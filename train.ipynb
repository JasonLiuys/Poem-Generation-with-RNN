{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T12:35:27.272077Z",
     "start_time": "2019-05-09T12:07:37.452918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唐诗数据集大小 : 232670\n",
      "5295\n",
      "WARNING:tensorflow:From /notebooks/Generate_Poem_with_RNN/model.py:51: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "(64, 33)\n",
      "step 0, loss is 8.574045181274414\n",
      "(64, 33)\n",
      "step 1, loss is 8.573363304138184\n",
      "(64, 33)\n",
      "step 2, loss is 8.572128295898438\n",
      "(64, 33)\n",
      "step 3, loss is 8.571191787719727\n",
      "(64, 33)\n",
      "step 4, loss is 8.569550514221191\n",
      "(64, 33)\n",
      "step 5, loss is 8.567244529724121\n",
      "(64, 33)\n",
      "step 6, loss is 8.564799308776855\n",
      "(64, 33)\n",
      "step 7, loss is 8.561384201049805\n",
      "(64, 33)\n",
      "step 8, loss is 8.559086799621582\n",
      "(64, 33)\n",
      "step 9, loss is 8.552078247070312\n",
      "(64, 33)\n",
      "step 10, loss is 8.545492172241211\n",
      "(64, 33)\n",
      "step 11, loss is 8.535004615783691\n",
      "(64, 33)\n",
      "step 12, loss is 8.520625114440918\n",
      "(64, 33)\n",
      "step 13, loss is 8.500568389892578\n",
      "(64, 33)\n",
      "step 14, loss is 8.475951194763184\n",
      "(64, 33)\n",
      "step 15, loss is 8.430252075195312\n",
      "(64, 33)\n",
      "step 16, loss is 8.362045288085938\n",
      "(64, 33)\n",
      "step 17, loss is 8.297121047973633\n",
      "(64, 33)\n",
      "step 18, loss is 8.240994453430176\n",
      "(64, 33)\n",
      "step 19, loss is 8.13673210144043\n",
      "(64, 33)\n",
      "step 20, loss is 8.097125053405762\n",
      "(64, 33)\n",
      "step 21, loss is 8.028202056884766\n",
      "(64, 33)\n",
      "step 22, loss is 7.9048075675964355\n",
      "(64, 33)\n",
      "step 23, loss is 7.792725563049316\n",
      "(64, 33)\n",
      "step 24, loss is 7.814651966094971\n",
      "(64, 33)\n",
      "step 25, loss is 7.678347110748291\n",
      "(64, 33)\n",
      "step 26, loss is 7.6467156410217285\n",
      "(64, 33)\n",
      "step 27, loss is 7.504639625549316\n",
      "(64, 33)\n",
      "step 28, loss is 7.476413249969482\n",
      "(64, 33)\n",
      "step 29, loss is 7.381346702575684\n",
      "(64, 33)\n",
      "step 30, loss is 7.276610851287842\n",
      "(64, 33)\n",
      "step 31, loss is 7.149212837219238\n",
      "(64, 33)\n",
      "step 32, loss is 7.210282802581787\n",
      "(64, 33)\n",
      "step 33, loss is 7.070953369140625\n",
      "(64, 33)\n",
      "step 34, loss is 7.151586532592773\n",
      "(64, 33)\n",
      "step 35, loss is 7.012917518615723\n",
      "(64, 33)\n",
      "step 36, loss is 6.945773601531982\n",
      "(64, 33)\n",
      "step 37, loss is 6.847123622894287\n",
      "(64, 33)\n",
      "step 38, loss is 6.879998207092285\n",
      "(64, 33)\n",
      "step 39, loss is 6.77623176574707\n",
      "(64, 33)\n",
      "step 40, loss is 6.716479301452637\n",
      "(64, 33)\n",
      "step 41, loss is 6.854119300842285\n",
      "(64, 33)\n",
      "step 42, loss is 6.5798540115356445\n",
      "(64, 33)\n",
      "step 43, loss is 6.632065296173096\n",
      "(64, 33)\n",
      "step 44, loss is 6.599300861358643\n",
      "(64, 33)\n",
      "step 45, loss is 6.560258865356445\n",
      "(64, 33)\n",
      "step 46, loss is 6.4449286460876465\n",
      "(64, 33)\n",
      "step 47, loss is 6.4583048820495605\n",
      "(64, 33)\n",
      "step 48, loss is 6.581235408782959\n",
      "(64, 33)\n",
      "step 49, loss is 6.4015913009643555\n",
      "(64, 33)\n",
      "step 50, loss is 6.422438144683838\n",
      "(64, 33)\n",
      "step 51, loss is 6.442099571228027\n",
      "(64, 33)\n",
      "step 52, loss is 6.050407886505127\n",
      "(64, 33)\n",
      "step 53, loss is 6.276885986328125\n",
      "(64, 33)\n",
      "step 54, loss is 6.275015830993652\n",
      "(64, 33)\n",
      "step 55, loss is 6.472949504852295\n",
      "(64, 33)\n",
      "step 56, loss is 6.209045886993408\n",
      "(64, 33)\n",
      "step 57, loss is 6.330308437347412\n",
      "(64, 33)\n",
      "step 58, loss is 6.227443218231201\n",
      "(64, 33)\n",
      "step 59, loss is 6.220668792724609\n",
      "(64, 33)\n",
      "step 60, loss is 6.342414379119873\n",
      "(64, 33)\n",
      "step 61, loss is 6.291831970214844\n",
      "(64, 33)\n",
      "step 62, loss is 6.293200492858887\n",
      "(64, 33)\n",
      "step 63, loss is 6.347331523895264\n",
      "(64, 33)\n",
      "step 64, loss is 6.390728950500488\n",
      "(64, 33)\n",
      "step 65, loss is 6.320899486541748\n",
      "(64, 33)\n",
      "step 66, loss is 6.323579788208008\n",
      "(64, 33)\n",
      "step 67, loss is 6.157156467437744\n",
      "(64, 33)\n",
      "step 68, loss is 6.196264743804932\n",
      "(64, 33)\n",
      "step 69, loss is 6.296522617340088\n",
      "(64, 33)\n",
      "step 70, loss is 6.421402454376221\n",
      "(64, 33)\n",
      "step 71, loss is 6.376655101776123\n",
      "(64, 33)\n",
      "step 72, loss is 6.124177932739258\n",
      "(64, 33)\n",
      "step 73, loss is 6.208024024963379\n",
      "(64, 33)\n",
      "step 74, loss is 6.263961315155029\n",
      "(64, 33)\n",
      "step 75, loss is 6.248686790466309\n",
      "(64, 33)\n",
      "step 76, loss is 6.288157939910889\n",
      "(64, 33)\n",
      "step 77, loss is 5.980845928192139\n",
      "(64, 33)\n",
      "step 78, loss is 6.243094444274902\n",
      "(64, 33)\n",
      "step 79, loss is 6.149729251861572\n",
      "(64, 33)\n",
      "step 80, loss is 6.1358866691589355\n",
      "(64, 33)\n",
      "step 81, loss is 6.3336663246154785\n",
      "(64, 33)\n",
      "step 82, loss is 6.346620559692383\n",
      "(64, 33)\n",
      "step 83, loss is 6.254512786865234\n",
      "(64, 33)\n",
      "step 84, loss is 6.2347941398620605\n",
      "(64, 33)\n",
      "step 85, loss is 6.17055606842041\n",
      "(64, 33)\n",
      "step 86, loss is 6.116607666015625\n",
      "(64, 33)\n",
      "step 87, loss is 6.228815078735352\n",
      "(64, 33)\n",
      "step 88, loss is 6.31164026260376\n",
      "(64, 33)\n",
      "step 89, loss is 6.2552385330200195\n",
      "(64, 33)\n",
      "step 90, loss is 6.289921283721924\n",
      "(64, 33)\n",
      "step 91, loss is 6.323274612426758\n",
      "(64, 33)\n",
      "step 92, loss is 6.333782196044922\n",
      "(64, 33)\n",
      "step 93, loss is 6.234375953674316\n",
      "(64, 33)\n",
      "step 94, loss is 6.198562145233154\n",
      "(64, 33)\n",
      "step 95, loss is 6.120059967041016\n",
      "(64, 33)\n",
      "step 96, loss is 6.244517803192139\n",
      "(64, 33)\n",
      "step 97, loss is 6.062417984008789\n",
      "(64, 33)\n",
      "step 98, loss is 6.260678768157959\n",
      "(64, 33)\n",
      "step 99, loss is 6.246001720428467\n",
      "(64, 33)\n",
      "step 100, loss is 6.1956095695495605\n",
      "(64, 33)\n",
      "step 101, loss is 6.31459903717041\n",
      "(64, 33)\n",
      "step 102, loss is 6.169218063354492\n",
      "(64, 33)\n",
      "step 103, loss is 6.1444411277771\n",
      "(64, 33)\n",
      "step 104, loss is 6.235686302185059\n",
      "(64, 33)\n",
      "step 105, loss is 6.143805027008057\n",
      "(64, 33)\n",
      "step 106, loss is 6.10289192199707\n",
      "(64, 33)\n",
      "step 107, loss is 6.212996482849121\n",
      "(64, 33)\n",
      "step 108, loss is 6.080496311187744\n",
      "(64, 33)\n",
      "step 109, loss is 6.151723861694336\n",
      "(64, 33)\n",
      "step 110, loss is 6.098781585693359\n",
      "(64, 33)\n",
      "step 111, loss is 6.2131733894348145\n",
      "(64, 33)\n",
      "step 112, loss is 6.493963718414307\n",
      "(64, 33)\n",
      "step 113, loss is 6.393325328826904\n",
      "(64, 33)\n",
      "step 114, loss is 6.113799571990967\n",
      "(64, 33)\n",
      "step 115, loss is 6.066450119018555\n",
      "(64, 33)\n",
      "step 116, loss is 6.293789863586426\n",
      "(64, 33)\n",
      "step 117, loss is 6.179272174835205\n",
      "(64, 33)\n",
      "step 118, loss is 6.38866662979126\n",
      "(64, 33)\n",
      "step 119, loss is 6.072697639465332\n",
      "(64, 33)\n",
      "step 120, loss is 6.242691516876221\n",
      "(64, 33)\n",
      "step 121, loss is 6.190683364868164\n",
      "(64, 33)\n",
      "step 122, loss is 6.2201008796691895\n",
      "(64, 33)\n",
      "step 123, loss is 6.229423999786377\n",
      "(64, 33)\n",
      "step 124, loss is 6.333484172821045\n",
      "(64, 33)\n",
      "step 125, loss is 6.113994598388672\n",
      "(64, 33)\n",
      "step 126, loss is 6.000428676605225\n",
      "(64, 33)\n",
      "step 127, loss is 6.157861709594727\n",
      "(64, 33)\n",
      "step 128, loss is 6.171262741088867\n",
      "(64, 33)\n",
      "step 129, loss is 6.280547142028809\n",
      "(64, 33)\n",
      "step 130, loss is 5.92006778717041\n",
      "(64, 33)\n",
      "step 131, loss is 6.10038948059082\n",
      "(64, 33)\n",
      "step 132, loss is 6.270833492279053\n",
      "(64, 33)\n",
      "step 133, loss is 6.15023946762085\n",
      "(64, 33)\n",
      "step 134, loss is 6.146078586578369\n",
      "(64, 33)\n",
      "step 135, loss is 6.194433212280273\n",
      "(64, 33)\n",
      "step 136, loss is 6.220240116119385\n",
      "(64, 33)\n",
      "step 137, loss is 5.986458778381348\n",
      "(64, 33)\n",
      "step 138, loss is 6.176821231842041\n",
      "(64, 33)\n",
      "step 139, loss is 6.314891815185547\n",
      "(64, 33)\n",
      "step 140, loss is 6.239377975463867\n",
      "(64, 33)\n",
      "step 141, loss is 6.258051872253418\n",
      "(64, 33)\n",
      "step 142, loss is 6.180008411407471\n",
      "(64, 33)\n",
      "step 143, loss is 6.117742538452148\n",
      "(64, 33)\n",
      "step 144, loss is 6.139341831207275\n",
      "(64, 33)\n",
      "step 145, loss is 6.200750827789307\n",
      "(64, 33)\n",
      "step 146, loss is 6.166755676269531\n",
      "(64, 33)\n",
      "step 147, loss is 6.286452293395996\n",
      "(64, 33)\n",
      "step 148, loss is 6.190199375152588\n",
      "(64, 33)\n",
      "step 149, loss is 6.137852668762207\n",
      "(64, 33)\n",
      "step 150, loss is 6.256708145141602\n",
      "(64, 33)\n",
      "step 151, loss is 6.2050886154174805\n",
      "(64, 33)\n",
      "step 152, loss is 6.057406902313232\n",
      "(64, 33)\n",
      "step 153, loss is 6.1026458740234375\n",
      "(64, 33)\n",
      "step 154, loss is 6.260125160217285\n",
      "(64, 33)\n",
      "step 155, loss is 6.125376224517822\n",
      "(64, 33)\n",
      "step 156, loss is 6.170436382293701\n",
      "(64, 33)\n",
      "step 157, loss is 6.0521368980407715\n",
      "(64, 33)\n",
      "step 158, loss is 6.120733737945557\n",
      "(64, 33)\n",
      "step 159, loss is 6.169375419616699\n",
      "(64, 33)\n",
      "step 160, loss is 6.100857734680176\n",
      "(64, 33)\n",
      "step 161, loss is 6.255457401275635\n",
      "(64, 33)\n",
      "step 162, loss is 6.153663635253906\n",
      "(64, 33)\n",
      "step 163, loss is 6.189310550689697\n",
      "(64, 33)\n",
      "step 164, loss is 6.257063865661621\n",
      "(64, 33)\n",
      "step 165, loss is 6.168361186981201\n",
      "(64, 33)\n",
      "step 166, loss is 6.071835517883301\n",
      "(64, 33)\n",
      "step 167, loss is 6.24180269241333\n",
      "(64, 33)\n",
      "step 168, loss is 6.2587714195251465\n",
      "(64, 33)\n",
      "step 169, loss is 6.219467639923096\n",
      "(64, 33)\n",
      "step 170, loss is 6.073169231414795\n",
      "(64, 33)\n",
      "step 171, loss is 6.178515911102295\n",
      "(64, 33)\n",
      "step 172, loss is 6.086398124694824\n",
      "(64, 33)\n",
      "step 173, loss is 6.0770745277404785\n",
      "(64, 33)\n",
      "step 174, loss is 6.223300933837891\n",
      "(64, 33)\n",
      "step 175, loss is 6.121365547180176\n",
      "(64, 33)\n",
      "step 176, loss is 6.239099502563477\n",
      "(64, 33)\n",
      "step 177, loss is 6.099745273590088\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 178, loss is 6.169262409210205\n",
      "(64, 33)\n",
      "step 179, loss is 6.358534336090088\n",
      "(64, 33)\n",
      "step 180, loss is 6.155564308166504\n",
      "(64, 33)\n",
      "step 181, loss is 6.030854225158691\n",
      "(64, 33)\n",
      "step 182, loss is 6.172558307647705\n",
      "(64, 33)\n",
      "step 183, loss is 6.302604675292969\n",
      "(64, 33)\n",
      "step 184, loss is 6.060722351074219\n",
      "(64, 33)\n",
      "step 185, loss is 6.234328746795654\n",
      "(64, 33)\n",
      "step 186, loss is 6.097503662109375\n",
      "(64, 33)\n",
      "step 187, loss is 6.366125106811523\n",
      "(64, 33)\n",
      "step 188, loss is 6.2616801261901855\n",
      "(64, 33)\n",
      "step 189, loss is 6.031935214996338\n",
      "(64, 33)\n",
      "step 190, loss is 6.238527774810791\n",
      "(64, 33)\n",
      "step 191, loss is 6.066895008087158\n",
      "(64, 33)\n",
      "step 192, loss is 6.063211441040039\n",
      "(64, 33)\n",
      "step 193, loss is 6.327133655548096\n",
      "(64, 33)\n",
      "step 194, loss is 6.211902141571045\n",
      "(64, 33)\n",
      "step 195, loss is 6.130287170410156\n",
      "(64, 33)\n",
      "step 196, loss is 6.207088470458984\n",
      "(64, 33)\n",
      "step 197, loss is 6.414594650268555\n",
      "(64, 33)\n",
      "step 198, loss is 6.106288909912109\n",
      "(64, 33)\n",
      "step 199, loss is 6.0029802322387695\n",
      "(64, 33)\n",
      "step 200, loss is 6.29576301574707\n",
      "(64, 33)\n",
      "step 201, loss is 6.232110977172852\n",
      "(64, 33)\n",
      "step 202, loss is 6.063342571258545\n",
      "(64, 33)\n",
      "step 203, loss is 6.228941917419434\n",
      "(64, 33)\n",
      "step 204, loss is 6.122361183166504\n",
      "(64, 33)\n",
      "step 205, loss is 6.038455009460449\n",
      "(64, 33)\n",
      "step 206, loss is 6.106658935546875\n",
      "(64, 33)\n",
      "step 207, loss is 6.0452752113342285\n",
      "(64, 33)\n",
      "step 208, loss is 6.305345058441162\n",
      "(64, 33)\n",
      "step 209, loss is 6.278191566467285\n",
      "(64, 33)\n",
      "step 210, loss is 6.083390712738037\n",
      "(64, 33)\n",
      "step 211, loss is 6.297312259674072\n",
      "(64, 33)\n",
      "step 212, loss is 6.21146297454834\n",
      "(64, 33)\n",
      "step 213, loss is 6.150989055633545\n",
      "(64, 33)\n",
      "step 214, loss is 6.245087146759033\n",
      "(64, 33)\n",
      "step 215, loss is 6.233976364135742\n",
      "(64, 33)\n",
      "step 216, loss is 6.2830119132995605\n",
      "(64, 33)\n",
      "step 217, loss is 6.217170715332031\n",
      "(64, 33)\n",
      "step 218, loss is 6.380251884460449\n",
      "(64, 33)\n",
      "step 219, loss is 6.1742024421691895\n",
      "(64, 33)\n",
      "step 220, loss is 6.207119941711426\n",
      "(64, 33)\n",
      "step 221, loss is 6.179742336273193\n",
      "(64, 33)\n",
      "step 222, loss is 6.228104114532471\n",
      "(64, 33)\n",
      "step 223, loss is 6.053518772125244\n",
      "(64, 33)\n",
      "step 224, loss is 6.041421413421631\n",
      "(64, 33)\n",
      "step 225, loss is 6.234981060028076\n",
      "(64, 33)\n",
      "step 226, loss is 6.1472859382629395\n",
      "(64, 33)\n",
      "step 227, loss is 6.463413238525391\n",
      "(64, 33)\n",
      "step 228, loss is 6.104283332824707\n",
      "(64, 33)\n",
      "step 229, loss is 6.1157050132751465\n",
      "(64, 33)\n",
      "step 230, loss is 6.189449310302734\n",
      "(64, 33)\n",
      "step 231, loss is 6.014934539794922\n",
      "(64, 33)\n",
      "step 232, loss is 6.05021333694458\n",
      "(64, 33)\n",
      "step 233, loss is 6.2128424644470215\n",
      "(64, 33)\n",
      "step 234, loss is 6.253786087036133\n",
      "(64, 33)\n",
      "step 235, loss is 6.100109577178955\n",
      "(64, 33)\n",
      "step 236, loss is 6.345608711242676\n",
      "(64, 33)\n",
      "step 237, loss is 6.101340293884277\n",
      "(64, 33)\n",
      "step 238, loss is 6.232438564300537\n",
      "(64, 33)\n",
      "step 239, loss is 6.288766384124756\n",
      "(64, 33)\n",
      "step 240, loss is 6.214042663574219\n",
      "(64, 33)\n",
      "step 241, loss is 6.23421573638916\n",
      "(64, 33)\n",
      "step 242, loss is 6.085737705230713\n",
      "(64, 33)\n",
      "step 243, loss is 6.234157562255859\n",
      "(64, 33)\n",
      "step 244, loss is 6.2002272605896\n",
      "(64, 33)\n",
      "step 245, loss is 6.177260875701904\n",
      "(64, 33)\n",
      "step 246, loss is 6.169960975646973\n",
      "(64, 33)\n",
      "step 247, loss is 6.283442497253418\n",
      "(64, 33)\n",
      "step 248, loss is 5.961784362792969\n",
      "(64, 33)\n",
      "step 249, loss is 6.1788835525512695\n",
      "(64, 33)\n",
      "step 250, loss is 6.174925804138184\n",
      "(64, 33)\n",
      "step 251, loss is 6.286487102508545\n",
      "(64, 33)\n",
      "step 252, loss is 6.148355960845947\n",
      "(64, 33)\n",
      "step 253, loss is 5.97725772857666\n",
      "(64, 33)\n",
      "step 254, loss is 6.170838832855225\n",
      "(64, 33)\n",
      "step 255, loss is 6.166593074798584\n",
      "(64, 33)\n",
      "step 256, loss is 6.271458625793457\n",
      "(64, 33)\n",
      "step 257, loss is 5.955837249755859\n",
      "(64, 33)\n",
      "step 258, loss is 6.183285713195801\n",
      "(64, 33)\n",
      "step 259, loss is 6.164989948272705\n",
      "(64, 33)\n",
      "step 260, loss is 6.292754650115967\n",
      "(64, 33)\n",
      "step 261, loss is 6.2274088859558105\n",
      "(64, 33)\n",
      "step 262, loss is 6.0837626457214355\n",
      "(64, 33)\n",
      "step 263, loss is 6.0693206787109375\n",
      "(64, 33)\n",
      "step 264, loss is 6.110736846923828\n",
      "(64, 33)\n",
      "step 265, loss is 5.989901542663574\n",
      "(64, 33)\n",
      "step 266, loss is 6.281705856323242\n",
      "(64, 33)\n",
      "step 267, loss is 6.273739814758301\n",
      "(64, 33)\n",
      "step 268, loss is 6.103854656219482\n",
      "(64, 33)\n",
      "step 269, loss is 6.139742851257324\n",
      "(64, 33)\n",
      "step 270, loss is 6.026991844177246\n",
      "(64, 33)\n",
      "step 271, loss is 6.216887950897217\n",
      "(64, 33)\n",
      "step 272, loss is 6.323433876037598\n",
      "(64, 33)\n",
      "step 273, loss is 6.276796340942383\n",
      "(64, 33)\n",
      "step 274, loss is 6.157762050628662\n",
      "(64, 33)\n",
      "step 275, loss is 6.293460845947266\n",
      "(64, 33)\n",
      "step 276, loss is 6.105273723602295\n",
      "(64, 33)\n",
      "step 277, loss is 6.294471740722656\n",
      "(64, 33)\n",
      "step 278, loss is 6.3478569984436035\n",
      "(64, 33)\n",
      "step 279, loss is 6.210142612457275\n",
      "(64, 33)\n",
      "step 280, loss is 6.3471455574035645\n",
      "(64, 33)\n",
      "step 281, loss is 6.2167487144470215\n",
      "(64, 33)\n",
      "step 282, loss is 6.313202381134033\n",
      "(64, 33)\n",
      "step 283, loss is 6.1568284034729\n",
      "(64, 33)\n",
      "step 284, loss is 6.220580577850342\n",
      "(64, 33)\n",
      "step 285, loss is 6.242779731750488\n",
      "(64, 33)\n",
      "step 286, loss is 6.063933372497559\n",
      "(64, 33)\n",
      "step 287, loss is 6.254521369934082\n",
      "(64, 33)\n",
      "step 288, loss is 6.259210109710693\n",
      "(64, 33)\n",
      "step 289, loss is 6.216181755065918\n",
      "(64, 33)\n",
      "step 290, loss is 6.185847282409668\n",
      "(64, 33)\n",
      "step 291, loss is 6.081629276275635\n",
      "(64, 33)\n",
      "step 292, loss is 6.104914665222168\n",
      "(64, 33)\n",
      "step 293, loss is 6.100998401641846\n",
      "(64, 33)\n",
      "step 294, loss is 6.204397678375244\n",
      "(64, 33)\n",
      "step 295, loss is 6.048821449279785\n",
      "(64, 33)\n",
      "step 296, loss is 6.183058261871338\n",
      "(64, 33)\n",
      "step 297, loss is 6.2721123695373535\n",
      "(64, 33)\n",
      "step 298, loss is 6.326733112335205\n",
      "(64, 33)\n",
      "step 299, loss is 6.303781032562256\n",
      "(64, 33)\n",
      "step 300, loss is 6.193127155303955\n",
      "(64, 33)\n",
      "step 301, loss is 6.098174095153809\n",
      "(64, 33)\n",
      "step 302, loss is 6.0959553718566895\n",
      "(64, 33)\n",
      "step 303, loss is 6.139961242675781\n",
      "(64, 33)\n",
      "step 304, loss is 6.112776279449463\n",
      "(64, 33)\n",
      "step 305, loss is 6.392440319061279\n",
      "(64, 33)\n",
      "step 306, loss is 6.1058526039123535\n",
      "(64, 33)\n",
      "step 307, loss is 6.350447654724121\n",
      "(64, 33)\n",
      "step 308, loss is 6.184840679168701\n",
      "(64, 33)\n",
      "step 309, loss is 6.138742923736572\n",
      "(64, 33)\n",
      "step 310, loss is 6.1585211753845215\n",
      "(64, 33)\n",
      "step 311, loss is 6.088718891143799\n",
      "(64, 33)\n",
      "step 312, loss is 6.201475143432617\n",
      "(64, 33)\n",
      "step 313, loss is 6.129335403442383\n",
      "(64, 33)\n",
      "step 314, loss is 6.084052562713623\n",
      "(64, 33)\n",
      "step 315, loss is 6.043946743011475\n",
      "(64, 33)\n",
      "step 316, loss is 6.241336822509766\n",
      "(64, 33)\n",
      "step 317, loss is 6.318661689758301\n",
      "(64, 33)\n",
      "step 318, loss is 6.1502509117126465\n",
      "(64, 33)\n",
      "step 319, loss is 6.230928421020508\n",
      "(64, 33)\n",
      "step 320, loss is 6.180171966552734\n",
      "(64, 33)\n",
      "step 321, loss is 6.085034370422363\n",
      "(64, 33)\n",
      "step 322, loss is 6.217235565185547\n",
      "(64, 33)\n",
      "step 323, loss is 6.283015251159668\n",
      "(64, 33)\n",
      "step 324, loss is 6.3531341552734375\n",
      "(64, 33)\n",
      "step 325, loss is 6.0650506019592285\n",
      "(64, 33)\n",
      "step 326, loss is 6.1348772048950195\n",
      "(64, 33)\n",
      "step 327, loss is 6.078497886657715\n",
      "(64, 33)\n",
      "step 328, loss is 5.929532527923584\n",
      "(64, 33)\n",
      "step 329, loss is 6.1436028480529785\n",
      "(64, 33)\n",
      "step 330, loss is 6.185276031494141\n",
      "(64, 33)\n",
      "step 331, loss is 6.216601848602295\n",
      "(64, 33)\n",
      "step 332, loss is 6.152285099029541\n",
      "(64, 33)\n",
      "step 333, loss is 6.164695739746094\n",
      "(64, 33)\n",
      "step 334, loss is 6.403231143951416\n",
      "(64, 33)\n",
      "step 335, loss is 6.336357116699219\n",
      "(64, 33)\n",
      "step 336, loss is 6.094886779785156\n",
      "(64, 33)\n",
      "step 337, loss is 6.2142558097839355\n",
      "(64, 33)\n",
      "step 338, loss is 6.076510906219482\n",
      "(64, 33)\n",
      "step 339, loss is 6.2072367668151855\n",
      "(64, 33)\n",
      "step 340, loss is 6.187461853027344\n",
      "(64, 33)\n",
      "step 341, loss is 6.238025665283203\n",
      "(64, 33)\n",
      "step 342, loss is 6.136006832122803\n",
      "(64, 33)\n",
      "step 343, loss is 6.225440979003906\n",
      "(64, 33)\n",
      "step 344, loss is 6.1343607902526855\n",
      "(64, 33)\n",
      "step 345, loss is 6.10435676574707\n",
      "(64, 33)\n",
      "step 346, loss is 6.181081295013428\n",
      "(64, 33)\n",
      "step 347, loss is 6.139754295349121\n",
      "(64, 33)\n",
      "step 348, loss is 6.277882099151611\n",
      "(64, 33)\n",
      "step 349, loss is 6.252665996551514\n",
      "(64, 33)\n",
      "step 350, loss is 6.1653618812561035\n",
      "(64, 33)\n",
      "step 351, loss is 6.153172492980957\n",
      "(64, 33)\n",
      "step 352, loss is 6.143765926361084\n",
      "(64, 33)\n",
      "step 353, loss is 6.13337516784668\n",
      "(64, 33)\n",
      "step 354, loss is 6.254054546356201\n",
      "(64, 33)\n",
      "step 355, loss is 6.116842269897461\n",
      "(64, 33)\n",
      "step 356, loss is 6.247245788574219\n",
      "(64, 33)\n",
      "step 357, loss is 6.190577507019043\n",
      "(64, 33)\n",
      "step 358, loss is 6.052667617797852\n",
      "(64, 33)\n",
      "step 359, loss is 5.940025329589844\n",
      "(64, 33)\n",
      "step 360, loss is 6.165611743927002\n",
      "(64, 33)\n",
      "step 361, loss is 6.196105003356934\n",
      "(64, 33)\n",
      "step 362, loss is 6.254866123199463\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 363, loss is 6.160124778747559\n",
      "(64, 33)\n",
      "step 364, loss is 6.011058807373047\n",
      "(64, 33)\n",
      "step 365, loss is 6.124341011047363\n",
      "(64, 33)\n",
      "step 366, loss is 6.201047897338867\n",
      "(64, 33)\n",
      "step 367, loss is 6.123391628265381\n",
      "(64, 33)\n",
      "step 368, loss is 6.278592109680176\n",
      "(64, 33)\n",
      "step 369, loss is 6.18341064453125\n",
      "(64, 33)\n",
      "step 370, loss is 6.129868984222412\n",
      "(64, 33)\n",
      "step 371, loss is 6.250813007354736\n",
      "(64, 33)\n",
      "step 372, loss is 6.325072765350342\n",
      "(64, 33)\n",
      "step 373, loss is 6.217570781707764\n",
      "(64, 33)\n",
      "step 374, loss is 6.363708019256592\n",
      "(64, 33)\n",
      "step 375, loss is 6.1338114738464355\n",
      "(64, 33)\n",
      "step 376, loss is 6.184660911560059\n",
      "(64, 33)\n",
      "step 377, loss is 6.154579162597656\n",
      "(64, 33)\n",
      "step 378, loss is 6.117729187011719\n",
      "(64, 33)\n",
      "step 379, loss is 6.173042297363281\n",
      "(64, 33)\n",
      "step 380, loss is 6.146421909332275\n",
      "(64, 33)\n",
      "step 381, loss is 6.322834014892578\n",
      "(64, 33)\n",
      "step 382, loss is 6.058135032653809\n",
      "(64, 33)\n",
      "step 383, loss is 6.030667304992676\n",
      "(64, 33)\n",
      "step 384, loss is 6.217803955078125\n",
      "(64, 33)\n",
      "step 385, loss is 6.222099781036377\n",
      "(64, 33)\n",
      "step 386, loss is 6.082845687866211\n",
      "(64, 33)\n",
      "step 387, loss is 6.2389960289001465\n",
      "(64, 33)\n",
      "step 388, loss is 6.139612674713135\n",
      "(64, 33)\n",
      "step 389, loss is 6.247154235839844\n",
      "(64, 33)\n",
      "step 390, loss is 6.297320365905762\n",
      "(64, 33)\n",
      "step 391, loss is 6.093034267425537\n",
      "(64, 33)\n",
      "step 392, loss is 6.318953037261963\n",
      "(64, 33)\n",
      "step 393, loss is 6.096944808959961\n",
      "(64, 33)\n",
      "step 394, loss is 6.225925445556641\n",
      "(64, 33)\n",
      "step 395, loss is 6.14772891998291\n",
      "(64, 33)\n",
      "step 396, loss is 6.09335470199585\n",
      "(64, 33)\n",
      "step 397, loss is 6.207771301269531\n",
      "(64, 33)\n",
      "step 398, loss is 6.222706317901611\n",
      "(64, 33)\n",
      "step 399, loss is 6.173922061920166\n",
      "(64, 33)\n",
      "step 400, loss is 6.151917934417725\n",
      "(64, 33)\n",
      "step 401, loss is 6.10643196105957\n",
      "(64, 33)\n",
      "step 402, loss is 6.094108581542969\n",
      "(64, 33)\n",
      "step 403, loss is 6.367820739746094\n",
      "(64, 33)\n",
      "step 404, loss is 6.170735836029053\n",
      "(64, 33)\n",
      "step 405, loss is 6.2744646072387695\n",
      "(64, 33)\n",
      "step 406, loss is 6.152687072753906\n",
      "(64, 33)\n",
      "step 407, loss is 6.257362365722656\n",
      "(64, 33)\n",
      "step 408, loss is 6.195082187652588\n",
      "(64, 33)\n",
      "step 409, loss is 6.294823169708252\n",
      "(64, 33)\n",
      "step 410, loss is 6.354919910430908\n",
      "(64, 33)\n",
      "step 411, loss is 6.246160507202148\n",
      "(64, 33)\n",
      "step 412, loss is 6.148719787597656\n",
      "(64, 33)\n",
      "step 413, loss is 6.36638879776001\n",
      "(64, 33)\n",
      "step 414, loss is 6.025509834289551\n",
      "(64, 33)\n",
      "step 415, loss is 6.094590663909912\n",
      "(64, 33)\n",
      "step 416, loss is 5.988581657409668\n",
      "(64, 33)\n",
      "step 417, loss is 6.323864459991455\n",
      "(64, 33)\n",
      "step 418, loss is 6.21885347366333\n",
      "(64, 33)\n",
      "step 419, loss is 6.109523296356201\n",
      "(64, 33)\n",
      "step 420, loss is 6.241354465484619\n",
      "(64, 33)\n",
      "step 421, loss is 6.182031631469727\n",
      "(64, 33)\n",
      "step 422, loss is 6.32343053817749\n",
      "(64, 33)\n",
      "step 423, loss is 6.078639984130859\n",
      "(64, 33)\n",
      "step 424, loss is 6.2953901290893555\n",
      "(64, 33)\n",
      "step 425, loss is 6.238678932189941\n",
      "(64, 33)\n",
      "step 426, loss is 6.347696304321289\n",
      "(64, 33)\n",
      "step 427, loss is 6.1909685134887695\n",
      "(64, 33)\n",
      "step 428, loss is 6.013021945953369\n",
      "(64, 33)\n",
      "step 429, loss is 6.290536403656006\n",
      "(64, 33)\n",
      "step 430, loss is 6.2166266441345215\n",
      "(64, 33)\n",
      "step 431, loss is 6.110634803771973\n",
      "(64, 33)\n",
      "step 432, loss is 6.149375915527344\n",
      "(64, 33)\n",
      "step 433, loss is 6.149478912353516\n",
      "(64, 33)\n",
      "step 434, loss is 6.018453598022461\n",
      "(64, 33)\n",
      "step 435, loss is 6.28117036819458\n",
      "(64, 33)\n",
      "step 436, loss is 6.102743625640869\n",
      "(64, 33)\n",
      "step 437, loss is 6.0756120681762695\n",
      "(64, 33)\n",
      "step 438, loss is 6.152719020843506\n",
      "(64, 33)\n",
      "step 439, loss is 6.028067111968994\n",
      "(64, 33)\n",
      "step 440, loss is 6.068625450134277\n",
      "(64, 33)\n",
      "step 441, loss is 6.2080583572387695\n",
      "(64, 33)\n",
      "step 442, loss is 6.088449954986572\n",
      "(64, 33)\n",
      "step 443, loss is 6.175716876983643\n",
      "(64, 33)\n",
      "step 444, loss is 6.373081684112549\n",
      "(64, 33)\n",
      "step 445, loss is 6.066197395324707\n",
      "(64, 33)\n",
      "step 446, loss is 6.1456990242004395\n",
      "(64, 33)\n",
      "step 447, loss is 6.110026836395264\n",
      "(64, 33)\n",
      "step 448, loss is 6.029117584228516\n",
      "(64, 33)\n",
      "step 449, loss is 6.192966461181641\n",
      "(64, 33)\n",
      "step 450, loss is 6.120571136474609\n",
      "(64, 33)\n",
      "step 451, loss is 6.237133979797363\n",
      "(64, 33)\n",
      "step 452, loss is 6.1313157081604\n",
      "(64, 33)\n",
      "step 453, loss is 5.993168354034424\n",
      "(64, 33)\n",
      "step 454, loss is 6.1768388748168945\n",
      "(64, 33)\n",
      "step 455, loss is 6.026745319366455\n",
      "(64, 33)\n",
      "step 456, loss is 6.133144855499268\n",
      "(64, 33)\n",
      "step 457, loss is 6.182334899902344\n",
      "(64, 33)\n",
      "step 458, loss is 6.073970794677734\n",
      "(64, 33)\n",
      "step 459, loss is 6.305145263671875\n",
      "(64, 33)\n",
      "step 460, loss is 6.087421894073486\n",
      "(64, 33)\n",
      "step 461, loss is 6.143516540527344\n",
      "(64, 33)\n",
      "step 462, loss is 6.098926544189453\n",
      "(64, 33)\n",
      "step 463, loss is 5.983146667480469\n",
      "(64, 33)\n",
      "step 464, loss is 5.985475063323975\n",
      "(64, 33)\n",
      "step 465, loss is 6.174524307250977\n",
      "(64, 33)\n",
      "step 466, loss is 6.274659633636475\n",
      "(64, 33)\n",
      "step 467, loss is 6.234044075012207\n",
      "(64, 33)\n",
      "step 468, loss is 5.998202800750732\n",
      "(64, 33)\n",
      "step 469, loss is 6.289790153503418\n",
      "(64, 33)\n",
      "step 470, loss is 6.366012096405029\n",
      "(64, 33)\n",
      "step 471, loss is 6.230422019958496\n",
      "(64, 33)\n",
      "step 472, loss is 6.149965763092041\n",
      "(64, 33)\n",
      "step 473, loss is 6.169100761413574\n",
      "(64, 33)\n",
      "step 474, loss is 6.128398418426514\n",
      "(64, 33)\n",
      "step 475, loss is 5.941235542297363\n",
      "(64, 33)\n",
      "step 476, loss is 6.133027076721191\n",
      "(64, 33)\n",
      "step 477, loss is 6.095785140991211\n",
      "(64, 33)\n",
      "step 478, loss is 6.134416103363037\n",
      "(64, 33)\n",
      "step 479, loss is 6.243346214294434\n",
      "(64, 33)\n",
      "step 480, loss is 6.123307228088379\n",
      "(64, 33)\n",
      "step 481, loss is 6.168466091156006\n",
      "(64, 33)\n",
      "step 482, loss is 6.011904239654541\n",
      "(64, 33)\n",
      "step 483, loss is 6.080419540405273\n",
      "(64, 33)\n",
      "step 484, loss is 5.996409893035889\n",
      "(64, 33)\n",
      "step 485, loss is 6.134341716766357\n",
      "(64, 33)\n",
      "step 486, loss is 6.1298394203186035\n",
      "(64, 33)\n",
      "step 487, loss is 5.930207252502441\n",
      "(64, 33)\n",
      "step 488, loss is 6.186544895172119\n",
      "(64, 33)\n",
      "step 489, loss is 6.1523823738098145\n",
      "(64, 33)\n",
      "step 490, loss is 6.076362133026123\n",
      "(64, 33)\n",
      "step 491, loss is 6.22158670425415\n",
      "(64, 33)\n",
      "step 492, loss is 6.070499897003174\n",
      "(64, 33)\n",
      "step 493, loss is 6.175990104675293\n",
      "(64, 33)\n",
      "step 494, loss is 6.184453010559082\n",
      "(64, 33)\n",
      "step 495, loss is 6.007383346557617\n",
      "(64, 33)\n",
      "step 496, loss is 6.2169413566589355\n",
      "(64, 33)\n",
      "step 497, loss is 5.929062366485596\n",
      "(64, 33)\n",
      "step 498, loss is 6.145601272583008\n",
      "(64, 33)\n",
      "step 499, loss is 5.991121292114258\n",
      "(64, 33)\n",
      "step 500, loss is 6.111276149749756\n",
      "(64, 33)\n",
      "step 501, loss is 6.093441486358643\n",
      "(64, 33)\n",
      "step 502, loss is 5.965203285217285\n",
      "(64, 33)\n",
      "step 503, loss is 6.069639205932617\n",
      "(64, 33)\n",
      "step 504, loss is 6.156867027282715\n",
      "(64, 33)\n",
      "step 505, loss is 5.976796627044678\n",
      "(64, 33)\n",
      "step 506, loss is 5.916108131408691\n",
      "(64, 33)\n",
      "step 507, loss is 5.830966949462891\n",
      "(64, 33)\n",
      "step 508, loss is 6.087417125701904\n",
      "(64, 33)\n",
      "step 509, loss is 6.0472731590271\n",
      "(64, 33)\n",
      "step 510, loss is 6.123958587646484\n",
      "(64, 33)\n",
      "step 511, loss is 6.003881454467773\n",
      "(64, 33)\n",
      "step 512, loss is 6.0782389640808105\n",
      "(64, 33)\n",
      "step 513, loss is 5.983201503753662\n",
      "(64, 33)\n",
      "step 514, loss is 5.810308456420898\n",
      "(64, 33)\n",
      "step 515, loss is 5.964336395263672\n",
      "(64, 33)\n",
      "step 516, loss is 6.056306838989258\n",
      "(64, 33)\n",
      "step 517, loss is 6.152743816375732\n",
      "(64, 33)\n",
      "step 518, loss is 5.979422569274902\n",
      "(64, 33)\n",
      "step 519, loss is 5.957693576812744\n",
      "(64, 33)\n",
      "step 520, loss is 6.10781192779541\n",
      "(64, 33)\n",
      "step 521, loss is 5.933151721954346\n",
      "(64, 33)\n",
      "step 522, loss is 5.929933071136475\n",
      "(64, 33)\n",
      "step 523, loss is 5.948186874389648\n",
      "(64, 33)\n",
      "step 524, loss is 5.965912342071533\n",
      "(64, 33)\n",
      "step 525, loss is 5.936618328094482\n",
      "(64, 33)\n",
      "step 526, loss is 6.143813133239746\n",
      "(64, 33)\n",
      "step 527, loss is 6.103809833526611\n",
      "(64, 33)\n",
      "step 528, loss is 5.9893903732299805\n",
      "(64, 33)\n",
      "step 529, loss is 6.08096981048584\n",
      "(64, 33)\n",
      "step 530, loss is 6.16765832901001\n",
      "(64, 33)\n",
      "step 531, loss is 6.061730861663818\n",
      "(64, 33)\n",
      "step 532, loss is 6.059188365936279\n",
      "(64, 33)\n",
      "step 533, loss is 6.18564510345459\n",
      "(64, 33)\n",
      "step 534, loss is 6.015059471130371\n",
      "(64, 33)\n",
      "step 535, loss is 6.017812252044678\n",
      "(64, 33)\n",
      "step 536, loss is 6.037445068359375\n",
      "(64, 33)\n",
      "step 537, loss is 5.984740734100342\n",
      "(64, 33)\n",
      "step 538, loss is 6.140303134918213\n",
      "(64, 33)\n",
      "step 539, loss is 6.140966892242432\n",
      "(64, 33)\n",
      "step 540, loss is 6.197175025939941\n",
      "(64, 33)\n",
      "step 541, loss is 6.093510627746582\n",
      "(64, 33)\n",
      "step 542, loss is 6.070623397827148\n",
      "(64, 33)\n",
      "step 543, loss is 6.0283122062683105\n",
      "(64, 33)\n",
      "step 544, loss is 6.081099033355713\n",
      "(64, 33)\n",
      "step 545, loss is 6.005882263183594\n",
      "(64, 33)\n",
      "step 546, loss is 6.057256698608398\n",
      "(64, 33)\n",
      "step 547, loss is 6.05669641494751\n",
      "(64, 33)\n",
      "step 548, loss is 6.026492595672607\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 549, loss is 6.07673978805542\n",
      "(64, 33)\n",
      "step 550, loss is 5.93206787109375\n",
      "(64, 33)\n",
      "step 551, loss is 5.799814701080322\n",
      "(64, 33)\n",
      "step 552, loss is 5.996934413909912\n",
      "(64, 33)\n",
      "step 553, loss is 5.969272136688232\n",
      "(64, 33)\n",
      "step 554, loss is 5.901767253875732\n",
      "(64, 33)\n",
      "step 555, loss is 6.01340913772583\n",
      "(64, 33)\n",
      "step 556, loss is 5.903517723083496\n",
      "(64, 33)\n",
      "step 557, loss is 6.050293922424316\n",
      "(64, 33)\n",
      "step 558, loss is 5.8379926681518555\n",
      "(64, 33)\n",
      "step 559, loss is 5.891807556152344\n",
      "(64, 33)\n",
      "step 560, loss is 5.766673564910889\n",
      "(64, 33)\n",
      "step 561, loss is 6.048394203186035\n",
      "(64, 33)\n",
      "step 562, loss is 5.7934699058532715\n",
      "(64, 33)\n",
      "step 563, loss is 5.921345233917236\n",
      "(64, 33)\n",
      "step 564, loss is 6.106766223907471\n",
      "(64, 33)\n",
      "step 565, loss is 5.995359420776367\n",
      "(64, 33)\n",
      "step 566, loss is 5.998579978942871\n",
      "(64, 33)\n",
      "step 567, loss is 5.8274688720703125\n",
      "(64, 33)\n",
      "step 568, loss is 6.063933372497559\n",
      "(64, 33)\n",
      "step 569, loss is 5.970519542694092\n",
      "(64, 33)\n",
      "step 570, loss is 5.9965314865112305\n",
      "(64, 33)\n",
      "step 571, loss is 5.983310699462891\n",
      "(64, 33)\n",
      "step 572, loss is 6.071989059448242\n",
      "(64, 33)\n",
      "step 573, loss is 6.0141096115112305\n",
      "(64, 33)\n",
      "step 574, loss is 5.927181243896484\n",
      "(64, 33)\n",
      "step 575, loss is 6.110198974609375\n",
      "(64, 33)\n",
      "step 576, loss is 5.778639316558838\n",
      "(64, 33)\n",
      "step 577, loss is 5.956382751464844\n",
      "(64, 33)\n",
      "step 578, loss is 5.922019004821777\n",
      "(64, 33)\n",
      "step 579, loss is 5.879483222961426\n",
      "(64, 33)\n",
      "step 580, loss is 5.903167247772217\n",
      "(64, 33)\n",
      "step 581, loss is 5.969883441925049\n",
      "(64, 33)\n",
      "step 582, loss is 6.0362772941589355\n",
      "(64, 33)\n",
      "step 583, loss is 6.045287609100342\n",
      "(64, 33)\n",
      "step 584, loss is 6.012449264526367\n",
      "(64, 33)\n",
      "step 585, loss is 5.921846389770508\n",
      "(64, 33)\n",
      "step 586, loss is 5.936413288116455\n",
      "(64, 33)\n",
      "step 587, loss is 6.047796249389648\n",
      "(64, 33)\n",
      "step 588, loss is 6.041834831237793\n",
      "(64, 33)\n",
      "step 589, loss is 5.728238105773926\n",
      "(64, 33)\n",
      "step 590, loss is 5.956944465637207\n",
      "(64, 33)\n",
      "step 591, loss is 5.844064712524414\n",
      "(64, 33)\n",
      "step 592, loss is 6.037478923797607\n",
      "(64, 33)\n",
      "step 593, loss is 5.687926769256592\n",
      "(64, 33)\n",
      "step 594, loss is 6.0189948081970215\n",
      "(64, 33)\n",
      "step 595, loss is 5.774251461029053\n",
      "(64, 33)\n",
      "step 596, loss is 6.059238433837891\n",
      "(64, 33)\n",
      "step 597, loss is 5.965520858764648\n",
      "(64, 33)\n",
      "step 598, loss is 5.751977920532227\n",
      "(64, 33)\n",
      "step 599, loss is 5.770932197570801\n",
      "(64, 33)\n",
      "step 600, loss is 5.909412860870361\n",
      "(64, 33)\n",
      "step 601, loss is 5.960775852203369\n",
      "(64, 33)\n",
      "step 602, loss is 5.854341983795166\n",
      "(64, 33)\n",
      "step 603, loss is 5.915085792541504\n",
      "(64, 33)\n",
      "step 604, loss is 5.878136157989502\n",
      "(64, 33)\n",
      "step 605, loss is 5.64316463470459\n",
      "(64, 33)\n",
      "step 606, loss is 5.927331924438477\n",
      "(64, 33)\n",
      "step 607, loss is 5.9693217277526855\n",
      "(64, 33)\n",
      "step 608, loss is 5.7609124183654785\n",
      "(64, 33)\n",
      "step 609, loss is 5.883481502532959\n",
      "(64, 33)\n",
      "step 610, loss is 5.91013765335083\n",
      "(64, 33)\n",
      "step 611, loss is 5.905702590942383\n",
      "(64, 33)\n",
      "step 612, loss is 5.820335388183594\n",
      "(64, 33)\n",
      "step 613, loss is 6.0778093338012695\n",
      "(64, 33)\n",
      "step 614, loss is 5.722563743591309\n",
      "(64, 33)\n",
      "step 615, loss is 5.9700751304626465\n",
      "(64, 33)\n",
      "step 616, loss is 5.786405086517334\n",
      "(64, 33)\n",
      "step 617, loss is 5.949733734130859\n",
      "(64, 33)\n",
      "step 618, loss is 5.8539605140686035\n",
      "(64, 33)\n",
      "step 619, loss is 5.911191463470459\n",
      "(64, 33)\n",
      "step 620, loss is 5.765543460845947\n",
      "(64, 33)\n",
      "step 621, loss is 5.828834056854248\n",
      "(64, 33)\n",
      "step 622, loss is 6.0514421463012695\n",
      "(64, 33)\n",
      "step 623, loss is 5.993037700653076\n",
      "(64, 33)\n",
      "step 624, loss is 5.825078010559082\n",
      "(64, 33)\n",
      "step 625, loss is 6.014908313751221\n",
      "(64, 33)\n",
      "step 626, loss is 5.7535810470581055\n",
      "(64, 33)\n",
      "step 627, loss is 5.828427314758301\n",
      "(64, 33)\n",
      "step 628, loss is 5.970699310302734\n",
      "(64, 33)\n",
      "step 629, loss is 5.931530952453613\n",
      "(64, 33)\n",
      "step 630, loss is 5.959537029266357\n",
      "(64, 33)\n",
      "step 631, loss is 5.933341979980469\n",
      "(64, 33)\n",
      "step 632, loss is 5.927563190460205\n",
      "(64, 33)\n",
      "step 633, loss is 5.838050842285156\n",
      "(64, 33)\n",
      "step 634, loss is 5.909532070159912\n",
      "(64, 33)\n",
      "step 635, loss is 5.7074689865112305\n",
      "(64, 33)\n",
      "step 636, loss is 6.006152153015137\n",
      "(64, 33)\n",
      "step 637, loss is 5.962014198303223\n",
      "(64, 33)\n",
      "step 638, loss is 5.84131383895874\n",
      "(64, 33)\n",
      "step 639, loss is 5.784867763519287\n",
      "(64, 33)\n",
      "step 640, loss is 5.815290927886963\n",
      "(64, 33)\n",
      "step 641, loss is 5.892265319824219\n",
      "(64, 33)\n",
      "step 642, loss is 5.7294816970825195\n",
      "(64, 33)\n",
      "step 643, loss is 5.971898555755615\n",
      "(64, 33)\n",
      "step 644, loss is 5.77700138092041\n",
      "(64, 33)\n",
      "step 645, loss is 5.984001159667969\n",
      "(64, 33)\n",
      "step 646, loss is 5.964232444763184\n",
      "(64, 33)\n",
      "step 647, loss is 5.784889221191406\n",
      "(64, 33)\n",
      "step 648, loss is 5.842002868652344\n",
      "(64, 33)\n",
      "step 649, loss is 5.820785045623779\n",
      "(64, 33)\n",
      "step 650, loss is 5.823437213897705\n",
      "(64, 33)\n",
      "step 651, loss is 5.586348533630371\n",
      "(64, 33)\n",
      "step 652, loss is 5.713371276855469\n",
      "(64, 33)\n",
      "step 653, loss is 5.912298679351807\n",
      "(64, 33)\n",
      "step 654, loss is 5.832496643066406\n",
      "(64, 33)\n",
      "step 655, loss is 5.702723979949951\n",
      "(64, 33)\n",
      "step 656, loss is 5.777098655700684\n",
      "(64, 33)\n",
      "step 657, loss is 5.7033610343933105\n",
      "(64, 33)\n",
      "step 658, loss is 5.755026340484619\n",
      "(64, 33)\n",
      "step 659, loss is 5.930882930755615\n",
      "(64, 33)\n",
      "step 660, loss is 5.957472324371338\n",
      "(64, 33)\n",
      "step 661, loss is 5.827708721160889\n",
      "(64, 33)\n",
      "step 662, loss is 5.851278305053711\n",
      "(64, 33)\n",
      "step 663, loss is 5.640870094299316\n",
      "(64, 33)\n",
      "step 664, loss is 5.745985984802246\n",
      "(64, 33)\n",
      "step 665, loss is 5.834271430969238\n",
      "(64, 33)\n",
      "step 666, loss is 5.858985900878906\n",
      "(64, 33)\n",
      "step 667, loss is 5.760476589202881\n",
      "(64, 33)\n",
      "step 668, loss is 5.936783313751221\n",
      "(64, 33)\n",
      "step 669, loss is 5.92557430267334\n",
      "(64, 33)\n",
      "step 670, loss is 5.884282112121582\n",
      "(64, 33)\n",
      "step 671, loss is 5.886171340942383\n",
      "(64, 33)\n",
      "step 672, loss is 5.64727258682251\n",
      "(64, 33)\n",
      "step 673, loss is 5.747941970825195\n",
      "(64, 33)\n",
      "step 674, loss is 5.61740779876709\n",
      "(64, 33)\n",
      "step 675, loss is 5.912811756134033\n",
      "(64, 33)\n",
      "step 676, loss is 5.920655250549316\n",
      "(64, 33)\n",
      "step 677, loss is 5.892978191375732\n",
      "(64, 33)\n",
      "step 678, loss is 5.775099277496338\n",
      "(64, 33)\n",
      "step 679, loss is 5.806705951690674\n",
      "(64, 33)\n",
      "step 680, loss is 5.848033428192139\n",
      "(64, 33)\n",
      "step 681, loss is 5.858123302459717\n",
      "(64, 33)\n",
      "step 682, loss is 5.768712997436523\n",
      "(64, 33)\n",
      "step 683, loss is 5.68723726272583\n",
      "(64, 33)\n",
      "step 684, loss is 5.785041332244873\n",
      "(64, 33)\n",
      "step 685, loss is 5.733475208282471\n",
      "(64, 33)\n",
      "step 686, loss is 5.821279525756836\n",
      "(64, 33)\n",
      "step 687, loss is 5.756479740142822\n",
      "(64, 33)\n",
      "step 688, loss is 5.644453048706055\n",
      "(64, 33)\n",
      "step 689, loss is 5.790055274963379\n",
      "(64, 33)\n",
      "step 690, loss is 5.7021050453186035\n",
      "(64, 33)\n",
      "step 691, loss is 5.8543853759765625\n",
      "(64, 33)\n",
      "step 692, loss is 5.866257667541504\n",
      "(64, 33)\n",
      "step 693, loss is 5.7892045974731445\n",
      "(64, 33)\n",
      "step 694, loss is 5.709371566772461\n",
      "(64, 33)\n",
      "step 695, loss is 5.699639320373535\n",
      "(64, 33)\n",
      "step 696, loss is 5.65068244934082\n",
      "(64, 33)\n",
      "step 697, loss is 5.7208476066589355\n",
      "(64, 33)\n",
      "step 698, loss is 5.830595970153809\n",
      "(64, 33)\n",
      "step 699, loss is 5.6522297859191895\n",
      "(64, 33)\n",
      "step 700, loss is 5.8160176277160645\n",
      "(64, 33)\n",
      "step 701, loss is 5.732979774475098\n",
      "(64, 33)\n",
      "step 702, loss is 5.739193916320801\n",
      "(64, 33)\n",
      "step 703, loss is 5.710048675537109\n",
      "(64, 33)\n",
      "step 704, loss is 5.62138032913208\n",
      "(64, 33)\n",
      "step 705, loss is 6.0158610343933105\n",
      "(64, 33)\n",
      "step 706, loss is 5.783100128173828\n",
      "(64, 33)\n",
      "step 707, loss is 5.661763668060303\n",
      "(64, 33)\n",
      "step 708, loss is 5.630868911743164\n",
      "(64, 33)\n",
      "step 709, loss is 5.690921783447266\n",
      "(64, 33)\n",
      "step 710, loss is 5.672510147094727\n",
      "(64, 33)\n",
      "step 711, loss is 5.7127461433410645\n",
      "(64, 33)\n",
      "step 712, loss is 5.796600341796875\n",
      "(64, 33)\n",
      "step 713, loss is 5.78338623046875\n",
      "(64, 33)\n",
      "step 714, loss is 5.788487911224365\n",
      "(64, 33)\n",
      "step 715, loss is 5.721564769744873\n",
      "(64, 33)\n",
      "step 716, loss is 5.6813063621521\n",
      "(64, 33)\n",
      "step 717, loss is 5.932619094848633\n",
      "(64, 33)\n",
      "step 718, loss is 5.799485206604004\n",
      "(64, 33)\n",
      "step 719, loss is 5.882180690765381\n",
      "(64, 33)\n",
      "step 720, loss is 5.724860191345215\n",
      "(64, 33)\n",
      "step 721, loss is 5.736159324645996\n",
      "(64, 33)\n",
      "step 722, loss is 5.642796993255615\n",
      "(64, 33)\n",
      "step 723, loss is 5.703186511993408\n",
      "(64, 33)\n",
      "step 724, loss is 5.779138565063477\n",
      "(64, 33)\n",
      "step 725, loss is 5.756897449493408\n",
      "(64, 33)\n",
      "step 726, loss is 5.872992515563965\n",
      "(64, 33)\n",
      "step 727, loss is 5.689628601074219\n",
      "(64, 33)\n",
      "step 728, loss is 5.659489631652832\n",
      "(64, 33)\n",
      "step 729, loss is 5.859530448913574\n",
      "(64, 33)\n",
      "step 730, loss is 5.805126190185547\n",
      "(64, 33)\n",
      "step 731, loss is 5.660501956939697\n",
      "(64, 33)\n",
      "step 732, loss is 5.740261077880859\n",
      "(64, 33)\n",
      "step 733, loss is 5.674253463745117\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 734, loss is 5.7412543296813965\n",
      "(64, 33)\n",
      "step 735, loss is 5.8143134117126465\n",
      "(64, 33)\n",
      "step 736, loss is 5.6424736976623535\n",
      "(64, 33)\n",
      "step 737, loss is 5.693970680236816\n",
      "(64, 33)\n",
      "step 738, loss is 5.700714111328125\n",
      "(64, 33)\n",
      "step 739, loss is 5.808602809906006\n",
      "(64, 33)\n",
      "step 740, loss is 5.967475891113281\n",
      "(64, 33)\n",
      "step 741, loss is 5.74840784072876\n",
      "(64, 33)\n",
      "step 742, loss is 5.868999004364014\n",
      "(64, 33)\n",
      "step 743, loss is 5.679218769073486\n",
      "(64, 33)\n",
      "step 744, loss is 5.957379341125488\n",
      "(64, 33)\n",
      "step 745, loss is 5.799756050109863\n",
      "(64, 33)\n",
      "step 746, loss is 5.584260940551758\n",
      "(64, 33)\n",
      "step 747, loss is 5.829229831695557\n",
      "(64, 33)\n",
      "step 748, loss is 5.762981414794922\n",
      "(64, 33)\n",
      "step 749, loss is 5.863596439361572\n",
      "(64, 33)\n",
      "step 750, loss is 5.717973232269287\n",
      "(64, 33)\n",
      "step 751, loss is 5.464476108551025\n",
      "(64, 33)\n",
      "step 752, loss is 5.678736686706543\n",
      "(64, 33)\n",
      "step 753, loss is 5.706688404083252\n",
      "(64, 33)\n",
      "step 754, loss is 5.761323928833008\n",
      "(64, 33)\n",
      "step 755, loss is 5.602425575256348\n",
      "(64, 33)\n",
      "step 756, loss is 5.771237373352051\n",
      "(64, 33)\n",
      "step 757, loss is 5.4410295486450195\n",
      "(64, 33)\n",
      "step 758, loss is 5.75097131729126\n",
      "(64, 33)\n",
      "step 759, loss is 5.743777751922607\n",
      "(64, 33)\n",
      "step 760, loss is 5.714128017425537\n",
      "(64, 33)\n",
      "step 761, loss is 5.6657185554504395\n",
      "(64, 33)\n",
      "step 762, loss is 5.718034744262695\n",
      "(64, 33)\n",
      "step 763, loss is 5.913830280303955\n",
      "(64, 33)\n",
      "step 764, loss is 5.758101940155029\n",
      "(64, 33)\n",
      "step 765, loss is 5.581223487854004\n",
      "(64, 33)\n",
      "step 766, loss is 5.72353458404541\n",
      "(64, 33)\n",
      "step 767, loss is 5.87584924697876\n",
      "(64, 33)\n",
      "step 768, loss is 5.759479999542236\n",
      "(64, 33)\n",
      "step 769, loss is 5.783950328826904\n",
      "(64, 33)\n",
      "step 770, loss is 5.703868389129639\n",
      "(64, 33)\n",
      "step 771, loss is 5.797575950622559\n",
      "(64, 33)\n",
      "step 772, loss is 5.7426252365112305\n",
      "(64, 33)\n",
      "step 773, loss is 5.737882614135742\n",
      "(64, 33)\n",
      "step 774, loss is 5.811542987823486\n",
      "(64, 33)\n",
      "step 775, loss is 5.802896976470947\n",
      "(64, 33)\n",
      "step 776, loss is 5.9531426429748535\n",
      "(64, 33)\n",
      "step 777, loss is 5.802040100097656\n",
      "(64, 33)\n",
      "step 778, loss is 5.688498497009277\n",
      "(64, 33)\n",
      "step 779, loss is 5.7593560218811035\n",
      "(64, 33)\n",
      "step 780, loss is 5.76513671875\n",
      "(64, 33)\n",
      "step 781, loss is 5.748648166656494\n",
      "(64, 33)\n",
      "step 782, loss is 5.8227338790893555\n",
      "(64, 33)\n",
      "step 783, loss is 5.652231216430664\n",
      "(64, 33)\n",
      "step 784, loss is 5.614900588989258\n",
      "(64, 33)\n",
      "step 785, loss is 5.793116092681885\n",
      "(64, 33)\n",
      "step 786, loss is 5.856914043426514\n",
      "(64, 33)\n",
      "step 787, loss is 5.778985023498535\n",
      "(64, 33)\n",
      "step 788, loss is 5.910647869110107\n",
      "(64, 33)\n",
      "step 789, loss is 5.807368755340576\n",
      "(64, 33)\n",
      "step 790, loss is 5.6426615715026855\n",
      "(64, 33)\n",
      "step 791, loss is 5.800077438354492\n",
      "(64, 33)\n",
      "step 792, loss is 5.695401191711426\n",
      "(64, 33)\n",
      "step 793, loss is 5.898138999938965\n",
      "(64, 33)\n",
      "step 794, loss is 5.596475601196289\n",
      "(64, 33)\n",
      "step 795, loss is 5.800203800201416\n",
      "(64, 33)\n",
      "step 796, loss is 5.769179344177246\n",
      "(64, 33)\n",
      "step 797, loss is 5.96187162399292\n",
      "(64, 33)\n",
      "step 798, loss is 5.849955081939697\n",
      "(64, 33)\n",
      "step 799, loss is 5.53061580657959\n",
      "(64, 33)\n",
      "step 800, loss is 5.814236640930176\n",
      "(64, 33)\n",
      "step 801, loss is 5.689006328582764\n",
      "(64, 33)\n",
      "step 802, loss is 5.851837635040283\n",
      "(64, 33)\n",
      "step 803, loss is 5.690608978271484\n",
      "(64, 33)\n",
      "step 804, loss is 5.8825297355651855\n",
      "(64, 33)\n",
      "step 805, loss is 5.818279266357422\n",
      "(64, 33)\n",
      "step 806, loss is 5.843862533569336\n",
      "(64, 33)\n",
      "step 807, loss is 5.778592109680176\n",
      "(64, 33)\n",
      "step 808, loss is 5.707792282104492\n",
      "(64, 33)\n",
      "step 809, loss is 5.769158840179443\n",
      "(64, 33)\n",
      "step 810, loss is 5.835421085357666\n",
      "(64, 33)\n",
      "step 811, loss is 5.858111381530762\n",
      "(64, 33)\n",
      "step 812, loss is 5.704051494598389\n",
      "(64, 33)\n",
      "step 813, loss is 5.606413841247559\n",
      "(64, 33)\n",
      "step 814, loss is 5.762548923492432\n",
      "(64, 33)\n",
      "step 815, loss is 5.767515659332275\n",
      "(64, 33)\n",
      "step 816, loss is 5.869867324829102\n",
      "(64, 33)\n",
      "step 817, loss is 5.645092487335205\n",
      "(64, 33)\n",
      "step 818, loss is 5.5931596755981445\n",
      "(64, 33)\n",
      "step 819, loss is 5.86916446685791\n",
      "(64, 33)\n",
      "step 820, loss is 5.665058612823486\n",
      "(64, 33)\n",
      "step 821, loss is 5.883911609649658\n",
      "(64, 33)\n",
      "step 822, loss is 5.789033889770508\n",
      "(64, 33)\n",
      "step 823, loss is 5.718160629272461\n",
      "(64, 33)\n",
      "step 824, loss is 5.6824164390563965\n",
      "(64, 33)\n",
      "step 825, loss is 5.829334259033203\n",
      "(64, 33)\n",
      "step 826, loss is 5.575135231018066\n",
      "(64, 33)\n",
      "step 827, loss is 5.588932991027832\n",
      "(64, 33)\n",
      "step 828, loss is 5.826356887817383\n",
      "(64, 33)\n",
      "step 829, loss is 5.611141204833984\n",
      "(64, 33)\n",
      "step 830, loss is 5.772531032562256\n",
      "(64, 33)\n",
      "step 831, loss is 5.918862342834473\n",
      "(64, 33)\n",
      "step 832, loss is 5.618868827819824\n",
      "(64, 33)\n",
      "step 833, loss is 5.654304504394531\n",
      "(64, 33)\n",
      "step 834, loss is 5.89155912399292\n",
      "(64, 33)\n",
      "step 835, loss is 5.7764692306518555\n",
      "(64, 33)\n",
      "step 836, loss is 5.9147047996521\n",
      "(64, 33)\n",
      "step 837, loss is 5.65329122543335\n",
      "(64, 33)\n",
      "step 838, loss is 5.754817962646484\n",
      "(64, 33)\n",
      "step 839, loss is 5.712334632873535\n",
      "(64, 33)\n",
      "step 840, loss is 5.6138410568237305\n",
      "(64, 33)\n",
      "step 841, loss is 5.805854320526123\n",
      "(64, 33)\n",
      "step 842, loss is 5.743005752563477\n",
      "(64, 33)\n",
      "step 843, loss is 5.785238265991211\n",
      "(64, 33)\n",
      "step 844, loss is 5.829895973205566\n",
      "(64, 33)\n",
      "step 845, loss is 5.5787129402160645\n",
      "(64, 33)\n",
      "step 846, loss is 5.771096229553223\n",
      "(64, 33)\n",
      "step 847, loss is 5.715762138366699\n",
      "(64, 33)\n",
      "step 848, loss is 5.859526634216309\n",
      "(64, 33)\n",
      "step 849, loss is 5.781345844268799\n",
      "(64, 33)\n",
      "step 850, loss is 5.735227584838867\n",
      "(64, 33)\n",
      "step 851, loss is 5.7395501136779785\n",
      "(64, 33)\n",
      "step 852, loss is 5.800673961639404\n",
      "(64, 33)\n",
      "step 853, loss is 5.769686698913574\n",
      "(64, 33)\n",
      "step 854, loss is 5.6645307540893555\n",
      "(64, 33)\n",
      "step 855, loss is 5.793112277984619\n",
      "(64, 33)\n",
      "step 856, loss is 5.725883483886719\n",
      "(64, 33)\n",
      "step 857, loss is 5.7874860763549805\n",
      "(64, 33)\n",
      "step 858, loss is 5.8261494636535645\n",
      "(64, 33)\n",
      "step 859, loss is 5.776689052581787\n",
      "(64, 33)\n",
      "step 860, loss is 5.61725378036499\n",
      "(64, 33)\n",
      "step 861, loss is 5.940685272216797\n",
      "(64, 33)\n",
      "step 862, loss is 5.93443489074707\n",
      "(64, 33)\n",
      "step 863, loss is 5.575051307678223\n",
      "(64, 33)\n",
      "step 864, loss is 5.71744966506958\n",
      "(64, 33)\n",
      "step 865, loss is 5.844525337219238\n",
      "(64, 33)\n",
      "step 866, loss is 5.590476989746094\n",
      "(64, 33)\n",
      "step 867, loss is 5.843631744384766\n",
      "(64, 33)\n",
      "step 868, loss is 5.8776936531066895\n",
      "(64, 33)\n",
      "step 869, loss is 5.793659210205078\n",
      "(64, 33)\n",
      "step 870, loss is 5.81798791885376\n",
      "(64, 33)\n",
      "step 871, loss is 5.673644065856934\n",
      "(64, 33)\n",
      "step 872, loss is 5.649291038513184\n",
      "(64, 33)\n",
      "step 873, loss is 5.492061138153076\n",
      "(64, 33)\n",
      "step 874, loss is 5.586982727050781\n",
      "(64, 33)\n",
      "step 875, loss is 5.886527061462402\n",
      "(64, 33)\n",
      "step 876, loss is 5.7418060302734375\n",
      "(64, 33)\n",
      "step 877, loss is 5.679990768432617\n",
      "(64, 33)\n",
      "step 878, loss is 5.697742938995361\n",
      "(64, 33)\n",
      "step 879, loss is 5.871255874633789\n",
      "(64, 33)\n",
      "step 880, loss is 5.87961483001709\n",
      "(64, 33)\n",
      "step 881, loss is 5.812794208526611\n",
      "(64, 33)\n",
      "step 882, loss is 5.738165855407715\n",
      "(64, 33)\n",
      "step 883, loss is 5.765005588531494\n",
      "(64, 33)\n",
      "step 884, loss is 5.690032482147217\n",
      "(64, 33)\n",
      "step 885, loss is 5.78501558303833\n",
      "(64, 33)\n",
      "step 886, loss is 5.699060916900635\n",
      "(64, 33)\n",
      "step 887, loss is 5.78580904006958\n",
      "(64, 33)\n",
      "step 888, loss is 5.540246486663818\n",
      "(64, 33)\n",
      "step 889, loss is 5.723338603973389\n",
      "(64, 33)\n",
      "step 890, loss is 5.690986156463623\n",
      "(64, 33)\n",
      "step 891, loss is 5.794195652008057\n",
      "(64, 33)\n",
      "step 892, loss is 5.548551082611084\n",
      "(64, 33)\n",
      "step 893, loss is 5.8077826499938965\n",
      "(64, 33)\n",
      "step 894, loss is 5.641744613647461\n",
      "(64, 33)\n",
      "step 895, loss is 5.846450328826904\n",
      "(64, 33)\n",
      "step 896, loss is 5.794300079345703\n",
      "(64, 33)\n",
      "step 897, loss is 5.561273097991943\n",
      "(64, 33)\n",
      "step 898, loss is 5.577291965484619\n",
      "(64, 33)\n",
      "step 899, loss is 5.758216381072998\n",
      "(64, 33)\n",
      "step 900, loss is 5.576366901397705\n",
      "(64, 33)\n",
      "step 901, loss is 5.788189888000488\n",
      "(64, 33)\n",
      "step 902, loss is 5.6487812995910645\n",
      "(64, 33)\n",
      "step 903, loss is 5.682660102844238\n",
      "(64, 33)\n",
      "step 904, loss is 5.791725158691406\n",
      "(64, 33)\n",
      "step 905, loss is 5.842142581939697\n",
      "(64, 33)\n",
      "step 906, loss is 5.840668678283691\n",
      "(64, 33)\n",
      "step 907, loss is 5.811622619628906\n",
      "(64, 33)\n",
      "step 908, loss is 6.003966331481934\n",
      "(64, 33)\n",
      "step 909, loss is 5.743937015533447\n",
      "(64, 33)\n",
      "step 910, loss is 5.896313190460205\n",
      "(64, 33)\n",
      "step 911, loss is 5.953250408172607\n",
      "(64, 33)\n",
      "step 912, loss is 5.7803120613098145\n",
      "(64, 33)\n",
      "step 913, loss is 5.626458168029785\n",
      "(64, 33)\n",
      "step 914, loss is 5.647904872894287\n",
      "(64, 33)\n",
      "step 915, loss is 5.490819454193115\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 916, loss is 5.755041599273682\n",
      "(64, 33)\n",
      "step 917, loss is 5.629760265350342\n",
      "(64, 33)\n",
      "step 918, loss is 5.908602714538574\n",
      "(64, 33)\n",
      "step 919, loss is 5.68350887298584\n",
      "(64, 33)\n",
      "step 920, loss is 5.398185729980469\n",
      "(64, 33)\n",
      "step 921, loss is 5.681220054626465\n",
      "(64, 33)\n",
      "step 922, loss is 5.470965385437012\n",
      "(64, 33)\n",
      "step 923, loss is 5.589393615722656\n",
      "(64, 33)\n",
      "step 924, loss is 5.6024651527404785\n",
      "(64, 33)\n",
      "step 925, loss is 5.716646194458008\n",
      "(64, 33)\n",
      "step 926, loss is 5.7300639152526855\n",
      "(64, 33)\n",
      "step 927, loss is 5.529717445373535\n",
      "(64, 33)\n",
      "step 928, loss is 5.853202819824219\n",
      "(64, 33)\n",
      "step 929, loss is 5.7851786613464355\n",
      "(64, 33)\n",
      "step 930, loss is 5.632996082305908\n",
      "(64, 33)\n",
      "step 931, loss is 5.766078472137451\n",
      "(64, 33)\n",
      "step 932, loss is 5.778919696807861\n",
      "(64, 33)\n",
      "step 933, loss is 5.7963433265686035\n",
      "(64, 33)\n",
      "step 934, loss is 5.71209192276001\n",
      "(64, 33)\n",
      "step 935, loss is 5.673165798187256\n",
      "(64, 33)\n",
      "step 936, loss is 5.84104585647583\n",
      "(64, 33)\n",
      "step 937, loss is 5.694160461425781\n",
      "(64, 33)\n",
      "step 938, loss is 5.746796607971191\n",
      "(64, 33)\n",
      "step 939, loss is 5.798692226409912\n",
      "(64, 33)\n",
      "step 940, loss is 5.65641450881958\n",
      "(64, 33)\n",
      "step 941, loss is 5.79371976852417\n",
      "(64, 33)\n",
      "step 942, loss is 5.940603733062744\n",
      "(64, 33)\n",
      "step 943, loss is 5.608920097351074\n",
      "(64, 33)\n",
      "step 944, loss is 5.578336238861084\n",
      "(64, 33)\n",
      "step 945, loss is 5.689626216888428\n",
      "(64, 33)\n",
      "step 946, loss is 5.635648250579834\n",
      "(64, 33)\n",
      "step 947, loss is 5.59403133392334\n",
      "(64, 33)\n",
      "step 948, loss is 5.5464301109313965\n",
      "(64, 33)\n",
      "step 949, loss is 5.485444068908691\n",
      "(64, 33)\n",
      "step 950, loss is 5.548036575317383\n",
      "(64, 33)\n",
      "step 951, loss is 5.731300354003906\n",
      "(64, 33)\n",
      "step 952, loss is 5.476018905639648\n",
      "(64, 33)\n",
      "step 953, loss is 5.636949062347412\n",
      "(64, 33)\n",
      "step 954, loss is 5.588586807250977\n",
      "(64, 33)\n",
      "step 955, loss is 5.760076522827148\n",
      "(64, 33)\n",
      "step 956, loss is 5.697712421417236\n",
      "(64, 33)\n",
      "step 957, loss is 5.584812164306641\n",
      "(64, 33)\n",
      "step 958, loss is 5.837174892425537\n",
      "(64, 33)\n",
      "step 959, loss is 5.72921085357666\n",
      "(64, 33)\n",
      "step 960, loss is 5.827663421630859\n",
      "(64, 33)\n",
      "step 961, loss is 5.755784511566162\n",
      "(64, 33)\n",
      "step 962, loss is 5.803904056549072\n",
      "(64, 33)\n",
      "step 963, loss is 5.7474470138549805\n",
      "(64, 33)\n",
      "step 964, loss is 5.740235805511475\n",
      "(64, 33)\n",
      "step 965, loss is 5.678457260131836\n",
      "(64, 33)\n",
      "step 966, loss is 5.61090087890625\n",
      "(64, 33)\n",
      "step 967, loss is 5.614599704742432\n",
      "(64, 33)\n",
      "step 968, loss is 5.526838779449463\n",
      "(64, 33)\n",
      "step 969, loss is 5.345481872558594\n",
      "(64, 33)\n",
      "step 970, loss is 5.716281414031982\n",
      "(64, 33)\n",
      "step 971, loss is 5.557559013366699\n",
      "(64, 33)\n",
      "step 972, loss is 5.71968412399292\n",
      "(64, 33)\n",
      "step 973, loss is 5.761293411254883\n",
      "(64, 33)\n",
      "step 974, loss is 5.665769577026367\n",
      "(64, 33)\n",
      "step 975, loss is 5.411979675292969\n",
      "(64, 33)\n",
      "step 976, loss is 5.625686168670654\n",
      "(64, 33)\n",
      "step 977, loss is 5.759379863739014\n",
      "(64, 33)\n",
      "step 978, loss is 5.529425621032715\n",
      "(64, 33)\n",
      "step 979, loss is 5.734084606170654\n",
      "(64, 33)\n",
      "step 980, loss is 5.562510967254639\n",
      "(64, 33)\n",
      "step 981, loss is 5.70601749420166\n",
      "(64, 33)\n",
      "step 982, loss is 5.842607498168945\n",
      "(64, 33)\n",
      "step 983, loss is 5.786857604980469\n",
      "(64, 33)\n",
      "step 984, loss is 5.758344650268555\n",
      "(64, 33)\n",
      "step 985, loss is 5.626169204711914\n",
      "(64, 33)\n",
      "step 986, loss is 5.584425449371338\n",
      "(64, 33)\n",
      "step 987, loss is 5.711768627166748\n",
      "(64, 33)\n",
      "step 988, loss is 5.538201332092285\n",
      "(64, 33)\n",
      "step 989, loss is 5.72749662399292\n",
      "(64, 33)\n",
      "step 990, loss is 5.550784587860107\n",
      "(64, 33)\n",
      "step 991, loss is 5.760283470153809\n",
      "(64, 33)\n",
      "step 992, loss is 5.571753978729248\n",
      "(64, 33)\n",
      "step 993, loss is 5.684170246124268\n",
      "(64, 33)\n",
      "step 994, loss is 5.8000617027282715\n",
      "(64, 33)\n",
      "step 995, loss is 5.63014030456543\n",
      "(64, 33)\n",
      "step 996, loss is 5.572899341583252\n",
      "(64, 33)\n",
      "step 997, loss is 5.651052951812744\n",
      "(64, 33)\n",
      "step 998, loss is 5.750644207000732\n",
      "(64, 33)\n",
      "step 999, loss is 5.755465030670166\n",
      "(64, 33)\n",
      "step 1000, loss is 5.666390895843506\n",
      "(64, 33)\n",
      "step 1001, loss is 5.536688804626465\n",
      "(64, 33)\n",
      "step 1002, loss is 5.379565238952637\n",
      "(64, 33)\n",
      "step 1003, loss is 5.933354377746582\n",
      "(64, 33)\n",
      "step 1004, loss is 5.784013271331787\n",
      "(64, 33)\n",
      "step 1005, loss is 5.494389057159424\n",
      "(64, 33)\n",
      "step 1006, loss is 5.769594192504883\n",
      "(64, 33)\n",
      "step 1007, loss is 5.524543285369873\n",
      "(64, 33)\n",
      "step 1008, loss is 5.7514519691467285\n",
      "(64, 33)\n",
      "step 1009, loss is 5.602992057800293\n",
      "(64, 33)\n",
      "step 1010, loss is 5.632179260253906\n",
      "(64, 33)\n",
      "step 1011, loss is 5.581368446350098\n",
      "(64, 33)\n",
      "step 1012, loss is 5.768363952636719\n",
      "(64, 33)\n",
      "step 1013, loss is 5.716917514801025\n",
      "(64, 33)\n",
      "step 1014, loss is 5.820608615875244\n",
      "(64, 33)\n",
      "step 1015, loss is 5.643673419952393\n",
      "(64, 33)\n",
      "step 1016, loss is 5.6958112716674805\n",
      "(64, 33)\n",
      "step 1017, loss is 5.568108081817627\n",
      "(64, 33)\n",
      "step 1018, loss is 5.578888416290283\n",
      "(64, 33)\n",
      "step 1019, loss is 5.747769832611084\n",
      "(64, 33)\n",
      "step 1020, loss is 5.736077785491943\n",
      "(64, 33)\n",
      "step 1021, loss is 5.599865436553955\n",
      "(64, 33)\n",
      "step 1022, loss is 5.740411281585693\n",
      "(64, 33)\n",
      "step 1023, loss is 5.53543758392334\n",
      "(64, 33)\n",
      "step 1024, loss is 5.461453437805176\n",
      "(64, 33)\n",
      "step 1025, loss is 5.7202982902526855\n",
      "(64, 33)\n",
      "step 1026, loss is 5.651614189147949\n",
      "(64, 33)\n",
      "step 1027, loss is 5.730448246002197\n",
      "(64, 33)\n",
      "step 1028, loss is 5.499813079833984\n",
      "(64, 33)\n",
      "step 1029, loss is 5.644445896148682\n",
      "(64, 33)\n",
      "step 1030, loss is 5.589888572692871\n",
      "(64, 33)\n",
      "step 1031, loss is 5.466681480407715\n",
      "(64, 33)\n",
      "step 1032, loss is 5.530762195587158\n",
      "(64, 33)\n",
      "step 1033, loss is 5.704164028167725\n",
      "(64, 33)\n",
      "step 1034, loss is 5.708789825439453\n",
      "(64, 33)\n",
      "step 1035, loss is 5.696316719055176\n",
      "(64, 33)\n",
      "step 1036, loss is 5.553586483001709\n",
      "(64, 33)\n",
      "step 1037, loss is 5.583292007446289\n",
      "(64, 33)\n",
      "step 1038, loss is 5.732783317565918\n",
      "(64, 33)\n",
      "step 1039, loss is 5.702078819274902\n",
      "(64, 33)\n",
      "step 1040, loss is 5.622768402099609\n",
      "(64, 33)\n",
      "step 1041, loss is 5.576906681060791\n",
      "(64, 33)\n",
      "step 1042, loss is 5.6728196144104\n",
      "(64, 33)\n",
      "step 1043, loss is 5.587024688720703\n",
      "(64, 33)\n",
      "step 1044, loss is 5.5847249031066895\n",
      "(64, 33)\n",
      "step 1045, loss is 5.598374843597412\n",
      "(64, 33)\n",
      "step 1046, loss is 5.579122543334961\n",
      "(64, 33)\n",
      "step 1047, loss is 5.779007911682129\n",
      "(64, 33)\n",
      "step 1048, loss is 5.520471096038818\n",
      "(64, 33)\n",
      "step 1049, loss is 5.738925457000732\n",
      "(64, 33)\n",
      "step 1050, loss is 5.7246198654174805\n",
      "(64, 33)\n",
      "step 1051, loss is 5.6584367752075195\n",
      "(64, 33)\n",
      "step 1052, loss is 5.642848014831543\n",
      "(64, 33)\n",
      "step 1053, loss is 5.541284084320068\n",
      "(64, 33)\n",
      "step 1054, loss is 5.749509811401367\n",
      "(64, 33)\n",
      "step 1055, loss is 5.729641437530518\n",
      "(64, 33)\n",
      "step 1056, loss is 5.6894049644470215\n",
      "(64, 33)\n",
      "step 1057, loss is 5.660638809204102\n",
      "(64, 33)\n",
      "step 1058, loss is 5.862253189086914\n",
      "(64, 33)\n",
      "step 1059, loss is 5.7480387687683105\n",
      "(64, 33)\n",
      "step 1060, loss is 5.6689066886901855\n",
      "(64, 33)\n",
      "step 1061, loss is 5.586192607879639\n",
      "(64, 33)\n",
      "step 1062, loss is 5.669717788696289\n",
      "(64, 33)\n",
      "step 1063, loss is 5.520778656005859\n",
      "(64, 33)\n",
      "step 1064, loss is 5.783135890960693\n",
      "(64, 33)\n",
      "step 1065, loss is 5.5430498123168945\n",
      "(64, 33)\n",
      "step 1066, loss is 5.7235798835754395\n",
      "(64, 33)\n",
      "step 1067, loss is 5.74591064453125\n",
      "(64, 33)\n",
      "step 1068, loss is 5.826443672180176\n",
      "(64, 33)\n",
      "step 1069, loss is 5.649637222290039\n",
      "(64, 33)\n",
      "step 1070, loss is 5.657607078552246\n",
      "(64, 33)\n",
      "step 1071, loss is 5.589384078979492\n",
      "(64, 33)\n",
      "step 1072, loss is 5.849588394165039\n",
      "(64, 33)\n",
      "step 1073, loss is 5.6135172843933105\n",
      "(64, 33)\n",
      "step 1074, loss is 5.7047553062438965\n",
      "(64, 33)\n",
      "step 1075, loss is 5.700357913970947\n",
      "(64, 33)\n",
      "step 1076, loss is 5.803160190582275\n",
      "(64, 33)\n",
      "step 1077, loss is 5.737534046173096\n",
      "(64, 33)\n",
      "step 1078, loss is 5.710166931152344\n",
      "(64, 33)\n",
      "step 1079, loss is 5.6192755699157715\n",
      "(64, 33)\n",
      "step 1080, loss is 5.726812839508057\n",
      "(64, 33)\n",
      "step 1081, loss is 5.643253803253174\n",
      "(64, 33)\n",
      "step 1082, loss is 5.7259721755981445\n",
      "(64, 33)\n",
      "step 1083, loss is 5.762957572937012\n",
      "(64, 33)\n",
      "step 1084, loss is 5.626354694366455\n",
      "(64, 33)\n",
      "step 1085, loss is 5.523835182189941\n",
      "(64, 33)\n",
      "step 1086, loss is 5.758967876434326\n",
      "(64, 33)\n",
      "step 1087, loss is 5.483155727386475\n",
      "(64, 33)\n",
      "step 1088, loss is 5.780330657958984\n",
      "(64, 33)\n",
      "step 1089, loss is 5.690496444702148\n",
      "(64, 33)\n",
      "step 1090, loss is 5.8579254150390625\n",
      "(64, 33)\n",
      "step 1091, loss is 5.751775741577148\n",
      "(64, 33)\n",
      "step 1092, loss is 5.806171894073486\n",
      "(64, 33)\n",
      "step 1093, loss is 5.741275310516357\n",
      "(64, 33)\n",
      "step 1094, loss is 5.540945529937744\n",
      "(64, 33)\n",
      "step 1095, loss is 5.563249111175537\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1096, loss is 5.774439334869385\n",
      "(64, 33)\n",
      "step 1097, loss is 5.694890975952148\n",
      "(64, 33)\n",
      "step 1098, loss is 5.6488356590271\n",
      "(64, 33)\n",
      "step 1099, loss is 5.438436031341553\n",
      "(64, 33)\n",
      "step 1100, loss is 5.766408443450928\n",
      "(64, 33)\n",
      "step 1101, loss is 5.492663383483887\n",
      "(64, 33)\n",
      "step 1102, loss is 5.377962589263916\n",
      "(64, 33)\n",
      "step 1103, loss is 5.679097652435303\n",
      "(64, 33)\n",
      "step 1104, loss is 5.641822814941406\n",
      "(64, 33)\n",
      "step 1105, loss is 5.683517932891846\n",
      "(64, 33)\n",
      "step 1106, loss is 5.641879081726074\n",
      "(64, 33)\n",
      "step 1107, loss is 5.791055202484131\n",
      "(64, 33)\n",
      "step 1108, loss is 5.862308502197266\n",
      "(64, 33)\n",
      "step 1109, loss is 5.788090705871582\n",
      "(64, 33)\n",
      "step 1110, loss is 5.628444671630859\n",
      "(64, 33)\n",
      "step 1111, loss is 5.635224342346191\n",
      "(64, 33)\n",
      "step 1112, loss is 5.551355361938477\n",
      "(64, 33)\n",
      "step 1113, loss is 5.582783222198486\n",
      "(64, 33)\n",
      "step 1114, loss is 5.822865009307861\n",
      "(64, 33)\n",
      "step 1115, loss is 5.8259358406066895\n",
      "(64, 33)\n",
      "step 1116, loss is 5.49807071685791\n",
      "(64, 33)\n",
      "step 1117, loss is 5.5010809898376465\n",
      "(64, 33)\n",
      "step 1118, loss is 5.700681209564209\n",
      "(64, 33)\n",
      "step 1119, loss is 5.65755033493042\n",
      "(64, 33)\n",
      "step 1120, loss is 5.817619323730469\n",
      "(64, 33)\n",
      "step 1121, loss is 5.595690727233887\n",
      "(64, 33)\n",
      "step 1122, loss is 5.652490615844727\n",
      "(64, 33)\n",
      "step 1123, loss is 5.579582214355469\n",
      "(64, 33)\n",
      "step 1124, loss is 5.449821949005127\n",
      "(64, 33)\n",
      "step 1125, loss is 5.784492492675781\n",
      "(64, 33)\n",
      "step 1126, loss is 5.494609355926514\n",
      "(64, 33)\n",
      "step 1127, loss is 5.70325231552124\n",
      "(64, 33)\n",
      "step 1128, loss is 5.70790433883667\n",
      "(64, 33)\n",
      "step 1129, loss is 5.712618827819824\n",
      "(64, 33)\n",
      "step 1130, loss is 5.554004669189453\n",
      "(64, 33)\n",
      "step 1131, loss is 5.648098945617676\n",
      "(64, 33)\n",
      "step 1132, loss is 5.602205753326416\n",
      "(64, 33)\n",
      "step 1133, loss is 5.554139137268066\n",
      "(64, 33)\n",
      "step 1134, loss is 5.597692966461182\n",
      "(64, 33)\n",
      "step 1135, loss is 5.670154094696045\n",
      "(64, 33)\n",
      "step 1136, loss is 5.792407989501953\n",
      "(64, 33)\n",
      "step 1137, loss is 5.6744303703308105\n",
      "(64, 33)\n",
      "step 1138, loss is 5.76749324798584\n",
      "(64, 33)\n",
      "step 1139, loss is 5.287319183349609\n",
      "(64, 33)\n",
      "step 1140, loss is 5.694282531738281\n",
      "(64, 33)\n",
      "step 1141, loss is 5.5837812423706055\n",
      "(64, 33)\n",
      "step 1142, loss is 5.6266021728515625\n",
      "(64, 33)\n",
      "step 1143, loss is 5.503708839416504\n",
      "(64, 33)\n",
      "step 1144, loss is 5.544352054595947\n",
      "(64, 33)\n",
      "step 1145, loss is 5.57855224609375\n",
      "(64, 33)\n",
      "step 1146, loss is 5.6991682052612305\n",
      "(64, 33)\n",
      "step 1147, loss is 5.744110107421875\n",
      "(64, 33)\n",
      "step 1148, loss is 5.6220293045043945\n",
      "(64, 33)\n",
      "step 1149, loss is 5.650845050811768\n",
      "(64, 33)\n",
      "step 1150, loss is 5.537439823150635\n",
      "(64, 33)\n",
      "step 1151, loss is 5.6567535400390625\n",
      "(64, 33)\n",
      "step 1152, loss is 5.902892589569092\n",
      "(64, 33)\n",
      "step 1153, loss is 5.582473278045654\n",
      "(64, 33)\n",
      "step 1154, loss is 5.686751365661621\n",
      "(64, 33)\n",
      "step 1155, loss is 5.779435157775879\n",
      "(64, 33)\n",
      "step 1156, loss is 5.5603928565979\n",
      "(64, 33)\n",
      "step 1157, loss is 5.753204345703125\n",
      "(64, 33)\n",
      "step 1158, loss is 5.619151592254639\n",
      "(64, 33)\n",
      "step 1159, loss is 5.82463264465332\n",
      "(64, 33)\n",
      "step 1160, loss is 5.692641258239746\n",
      "(64, 33)\n",
      "step 1161, loss is 5.645554542541504\n",
      "(64, 33)\n",
      "step 1162, loss is 5.65542459487915\n",
      "(64, 33)\n",
      "step 1163, loss is 5.607025146484375\n",
      "(64, 33)\n",
      "step 1164, loss is 5.626896858215332\n",
      "(64, 33)\n",
      "step 1165, loss is 5.579824447631836\n",
      "(64, 33)\n",
      "step 1166, loss is 5.601938247680664\n",
      "(64, 33)\n",
      "step 1167, loss is 5.623930931091309\n",
      "(64, 33)\n",
      "step 1168, loss is 5.768832206726074\n",
      "(64, 33)\n",
      "step 1169, loss is 5.695642471313477\n",
      "(64, 33)\n",
      "step 1170, loss is 5.442896366119385\n",
      "(64, 33)\n",
      "step 1171, loss is 5.67677116394043\n",
      "(64, 33)\n",
      "step 1172, loss is 5.681120872497559\n",
      "(64, 33)\n",
      "step 1173, loss is 5.556524753570557\n",
      "(64, 33)\n",
      "step 1174, loss is 5.543429374694824\n",
      "(64, 33)\n",
      "step 1175, loss is 5.512112617492676\n",
      "(64, 33)\n",
      "step 1176, loss is 5.590841293334961\n",
      "(64, 33)\n",
      "step 1177, loss is 5.542227268218994\n",
      "(64, 33)\n",
      "step 1178, loss is 5.694162845611572\n",
      "(64, 33)\n",
      "step 1179, loss is 5.541553020477295\n",
      "(64, 33)\n",
      "step 1180, loss is 5.411301136016846\n",
      "(64, 33)\n",
      "step 1181, loss is 5.628411769866943\n",
      "(64, 33)\n",
      "step 1182, loss is 5.714603424072266\n",
      "(64, 33)\n",
      "step 1183, loss is 5.674188613891602\n",
      "(64, 33)\n",
      "step 1184, loss is 5.528576850891113\n",
      "(64, 33)\n",
      "step 1185, loss is 5.705927848815918\n",
      "(64, 33)\n",
      "step 1186, loss is 5.65828800201416\n",
      "(64, 33)\n",
      "step 1187, loss is 5.416726589202881\n",
      "(64, 33)\n",
      "step 1188, loss is 5.559546947479248\n",
      "(64, 33)\n",
      "step 1189, loss is 5.784128189086914\n",
      "(64, 33)\n",
      "step 1190, loss is 5.767866134643555\n",
      "(64, 33)\n",
      "step 1191, loss is 5.704221248626709\n",
      "(64, 33)\n",
      "step 1192, loss is 5.550923824310303\n",
      "(64, 33)\n",
      "step 1193, loss is 5.816036701202393\n",
      "(64, 33)\n",
      "step 1194, loss is 5.666608810424805\n",
      "(64, 33)\n",
      "step 1195, loss is 5.783819675445557\n",
      "(64, 33)\n",
      "step 1196, loss is 5.70378303527832\n",
      "(64, 33)\n",
      "step 1197, loss is 5.58377742767334\n",
      "(64, 33)\n",
      "step 1198, loss is 5.693788528442383\n",
      "(64, 33)\n",
      "step 1199, loss is 5.581258296966553\n",
      "(64, 33)\n",
      "step 1200, loss is 5.7064900398254395\n",
      "(64, 33)\n",
      "step 1201, loss is 5.771012306213379\n",
      "(64, 33)\n",
      "step 1202, loss is 5.632466793060303\n",
      "(64, 33)\n",
      "step 1203, loss is 5.598645210266113\n",
      "(64, 33)\n",
      "step 1204, loss is 5.627398490905762\n",
      "(64, 33)\n",
      "step 1205, loss is 5.68916654586792\n",
      "(64, 33)\n",
      "step 1206, loss is 5.74736213684082\n",
      "(64, 33)\n",
      "step 1207, loss is 5.677947044372559\n",
      "(64, 33)\n",
      "step 1208, loss is 5.662483215332031\n",
      "(64, 33)\n",
      "step 1209, loss is 5.612130165100098\n",
      "(64, 33)\n",
      "step 1210, loss is 5.839346408843994\n",
      "(64, 33)\n",
      "step 1211, loss is 5.630361557006836\n",
      "(64, 33)\n",
      "step 1212, loss is 5.605988025665283\n",
      "(64, 33)\n",
      "step 1213, loss is 5.681489944458008\n",
      "(64, 33)\n",
      "step 1214, loss is 5.7411274909973145\n",
      "(64, 33)\n",
      "step 1215, loss is 5.549361705780029\n",
      "(64, 33)\n",
      "step 1216, loss is 5.6080145835876465\n",
      "(64, 33)\n",
      "step 1217, loss is 5.527144908905029\n",
      "(64, 33)\n",
      "step 1218, loss is 5.776554584503174\n",
      "(64, 33)\n",
      "step 1219, loss is 5.565604209899902\n",
      "(64, 33)\n",
      "step 1220, loss is 5.640477180480957\n",
      "(64, 33)\n",
      "step 1221, loss is 5.540804862976074\n",
      "(64, 33)\n",
      "step 1222, loss is 5.415581226348877\n",
      "(64, 33)\n",
      "step 1223, loss is 5.689868450164795\n",
      "(64, 33)\n",
      "step 1224, loss is 5.491761207580566\n",
      "(64, 33)\n",
      "step 1225, loss is 5.642689228057861\n",
      "(64, 33)\n",
      "step 1226, loss is 5.464205741882324\n",
      "(64, 33)\n",
      "step 1227, loss is 5.854540824890137\n",
      "(64, 33)\n",
      "step 1228, loss is 5.574405670166016\n",
      "(64, 33)\n",
      "step 1229, loss is 5.777061939239502\n",
      "(64, 33)\n",
      "step 1230, loss is 5.481929302215576\n",
      "(64, 33)\n",
      "step 1231, loss is 5.557274341583252\n",
      "(64, 33)\n",
      "step 1232, loss is 5.594089031219482\n",
      "(64, 33)\n",
      "step 1233, loss is 5.594906330108643\n",
      "(64, 33)\n",
      "step 1234, loss is 5.554723739624023\n",
      "(64, 33)\n",
      "step 1235, loss is 5.66839599609375\n",
      "(64, 33)\n",
      "step 1236, loss is 5.703840732574463\n",
      "(64, 33)\n",
      "step 1237, loss is 5.476563453674316\n",
      "(64, 33)\n",
      "step 1238, loss is 5.774401664733887\n",
      "(64, 33)\n",
      "step 1239, loss is 5.564585208892822\n",
      "(64, 33)\n",
      "step 1240, loss is 5.541642665863037\n",
      "(64, 33)\n",
      "step 1241, loss is 5.79110050201416\n",
      "(64, 33)\n",
      "step 1242, loss is 5.659185886383057\n",
      "(64, 33)\n",
      "step 1243, loss is 5.617143154144287\n",
      "(64, 33)\n",
      "step 1244, loss is 5.4809465408325195\n",
      "(64, 33)\n",
      "step 1245, loss is 5.707040309906006\n",
      "(64, 33)\n",
      "step 1246, loss is 5.653408527374268\n",
      "(64, 33)\n",
      "step 1247, loss is 5.684357643127441\n",
      "(64, 33)\n",
      "step 1248, loss is 5.54268741607666\n",
      "(64, 33)\n",
      "step 1249, loss is 5.606142044067383\n",
      "(64, 33)\n",
      "step 1250, loss is 5.488458156585693\n",
      "(64, 33)\n",
      "step 1251, loss is 5.611408233642578\n",
      "(64, 33)\n",
      "step 1252, loss is 5.625587463378906\n",
      "(64, 33)\n",
      "step 1253, loss is 5.800267219543457\n",
      "(64, 33)\n",
      "step 1254, loss is 5.622969150543213\n",
      "(64, 33)\n",
      "step 1255, loss is 5.657864570617676\n",
      "(64, 33)\n",
      "step 1256, loss is 5.475799083709717\n",
      "(64, 33)\n",
      "step 1257, loss is 5.7062458992004395\n",
      "(64, 33)\n",
      "step 1258, loss is 5.621767044067383\n",
      "(64, 33)\n",
      "step 1259, loss is 5.7429094314575195\n",
      "(64, 33)\n",
      "step 1260, loss is 5.628752708435059\n",
      "(64, 33)\n",
      "step 1261, loss is 5.578372955322266\n",
      "(64, 33)\n",
      "step 1262, loss is 5.628386974334717\n",
      "(64, 33)\n",
      "step 1263, loss is 5.346066474914551\n",
      "(64, 33)\n",
      "step 1264, loss is 5.741368293762207\n",
      "(64, 33)\n",
      "step 1265, loss is 5.552886962890625\n",
      "(64, 33)\n",
      "step 1266, loss is 5.726617336273193\n",
      "(64, 33)\n",
      "step 1267, loss is 5.469184875488281\n",
      "(64, 33)\n",
      "step 1268, loss is 5.732817649841309\n",
      "(64, 33)\n",
      "step 1269, loss is 5.473920822143555\n",
      "(64, 33)\n",
      "step 1270, loss is 5.786281585693359\n",
      "(64, 33)\n",
      "step 1271, loss is 5.496263027191162\n",
      "(64, 33)\n",
      "step 1272, loss is 5.643990516662598\n",
      "(64, 33)\n",
      "step 1273, loss is 5.379117012023926\n",
      "(64, 33)\n",
      "step 1274, loss is 5.602746963500977\n",
      "(64, 33)\n",
      "step 1275, loss is 5.648943901062012\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1276, loss is 5.690736770629883\n",
      "(64, 33)\n",
      "step 1277, loss is 5.618546009063721\n",
      "(64, 33)\n",
      "step 1278, loss is 5.6567583084106445\n",
      "(64, 33)\n",
      "step 1279, loss is 5.623497486114502\n",
      "(64, 33)\n",
      "step 1280, loss is 5.606485843658447\n",
      "(64, 33)\n",
      "step 1281, loss is 5.644579887390137\n",
      "(64, 33)\n",
      "step 1282, loss is 5.3800530433654785\n",
      "(64, 33)\n",
      "step 1283, loss is 5.5960612297058105\n",
      "(64, 33)\n",
      "step 1284, loss is 5.634324550628662\n",
      "(64, 33)\n",
      "step 1285, loss is 5.543731212615967\n",
      "(64, 33)\n",
      "step 1286, loss is 5.6667070388793945\n",
      "(64, 33)\n",
      "step 1287, loss is 5.523138999938965\n",
      "(64, 33)\n",
      "step 1288, loss is 5.416172981262207\n",
      "(64, 33)\n",
      "step 1289, loss is 5.560362815856934\n",
      "(64, 33)\n",
      "step 1290, loss is 5.924117565155029\n",
      "(64, 33)\n",
      "step 1291, loss is 5.6353654861450195\n",
      "(64, 33)\n",
      "step 1292, loss is 5.613630771636963\n",
      "(64, 33)\n",
      "step 1293, loss is 5.312739372253418\n",
      "(64, 33)\n",
      "step 1294, loss is 5.89674186706543\n",
      "(64, 33)\n",
      "step 1295, loss is 5.521156311035156\n",
      "(64, 33)\n",
      "step 1296, loss is 5.756378650665283\n",
      "(64, 33)\n",
      "step 1297, loss is 5.598721981048584\n",
      "(64, 33)\n",
      "step 1298, loss is 5.825326919555664\n",
      "(64, 33)\n",
      "step 1299, loss is 5.6880574226379395\n",
      "(64, 33)\n",
      "step 1300, loss is 5.620058059692383\n",
      "(64, 33)\n",
      "step 1301, loss is 5.6832146644592285\n",
      "(64, 33)\n",
      "step 1302, loss is 5.339512825012207\n",
      "(64, 33)\n",
      "step 1303, loss is 5.488119125366211\n",
      "(64, 33)\n",
      "step 1304, loss is 5.553398132324219\n",
      "(64, 33)\n",
      "step 1305, loss is 5.554420471191406\n",
      "(64, 33)\n",
      "step 1306, loss is 5.5192975997924805\n",
      "(64, 33)\n",
      "step 1307, loss is 5.546104431152344\n",
      "(64, 33)\n",
      "step 1308, loss is 5.707259178161621\n",
      "(64, 33)\n",
      "step 1309, loss is 5.5502610206604\n",
      "(64, 33)\n",
      "step 1310, loss is 5.607649803161621\n",
      "(64, 33)\n",
      "step 1311, loss is 5.486767292022705\n",
      "(64, 33)\n",
      "step 1312, loss is 5.624846935272217\n",
      "(64, 33)\n",
      "step 1313, loss is 5.5942301750183105\n",
      "(64, 33)\n",
      "step 1314, loss is 5.522710800170898\n",
      "(64, 33)\n",
      "step 1315, loss is 5.387263774871826\n",
      "(64, 33)\n",
      "step 1316, loss is 5.65496301651001\n",
      "(64, 33)\n",
      "step 1317, loss is 5.703135967254639\n",
      "(64, 33)\n",
      "step 1318, loss is 5.576855182647705\n",
      "(64, 33)\n",
      "step 1319, loss is 5.663919448852539\n",
      "(64, 33)\n",
      "step 1320, loss is 5.609880447387695\n",
      "(64, 33)\n",
      "step 1321, loss is 5.56398868560791\n",
      "(64, 33)\n",
      "step 1322, loss is 5.64603853225708\n",
      "(64, 33)\n",
      "step 1323, loss is 5.704329967498779\n",
      "(64, 33)\n",
      "step 1324, loss is 5.71774959564209\n",
      "(64, 33)\n",
      "step 1325, loss is 5.596974849700928\n",
      "(64, 33)\n",
      "step 1326, loss is 5.825874328613281\n",
      "(64, 33)\n",
      "step 1327, loss is 5.657700538635254\n",
      "(64, 33)\n",
      "step 1328, loss is 5.363774299621582\n",
      "(64, 33)\n",
      "step 1329, loss is 5.649202823638916\n",
      "(64, 33)\n",
      "step 1330, loss is 5.767329216003418\n",
      "(64, 33)\n",
      "step 1331, loss is 5.482100486755371\n",
      "(64, 33)\n",
      "step 1332, loss is 5.640668869018555\n",
      "(64, 33)\n",
      "step 1333, loss is 5.621151924133301\n",
      "(64, 33)\n",
      "step 1334, loss is 5.596606731414795\n",
      "(64, 33)\n",
      "step 1335, loss is 5.372713088989258\n",
      "(64, 33)\n",
      "step 1336, loss is 5.701255798339844\n",
      "(64, 33)\n",
      "step 1337, loss is 5.6414265632629395\n",
      "(64, 33)\n",
      "step 1338, loss is 5.605674743652344\n",
      "(64, 33)\n",
      "step 1339, loss is 5.470564365386963\n",
      "(64, 33)\n",
      "step 1340, loss is 5.567699909210205\n",
      "(64, 33)\n",
      "step 1341, loss is 5.5513505935668945\n",
      "(64, 33)\n",
      "step 1342, loss is 5.662078857421875\n",
      "(64, 33)\n",
      "step 1343, loss is 5.423274040222168\n",
      "(64, 33)\n",
      "step 1344, loss is 5.825858116149902\n",
      "(64, 33)\n",
      "step 1345, loss is 5.52805233001709\n",
      "(64, 33)\n",
      "step 1346, loss is 5.809021949768066\n",
      "(64, 33)\n",
      "step 1347, loss is 5.726768970489502\n",
      "(64, 33)\n",
      "step 1348, loss is 5.593470573425293\n",
      "(64, 33)\n",
      "step 1349, loss is 5.595644950866699\n",
      "(64, 33)\n",
      "step 1350, loss is 5.703605651855469\n",
      "(64, 33)\n",
      "step 1351, loss is 5.464118480682373\n",
      "(64, 33)\n",
      "step 1352, loss is 5.714560031890869\n",
      "(64, 33)\n",
      "step 1353, loss is 5.621511459350586\n",
      "(64, 33)\n",
      "step 1354, loss is 5.818474769592285\n",
      "(64, 33)\n",
      "step 1355, loss is 5.536185264587402\n",
      "(64, 33)\n",
      "step 1356, loss is 5.581263065338135\n",
      "(64, 33)\n",
      "step 1357, loss is 5.599332809448242\n",
      "(64, 33)\n",
      "step 1358, loss is 5.736110210418701\n",
      "(64, 33)\n",
      "step 1359, loss is 5.681224822998047\n",
      "(64, 33)\n",
      "step 1360, loss is 5.55397891998291\n",
      "(64, 33)\n",
      "step 1361, loss is 5.573551654815674\n",
      "(64, 33)\n",
      "step 1362, loss is 5.662113189697266\n",
      "(64, 33)\n",
      "step 1363, loss is 5.516700744628906\n",
      "(64, 33)\n",
      "step 1364, loss is 5.644025802612305\n",
      "(64, 33)\n",
      "step 1365, loss is 5.6748046875\n",
      "(64, 33)\n",
      "step 1366, loss is 5.561966896057129\n",
      "(64, 33)\n",
      "step 1367, loss is 5.487153053283691\n",
      "(64, 33)\n",
      "step 1368, loss is 5.811811447143555\n",
      "(64, 33)\n",
      "step 1369, loss is 5.861578941345215\n",
      "(64, 33)\n",
      "step 1370, loss is 5.481849670410156\n",
      "(64, 33)\n",
      "step 1371, loss is 5.639641284942627\n",
      "(64, 33)\n",
      "step 1372, loss is 5.504976749420166\n",
      "(64, 33)\n",
      "step 1373, loss is 5.638875484466553\n",
      "(64, 33)\n",
      "step 1374, loss is 5.75830078125\n",
      "(64, 33)\n",
      "step 1375, loss is 5.639326095581055\n",
      "(64, 33)\n",
      "step 1376, loss is 5.517666816711426\n",
      "(64, 33)\n",
      "step 1377, loss is 5.432563781738281\n",
      "(64, 33)\n",
      "step 1378, loss is 5.474658012390137\n",
      "(64, 33)\n",
      "step 1379, loss is 5.5699238777160645\n",
      "(64, 33)\n",
      "step 1380, loss is 5.475481986999512\n",
      "(64, 33)\n",
      "step 1381, loss is 5.741583824157715\n",
      "(64, 33)\n",
      "step 1382, loss is 5.574333190917969\n",
      "(64, 33)\n",
      "step 1383, loss is 5.645102500915527\n",
      "(64, 33)\n",
      "step 1384, loss is 5.5880889892578125\n",
      "(64, 33)\n",
      "step 1385, loss is 5.632046222686768\n",
      "(64, 33)\n",
      "step 1386, loss is 5.651817321777344\n",
      "(64, 33)\n",
      "step 1387, loss is 5.529120445251465\n",
      "(64, 33)\n",
      "step 1388, loss is 5.554265022277832\n",
      "(64, 33)\n",
      "step 1389, loss is 5.687161922454834\n",
      "(64, 33)\n",
      "step 1390, loss is 5.659709930419922\n",
      "(64, 33)\n",
      "step 1391, loss is 5.50205659866333\n",
      "(64, 33)\n",
      "step 1392, loss is 5.472909450531006\n",
      "(64, 33)\n",
      "step 1393, loss is 5.57670259475708\n",
      "(64, 33)\n",
      "step 1394, loss is 5.588010311126709\n",
      "(64, 33)\n",
      "step 1395, loss is 5.5049262046813965\n",
      "(64, 33)\n",
      "step 1396, loss is 5.620145797729492\n",
      "(64, 33)\n",
      "step 1397, loss is 5.688335418701172\n",
      "(64, 33)\n",
      "step 1398, loss is 5.558457374572754\n",
      "(64, 33)\n",
      "step 1399, loss is 5.653682708740234\n",
      "(64, 33)\n",
      "step 1400, loss is 5.582729816436768\n",
      "(64, 33)\n",
      "step 1401, loss is 5.240695953369141\n",
      "(64, 33)\n",
      "step 1402, loss is 5.637299537658691\n",
      "(64, 33)\n",
      "step 1403, loss is 5.6264824867248535\n",
      "(64, 33)\n",
      "step 1404, loss is 5.551566123962402\n",
      "(64, 33)\n",
      "step 1405, loss is 5.5173211097717285\n",
      "(64, 33)\n",
      "step 1406, loss is 5.567645072937012\n",
      "(64, 33)\n",
      "step 1407, loss is 5.453835964202881\n",
      "(64, 33)\n",
      "step 1408, loss is 5.48154878616333\n",
      "(64, 33)\n",
      "step 1409, loss is 5.71462869644165\n",
      "(64, 33)\n",
      "step 1410, loss is 5.553622245788574\n",
      "(64, 33)\n",
      "step 1411, loss is 5.563244819641113\n",
      "(64, 33)\n",
      "step 1412, loss is 5.510392189025879\n",
      "(64, 33)\n",
      "step 1413, loss is 5.566464424133301\n",
      "(64, 33)\n",
      "step 1414, loss is 5.648918151855469\n",
      "(64, 33)\n",
      "step 1415, loss is 5.600916862487793\n",
      "(64, 33)\n",
      "step 1416, loss is 5.402353763580322\n",
      "(64, 33)\n",
      "step 1417, loss is 5.602498531341553\n",
      "(64, 33)\n",
      "step 1418, loss is 5.675334453582764\n",
      "(64, 33)\n",
      "step 1419, loss is 5.748741149902344\n",
      "(64, 33)\n",
      "step 1420, loss is 5.608603000640869\n",
      "(64, 33)\n",
      "step 1421, loss is 5.619134902954102\n",
      "(64, 33)\n",
      "step 1422, loss is 5.5028605461120605\n",
      "(64, 33)\n",
      "step 1423, loss is 5.678778171539307\n",
      "(64, 33)\n",
      "step 1424, loss is 5.627948760986328\n",
      "(64, 33)\n",
      "step 1425, loss is 5.73346471786499\n",
      "(64, 33)\n",
      "step 1426, loss is 5.556835651397705\n",
      "(64, 33)\n",
      "step 1427, loss is 5.481396675109863\n",
      "(64, 33)\n",
      "step 1428, loss is 5.534436225891113\n",
      "(64, 33)\n",
      "step 1429, loss is 5.538275718688965\n",
      "(64, 33)\n",
      "step 1430, loss is 5.592832565307617\n",
      "(64, 33)\n",
      "step 1431, loss is 5.523550510406494\n",
      "(64, 33)\n",
      "step 1432, loss is 5.395702838897705\n",
      "(64, 33)\n",
      "step 1433, loss is 5.470178127288818\n",
      "(64, 33)\n",
      "step 1434, loss is 5.490828990936279\n",
      "(64, 33)\n",
      "step 1435, loss is 5.554356098175049\n",
      "(64, 33)\n",
      "step 1436, loss is 5.390929698944092\n",
      "(64, 33)\n",
      "step 1437, loss is 5.657588958740234\n",
      "(64, 33)\n",
      "step 1438, loss is 5.6700544357299805\n",
      "(64, 33)\n",
      "step 1439, loss is 5.803959846496582\n",
      "(64, 33)\n",
      "step 1440, loss is 5.545985221862793\n",
      "(64, 33)\n",
      "step 1441, loss is 5.584051132202148\n",
      "(64, 33)\n",
      "step 1442, loss is 5.620048522949219\n",
      "(64, 33)\n",
      "step 1443, loss is 5.649691104888916\n",
      "(64, 33)\n",
      "step 1444, loss is 5.839145183563232\n",
      "(64, 33)\n",
      "step 1445, loss is 5.4541850090026855\n",
      "(64, 33)\n",
      "step 1446, loss is 5.545215606689453\n",
      "(64, 33)\n",
      "step 1447, loss is 5.563955307006836\n",
      "(64, 33)\n",
      "step 1448, loss is 5.63839054107666\n",
      "(64, 33)\n",
      "step 1449, loss is 5.518730163574219\n",
      "(64, 33)\n",
      "step 1450, loss is 5.4598708152771\n",
      "(64, 33)\n",
      "step 1451, loss is 5.528716564178467\n",
      "(64, 33)\n",
      "step 1452, loss is 5.842442989349365\n",
      "(64, 33)\n",
      "step 1453, loss is 5.502892971038818\n",
      "(64, 33)\n",
      "step 1454, loss is 5.5543622970581055\n",
      "(64, 33)\n",
      "step 1455, loss is 5.56896448135376\n",
      "(64, 33)\n",
      "step 1456, loss is 5.6968488693237305\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1457, loss is 5.504733085632324\n",
      "(64, 33)\n",
      "step 1458, loss is 5.382124423980713\n",
      "(64, 33)\n",
      "step 1459, loss is 5.614474773406982\n",
      "(64, 33)\n",
      "step 1460, loss is 5.533279895782471\n",
      "(64, 33)\n",
      "step 1461, loss is 5.581542015075684\n",
      "(64, 33)\n",
      "step 1462, loss is 5.8698625564575195\n",
      "(64, 33)\n",
      "step 1463, loss is 5.469451904296875\n",
      "(64, 33)\n",
      "step 1464, loss is 5.487638473510742\n",
      "(64, 33)\n",
      "step 1465, loss is 5.537704944610596\n",
      "(64, 33)\n",
      "step 1466, loss is 5.609918117523193\n",
      "(64, 33)\n",
      "step 1467, loss is 5.770253658294678\n",
      "(64, 33)\n",
      "step 1468, loss is 5.51154088973999\n",
      "(64, 33)\n",
      "step 1469, loss is 5.470798492431641\n",
      "(64, 33)\n",
      "step 1470, loss is 5.444143295288086\n",
      "(64, 33)\n",
      "step 1471, loss is 5.463944911956787\n",
      "(64, 33)\n",
      "step 1472, loss is 5.300078868865967\n",
      "(64, 33)\n",
      "step 1473, loss is 5.646205902099609\n",
      "(64, 33)\n",
      "step 1474, loss is 5.446556568145752\n",
      "(64, 33)\n",
      "step 1475, loss is 5.601970195770264\n",
      "(64, 33)\n",
      "step 1476, loss is 5.589422702789307\n",
      "(64, 33)\n",
      "step 1477, loss is 5.541726589202881\n",
      "(64, 33)\n",
      "step 1478, loss is 5.441274166107178\n",
      "(64, 33)\n",
      "step 1479, loss is 5.567911624908447\n",
      "(64, 33)\n",
      "step 1480, loss is 5.51629114151001\n",
      "(64, 33)\n",
      "step 1481, loss is 5.585653781890869\n",
      "(64, 33)\n",
      "step 1482, loss is 5.645394802093506\n",
      "(64, 33)\n",
      "step 1483, loss is 5.8036885261535645\n",
      "(64, 33)\n",
      "step 1484, loss is 5.651893138885498\n",
      "(64, 33)\n",
      "step 1485, loss is 5.638938903808594\n",
      "(64, 33)\n",
      "step 1486, loss is 5.567605972290039\n",
      "(64, 33)\n",
      "step 1487, loss is 5.524982929229736\n",
      "(64, 33)\n",
      "step 1488, loss is 5.315119743347168\n",
      "(64, 33)\n",
      "step 1489, loss is 5.5095391273498535\n",
      "(64, 33)\n",
      "step 1490, loss is 5.572601795196533\n",
      "(64, 33)\n",
      "step 1491, loss is 5.356448650360107\n",
      "(64, 33)\n",
      "step 1492, loss is 5.492299556732178\n",
      "(64, 33)\n",
      "step 1493, loss is 5.65722131729126\n",
      "(64, 33)\n",
      "step 1494, loss is 5.658416748046875\n",
      "(64, 33)\n",
      "step 1495, loss is 5.4319868087768555\n",
      "(64, 33)\n",
      "step 1496, loss is 5.36529541015625\n",
      "(64, 33)\n",
      "step 1497, loss is 5.451506614685059\n",
      "(64, 33)\n",
      "step 1498, loss is 5.434138774871826\n",
      "(64, 33)\n",
      "step 1499, loss is 5.529951095581055\n",
      "(64, 33)\n",
      "step 1500, loss is 5.578036785125732\n",
      "(64, 33)\n",
      "step 1501, loss is 5.608251571655273\n",
      "(64, 33)\n",
      "step 1502, loss is 5.471176624298096\n",
      "(64, 33)\n",
      "step 1503, loss is 5.593902111053467\n",
      "(64, 33)\n",
      "step 1504, loss is 5.492433071136475\n",
      "(64, 33)\n",
      "step 1505, loss is 5.597402095794678\n",
      "(64, 33)\n",
      "step 1506, loss is 5.582033634185791\n",
      "(64, 33)\n",
      "step 1507, loss is 5.4935221672058105\n",
      "(64, 33)\n",
      "step 1508, loss is 5.609766960144043\n",
      "(64, 33)\n",
      "step 1509, loss is 5.680880069732666\n",
      "(64, 33)\n",
      "step 1510, loss is 5.546092987060547\n",
      "(64, 33)\n",
      "step 1511, loss is 5.517168998718262\n",
      "(64, 33)\n",
      "step 1512, loss is 5.611400127410889\n",
      "(64, 33)\n",
      "step 1513, loss is 5.599961757659912\n",
      "(64, 33)\n",
      "step 1514, loss is 5.436519622802734\n",
      "(64, 33)\n",
      "step 1515, loss is 5.644917964935303\n",
      "(64, 33)\n",
      "step 1516, loss is 5.69600248336792\n",
      "(64, 33)\n",
      "step 1517, loss is 5.53408145904541\n",
      "(64, 33)\n",
      "step 1518, loss is 5.516862869262695\n",
      "(64, 33)\n",
      "step 1519, loss is 5.548288822174072\n",
      "(64, 33)\n",
      "step 1520, loss is 5.359434127807617\n",
      "(64, 33)\n",
      "step 1521, loss is 5.629490852355957\n",
      "(64, 33)\n",
      "step 1522, loss is 5.466121673583984\n",
      "(64, 33)\n",
      "step 1523, loss is 5.475009918212891\n",
      "(64, 33)\n",
      "step 1524, loss is 5.706215858459473\n",
      "(64, 33)\n",
      "step 1525, loss is 5.43743896484375\n",
      "(64, 33)\n",
      "step 1526, loss is 5.527230262756348\n",
      "(64, 33)\n",
      "step 1527, loss is 5.488539695739746\n",
      "(64, 33)\n",
      "step 1528, loss is 5.440548896789551\n",
      "(64, 33)\n",
      "step 1529, loss is 5.480060577392578\n",
      "(64, 33)\n",
      "step 1530, loss is 5.767040729522705\n",
      "(64, 33)\n",
      "step 1531, loss is 5.5422868728637695\n",
      "(64, 33)\n",
      "step 1532, loss is 5.461691379547119\n",
      "(64, 33)\n",
      "step 1533, loss is 5.467375755310059\n",
      "(64, 33)\n",
      "step 1534, loss is 5.594850540161133\n",
      "(64, 33)\n",
      "step 1535, loss is 5.230766296386719\n",
      "(64, 33)\n",
      "step 1536, loss is 5.298587799072266\n",
      "(64, 33)\n",
      "step 1537, loss is 5.724621295928955\n",
      "(64, 33)\n",
      "step 1538, loss is 5.426458358764648\n",
      "(64, 33)\n",
      "step 1539, loss is 5.566704750061035\n",
      "(64, 33)\n",
      "step 1540, loss is 5.483459949493408\n",
      "(64, 33)\n",
      "step 1541, loss is 5.630373477935791\n",
      "(64, 33)\n",
      "step 1542, loss is 5.2088704109191895\n",
      "(64, 33)\n",
      "step 1543, loss is 5.508062362670898\n",
      "(64, 33)\n",
      "step 1544, loss is 5.774918556213379\n",
      "(64, 33)\n",
      "step 1545, loss is 5.405409336090088\n",
      "(64, 33)\n",
      "step 1546, loss is 5.771453857421875\n",
      "(64, 33)\n",
      "step 1547, loss is 5.531198978424072\n",
      "(64, 33)\n",
      "step 1548, loss is 5.56919527053833\n",
      "(64, 33)\n",
      "step 1549, loss is 5.3959784507751465\n",
      "(64, 33)\n",
      "step 1550, loss is 5.326922416687012\n",
      "(64, 33)\n",
      "step 1551, loss is 5.668071269989014\n",
      "(64, 33)\n",
      "step 1552, loss is 5.609958171844482\n",
      "(64, 33)\n",
      "step 1553, loss is 5.479257583618164\n",
      "(64, 33)\n",
      "step 1554, loss is 5.589212894439697\n",
      "(64, 33)\n",
      "step 1555, loss is 5.44517183303833\n",
      "(64, 33)\n",
      "step 1556, loss is 5.521248817443848\n",
      "(64, 33)\n",
      "step 1557, loss is 5.251520156860352\n",
      "(64, 33)\n",
      "step 1558, loss is 5.670644283294678\n",
      "(64, 33)\n",
      "step 1559, loss is 5.5511393547058105\n",
      "(64, 33)\n",
      "step 1560, loss is 5.457847595214844\n",
      "(64, 33)\n",
      "step 1561, loss is 5.298713207244873\n",
      "(64, 33)\n",
      "step 1562, loss is 5.799758434295654\n",
      "(64, 33)\n",
      "step 1563, loss is 5.2649993896484375\n",
      "(64, 33)\n",
      "step 1564, loss is 5.497354984283447\n",
      "(64, 33)\n",
      "step 1565, loss is 5.3889360427856445\n",
      "(64, 33)\n",
      "step 1566, loss is 5.570518493652344\n",
      "(64, 33)\n",
      "step 1567, loss is 5.57005500793457\n",
      "(64, 33)\n",
      "step 1568, loss is 5.630356311798096\n",
      "(64, 33)\n",
      "step 1569, loss is 5.45412540435791\n",
      "(64, 33)\n",
      "step 1570, loss is 5.404554843902588\n",
      "(64, 33)\n",
      "step 1571, loss is 5.347502708435059\n",
      "(64, 33)\n",
      "step 1572, loss is 5.5506720542907715\n",
      "(64, 33)\n",
      "step 1573, loss is 5.429647445678711\n",
      "(64, 33)\n",
      "step 1574, loss is 5.617808818817139\n",
      "(64, 33)\n",
      "step 1575, loss is 5.527708053588867\n",
      "(64, 33)\n",
      "step 1576, loss is 5.627568244934082\n",
      "(64, 33)\n",
      "step 1577, loss is 5.558140754699707\n",
      "(64, 33)\n",
      "step 1578, loss is 5.3540263175964355\n",
      "(64, 33)\n",
      "step 1579, loss is 5.614382743835449\n",
      "(64, 33)\n",
      "step 1580, loss is 5.676507472991943\n",
      "(64, 33)\n",
      "step 1581, loss is 5.496395587921143\n",
      "(64, 33)\n",
      "step 1582, loss is 5.297935962677002\n",
      "(64, 33)\n",
      "step 1583, loss is 5.409996509552002\n",
      "(64, 33)\n",
      "step 1584, loss is 5.402716636657715\n",
      "(64, 33)\n",
      "step 1585, loss is 5.392145156860352\n",
      "(64, 33)\n",
      "step 1586, loss is 5.418332099914551\n",
      "(64, 33)\n",
      "step 1587, loss is 5.417639255523682\n",
      "(64, 33)\n",
      "step 1588, loss is 5.3549628257751465\n",
      "(64, 33)\n",
      "step 1589, loss is 5.369452476501465\n",
      "(64, 33)\n",
      "step 1590, loss is 5.319725036621094\n",
      "(64, 33)\n",
      "step 1591, loss is 5.4266486167907715\n",
      "(64, 33)\n",
      "step 1592, loss is 5.463205814361572\n",
      "(64, 33)\n",
      "step 1593, loss is 5.555612087249756\n",
      "(64, 33)\n",
      "step 1594, loss is 5.2620062828063965\n",
      "(64, 33)\n",
      "step 1595, loss is 5.31061315536499\n",
      "(64, 33)\n",
      "step 1596, loss is 5.643483638763428\n",
      "(64, 33)\n",
      "step 1597, loss is 5.649839401245117\n",
      "(64, 33)\n",
      "step 1598, loss is 5.486023902893066\n",
      "(64, 33)\n",
      "step 1599, loss is 5.535453796386719\n",
      "(64, 33)\n",
      "step 1600, loss is 5.377519130706787\n",
      "(64, 33)\n",
      "step 1601, loss is 5.476868152618408\n",
      "(64, 33)\n",
      "step 1602, loss is 5.402499675750732\n",
      "(64, 33)\n",
      "step 1603, loss is 5.50390625\n",
      "(64, 33)\n",
      "step 1604, loss is 5.43839168548584\n",
      "(64, 33)\n",
      "step 1605, loss is 5.623141288757324\n",
      "(64, 33)\n",
      "step 1606, loss is 5.3044023513793945\n",
      "(64, 33)\n",
      "step 1607, loss is 5.458044528961182\n",
      "(64, 33)\n",
      "step 1608, loss is 5.3450927734375\n",
      "(64, 33)\n",
      "step 1609, loss is 5.5767717361450195\n",
      "(64, 33)\n",
      "step 1610, loss is 5.40494441986084\n",
      "(64, 33)\n",
      "step 1611, loss is 5.390800952911377\n",
      "(64, 33)\n",
      "step 1612, loss is 5.444087982177734\n",
      "(64, 33)\n",
      "step 1613, loss is 5.3061676025390625\n",
      "(64, 33)\n",
      "step 1614, loss is 5.606567859649658\n",
      "(64, 33)\n",
      "step 1615, loss is 5.554790019989014\n",
      "(64, 33)\n",
      "step 1616, loss is 5.577828884124756\n",
      "(64, 33)\n",
      "step 1617, loss is 5.366174697875977\n",
      "(64, 33)\n",
      "step 1618, loss is 5.556517124176025\n",
      "(64, 33)\n",
      "step 1619, loss is 5.435275077819824\n",
      "(64, 33)\n",
      "step 1620, loss is 5.388396263122559\n",
      "(64, 33)\n",
      "step 1621, loss is 5.448760986328125\n",
      "(64, 33)\n",
      "step 1622, loss is 5.588099002838135\n",
      "(64, 33)\n",
      "step 1623, loss is 5.423923015594482\n",
      "(64, 33)\n",
      "step 1624, loss is 5.368179798126221\n",
      "(64, 33)\n",
      "step 1625, loss is 5.594260215759277\n",
      "(64, 33)\n",
      "step 1626, loss is 5.485697269439697\n",
      "(64, 33)\n",
      "step 1627, loss is 5.485742092132568\n",
      "(64, 33)\n",
      "step 1628, loss is 5.51468563079834\n",
      "(64, 33)\n",
      "step 1629, loss is 5.3853678703308105\n",
      "(64, 33)\n",
      "step 1630, loss is 5.524036884307861\n",
      "(64, 33)\n",
      "step 1631, loss is 5.663432598114014\n",
      "(64, 33)\n",
      "step 1632, loss is 5.404258728027344\n",
      "(64, 33)\n",
      "step 1633, loss is 5.470510959625244\n",
      "(64, 33)\n",
      "step 1634, loss is 5.296122074127197\n",
      "(64, 33)\n",
      "step 1635, loss is 5.515809535980225\n",
      "(64, 33)\n",
      "step 1636, loss is 5.661606788635254\n",
      "(64, 33)\n",
      "step 1637, loss is 5.288604259490967\n",
      "(64, 33)\n",
      "step 1638, loss is 5.3908257484436035\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1639, loss is 5.387126445770264\n",
      "(64, 33)\n",
      "step 1640, loss is 5.5127763748168945\n",
      "(64, 33)\n",
      "step 1641, loss is 5.691143035888672\n",
      "(64, 33)\n",
      "step 1642, loss is 5.2548699378967285\n",
      "(64, 33)\n",
      "step 1643, loss is 5.615933418273926\n",
      "(64, 33)\n",
      "step 1644, loss is 5.5687432289123535\n",
      "(64, 33)\n",
      "step 1645, loss is 5.494499206542969\n",
      "(64, 33)\n",
      "step 1646, loss is 5.5226569175720215\n",
      "(64, 33)\n",
      "step 1647, loss is 5.326516151428223\n",
      "(64, 33)\n",
      "step 1648, loss is 5.4021897315979\n",
      "(64, 33)\n",
      "step 1649, loss is 5.242424964904785\n",
      "(64, 33)\n",
      "step 1650, loss is 5.177421569824219\n",
      "(64, 33)\n",
      "step 1651, loss is 5.6812520027160645\n",
      "(64, 33)\n",
      "step 1652, loss is 5.574972629547119\n",
      "(64, 33)\n",
      "step 1653, loss is 5.402353763580322\n",
      "(64, 33)\n",
      "step 1654, loss is 5.41415548324585\n",
      "(64, 33)\n",
      "step 1655, loss is 5.539464950561523\n",
      "(64, 33)\n",
      "step 1656, loss is 5.344776153564453\n",
      "(64, 33)\n",
      "step 1657, loss is 5.458143711090088\n",
      "(64, 33)\n",
      "step 1658, loss is 5.383467674255371\n",
      "(64, 33)\n",
      "step 1659, loss is 5.4813008308410645\n",
      "(64, 33)\n",
      "step 1660, loss is 5.34995174407959\n",
      "(64, 33)\n",
      "step 1661, loss is 5.233872890472412\n",
      "(64, 33)\n",
      "step 1662, loss is 5.510021209716797\n",
      "(64, 33)\n",
      "step 1663, loss is 5.294969081878662\n",
      "(64, 33)\n",
      "step 1664, loss is 5.352716445922852\n",
      "(64, 33)\n",
      "step 1665, loss is 5.478818416595459\n",
      "(64, 33)\n",
      "step 1666, loss is 5.400031566619873\n",
      "(64, 33)\n",
      "step 1667, loss is 5.377934455871582\n",
      "(64, 33)\n",
      "step 1668, loss is 5.375699043273926\n",
      "(64, 33)\n",
      "step 1669, loss is 5.615877628326416\n",
      "(64, 33)\n",
      "step 1670, loss is 5.33074951171875\n",
      "(64, 33)\n",
      "step 1671, loss is 5.414316177368164\n",
      "(64, 33)\n",
      "step 1672, loss is 5.2447991371154785\n",
      "(64, 33)\n",
      "step 1673, loss is 5.354053020477295\n",
      "(64, 33)\n",
      "step 1674, loss is 5.373687267303467\n",
      "(64, 33)\n",
      "step 1675, loss is 5.310981750488281\n",
      "(64, 33)\n",
      "step 1676, loss is 5.343269348144531\n",
      "(64, 33)\n",
      "step 1677, loss is 5.4270172119140625\n",
      "(64, 33)\n",
      "step 1678, loss is 5.476053237915039\n",
      "(64, 33)\n",
      "step 1679, loss is 5.251116752624512\n",
      "(64, 33)\n",
      "step 1680, loss is 5.448527812957764\n",
      "(64, 33)\n",
      "step 1681, loss is 5.148860931396484\n",
      "(64, 33)\n",
      "step 1682, loss is 5.411258697509766\n",
      "(64, 33)\n",
      "step 1683, loss is 5.437214374542236\n",
      "(64, 33)\n",
      "step 1684, loss is 5.45383358001709\n",
      "(64, 33)\n",
      "step 1685, loss is 5.330604553222656\n",
      "(64, 33)\n",
      "step 1686, loss is 5.421017169952393\n",
      "(64, 33)\n",
      "step 1687, loss is 5.465980529785156\n",
      "(64, 33)\n",
      "step 1688, loss is 5.399210453033447\n",
      "(64, 33)\n",
      "step 1689, loss is 5.444035530090332\n",
      "(64, 33)\n",
      "step 1690, loss is 5.486819744110107\n",
      "(64, 33)\n",
      "step 1691, loss is 5.378962993621826\n",
      "(64, 33)\n",
      "step 1692, loss is 5.5590291023254395\n",
      "(64, 33)\n",
      "step 1693, loss is 5.477852821350098\n",
      "(64, 33)\n",
      "step 1694, loss is 5.514976501464844\n",
      "(64, 33)\n",
      "step 1695, loss is 5.343900680541992\n",
      "(64, 33)\n",
      "step 1696, loss is 5.284681797027588\n",
      "(64, 33)\n",
      "step 1697, loss is 5.471404075622559\n",
      "(64, 33)\n",
      "step 1698, loss is 5.472311496734619\n",
      "(64, 33)\n",
      "step 1699, loss is 5.233249664306641\n",
      "(64, 33)\n",
      "step 1700, loss is 5.2710466384887695\n",
      "(64, 33)\n",
      "step 1701, loss is 5.305850028991699\n",
      "(64, 33)\n",
      "step 1702, loss is 5.501402378082275\n",
      "(64, 33)\n",
      "step 1703, loss is 5.545682907104492\n",
      "(64, 33)\n",
      "step 1704, loss is 5.375074863433838\n",
      "(64, 33)\n",
      "step 1705, loss is 5.328484058380127\n",
      "(64, 33)\n",
      "step 1706, loss is 5.406839370727539\n",
      "(64, 33)\n",
      "step 1707, loss is 5.3445940017700195\n",
      "(64, 33)\n",
      "step 1708, loss is 5.4229631423950195\n",
      "(64, 33)\n",
      "step 1709, loss is 5.36092472076416\n",
      "(64, 33)\n",
      "step 1710, loss is 5.499693393707275\n",
      "(64, 33)\n",
      "step 1711, loss is 5.4463701248168945\n",
      "(64, 33)\n",
      "step 1712, loss is 5.18140172958374\n",
      "(64, 33)\n",
      "step 1713, loss is 5.486215114593506\n",
      "(64, 33)\n",
      "step 1714, loss is 5.22081184387207\n",
      "(64, 33)\n",
      "step 1715, loss is 5.444735050201416\n",
      "(64, 33)\n",
      "step 1716, loss is 5.485769748687744\n",
      "(64, 33)\n",
      "step 1717, loss is 5.434720516204834\n",
      "(64, 33)\n",
      "step 1718, loss is 5.340640068054199\n",
      "(64, 33)\n",
      "step 1719, loss is 5.340792179107666\n",
      "(64, 33)\n",
      "step 1720, loss is 5.385732173919678\n",
      "(64, 33)\n",
      "step 1721, loss is 5.479822158813477\n",
      "(64, 33)\n",
      "step 1722, loss is 5.169403553009033\n",
      "(64, 33)\n",
      "step 1723, loss is 5.311059951782227\n",
      "(64, 33)\n",
      "step 1724, loss is 5.5679240226745605\n",
      "(64, 33)\n",
      "step 1725, loss is 5.442488670349121\n",
      "(64, 33)\n",
      "step 1726, loss is 5.42002534866333\n",
      "(64, 33)\n",
      "step 1727, loss is 5.33718204498291\n",
      "(64, 33)\n",
      "step 1728, loss is 5.218982696533203\n",
      "(64, 33)\n",
      "step 1729, loss is 5.203375339508057\n",
      "(64, 33)\n",
      "step 1730, loss is 5.649999141693115\n",
      "(64, 33)\n",
      "step 1731, loss is 5.346966743469238\n",
      "(64, 33)\n",
      "step 1732, loss is 5.351975917816162\n",
      "(64, 33)\n",
      "step 1733, loss is 5.327722549438477\n",
      "(64, 33)\n",
      "step 1734, loss is 5.43454122543335\n",
      "(64, 33)\n",
      "step 1735, loss is 5.442320823669434\n",
      "(64, 33)\n",
      "step 1736, loss is 5.42372465133667\n",
      "(64, 33)\n",
      "step 1737, loss is 5.46252965927124\n",
      "(64, 33)\n",
      "step 1738, loss is 5.29967737197876\n",
      "(64, 33)\n",
      "step 1739, loss is 5.00929069519043\n",
      "(64, 33)\n",
      "step 1740, loss is 5.3854241371154785\n",
      "(64, 33)\n",
      "step 1741, loss is 5.410949230194092\n",
      "(64, 33)\n",
      "step 1742, loss is 5.482273578643799\n",
      "(64, 33)\n",
      "step 1743, loss is 5.409090042114258\n",
      "(64, 33)\n",
      "step 1744, loss is 5.172428607940674\n",
      "(64, 33)\n",
      "step 1745, loss is 5.483770847320557\n",
      "(64, 33)\n",
      "step 1746, loss is 5.254228591918945\n",
      "(64, 33)\n",
      "step 1747, loss is 5.273269176483154\n",
      "(64, 33)\n",
      "step 1748, loss is 5.377021789550781\n",
      "(64, 33)\n",
      "step 1749, loss is 5.33812952041626\n",
      "(64, 33)\n",
      "step 1750, loss is 5.285562038421631\n",
      "(64, 33)\n",
      "step 1751, loss is 5.236885070800781\n",
      "(64, 33)\n",
      "step 1752, loss is 5.5735368728637695\n",
      "(64, 33)\n",
      "step 1753, loss is 5.382009506225586\n",
      "(64, 33)\n",
      "step 1754, loss is 5.380897521972656\n",
      "(64, 33)\n",
      "step 1755, loss is 5.4285969734191895\n",
      "(64, 33)\n",
      "step 1756, loss is 5.341109275817871\n",
      "(64, 33)\n",
      "step 1757, loss is 5.360810279846191\n",
      "(64, 33)\n",
      "step 1758, loss is 5.500532627105713\n",
      "(64, 33)\n",
      "step 1759, loss is 5.359135150909424\n",
      "(64, 33)\n",
      "step 1760, loss is 5.4352593421936035\n",
      "(64, 33)\n",
      "step 1761, loss is 5.34736967086792\n",
      "(64, 33)\n",
      "step 1762, loss is 5.32376766204834\n",
      "(64, 33)\n",
      "step 1763, loss is 5.19755220413208\n",
      "(64, 33)\n",
      "step 1764, loss is 5.27708625793457\n",
      "(64, 33)\n",
      "step 1765, loss is 5.283462047576904\n",
      "(64, 33)\n",
      "step 1766, loss is 5.2875847816467285\n",
      "(64, 33)\n",
      "step 1767, loss is 5.34408712387085\n",
      "(64, 33)\n",
      "step 1768, loss is 5.405165672302246\n",
      "(64, 33)\n",
      "step 1769, loss is 5.464111328125\n",
      "(64, 33)\n",
      "step 1770, loss is 5.047544956207275\n",
      "(64, 33)\n",
      "step 1771, loss is 5.138942718505859\n",
      "(64, 33)\n",
      "step 1772, loss is 5.345282077789307\n",
      "(64, 33)\n",
      "step 1773, loss is 5.349465370178223\n",
      "(64, 33)\n",
      "step 1774, loss is 5.464999198913574\n",
      "(64, 33)\n",
      "step 1775, loss is 5.238674640655518\n",
      "(64, 33)\n",
      "step 1776, loss is 5.519145965576172\n",
      "(64, 33)\n",
      "step 1777, loss is 5.207597732543945\n",
      "(64, 33)\n",
      "step 1778, loss is 5.134716033935547\n",
      "(64, 33)\n",
      "step 1779, loss is 5.2849602699279785\n",
      "(64, 33)\n",
      "step 1780, loss is 5.350411415100098\n",
      "(64, 33)\n",
      "step 1781, loss is 5.255882263183594\n",
      "(64, 33)\n",
      "step 1782, loss is 5.395496845245361\n",
      "(64, 33)\n",
      "step 1783, loss is 5.3692851066589355\n",
      "(64, 33)\n",
      "step 1784, loss is 5.5399489402771\n",
      "(64, 33)\n",
      "step 1785, loss is 5.341902256011963\n",
      "(64, 33)\n",
      "step 1786, loss is 5.338545799255371\n",
      "(64, 33)\n",
      "step 1787, loss is 5.29763126373291\n",
      "(64, 33)\n",
      "step 1788, loss is 5.213927268981934\n",
      "(64, 33)\n",
      "step 1789, loss is 5.13132905960083\n",
      "(64, 33)\n",
      "step 1790, loss is 5.330018043518066\n",
      "(64, 33)\n",
      "step 1791, loss is 5.1552653312683105\n",
      "(64, 33)\n",
      "step 1792, loss is 5.3243408203125\n",
      "(64, 33)\n",
      "step 1793, loss is 5.371989727020264\n",
      "(64, 33)\n",
      "step 1794, loss is 5.398438453674316\n",
      "(64, 33)\n",
      "step 1795, loss is 5.4228949546813965\n",
      "(64, 33)\n",
      "step 1796, loss is 5.4218010902404785\n",
      "(64, 33)\n",
      "step 1797, loss is 5.265939712524414\n",
      "(64, 33)\n",
      "step 1798, loss is 5.358365535736084\n",
      "(64, 33)\n",
      "step 1799, loss is 5.203882217407227\n",
      "(64, 33)\n",
      "step 1800, loss is 5.4597578048706055\n",
      "(64, 33)\n",
      "step 1801, loss is 5.395666122436523\n",
      "(64, 33)\n",
      "step 1802, loss is 5.210500717163086\n",
      "(64, 33)\n",
      "step 1803, loss is 5.350722789764404\n",
      "(64, 33)\n",
      "step 1804, loss is 5.504297256469727\n",
      "(64, 33)\n",
      "step 1805, loss is 5.428079605102539\n",
      "(64, 33)\n",
      "step 1806, loss is 5.3024001121521\n",
      "(64, 33)\n",
      "step 1807, loss is 5.255666732788086\n",
      "(64, 33)\n",
      "step 1808, loss is 5.282993316650391\n",
      "(64, 33)\n",
      "step 1809, loss is 5.423833847045898\n",
      "(64, 33)\n",
      "step 1810, loss is 5.210338115692139\n",
      "(64, 33)\n",
      "step 1811, loss is 5.219961166381836\n",
      "(64, 33)\n",
      "step 1812, loss is 5.187911510467529\n",
      "(64, 33)\n",
      "step 1813, loss is 5.344871520996094\n",
      "(64, 33)\n",
      "step 1814, loss is 5.617159843444824\n",
      "(64, 33)\n",
      "step 1815, loss is 5.25497579574585\n",
      "(64, 33)\n",
      "step 1816, loss is 5.406562328338623\n",
      "(64, 33)\n",
      "step 1817, loss is 5.1736931800842285\n",
      "(64, 33)\n",
      "step 1818, loss is 5.296012878417969\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1819, loss is 5.247887134552002\n",
      "(64, 33)\n",
      "step 1820, loss is 5.15178918838501\n",
      "(64, 33)\n",
      "step 1821, loss is 5.216978073120117\n",
      "(64, 33)\n",
      "step 1822, loss is 5.337978363037109\n",
      "(64, 33)\n",
      "step 1823, loss is 5.338783264160156\n",
      "(64, 33)\n",
      "step 1824, loss is 5.442182540893555\n",
      "(64, 33)\n",
      "step 1825, loss is 5.2428297996521\n",
      "(64, 33)\n",
      "step 1826, loss is 5.319067001342773\n",
      "(64, 33)\n",
      "step 1827, loss is 5.2952799797058105\n",
      "(64, 33)\n",
      "step 1828, loss is 5.20261287689209\n",
      "(64, 33)\n",
      "step 1829, loss is 5.194371223449707\n",
      "(64, 33)\n",
      "step 1830, loss is 5.205880641937256\n",
      "(64, 33)\n",
      "step 1831, loss is 5.217259407043457\n",
      "(64, 33)\n",
      "step 1832, loss is 5.4086737632751465\n",
      "(64, 33)\n",
      "step 1833, loss is 5.332881927490234\n",
      "(64, 33)\n",
      "step 1834, loss is 5.1708292961120605\n",
      "(64, 33)\n",
      "step 1835, loss is 5.409220218658447\n",
      "(64, 33)\n",
      "step 1836, loss is 5.178466796875\n",
      "(64, 33)\n",
      "step 1837, loss is 5.197981357574463\n",
      "(64, 33)\n",
      "step 1838, loss is 5.3658552169799805\n",
      "(64, 33)\n",
      "step 1839, loss is 5.254124641418457\n",
      "(64, 33)\n",
      "step 1840, loss is 5.3159098625183105\n",
      "(64, 33)\n",
      "step 1841, loss is 5.371191024780273\n",
      "(64, 33)\n",
      "step 1842, loss is 5.341773986816406\n",
      "(64, 33)\n",
      "step 1843, loss is 5.388425350189209\n",
      "(64, 33)\n",
      "step 1844, loss is 5.287412166595459\n",
      "(64, 33)\n",
      "step 1845, loss is 5.126060485839844\n",
      "(64, 33)\n",
      "step 1846, loss is 5.419365882873535\n",
      "(64, 33)\n",
      "step 1847, loss is 5.251036167144775\n",
      "(64, 33)\n",
      "step 1848, loss is 5.3128275871276855\n",
      "(64, 33)\n",
      "step 1849, loss is 5.184446811676025\n",
      "(64, 33)\n",
      "step 1850, loss is 5.401924133300781\n",
      "(64, 33)\n",
      "step 1851, loss is 5.25059700012207\n",
      "(64, 33)\n",
      "step 1852, loss is 5.386308193206787\n",
      "(64, 33)\n",
      "step 1853, loss is 5.307360649108887\n",
      "(64, 33)\n",
      "step 1854, loss is 5.457422256469727\n",
      "(64, 33)\n",
      "step 1855, loss is 5.176150798797607\n",
      "(64, 33)\n",
      "step 1856, loss is 5.360361576080322\n",
      "(64, 33)\n",
      "step 1857, loss is 5.332146167755127\n",
      "(64, 33)\n",
      "step 1858, loss is 5.185975074768066\n",
      "(64, 33)\n",
      "step 1859, loss is 5.362795352935791\n",
      "(64, 33)\n",
      "step 1860, loss is 5.552894592285156\n",
      "(64, 33)\n",
      "step 1861, loss is 5.276675701141357\n",
      "(64, 33)\n",
      "step 1862, loss is 5.19976282119751\n",
      "(64, 33)\n",
      "step 1863, loss is 5.219141960144043\n",
      "(64, 33)\n",
      "step 1864, loss is 5.291733264923096\n",
      "(64, 33)\n",
      "step 1865, loss is 5.230775833129883\n",
      "(64, 33)\n",
      "step 1866, loss is 5.094353675842285\n",
      "(64, 33)\n",
      "step 1867, loss is 5.376434803009033\n",
      "(64, 33)\n",
      "step 1868, loss is 5.141666412353516\n",
      "(64, 33)\n",
      "step 1869, loss is 5.348883152008057\n",
      "(64, 33)\n",
      "step 1870, loss is 5.350035667419434\n",
      "(64, 33)\n",
      "step 1871, loss is 5.26644229888916\n",
      "(64, 33)\n",
      "step 1872, loss is 5.2619099617004395\n",
      "(64, 33)\n",
      "step 1873, loss is 5.304376602172852\n",
      "(64, 33)\n",
      "step 1874, loss is 5.2266411781311035\n",
      "(64, 33)\n",
      "step 1875, loss is 5.235912799835205\n",
      "(64, 33)\n",
      "step 1876, loss is 5.290268421173096\n",
      "(64, 33)\n",
      "step 1877, loss is 5.477297782897949\n",
      "(64, 33)\n",
      "step 1878, loss is 5.394680023193359\n",
      "(64, 33)\n",
      "step 1879, loss is 5.046152114868164\n",
      "(64, 33)\n",
      "step 1880, loss is 5.329090595245361\n",
      "(64, 33)\n",
      "step 1881, loss is 5.435303688049316\n",
      "(64, 33)\n",
      "step 1882, loss is 5.216803550720215\n",
      "(64, 33)\n",
      "step 1883, loss is 5.225173473358154\n",
      "(64, 33)\n",
      "step 1884, loss is 5.369499206542969\n",
      "(64, 33)\n",
      "step 1885, loss is 5.381817817687988\n",
      "(64, 33)\n",
      "step 1886, loss is 5.373923301696777\n",
      "(64, 33)\n",
      "step 1887, loss is 5.499352931976318\n",
      "(64, 33)\n",
      "step 1888, loss is 5.454896450042725\n",
      "(64, 33)\n",
      "step 1889, loss is 5.357655048370361\n",
      "(64, 33)\n",
      "step 1890, loss is 5.310428142547607\n",
      "(64, 33)\n",
      "step 1891, loss is 5.3043622970581055\n",
      "(64, 33)\n",
      "step 1892, loss is 5.294572830200195\n",
      "(64, 33)\n",
      "step 1893, loss is 5.21688175201416\n",
      "(64, 33)\n",
      "step 1894, loss is 5.415053844451904\n",
      "(64, 33)\n",
      "step 1895, loss is 5.280044078826904\n",
      "(64, 33)\n",
      "step 1896, loss is 5.277066230773926\n",
      "(64, 33)\n",
      "step 1897, loss is 5.4308552742004395\n",
      "(64, 33)\n",
      "step 1898, loss is 5.110297679901123\n",
      "(64, 33)\n",
      "step 1899, loss is 5.365132808685303\n",
      "(64, 33)\n",
      "step 1900, loss is 5.2397260665893555\n",
      "(64, 33)\n",
      "step 1901, loss is 5.259624004364014\n",
      "(64, 33)\n",
      "step 1902, loss is 5.212028503417969\n",
      "(64, 33)\n",
      "step 1903, loss is 5.169552803039551\n",
      "(64, 33)\n",
      "step 1904, loss is 5.286890983581543\n",
      "(64, 33)\n",
      "step 1905, loss is 5.213327884674072\n",
      "(64, 33)\n",
      "step 1906, loss is 5.3868632316589355\n",
      "(64, 33)\n",
      "step 1907, loss is 5.442145824432373\n",
      "(64, 33)\n",
      "step 1908, loss is 5.170663833618164\n",
      "(64, 33)\n",
      "step 1909, loss is 5.2758684158325195\n",
      "(64, 33)\n",
      "step 1910, loss is 5.51442289352417\n",
      "(64, 33)\n",
      "step 1911, loss is 5.247823715209961\n",
      "(64, 33)\n",
      "step 1912, loss is 5.295592308044434\n",
      "(64, 33)\n",
      "step 1913, loss is 5.126460075378418\n",
      "(64, 33)\n",
      "step 1914, loss is 5.12907075881958\n",
      "(64, 33)\n",
      "step 1915, loss is 5.177249431610107\n",
      "(64, 33)\n",
      "step 1916, loss is 5.205781936645508\n",
      "(64, 33)\n",
      "step 1917, loss is 5.384104251861572\n",
      "(64, 33)\n",
      "step 1918, loss is 5.233105182647705\n",
      "(64, 33)\n",
      "step 1919, loss is 5.3704142570495605\n",
      "(64, 33)\n",
      "step 1920, loss is 5.1542582511901855\n",
      "(64, 33)\n",
      "step 1921, loss is 5.428152084350586\n",
      "(64, 33)\n",
      "step 1922, loss is 5.408870697021484\n",
      "(64, 33)\n",
      "step 1923, loss is 5.459327697753906\n",
      "(64, 33)\n",
      "step 1924, loss is 5.201336860656738\n",
      "(64, 33)\n",
      "step 1925, loss is 5.321945667266846\n",
      "(64, 33)\n",
      "step 1926, loss is 5.291747570037842\n",
      "(64, 33)\n",
      "step 1927, loss is 5.342561721801758\n",
      "(64, 33)\n",
      "step 1928, loss is 5.272433757781982\n",
      "(64, 33)\n",
      "step 1929, loss is 5.231276035308838\n",
      "(64, 33)\n",
      "step 1930, loss is 5.253102779388428\n",
      "(64, 33)\n",
      "step 1931, loss is 5.222018241882324\n",
      "(64, 33)\n",
      "step 1932, loss is 5.334485054016113\n",
      "(64, 33)\n",
      "step 1933, loss is 5.2512688636779785\n",
      "(64, 33)\n",
      "step 1934, loss is 5.271999359130859\n",
      "(64, 33)\n",
      "step 1935, loss is 5.275271415710449\n",
      "(64, 33)\n",
      "step 1936, loss is 5.231879234313965\n",
      "(64, 33)\n",
      "step 1937, loss is 5.219184875488281\n",
      "(64, 33)\n",
      "step 1938, loss is 5.121659755706787\n",
      "(64, 33)\n",
      "step 1939, loss is 5.298530101776123\n",
      "(64, 33)\n",
      "step 1940, loss is 5.297695159912109\n",
      "(64, 33)\n",
      "step 1941, loss is 5.255402565002441\n",
      "(64, 33)\n",
      "step 1942, loss is 5.109967231750488\n",
      "(64, 33)\n",
      "step 1943, loss is 5.113799571990967\n",
      "(64, 33)\n",
      "step 1944, loss is 5.257687091827393\n",
      "(64, 33)\n",
      "step 1945, loss is 5.2961273193359375\n",
      "(64, 33)\n",
      "step 1946, loss is 5.354955673217773\n",
      "(64, 33)\n",
      "step 1947, loss is 5.071902751922607\n",
      "(64, 33)\n",
      "step 1948, loss is 5.254708766937256\n",
      "(64, 33)\n",
      "step 1949, loss is 5.401429176330566\n",
      "(64, 33)\n",
      "step 1950, loss is 5.164815425872803\n",
      "(64, 33)\n",
      "step 1951, loss is 5.592935085296631\n",
      "(64, 33)\n",
      "step 1952, loss is 5.184842109680176\n",
      "(64, 33)\n",
      "step 1953, loss is 5.117849826812744\n",
      "(64, 33)\n",
      "step 1954, loss is 5.329475402832031\n",
      "(64, 33)\n",
      "step 1955, loss is 5.058222770690918\n",
      "(64, 33)\n",
      "step 1956, loss is 5.575733661651611\n",
      "(64, 33)\n",
      "step 1957, loss is 5.276494026184082\n",
      "(64, 33)\n",
      "step 1958, loss is 5.287694931030273\n",
      "(64, 33)\n",
      "step 1959, loss is 5.463637828826904\n",
      "(64, 33)\n",
      "step 1960, loss is 5.4902143478393555\n",
      "(64, 33)\n",
      "step 1961, loss is 5.182283401489258\n",
      "(64, 33)\n",
      "step 1962, loss is 5.301233768463135\n",
      "(64, 33)\n",
      "step 1963, loss is 5.336616516113281\n",
      "(64, 33)\n",
      "step 1964, loss is 4.931399822235107\n",
      "(64, 33)\n",
      "step 1965, loss is 5.4808197021484375\n",
      "(64, 33)\n",
      "step 1966, loss is 5.235782623291016\n",
      "(64, 33)\n",
      "step 1967, loss is 5.49424409866333\n",
      "(64, 33)\n",
      "step 1968, loss is 5.342448711395264\n",
      "(64, 33)\n",
      "step 1969, loss is 5.1005473136901855\n",
      "(64, 33)\n",
      "step 1970, loss is 5.215718746185303\n",
      "(64, 33)\n",
      "step 1971, loss is 5.208043098449707\n",
      "(64, 33)\n",
      "step 1972, loss is 5.2507452964782715\n",
      "(64, 33)\n",
      "step 1973, loss is 5.194850921630859\n",
      "(64, 33)\n",
      "step 1974, loss is 5.478426933288574\n",
      "(64, 33)\n",
      "step 1975, loss is 5.0401740074157715\n",
      "(64, 33)\n",
      "step 1976, loss is 5.355581283569336\n",
      "(64, 33)\n",
      "step 1977, loss is 5.346321105957031\n",
      "(64, 33)\n",
      "step 1978, loss is 5.202151298522949\n",
      "(64, 33)\n",
      "step 1979, loss is 5.061640739440918\n",
      "(64, 33)\n",
      "step 1980, loss is 5.403444290161133\n",
      "(64, 33)\n",
      "step 1981, loss is 5.230173587799072\n",
      "(64, 33)\n",
      "step 1982, loss is 4.993897438049316\n",
      "(64, 33)\n",
      "step 1983, loss is 5.054995536804199\n",
      "(64, 33)\n",
      "step 1984, loss is 5.111554145812988\n",
      "(64, 33)\n",
      "step 1985, loss is 5.225553035736084\n",
      "(64, 33)\n",
      "step 1986, loss is 5.226352691650391\n",
      "(64, 33)\n",
      "step 1987, loss is 5.179432392120361\n",
      "(64, 33)\n",
      "step 1988, loss is 5.358453273773193\n",
      "(64, 33)\n",
      "step 1989, loss is 5.249543190002441\n",
      "(64, 33)\n",
      "step 1990, loss is 5.369531154632568\n",
      "(64, 33)\n",
      "step 1991, loss is 5.293307304382324\n",
      "(64, 33)\n",
      "step 1992, loss is 5.383118152618408\n",
      "(64, 33)\n",
      "step 1993, loss is 5.327918529510498\n",
      "(64, 33)\n",
      "step 1994, loss is 5.154422283172607\n",
      "(64, 33)\n",
      "step 1995, loss is 5.150567054748535\n",
      "(64, 33)\n",
      "step 1996, loss is 5.071569442749023\n",
      "(64, 33)\n",
      "step 1997, loss is 5.29189395904541\n",
      "(64, 33)\n",
      "step 1998, loss is 5.13298225402832\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1999, loss is 5.14487361907959\n",
      "(64, 33)\n",
      "step 2000, loss is 5.218346118927002\n",
      "(64, 33)\n",
      "step 2001, loss is 5.558961868286133\n",
      "(64, 33)\n",
      "step 2002, loss is 5.259971618652344\n",
      "(64, 33)\n",
      "step 2003, loss is 5.355580806732178\n",
      "(64, 33)\n",
      "step 2004, loss is 5.245382308959961\n",
      "(64, 33)\n",
      "step 2005, loss is 5.043735980987549\n",
      "(64, 33)\n",
      "step 2006, loss is 5.262947082519531\n",
      "(64, 33)\n",
      "step 2007, loss is 5.321941375732422\n",
      "(64, 33)\n",
      "step 2008, loss is 5.0738654136657715\n",
      "(64, 33)\n",
      "step 2009, loss is 5.307194232940674\n",
      "(64, 33)\n",
      "step 2010, loss is 5.246306419372559\n",
      "(64, 33)\n",
      "step 2011, loss is 5.255494594573975\n",
      "(64, 33)\n",
      "step 2012, loss is 5.215146064758301\n",
      "(64, 33)\n",
      "step 2013, loss is 5.282367706298828\n",
      "(64, 33)\n",
      "step 2014, loss is 5.255999565124512\n",
      "(64, 33)\n",
      "step 2015, loss is 5.261770725250244\n",
      "(64, 33)\n",
      "step 2016, loss is 5.494812965393066\n",
      "(64, 33)\n",
      "step 2017, loss is 5.185601234436035\n",
      "(64, 33)\n",
      "step 2018, loss is 5.267342567443848\n",
      "(64, 33)\n",
      "step 2019, loss is 5.1913838386535645\n",
      "(64, 33)\n",
      "step 2020, loss is 5.051438808441162\n",
      "(64, 33)\n",
      "step 2021, loss is 5.272552013397217\n",
      "(64, 33)\n",
      "step 2022, loss is 5.230316638946533\n",
      "(64, 33)\n",
      "step 2023, loss is 5.096350193023682\n",
      "(64, 33)\n",
      "step 2024, loss is 5.314847469329834\n",
      "(64, 33)\n",
      "step 2025, loss is 5.037481307983398\n",
      "(64, 33)\n",
      "step 2026, loss is 5.258385181427002\n",
      "(64, 33)\n",
      "step 2027, loss is 5.079959869384766\n",
      "(64, 33)\n",
      "step 2028, loss is 5.434921741485596\n",
      "(64, 33)\n",
      "step 2029, loss is 5.21608829498291\n",
      "(64, 33)\n",
      "step 2030, loss is 5.304994583129883\n",
      "(64, 33)\n",
      "step 2031, loss is 5.114906311035156\n",
      "(64, 33)\n",
      "step 2032, loss is 5.296698570251465\n",
      "(64, 33)\n",
      "step 2033, loss is 5.3572540283203125\n",
      "(64, 33)\n",
      "step 2034, loss is 5.373615741729736\n",
      "(64, 33)\n",
      "step 2035, loss is 5.266556262969971\n",
      "(64, 33)\n",
      "step 2036, loss is 5.3423919677734375\n",
      "(64, 33)\n",
      "step 2037, loss is 5.149575233459473\n",
      "(64, 33)\n",
      "step 2038, loss is 5.2730712890625\n",
      "(64, 33)\n",
      "step 2039, loss is 5.350954532623291\n",
      "(64, 33)\n",
      "step 2040, loss is 5.280802249908447\n",
      "(64, 33)\n",
      "step 2041, loss is 5.365963459014893\n",
      "(64, 33)\n",
      "step 2042, loss is 5.115141868591309\n",
      "(64, 33)\n",
      "step 2043, loss is 5.3650970458984375\n",
      "(64, 33)\n",
      "step 2044, loss is 5.2184834480285645\n",
      "(64, 33)\n",
      "step 2045, loss is 5.166961193084717\n",
      "(64, 33)\n",
      "step 2046, loss is 5.2360053062438965\n",
      "(64, 33)\n",
      "step 2047, loss is 5.035201072692871\n",
      "(64, 33)\n",
      "step 2048, loss is 5.108206272125244\n",
      "(64, 33)\n",
      "step 2049, loss is 5.178372859954834\n",
      "(64, 33)\n",
      "step 2050, loss is 5.1595683097839355\n",
      "(64, 33)\n",
      "step 2051, loss is 5.354667663574219\n",
      "(64, 33)\n",
      "step 2052, loss is 5.1770806312561035\n",
      "(64, 33)\n",
      "step 2053, loss is 5.151365280151367\n",
      "(64, 33)\n",
      "step 2054, loss is 5.076243877410889\n",
      "(64, 33)\n",
      "step 2055, loss is 5.191572189331055\n",
      "(64, 33)\n",
      "step 2056, loss is 5.3601837158203125\n",
      "(64, 33)\n",
      "step 2057, loss is 5.1679205894470215\n",
      "(64, 33)\n",
      "step 2058, loss is 5.148702144622803\n",
      "(64, 33)\n",
      "step 2059, loss is 5.118075370788574\n",
      "(64, 33)\n",
      "step 2060, loss is 5.048689365386963\n",
      "(64, 33)\n",
      "step 2061, loss is 5.361929416656494\n",
      "(64, 33)\n",
      "step 2062, loss is 5.220252990722656\n",
      "(64, 33)\n",
      "step 2063, loss is 5.533870697021484\n",
      "(64, 33)\n",
      "step 2064, loss is 5.2013373374938965\n",
      "(64, 33)\n",
      "step 2065, loss is 5.338894844055176\n",
      "(64, 33)\n",
      "step 2066, loss is 5.152315139770508\n",
      "(64, 33)\n",
      "step 2067, loss is 5.154144763946533\n",
      "(64, 33)\n",
      "step 2068, loss is 5.127914905548096\n",
      "(64, 33)\n",
      "step 2069, loss is 5.171576976776123\n",
      "(64, 33)\n",
      "step 2070, loss is 5.418447494506836\n",
      "(64, 33)\n",
      "step 2071, loss is 5.274170398712158\n",
      "(64, 33)\n",
      "step 2072, loss is 5.044098854064941\n",
      "(64, 33)\n",
      "step 2073, loss is 5.232724189758301\n",
      "(64, 33)\n",
      "step 2074, loss is 5.1219563484191895\n",
      "(64, 33)\n",
      "step 2075, loss is 5.235715389251709\n",
      "(64, 33)\n",
      "step 2076, loss is 5.320351600646973\n",
      "(64, 33)\n",
      "step 2077, loss is 5.2509074211120605\n",
      "(64, 33)\n",
      "step 2078, loss is 5.262585163116455\n",
      "(64, 33)\n",
      "step 2079, loss is 5.3157267570495605\n",
      "(64, 33)\n",
      "step 2080, loss is 5.181807041168213\n",
      "(64, 33)\n",
      "step 2081, loss is 5.3087358474731445\n",
      "(64, 33)\n",
      "step 2082, loss is 5.148414611816406\n",
      "(64, 33)\n",
      "step 2083, loss is 5.258337020874023\n",
      "(64, 33)\n",
      "step 2084, loss is 5.449821472167969\n",
      "(64, 33)\n",
      "step 2085, loss is 5.0938239097595215\n",
      "(64, 33)\n",
      "step 2086, loss is 5.211555480957031\n",
      "(64, 33)\n",
      "step 2087, loss is 5.068968772888184\n",
      "(64, 33)\n",
      "step 2088, loss is 5.223237037658691\n",
      "(64, 33)\n",
      "step 2089, loss is 5.423451900482178\n",
      "(64, 33)\n",
      "step 2090, loss is 5.396637916564941\n",
      "(64, 33)\n",
      "step 2091, loss is 5.03026008605957\n",
      "(64, 33)\n",
      "step 2092, loss is 5.115400314331055\n",
      "(64, 33)\n",
      "step 2093, loss is 5.3083600997924805\n",
      "(64, 33)\n",
      "step 2094, loss is 5.304373264312744\n",
      "(64, 33)\n",
      "step 2095, loss is 4.900701999664307\n",
      "(64, 33)\n",
      "step 2096, loss is 5.300775051116943\n",
      "(64, 33)\n",
      "step 2097, loss is 5.234862804412842\n",
      "(64, 33)\n",
      "step 2098, loss is 5.07805871963501\n",
      "(64, 33)\n",
      "step 2099, loss is 5.1715898513793945\n",
      "(64, 33)\n",
      "step 2100, loss is 5.138220310211182\n",
      "(64, 33)\n",
      "step 2101, loss is 5.3072509765625\n",
      "(64, 33)\n",
      "step 2102, loss is 5.399566173553467\n",
      "(64, 33)\n",
      "step 2103, loss is 5.165367126464844\n",
      "(64, 33)\n",
      "step 2104, loss is 5.03366231918335\n",
      "(64, 33)\n",
      "step 2105, loss is 5.174469947814941\n",
      "(64, 33)\n",
      "step 2106, loss is 5.221235275268555\n",
      "(64, 33)\n",
      "step 2107, loss is 5.354216575622559\n",
      "(64, 33)\n",
      "step 2108, loss is 5.131003379821777\n",
      "(64, 33)\n",
      "step 2109, loss is 5.193924427032471\n",
      "(64, 33)\n",
      "step 2110, loss is 5.14945650100708\n",
      "(64, 33)\n",
      "step 2111, loss is 5.33339262008667\n",
      "(64, 33)\n",
      "step 2112, loss is 5.115720748901367\n",
      "(64, 33)\n",
      "step 2113, loss is 5.195631504058838\n",
      "(64, 33)\n",
      "step 2114, loss is 5.319052696228027\n",
      "(64, 33)\n",
      "step 2115, loss is 5.2192816734313965\n",
      "(64, 33)\n",
      "step 2116, loss is 5.192019462585449\n",
      "(64, 33)\n",
      "step 2117, loss is 5.214139461517334\n",
      "(64, 33)\n",
      "step 2118, loss is 5.261756420135498\n",
      "(64, 33)\n",
      "step 2119, loss is 5.058712959289551\n",
      "(64, 33)\n",
      "step 2120, loss is 5.290821552276611\n",
      "(64, 33)\n",
      "step 2121, loss is 5.243823528289795\n",
      "(64, 33)\n",
      "step 2122, loss is 5.42185115814209\n",
      "(64, 33)\n",
      "step 2123, loss is 5.106955528259277\n",
      "(64, 33)\n",
      "step 2124, loss is 5.362675666809082\n",
      "(64, 33)\n",
      "step 2125, loss is 5.158149719238281\n",
      "(64, 33)\n",
      "step 2126, loss is 5.337553024291992\n",
      "(64, 33)\n",
      "step 2127, loss is 5.172731399536133\n",
      "(64, 33)\n",
      "step 2128, loss is 5.222132682800293\n",
      "(64, 33)\n",
      "step 2129, loss is 5.134006023406982\n",
      "(64, 33)\n",
      "step 2130, loss is 5.174923896789551\n",
      "(64, 33)\n",
      "step 2131, loss is 5.013208866119385\n",
      "(64, 33)\n",
      "step 2132, loss is 5.036020755767822\n",
      "(64, 33)\n",
      "step 2133, loss is 5.4383931159973145\n",
      "(64, 33)\n",
      "step 2134, loss is 5.244330883026123\n",
      "(64, 33)\n",
      "step 2135, loss is 5.176089763641357\n",
      "(64, 33)\n",
      "step 2136, loss is 5.389992713928223\n",
      "(64, 33)\n",
      "step 2137, loss is 5.212979316711426\n",
      "(64, 33)\n",
      "step 2138, loss is 5.202202320098877\n",
      "(64, 33)\n",
      "step 2139, loss is 5.065347194671631\n",
      "(64, 33)\n",
      "step 2140, loss is 5.157793045043945\n",
      "(64, 33)\n",
      "step 2141, loss is 5.123663425445557\n",
      "(64, 33)\n",
      "step 2142, loss is 5.23551607131958\n",
      "(64, 33)\n",
      "step 2143, loss is 5.234274387359619\n",
      "(64, 33)\n",
      "step 2144, loss is 5.3892011642456055\n",
      "(64, 33)\n",
      "step 2145, loss is 5.098899841308594\n",
      "(64, 33)\n",
      "step 2146, loss is 5.167481422424316\n",
      "(64, 33)\n",
      "step 2147, loss is 5.2153801918029785\n",
      "(64, 33)\n",
      "step 2148, loss is 5.074212551116943\n",
      "(64, 33)\n",
      "step 2149, loss is 5.323066711425781\n",
      "(64, 33)\n",
      "step 2150, loss is 5.2805986404418945\n",
      "(64, 33)\n",
      "step 2151, loss is 5.202343463897705\n",
      "(64, 33)\n",
      "step 2152, loss is 5.290695667266846\n",
      "(64, 33)\n",
      "step 2153, loss is 5.155467510223389\n",
      "(64, 33)\n",
      "step 2154, loss is 5.171473026275635\n",
      "(64, 33)\n",
      "step 2155, loss is 5.139005184173584\n",
      "(64, 33)\n",
      "step 2156, loss is 5.20492696762085\n",
      "(64, 33)\n",
      "step 2157, loss is 5.209207534790039\n",
      "(64, 33)\n",
      "step 2158, loss is 5.071954250335693\n",
      "(64, 33)\n",
      "step 2159, loss is 5.199522018432617\n",
      "(64, 33)\n",
      "step 2160, loss is 5.1583757400512695\n",
      "(64, 33)\n",
      "step 2161, loss is 5.155016899108887\n",
      "(64, 33)\n",
      "step 2162, loss is 5.147440433502197\n",
      "(64, 33)\n",
      "step 2163, loss is 5.200376033782959\n",
      "(64, 33)\n",
      "step 2164, loss is 5.2191596031188965\n",
      "(64, 33)\n",
      "step 2165, loss is 5.211668968200684\n",
      "(64, 33)\n",
      "step 2166, loss is 5.139902114868164\n",
      "(64, 33)\n",
      "step 2167, loss is 5.292102813720703\n",
      "(64, 33)\n",
      "step 2168, loss is 5.383825302124023\n",
      "(64, 33)\n",
      "step 2169, loss is 5.114729404449463\n",
      "(64, 33)\n",
      "step 2170, loss is 5.2354350090026855\n",
      "(64, 33)\n",
      "step 2171, loss is 4.97011661529541\n",
      "(64, 33)\n",
      "step 2172, loss is 5.410259246826172\n",
      "(64, 33)\n",
      "step 2173, loss is 5.171658515930176\n",
      "(64, 33)\n",
      "step 2174, loss is 5.103714466094971\n",
      "(64, 33)\n",
      "step 2175, loss is 5.247347354888916\n",
      "(64, 33)\n",
      "step 2176, loss is 5.173995494842529\n",
      "(64, 33)\n",
      "step 2177, loss is 5.215912342071533\n",
      "(64, 33)\n",
      "step 2178, loss is 5.390471458435059\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2179, loss is 5.161125183105469\n",
      "(64, 33)\n",
      "step 2180, loss is 5.271221160888672\n",
      "(64, 33)\n",
      "step 2181, loss is 5.312852382659912\n",
      "(64, 33)\n",
      "step 2182, loss is 5.131555557250977\n",
      "(64, 33)\n",
      "step 2183, loss is 5.20305061340332\n",
      "(64, 33)\n",
      "step 2184, loss is 5.174734592437744\n",
      "(64, 33)\n",
      "step 2185, loss is 5.274415493011475\n",
      "(64, 33)\n",
      "step 2186, loss is 5.154337406158447\n",
      "(64, 33)\n",
      "step 2187, loss is 4.923735618591309\n",
      "(64, 33)\n",
      "step 2188, loss is 5.25493049621582\n",
      "(64, 33)\n",
      "step 2189, loss is 5.185023307800293\n",
      "(64, 33)\n",
      "step 2190, loss is 5.399316310882568\n",
      "(64, 33)\n",
      "step 2191, loss is 5.2914557456970215\n",
      "(64, 33)\n",
      "step 2192, loss is 5.245028972625732\n",
      "(64, 33)\n",
      "step 2193, loss is 5.213831424713135\n",
      "(64, 33)\n",
      "step 2194, loss is 5.190288066864014\n",
      "(64, 33)\n",
      "step 2195, loss is 5.142498970031738\n",
      "(64, 33)\n",
      "step 2196, loss is 5.233530521392822\n",
      "(64, 33)\n",
      "step 2197, loss is 5.214533805847168\n",
      "(64, 33)\n",
      "step 2198, loss is 5.314021110534668\n",
      "(64, 33)\n",
      "step 2199, loss is 5.15926456451416\n",
      "(64, 33)\n",
      "step 2200, loss is 5.177774906158447\n",
      "(64, 33)\n",
      "step 2201, loss is 5.111212253570557\n",
      "(64, 33)\n",
      "step 2202, loss is 5.276296138763428\n",
      "(64, 33)\n",
      "step 2203, loss is 5.026081562042236\n",
      "(64, 33)\n",
      "step 2204, loss is 5.328314781188965\n",
      "(64, 33)\n",
      "step 2205, loss is 5.1963372230529785\n",
      "(64, 33)\n",
      "step 2206, loss is 5.119232654571533\n",
      "(64, 33)\n",
      "step 2207, loss is 5.4399871826171875\n",
      "(64, 33)\n",
      "step 2208, loss is 5.161558151245117\n",
      "(64, 33)\n",
      "step 2209, loss is 5.350740909576416\n",
      "(64, 33)\n",
      "step 2210, loss is 5.275692462921143\n",
      "(64, 33)\n",
      "step 2211, loss is 5.376502990722656\n",
      "(64, 33)\n",
      "step 2212, loss is 5.068801403045654\n",
      "(64, 33)\n",
      "step 2213, loss is 5.396294593811035\n",
      "(64, 33)\n",
      "step 2214, loss is 5.063319206237793\n",
      "(64, 33)\n",
      "step 2215, loss is 5.12302303314209\n",
      "(64, 33)\n",
      "step 2216, loss is 5.184520721435547\n",
      "(64, 33)\n",
      "step 2217, loss is 5.168715000152588\n",
      "(64, 33)\n",
      "step 2218, loss is 5.50072717666626\n",
      "(64, 33)\n",
      "step 2219, loss is 5.393892288208008\n",
      "(64, 33)\n",
      "step 2220, loss is 5.358058929443359\n",
      "(64, 33)\n",
      "step 2221, loss is 5.220717430114746\n",
      "(64, 33)\n",
      "step 2222, loss is 5.3041558265686035\n",
      "(64, 33)\n",
      "step 2223, loss is 5.240477561950684\n",
      "(64, 33)\n",
      "step 2224, loss is 4.983465671539307\n",
      "(64, 33)\n",
      "step 2225, loss is 5.04453706741333\n",
      "(64, 33)\n",
      "step 2226, loss is 4.898343563079834\n",
      "(64, 33)\n",
      "step 2227, loss is 5.346044063568115\n",
      "(64, 33)\n",
      "step 2228, loss is 5.240384101867676\n",
      "(64, 33)\n",
      "step 2229, loss is 5.243556499481201\n",
      "(64, 33)\n",
      "step 2230, loss is 5.345216751098633\n",
      "(64, 33)\n",
      "step 2231, loss is 5.294591426849365\n",
      "(64, 33)\n",
      "step 2232, loss is 5.139613151550293\n",
      "(64, 33)\n",
      "step 2233, loss is 5.068324089050293\n",
      "(64, 33)\n",
      "step 2234, loss is 5.178796768188477\n",
      "(64, 33)\n",
      "step 2235, loss is 5.060675144195557\n",
      "(64, 33)\n",
      "step 2236, loss is 5.3065314292907715\n",
      "(64, 33)\n",
      "step 2237, loss is 5.323358058929443\n",
      "(64, 33)\n",
      "step 2238, loss is 5.0810089111328125\n",
      "(64, 33)\n",
      "step 2239, loss is 5.17042875289917\n",
      "(64, 33)\n",
      "step 2240, loss is 5.18181037902832\n",
      "(64, 33)\n",
      "step 2241, loss is 5.144257068634033\n",
      "(64, 33)\n",
      "step 2242, loss is 5.214187145233154\n",
      "(64, 33)\n",
      "step 2243, loss is 5.388359546661377\n",
      "(64, 33)\n",
      "step 2244, loss is 5.182437896728516\n",
      "(64, 33)\n",
      "step 2245, loss is 5.23496675491333\n",
      "(64, 33)\n",
      "step 2246, loss is 5.224055290222168\n",
      "(64, 33)\n",
      "step 2247, loss is 5.1438374519348145\n",
      "(64, 33)\n",
      "step 2248, loss is 5.2946085929870605\n",
      "(64, 33)\n",
      "step 2249, loss is 5.297977924346924\n",
      "(64, 33)\n",
      "step 2250, loss is 5.124673366546631\n",
      "(64, 33)\n",
      "step 2251, loss is 5.160239219665527\n",
      "(64, 33)\n",
      "step 2252, loss is 5.3058600425720215\n",
      "(64, 33)\n",
      "step 2253, loss is 5.125802516937256\n",
      "(64, 33)\n",
      "step 2254, loss is 5.27788782119751\n",
      "(64, 33)\n",
      "step 2255, loss is 5.2494425773620605\n",
      "(64, 33)\n",
      "step 2256, loss is 5.188981533050537\n",
      "(64, 33)\n",
      "step 2257, loss is 5.160664081573486\n",
      "(64, 33)\n",
      "step 2258, loss is 5.060364246368408\n",
      "(64, 33)\n",
      "step 2259, loss is 5.1282830238342285\n",
      "(64, 33)\n",
      "step 2260, loss is 5.421852111816406\n",
      "(64, 33)\n",
      "step 2261, loss is 5.275385856628418\n",
      "(64, 33)\n",
      "step 2262, loss is 5.234586715698242\n",
      "(64, 33)\n",
      "step 2263, loss is 5.3246049880981445\n",
      "(64, 33)\n",
      "step 2264, loss is 5.344903469085693\n",
      "(64, 33)\n",
      "step 2265, loss is 5.207973003387451\n",
      "(64, 33)\n",
      "step 2266, loss is 5.022495746612549\n",
      "(64, 33)\n",
      "step 2267, loss is 5.064165115356445\n",
      "(64, 33)\n",
      "step 2268, loss is 5.238856315612793\n",
      "(64, 33)\n",
      "step 2269, loss is 5.289510726928711\n",
      "(64, 33)\n",
      "step 2270, loss is 5.2998366355896\n",
      "(64, 33)\n",
      "step 2271, loss is 4.928667068481445\n",
      "(64, 33)\n",
      "step 2272, loss is 5.397497177124023\n",
      "(64, 33)\n",
      "step 2273, loss is 5.261935234069824\n",
      "(64, 33)\n",
      "step 2274, loss is 5.184873580932617\n",
      "(64, 33)\n",
      "step 2275, loss is 5.394565105438232\n",
      "(64, 33)\n",
      "step 2276, loss is 5.1789937019348145\n",
      "(64, 33)\n",
      "step 2277, loss is 5.450669765472412\n",
      "(64, 33)\n",
      "step 2278, loss is 5.2491607666015625\n",
      "(64, 33)\n",
      "step 2279, loss is 5.227760314941406\n",
      "(64, 33)\n",
      "step 2280, loss is 5.2408928871154785\n",
      "(64, 33)\n",
      "step 2281, loss is 5.217958450317383\n",
      "(64, 33)\n",
      "step 2282, loss is 5.278801918029785\n",
      "(64, 33)\n",
      "step 2283, loss is 5.220933437347412\n",
      "(64, 33)\n",
      "step 2284, loss is 5.471248149871826\n",
      "(64, 33)\n",
      "step 2285, loss is 5.246039390563965\n",
      "(64, 33)\n",
      "step 2286, loss is 5.133859157562256\n",
      "(64, 33)\n",
      "step 2287, loss is 5.202746868133545\n",
      "(64, 33)\n",
      "step 2288, loss is 5.273041248321533\n",
      "(64, 33)\n",
      "step 2289, loss is 5.147400856018066\n",
      "(64, 33)\n",
      "step 2290, loss is 5.244284629821777\n",
      "(64, 33)\n",
      "step 2291, loss is 5.376884460449219\n",
      "(64, 33)\n",
      "step 2292, loss is 5.147985935211182\n",
      "(64, 33)\n",
      "step 2293, loss is 5.1199564933776855\n",
      "(64, 33)\n",
      "step 2294, loss is 5.275805473327637\n",
      "(64, 33)\n",
      "step 2295, loss is 5.088461399078369\n",
      "(64, 33)\n",
      "step 2296, loss is 5.352048873901367\n",
      "(64, 33)\n",
      "step 2297, loss is 5.209037780761719\n",
      "(64, 33)\n",
      "step 2298, loss is 5.133762836456299\n",
      "(64, 33)\n",
      "step 2299, loss is 5.127244472503662\n",
      "(64, 33)\n",
      "step 2300, loss is 5.082987308502197\n",
      "(64, 33)\n",
      "step 2301, loss is 5.296682357788086\n",
      "(64, 33)\n",
      "step 2302, loss is 5.2465033531188965\n",
      "(64, 33)\n",
      "step 2303, loss is 5.353151321411133\n",
      "(64, 33)\n",
      "step 2304, loss is 4.98512601852417\n",
      "(64, 33)\n",
      "step 2305, loss is 5.270051956176758\n",
      "(64, 33)\n",
      "step 2306, loss is 5.2731757164001465\n",
      "(64, 33)\n",
      "step 2307, loss is 5.0409932136535645\n",
      "(64, 33)\n",
      "step 2308, loss is 4.990751266479492\n",
      "(64, 33)\n",
      "step 2309, loss is 5.14695930480957\n",
      "(64, 33)\n",
      "step 2310, loss is 5.161989688873291\n",
      "(64, 33)\n",
      "step 2311, loss is 5.112231254577637\n",
      "(64, 33)\n",
      "step 2312, loss is 5.209544658660889\n",
      "(64, 33)\n",
      "step 2313, loss is 5.1400628089904785\n",
      "(64, 33)\n",
      "step 2314, loss is 5.142595291137695\n",
      "(64, 33)\n",
      "step 2315, loss is 5.2473835945129395\n",
      "(64, 33)\n",
      "step 2316, loss is 5.2841644287109375\n",
      "(64, 33)\n",
      "step 2317, loss is 5.485624313354492\n",
      "(64, 33)\n",
      "step 2318, loss is 5.325932025909424\n",
      "(64, 33)\n",
      "step 2319, loss is 5.330556869506836\n",
      "(64, 33)\n",
      "step 2320, loss is 5.141847133636475\n",
      "(64, 33)\n",
      "step 2321, loss is 5.136352062225342\n",
      "(64, 33)\n",
      "step 2322, loss is 5.238456726074219\n",
      "(64, 33)\n",
      "step 2323, loss is 5.381232738494873\n",
      "(64, 33)\n",
      "step 2324, loss is 5.362334251403809\n",
      "(64, 33)\n",
      "step 2325, loss is 5.279935359954834\n",
      "(64, 33)\n",
      "step 2326, loss is 5.259207248687744\n",
      "(64, 33)\n",
      "step 2327, loss is 5.229404449462891\n",
      "(64, 33)\n",
      "step 2328, loss is 5.188518047332764\n",
      "(64, 33)\n",
      "step 2329, loss is 5.256465911865234\n",
      "(64, 33)\n",
      "step 2330, loss is 5.133322238922119\n",
      "(64, 33)\n",
      "step 2331, loss is 5.190339088439941\n",
      "(64, 33)\n",
      "step 2332, loss is 5.1342549324035645\n",
      "(64, 33)\n",
      "step 2333, loss is 5.3395867347717285\n",
      "(64, 33)\n",
      "step 2334, loss is 5.1809797286987305\n",
      "(64, 33)\n",
      "step 2335, loss is 5.313719749450684\n",
      "(64, 33)\n",
      "step 2336, loss is 5.131987571716309\n",
      "(64, 33)\n",
      "step 2337, loss is 5.061375617980957\n",
      "(64, 33)\n",
      "step 2338, loss is 5.2222900390625\n",
      "(64, 33)\n",
      "step 2339, loss is 5.099857330322266\n",
      "(64, 33)\n",
      "step 2340, loss is 5.3029961585998535\n",
      "(64, 33)\n",
      "step 2341, loss is 5.138861179351807\n",
      "(64, 33)\n",
      "step 2342, loss is 5.160344123840332\n",
      "(64, 33)\n",
      "step 2343, loss is 5.217536926269531\n",
      "(64, 33)\n",
      "step 2344, loss is 5.227450370788574\n",
      "(64, 33)\n",
      "step 2345, loss is 5.102265357971191\n",
      "(64, 33)\n",
      "step 2346, loss is 5.328826904296875\n",
      "(64, 33)\n",
      "step 2347, loss is 5.1842522621154785\n",
      "(64, 33)\n",
      "step 2348, loss is 5.305942058563232\n",
      "(64, 33)\n",
      "step 2349, loss is 5.092825412750244\n",
      "(64, 33)\n",
      "step 2350, loss is 5.266716480255127\n",
      "(64, 33)\n",
      "step 2351, loss is 5.207416534423828\n",
      "(64, 33)\n",
      "step 2352, loss is 5.15790319442749\n",
      "(64, 33)\n",
      "step 2353, loss is 5.548023700714111\n",
      "(64, 33)\n",
      "step 2354, loss is 4.920276165008545\n",
      "(64, 33)\n",
      "step 2355, loss is 5.143529891967773\n",
      "(64, 33)\n",
      "step 2356, loss is 5.085851669311523\n",
      "(64, 33)\n",
      "step 2357, loss is 5.308279037475586\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2358, loss is 5.082294940948486\n",
      "(64, 33)\n",
      "step 2359, loss is 5.248485088348389\n",
      "(64, 33)\n",
      "step 2360, loss is 5.066792964935303\n",
      "(64, 33)\n",
      "step 2361, loss is 5.186058044433594\n",
      "(64, 33)\n",
      "step 2362, loss is 5.189615249633789\n",
      "(64, 33)\n",
      "step 2363, loss is 5.178016185760498\n",
      "(64, 33)\n",
      "step 2364, loss is 5.114608287811279\n",
      "(64, 33)\n",
      "step 2365, loss is 5.118071556091309\n",
      "(64, 33)\n",
      "step 2366, loss is 5.375510215759277\n",
      "(64, 33)\n",
      "step 2367, loss is 5.280965328216553\n",
      "(64, 33)\n",
      "step 2368, loss is 5.302886962890625\n",
      "(64, 33)\n",
      "step 2369, loss is 5.130187034606934\n",
      "(64, 33)\n",
      "step 2370, loss is 5.255542755126953\n",
      "(64, 33)\n",
      "step 2371, loss is 5.331747055053711\n",
      "(64, 33)\n",
      "step 2372, loss is 5.253929138183594\n",
      "(64, 33)\n",
      "step 2373, loss is 5.384982109069824\n",
      "(64, 33)\n",
      "step 2374, loss is 5.1655659675598145\n",
      "(64, 33)\n",
      "step 2375, loss is 5.390699863433838\n",
      "(64, 33)\n",
      "step 2376, loss is 5.142301559448242\n",
      "(64, 33)\n",
      "step 2377, loss is 5.345427989959717\n",
      "(64, 33)\n",
      "step 2378, loss is 5.232281684875488\n",
      "(64, 33)\n",
      "step 2379, loss is 5.097234725952148\n",
      "(64, 33)\n",
      "step 2380, loss is 5.43389892578125\n",
      "(64, 33)\n",
      "step 2381, loss is 5.213685512542725\n",
      "(64, 33)\n",
      "step 2382, loss is 5.292045593261719\n",
      "(64, 33)\n",
      "step 2383, loss is 5.151255130767822\n",
      "(64, 33)\n",
      "step 2384, loss is 5.234588623046875\n",
      "(64, 33)\n",
      "step 2385, loss is 5.265222072601318\n",
      "(64, 33)\n",
      "step 2386, loss is 5.259042263031006\n",
      "(64, 33)\n",
      "step 2387, loss is 5.237339019775391\n",
      "(64, 33)\n",
      "step 2388, loss is 5.193103790283203\n",
      "(64, 33)\n",
      "step 2389, loss is 5.0991411209106445\n",
      "(64, 33)\n",
      "step 2390, loss is 5.0301923751831055\n",
      "(64, 33)\n",
      "step 2391, loss is 5.210034370422363\n",
      "(64, 33)\n",
      "step 2392, loss is 5.497395038604736\n",
      "(64, 33)\n",
      "step 2393, loss is 5.122544765472412\n",
      "(64, 33)\n",
      "step 2394, loss is 5.191166877746582\n",
      "(64, 33)\n",
      "step 2395, loss is 5.2256011962890625\n",
      "(64, 33)\n",
      "step 2396, loss is 5.09855842590332\n",
      "(64, 33)\n",
      "step 2397, loss is 5.145393371582031\n",
      "(64, 33)\n",
      "step 2398, loss is 5.290859222412109\n",
      "(64, 33)\n",
      "step 2399, loss is 5.269054889678955\n",
      "(64, 33)\n",
      "step 2400, loss is 5.100971221923828\n",
      "(64, 33)\n",
      "step 2401, loss is 5.042993545532227\n",
      "(64, 33)\n",
      "step 2402, loss is 5.115915775299072\n",
      "(64, 33)\n",
      "step 2403, loss is 5.297732830047607\n",
      "(64, 33)\n",
      "step 2404, loss is 5.107058048248291\n",
      "(64, 33)\n",
      "step 2405, loss is 5.258904457092285\n",
      "(64, 33)\n",
      "step 2406, loss is 5.24189567565918\n",
      "(64, 33)\n",
      "step 2407, loss is 5.143388748168945\n",
      "(64, 33)\n",
      "step 2408, loss is 5.1116943359375\n",
      "(64, 33)\n",
      "step 2409, loss is 5.2957940101623535\n",
      "(64, 33)\n",
      "step 2410, loss is 5.115728855133057\n",
      "(64, 33)\n",
      "step 2411, loss is 5.109825134277344\n",
      "(64, 33)\n",
      "step 2412, loss is 5.290287017822266\n",
      "(64, 33)\n",
      "step 2413, loss is 5.292333602905273\n",
      "(64, 33)\n",
      "step 2414, loss is 5.191867351531982\n",
      "(64, 33)\n",
      "step 2415, loss is 5.24222993850708\n",
      "(64, 33)\n",
      "step 2416, loss is 5.198093414306641\n",
      "(64, 33)\n",
      "step 2417, loss is 5.246819972991943\n",
      "(64, 33)\n",
      "step 2418, loss is 5.139577388763428\n",
      "(64, 33)\n",
      "step 2419, loss is 5.442032814025879\n",
      "(64, 33)\n",
      "step 2420, loss is 5.172188758850098\n",
      "(64, 33)\n",
      "step 2421, loss is 5.229883670806885\n",
      "(64, 33)\n",
      "step 2422, loss is 5.04642915725708\n",
      "(64, 33)\n",
      "step 2423, loss is 5.326197147369385\n",
      "(64, 33)\n",
      "step 2424, loss is 5.266056537628174\n",
      "(64, 33)\n",
      "step 2425, loss is 5.239882469177246\n",
      "(64, 33)\n",
      "step 2426, loss is 5.06558895111084\n",
      "(64, 33)\n",
      "step 2427, loss is 5.253231525421143\n",
      "(64, 33)\n",
      "step 2428, loss is 5.40817403793335\n",
      "(64, 33)\n",
      "step 2429, loss is 5.247202396392822\n",
      "(64, 33)\n",
      "step 2430, loss is 5.26446533203125\n",
      "(64, 33)\n",
      "step 2431, loss is 5.196390628814697\n",
      "(64, 33)\n",
      "step 2432, loss is 5.294384002685547\n",
      "(64, 33)\n",
      "step 2433, loss is 5.245193004608154\n",
      "(64, 33)\n",
      "step 2434, loss is 5.189387321472168\n",
      "(64, 33)\n",
      "step 2435, loss is 5.060328006744385\n",
      "(64, 33)\n",
      "step 2436, loss is 5.091024398803711\n",
      "(64, 33)\n",
      "step 2437, loss is 5.1380534172058105\n",
      "(64, 33)\n",
      "step 2438, loss is 5.264432430267334\n",
      "(64, 33)\n",
      "step 2439, loss is 5.390361309051514\n",
      "(64, 33)\n",
      "step 2440, loss is 5.227560520172119\n",
      "(64, 33)\n",
      "step 2441, loss is 5.4199066162109375\n",
      "(64, 33)\n",
      "step 2442, loss is 5.178423881530762\n",
      "(64, 33)\n",
      "step 2443, loss is 5.322656154632568\n",
      "(64, 33)\n",
      "step 2444, loss is 5.110617160797119\n",
      "(64, 33)\n",
      "step 2445, loss is 5.213812351226807\n",
      "(64, 33)\n",
      "step 2446, loss is 5.144290924072266\n",
      "(64, 33)\n",
      "step 2447, loss is 5.03237771987915\n",
      "(64, 33)\n",
      "step 2448, loss is 5.184617042541504\n",
      "(64, 33)\n",
      "step 2449, loss is 5.209286212921143\n",
      "(64, 33)\n",
      "step 2450, loss is 5.0979485511779785\n",
      "(64, 33)\n",
      "step 2451, loss is 5.156185150146484\n",
      "(64, 33)\n",
      "step 2452, loss is 5.14668607711792\n",
      "(64, 33)\n",
      "step 2453, loss is 5.134620666503906\n",
      "(64, 33)\n",
      "step 2454, loss is 5.280877113342285\n",
      "(64, 33)\n",
      "step 2455, loss is 5.2055745124816895\n",
      "(64, 33)\n",
      "step 2456, loss is 5.287191867828369\n",
      "(64, 33)\n",
      "step 2457, loss is 5.1958537101745605\n",
      "(64, 33)\n",
      "step 2458, loss is 5.173407554626465\n",
      "(64, 33)\n",
      "step 2459, loss is 5.176762104034424\n",
      "(64, 33)\n",
      "step 2460, loss is 5.361896991729736\n",
      "(64, 33)\n",
      "step 2461, loss is 5.332742214202881\n",
      "(64, 33)\n",
      "step 2462, loss is 5.114212989807129\n",
      "(64, 33)\n",
      "step 2463, loss is 5.318941593170166\n",
      "(64, 33)\n",
      "step 2464, loss is 5.145045280456543\n",
      "(64, 33)\n",
      "step 2465, loss is 5.123310565948486\n",
      "(64, 33)\n",
      "step 2466, loss is 5.144843578338623\n",
      "(64, 33)\n",
      "step 2467, loss is 5.048611640930176\n",
      "(64, 33)\n",
      "step 2468, loss is 5.240942001342773\n",
      "(64, 33)\n",
      "step 2469, loss is 5.321235656738281\n",
      "(64, 33)\n",
      "step 2470, loss is 4.960416793823242\n",
      "(64, 33)\n",
      "step 2471, loss is 5.085117340087891\n",
      "(64, 33)\n",
      "step 2472, loss is 5.309412956237793\n",
      "(64, 33)\n",
      "step 2473, loss is 5.24106502532959\n",
      "(64, 33)\n",
      "step 2474, loss is 5.3394646644592285\n",
      "(64, 33)\n",
      "step 2475, loss is 5.29173469543457\n",
      "(64, 33)\n",
      "step 2476, loss is 5.057400226593018\n",
      "(64, 33)\n",
      "step 2477, loss is 5.454695701599121\n",
      "(64, 33)\n",
      "step 2478, loss is 5.294107913970947\n",
      "(64, 33)\n",
      "step 2479, loss is 5.154581069946289\n",
      "(64, 33)\n",
      "step 2480, loss is 5.037991523742676\n",
      "(64, 33)\n",
      "step 2481, loss is 5.177937984466553\n",
      "(64, 33)\n",
      "step 2482, loss is 5.037486553192139\n",
      "(64, 33)\n",
      "step 2483, loss is 5.287571907043457\n",
      "(64, 33)\n",
      "step 2484, loss is 5.035658359527588\n",
      "(64, 33)\n",
      "step 2485, loss is 5.182401180267334\n",
      "(64, 33)\n",
      "step 2486, loss is 5.269248962402344\n",
      "(64, 33)\n",
      "step 2487, loss is 5.202041149139404\n",
      "(64, 33)\n",
      "step 2488, loss is 5.146999359130859\n",
      "(64, 33)\n",
      "step 2489, loss is 5.1038970947265625\n",
      "(64, 33)\n",
      "step 2490, loss is 5.16865348815918\n",
      "(64, 33)\n",
      "step 2491, loss is 5.2378129959106445\n",
      "(64, 33)\n",
      "step 2492, loss is 5.156138896942139\n",
      "(64, 33)\n",
      "step 2493, loss is 5.141761779785156\n",
      "(64, 33)\n",
      "step 2494, loss is 5.3238525390625\n",
      "(64, 33)\n",
      "step 2495, loss is 5.099935054779053\n",
      "(64, 33)\n",
      "step 2496, loss is 5.249114513397217\n",
      "(64, 33)\n",
      "step 2497, loss is 5.183416366577148\n",
      "(64, 33)\n",
      "step 2498, loss is 5.286592483520508\n",
      "(64, 33)\n",
      "step 2499, loss is 5.222374439239502\n",
      "(64, 33)\n",
      "step 2500, loss is 5.203575134277344\n",
      "(64, 33)\n",
      "step 2501, loss is 5.365556240081787\n",
      "(64, 33)\n",
      "step 2502, loss is 5.154947280883789\n",
      "(64, 33)\n",
      "step 2503, loss is 5.181137561798096\n",
      "(64, 33)\n",
      "step 2504, loss is 5.025537490844727\n",
      "(64, 33)\n",
      "step 2505, loss is 5.265732288360596\n",
      "(64, 33)\n",
      "step 2506, loss is 5.069586277008057\n",
      "(64, 33)\n",
      "step 2507, loss is 5.024953365325928\n",
      "(64, 33)\n",
      "step 2508, loss is 5.097737789154053\n",
      "(64, 33)\n",
      "step 2509, loss is 5.231287956237793\n",
      "(64, 33)\n",
      "step 2510, loss is 5.146088123321533\n",
      "(64, 33)\n",
      "step 2511, loss is 5.2821946144104\n",
      "(64, 33)\n",
      "step 2512, loss is 5.318735599517822\n",
      "(64, 33)\n",
      "step 2513, loss is 5.170616149902344\n",
      "(64, 33)\n",
      "step 2514, loss is 5.117103576660156\n",
      "(64, 33)\n",
      "step 2515, loss is 5.2932233810424805\n",
      "(64, 33)\n",
      "step 2516, loss is 5.184939384460449\n",
      "(64, 33)\n",
      "step 2517, loss is 5.166384220123291\n",
      "(64, 33)\n",
      "step 2518, loss is 5.058345317840576\n",
      "(64, 33)\n",
      "step 2519, loss is 5.188155174255371\n",
      "(64, 33)\n",
      "step 2520, loss is 5.130655288696289\n",
      "(64, 33)\n",
      "step 2521, loss is 5.273518085479736\n",
      "(64, 33)\n",
      "step 2522, loss is 5.194674015045166\n",
      "(64, 33)\n",
      "step 2523, loss is 5.30562686920166\n",
      "(64, 33)\n",
      "step 2524, loss is 5.264037132263184\n",
      "(64, 33)\n",
      "step 2525, loss is 5.266145706176758\n",
      "(64, 33)\n",
      "step 2526, loss is 5.263417720794678\n",
      "(64, 33)\n",
      "step 2527, loss is 5.277542591094971\n",
      "(64, 33)\n",
      "step 2528, loss is 5.042839050292969\n",
      "(64, 33)\n",
      "step 2529, loss is 5.238835334777832\n",
      "(64, 33)\n",
      "step 2530, loss is 5.107283592224121\n",
      "(64, 33)\n",
      "step 2531, loss is 5.367114543914795\n",
      "(64, 33)\n",
      "step 2532, loss is 5.164242744445801\n",
      "(64, 33)\n",
      "step 2533, loss is 5.170347690582275\n",
      "(64, 33)\n",
      "step 2534, loss is 5.104269027709961\n",
      "(64, 33)\n",
      "step 2535, loss is 5.116919994354248\n",
      "(64, 33)\n",
      "step 2536, loss is 5.289897441864014\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2537, loss is 5.146803379058838\n",
      "(64, 33)\n",
      "step 2538, loss is 5.127845287322998\n",
      "(64, 33)\n",
      "step 2539, loss is 5.142539024353027\n",
      "(64, 33)\n",
      "step 2540, loss is 5.277360439300537\n",
      "(64, 33)\n",
      "step 2541, loss is 5.114207744598389\n",
      "(64, 33)\n",
      "step 2542, loss is 5.240241527557373\n",
      "(64, 33)\n",
      "step 2543, loss is 5.244504928588867\n",
      "(64, 33)\n",
      "step 2544, loss is 5.175506114959717\n",
      "(64, 33)\n",
      "step 2545, loss is 5.1438140869140625\n",
      "(64, 33)\n",
      "step 2546, loss is 5.13850212097168\n",
      "(64, 33)\n",
      "step 2547, loss is 5.335286617279053\n",
      "(64, 33)\n",
      "step 2548, loss is 5.3649582862854\n",
      "(64, 33)\n",
      "step 2549, loss is 5.314164638519287\n",
      "(64, 33)\n",
      "step 2550, loss is 5.3248162269592285\n",
      "(64, 33)\n",
      "step 2551, loss is 5.167961120605469\n",
      "(64, 33)\n",
      "step 2552, loss is 5.218998432159424\n",
      "(64, 33)\n",
      "step 2553, loss is 5.115922451019287\n",
      "(64, 33)\n",
      "step 2554, loss is 5.315290927886963\n",
      "(64, 33)\n",
      "step 2555, loss is 5.261508941650391\n",
      "(64, 33)\n",
      "step 2556, loss is 5.087327480316162\n",
      "(64, 33)\n",
      "step 2557, loss is 5.06745719909668\n",
      "(64, 33)\n",
      "step 2558, loss is 5.086096286773682\n",
      "(64, 33)\n",
      "step 2559, loss is 5.1569342613220215\n",
      "(64, 33)\n",
      "step 2560, loss is 5.198427200317383\n",
      "(64, 33)\n",
      "step 2561, loss is 5.167031288146973\n",
      "(64, 33)\n",
      "step 2562, loss is 5.329124450683594\n",
      "(64, 33)\n",
      "step 2563, loss is 5.048234462738037\n",
      "(64, 33)\n",
      "step 2564, loss is 5.185729503631592\n",
      "(64, 33)\n",
      "step 2565, loss is 5.2250518798828125\n",
      "(64, 33)\n",
      "step 2566, loss is 5.328010082244873\n",
      "(64, 33)\n",
      "step 2567, loss is 5.1324076652526855\n",
      "(64, 33)\n",
      "step 2568, loss is 5.076781272888184\n",
      "(64, 33)\n",
      "step 2569, loss is 5.190369606018066\n",
      "(64, 33)\n",
      "step 2570, loss is 5.287222385406494\n",
      "(64, 33)\n",
      "step 2571, loss is 5.313787937164307\n",
      "(64, 33)\n",
      "step 2572, loss is 5.110337734222412\n",
      "(64, 33)\n",
      "step 2573, loss is 5.189316272735596\n",
      "(64, 33)\n",
      "step 2574, loss is 5.153138160705566\n",
      "(64, 33)\n",
      "step 2575, loss is 5.220726013183594\n",
      "(64, 33)\n",
      "step 2576, loss is 5.160820007324219\n",
      "(64, 33)\n",
      "step 2577, loss is 5.22367000579834\n",
      "(64, 33)\n",
      "step 2578, loss is 4.96877384185791\n",
      "(64, 33)\n",
      "step 2579, loss is 5.032873153686523\n",
      "(64, 33)\n",
      "step 2580, loss is 5.037842750549316\n",
      "(64, 33)\n",
      "step 2581, loss is 5.240227699279785\n",
      "(64, 33)\n",
      "step 2582, loss is 5.239444732666016\n",
      "(64, 33)\n",
      "step 2583, loss is 5.345354080200195\n",
      "(64, 33)\n",
      "step 2584, loss is 5.109967231750488\n",
      "(64, 33)\n",
      "step 2585, loss is 5.251156806945801\n",
      "(64, 33)\n",
      "step 2586, loss is 5.341383934020996\n",
      "(64, 33)\n",
      "step 2587, loss is 5.14143705368042\n",
      "(64, 33)\n",
      "step 2588, loss is 5.0056681632995605\n",
      "(64, 33)\n",
      "step 2589, loss is 5.276485443115234\n",
      "(64, 33)\n",
      "step 2590, loss is 5.087516784667969\n",
      "(64, 33)\n",
      "step 2591, loss is 5.194853782653809\n",
      "(64, 33)\n",
      "step 2592, loss is 5.263114929199219\n",
      "(64, 33)\n",
      "step 2593, loss is 5.000102996826172\n",
      "(64, 33)\n",
      "step 2594, loss is 5.271841049194336\n",
      "(64, 33)\n",
      "step 2595, loss is 5.181740760803223\n",
      "(64, 33)\n",
      "step 2596, loss is 5.289143085479736\n",
      "(64, 33)\n",
      "step 2597, loss is 4.92205810546875\n",
      "(64, 33)\n",
      "step 2598, loss is 5.145168304443359\n",
      "(64, 33)\n",
      "step 2599, loss is 5.38016939163208\n",
      "(64, 33)\n",
      "step 2600, loss is 5.410923480987549\n",
      "(64, 33)\n",
      "step 2601, loss is 5.090785026550293\n",
      "(64, 33)\n",
      "step 2602, loss is 5.0204854011535645\n",
      "(64, 33)\n",
      "step 2603, loss is 4.928365707397461\n",
      "(64, 33)\n",
      "step 2604, loss is 5.153017520904541\n",
      "(64, 33)\n",
      "step 2605, loss is 5.107460975646973\n",
      "(64, 33)\n",
      "step 2606, loss is 5.268587112426758\n",
      "(64, 33)\n",
      "step 2607, loss is 5.262034893035889\n",
      "(64, 33)\n",
      "step 2608, loss is 5.102324962615967\n",
      "(64, 33)\n",
      "step 2609, loss is 5.149855613708496\n",
      "(64, 33)\n",
      "step 2610, loss is 5.000492095947266\n",
      "(64, 33)\n",
      "step 2611, loss is 5.201592922210693\n",
      "(64, 33)\n",
      "step 2612, loss is 5.327663421630859\n",
      "(64, 33)\n",
      "step 2613, loss is 5.275618553161621\n",
      "(64, 33)\n",
      "step 2614, loss is 5.354443550109863\n",
      "(64, 33)\n",
      "step 2615, loss is 4.992197036743164\n",
      "(64, 33)\n",
      "step 2616, loss is 5.081518173217773\n",
      "(64, 33)\n",
      "step 2617, loss is 5.237946510314941\n",
      "(64, 33)\n",
      "step 2618, loss is 5.113838195800781\n",
      "(64, 33)\n",
      "step 2619, loss is 5.206684112548828\n",
      "(64, 33)\n",
      "step 2620, loss is 5.246739864349365\n",
      "(64, 33)\n",
      "step 2621, loss is 5.170608043670654\n",
      "(64, 33)\n",
      "step 2622, loss is 5.314697265625\n",
      "(64, 33)\n",
      "step 2623, loss is 5.109556198120117\n",
      "(64, 33)\n",
      "step 2624, loss is 5.345803737640381\n",
      "(64, 33)\n",
      "step 2625, loss is 5.129570007324219\n",
      "(64, 33)\n",
      "step 2626, loss is 5.244494915008545\n",
      "(64, 33)\n",
      "step 2627, loss is 5.147899150848389\n",
      "(64, 33)\n",
      "step 2628, loss is 5.460991859436035\n",
      "(64, 33)\n",
      "step 2629, loss is 5.089510917663574\n",
      "(64, 33)\n",
      "step 2630, loss is 5.428456783294678\n",
      "(64, 33)\n",
      "step 2631, loss is 5.146387577056885\n",
      "(64, 33)\n",
      "step 2632, loss is 5.081191539764404\n",
      "(64, 33)\n",
      "step 2633, loss is 5.252677917480469\n",
      "(64, 33)\n",
      "step 2634, loss is 5.017428874969482\n",
      "(64, 33)\n",
      "step 2635, loss is 5.444947242736816\n",
      "(64, 33)\n",
      "step 2636, loss is 5.244653701782227\n",
      "(64, 33)\n",
      "step 2637, loss is 5.100154876708984\n",
      "(64, 33)\n",
      "step 2638, loss is 5.229288578033447\n",
      "(64, 33)\n",
      "step 2639, loss is 5.219760417938232\n",
      "(64, 33)\n",
      "step 2640, loss is 5.147132396697998\n",
      "(64, 33)\n",
      "step 2641, loss is 5.198617458343506\n",
      "(64, 33)\n",
      "step 2642, loss is 5.162997245788574\n",
      "(64, 33)\n",
      "step 2643, loss is 5.055948257446289\n",
      "(64, 33)\n",
      "step 2644, loss is 5.186269283294678\n",
      "(64, 33)\n",
      "step 2645, loss is 5.290665149688721\n",
      "(64, 33)\n",
      "step 2646, loss is 5.141030311584473\n",
      "(64, 33)\n",
      "step 2647, loss is 5.375572204589844\n",
      "(64, 33)\n",
      "step 2648, loss is 5.117227554321289\n",
      "(64, 33)\n",
      "step 2649, loss is 5.281414985656738\n",
      "(64, 33)\n",
      "step 2650, loss is 5.1371750831604\n",
      "(64, 33)\n",
      "step 2651, loss is 4.8474626541137695\n",
      "(64, 33)\n",
      "step 2652, loss is 5.153614521026611\n",
      "(64, 33)\n",
      "step 2653, loss is 5.1592020988464355\n",
      "(64, 33)\n",
      "step 2654, loss is 5.1208086013793945\n",
      "(64, 33)\n",
      "step 2655, loss is 5.148293972015381\n",
      "(64, 33)\n",
      "step 2656, loss is 5.288031578063965\n",
      "(64, 33)\n",
      "step 2657, loss is 5.2656025886535645\n",
      "(64, 33)\n",
      "step 2658, loss is 5.069301128387451\n",
      "(64, 33)\n",
      "step 2659, loss is 5.207729339599609\n",
      "(64, 33)\n",
      "step 2660, loss is 5.15927791595459\n",
      "(64, 33)\n",
      "step 2661, loss is 5.350975036621094\n",
      "(64, 33)\n",
      "step 2662, loss is 5.196115970611572\n",
      "(64, 33)\n",
      "step 2663, loss is 5.26846170425415\n",
      "(64, 33)\n",
      "step 2664, loss is 5.026513576507568\n",
      "(64, 33)\n",
      "step 2665, loss is 5.092409610748291\n",
      "(64, 33)\n",
      "step 2666, loss is 5.080175876617432\n",
      "(64, 33)\n",
      "step 2667, loss is 5.286312103271484\n",
      "(64, 33)\n",
      "step 2668, loss is 5.149550914764404\n",
      "(64, 33)\n",
      "step 2669, loss is 5.248600006103516\n",
      "(64, 33)\n",
      "step 2670, loss is 5.0379815101623535\n",
      "(64, 33)\n",
      "step 2671, loss is 5.143318176269531\n",
      "(64, 33)\n",
      "step 2672, loss is 5.293909072875977\n",
      "(64, 33)\n",
      "step 2673, loss is 5.141366958618164\n",
      "(64, 33)\n",
      "step 2674, loss is 5.141566753387451\n",
      "(64, 33)\n",
      "step 2675, loss is 5.114670276641846\n",
      "(64, 33)\n",
      "step 2676, loss is 5.276005744934082\n",
      "(64, 33)\n",
      "step 2677, loss is 5.121159076690674\n",
      "(64, 33)\n",
      "step 2678, loss is 5.145151138305664\n",
      "(64, 33)\n",
      "step 2679, loss is 5.351001262664795\n",
      "(64, 33)\n",
      "step 2680, loss is 5.271284580230713\n",
      "(64, 33)\n",
      "step 2681, loss is 5.077120304107666\n",
      "(64, 33)\n",
      "step 2682, loss is 5.20442533493042\n",
      "(64, 33)\n",
      "step 2683, loss is 5.286458492279053\n",
      "(64, 33)\n",
      "step 2684, loss is 5.087244987487793\n",
      "(64, 33)\n",
      "step 2685, loss is 5.131340980529785\n",
      "(64, 33)\n",
      "step 2686, loss is 5.389682769775391\n",
      "(64, 33)\n",
      "step 2687, loss is 5.315734386444092\n",
      "(64, 33)\n",
      "step 2688, loss is 5.278507232666016\n",
      "(64, 33)\n",
      "step 2689, loss is 5.345390319824219\n",
      "(64, 33)\n",
      "step 2690, loss is 5.333796977996826\n",
      "(64, 33)\n",
      "step 2691, loss is 5.182605266571045\n",
      "(64, 33)\n",
      "step 2692, loss is 4.977766036987305\n",
      "(64, 33)\n",
      "step 2693, loss is 5.232597351074219\n",
      "(64, 33)\n",
      "step 2694, loss is 5.060135841369629\n",
      "(64, 33)\n",
      "step 2695, loss is 5.341325283050537\n",
      "(64, 33)\n",
      "step 2696, loss is 5.328787803649902\n",
      "(64, 33)\n",
      "step 2697, loss is 5.351960182189941\n",
      "(64, 33)\n",
      "step 2698, loss is 5.254631042480469\n",
      "(64, 33)\n",
      "step 2699, loss is 5.155277729034424\n",
      "(64, 33)\n",
      "step 2700, loss is 5.156080722808838\n",
      "(64, 33)\n",
      "step 2701, loss is 5.263876914978027\n",
      "(64, 33)\n",
      "step 2702, loss is 5.151089668273926\n",
      "(64, 33)\n",
      "step 2703, loss is 5.201142311096191\n",
      "(64, 33)\n",
      "step 2704, loss is 5.242534637451172\n",
      "(64, 33)\n",
      "step 2705, loss is 5.029494762420654\n",
      "(64, 33)\n",
      "step 2706, loss is 5.163525104522705\n",
      "(64, 33)\n",
      "step 2707, loss is 5.206517219543457\n",
      "(64, 33)\n",
      "step 2708, loss is 5.025493144989014\n",
      "(64, 33)\n",
      "step 2709, loss is 5.197661876678467\n",
      "(64, 33)\n",
      "step 2710, loss is 5.037986755371094\n",
      "(64, 33)\n",
      "step 2711, loss is 5.120132923126221\n",
      "(64, 33)\n",
      "step 2712, loss is 5.060400009155273\n",
      "(64, 33)\n",
      "step 2713, loss is 5.1471028327941895\n",
      "(64, 33)\n",
      "step 2714, loss is 5.326588153839111\n",
      "(64, 33)\n",
      "step 2715, loss is 5.176573753356934\n",
      "(64, 33)\n",
      "step 2716, loss is 5.205604553222656\n",
      "(64, 33)\n",
      "step 2717, loss is 5.095871925354004\n",
      "(64, 33)\n",
      "step 2718, loss is 5.009240627288818\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2719, loss is 5.133358955383301\n",
      "(64, 33)\n",
      "step 2720, loss is 5.1597113609313965\n",
      "(64, 33)\n",
      "step 2721, loss is 5.258847236633301\n",
      "(64, 33)\n",
      "step 2722, loss is 5.119094371795654\n",
      "(64, 33)\n",
      "step 2723, loss is 5.097419738769531\n",
      "(64, 33)\n",
      "step 2724, loss is 5.309026718139648\n",
      "(64, 33)\n",
      "step 2725, loss is 5.1173858642578125\n",
      "(64, 33)\n",
      "step 2726, loss is 5.226528167724609\n",
      "(64, 33)\n",
      "step 2727, loss is 5.084784507751465\n",
      "(64, 33)\n",
      "step 2728, loss is 5.1438446044921875\n",
      "(64, 33)\n",
      "step 2729, loss is 5.3389201164245605\n",
      "(64, 33)\n",
      "step 2730, loss is 5.004566192626953\n",
      "(64, 33)\n",
      "step 2731, loss is 5.0618896484375\n",
      "(64, 33)\n",
      "step 2732, loss is 5.171270847320557\n",
      "(64, 33)\n",
      "step 2733, loss is 5.192812919616699\n",
      "(64, 33)\n",
      "step 2734, loss is 5.252357482910156\n",
      "(64, 33)\n",
      "step 2735, loss is 5.198034763336182\n",
      "(64, 33)\n",
      "step 2736, loss is 5.244514465332031\n",
      "(64, 33)\n",
      "step 2737, loss is 5.000870227813721\n",
      "(64, 33)\n",
      "step 2738, loss is 5.220676898956299\n",
      "(64, 33)\n",
      "step 2739, loss is 5.274623870849609\n",
      "(64, 33)\n",
      "step 2740, loss is 5.290549278259277\n",
      "(64, 33)\n",
      "step 2741, loss is 5.199952125549316\n",
      "(64, 33)\n",
      "step 2742, loss is 5.14213752746582\n",
      "(64, 33)\n",
      "step 2743, loss is 4.905947685241699\n",
      "(64, 33)\n",
      "step 2744, loss is 5.046804904937744\n",
      "(64, 33)\n",
      "step 2745, loss is 5.111372470855713\n",
      "(64, 33)\n",
      "step 2746, loss is 5.24418830871582\n",
      "(64, 33)\n",
      "step 2747, loss is 5.326074600219727\n",
      "(64, 33)\n",
      "step 2748, loss is 5.066611289978027\n",
      "(64, 33)\n",
      "step 2749, loss is 5.224565029144287\n",
      "(64, 33)\n",
      "step 2750, loss is 5.1046833992004395\n",
      "(64, 33)\n",
      "step 2751, loss is 5.10896110534668\n",
      "(64, 33)\n",
      "step 2752, loss is 5.223653793334961\n",
      "(64, 33)\n",
      "step 2753, loss is 5.41287899017334\n",
      "(64, 33)\n",
      "step 2754, loss is 5.0380635261535645\n",
      "(64, 33)\n",
      "step 2755, loss is 5.112699508666992\n",
      "(64, 33)\n",
      "step 2756, loss is 5.155142784118652\n",
      "(64, 33)\n",
      "step 2757, loss is 5.361336708068848\n",
      "(64, 33)\n",
      "step 2758, loss is 5.118481159210205\n",
      "(64, 33)\n",
      "step 2759, loss is 5.057039260864258\n",
      "(64, 33)\n",
      "step 2760, loss is 5.168049335479736\n",
      "(64, 33)\n",
      "step 2761, loss is 5.3916425704956055\n",
      "(64, 33)\n",
      "step 2762, loss is 5.110751152038574\n",
      "(64, 33)\n",
      "step 2763, loss is 5.250507354736328\n",
      "(64, 33)\n",
      "step 2764, loss is 5.317454814910889\n",
      "(64, 33)\n",
      "step 2765, loss is 5.2846903800964355\n",
      "(64, 33)\n",
      "step 2766, loss is 5.256592750549316\n",
      "(64, 33)\n",
      "step 2767, loss is 5.126666069030762\n",
      "(64, 33)\n",
      "step 2768, loss is 4.984734058380127\n",
      "(64, 33)\n",
      "step 2769, loss is 5.097556114196777\n",
      "(64, 33)\n",
      "step 2770, loss is 5.08767032623291\n",
      "(64, 33)\n",
      "step 2771, loss is 5.228613376617432\n",
      "(64, 33)\n",
      "step 2772, loss is 5.0330634117126465\n",
      "(64, 33)\n",
      "step 2773, loss is 5.120536804199219\n",
      "(64, 33)\n",
      "step 2774, loss is 5.205223560333252\n",
      "(64, 33)\n",
      "step 2775, loss is 5.437644958496094\n",
      "(64, 33)\n",
      "step 2776, loss is 5.260537147521973\n",
      "(64, 33)\n",
      "step 2777, loss is 5.280092716217041\n",
      "(64, 33)\n",
      "step 2778, loss is 5.163568019866943\n",
      "(64, 33)\n",
      "step 2779, loss is 5.16658878326416\n",
      "(64, 33)\n",
      "step 2780, loss is 5.220273017883301\n",
      "(64, 33)\n",
      "step 2781, loss is 5.4156413078308105\n",
      "(64, 33)\n",
      "step 2782, loss is 5.232265472412109\n",
      "(64, 33)\n",
      "step 2783, loss is 5.210531711578369\n",
      "(64, 33)\n",
      "step 2784, loss is 5.35389518737793\n",
      "(64, 33)\n",
      "step 2785, loss is 5.208584308624268\n",
      "(64, 33)\n",
      "step 2786, loss is 5.186453342437744\n",
      "(64, 33)\n",
      "step 2787, loss is 5.244655132293701\n",
      "(64, 33)\n",
      "step 2788, loss is 5.318177700042725\n",
      "(64, 33)\n",
      "step 2789, loss is 5.339871406555176\n",
      "(64, 33)\n",
      "step 2790, loss is 5.119297027587891\n",
      "(64, 33)\n",
      "step 2791, loss is 5.199420928955078\n",
      "(64, 33)\n",
      "step 2792, loss is 5.133780479431152\n",
      "(64, 33)\n",
      "step 2793, loss is 5.240834712982178\n",
      "(64, 33)\n",
      "step 2794, loss is 5.321251392364502\n",
      "(64, 33)\n",
      "step 2795, loss is 5.155133247375488\n",
      "(64, 33)\n",
      "step 2796, loss is 5.300760269165039\n",
      "(64, 33)\n",
      "step 2797, loss is 5.202775955200195\n",
      "(64, 33)\n",
      "step 2798, loss is 5.22886323928833\n",
      "(64, 33)\n",
      "step 2799, loss is 5.353549480438232\n",
      "(64, 33)\n",
      "step 2800, loss is 5.207960605621338\n",
      "(64, 33)\n",
      "step 2801, loss is 5.329400062561035\n",
      "(64, 33)\n",
      "step 2802, loss is 5.222815036773682\n",
      "(64, 33)\n",
      "step 2803, loss is 5.300863742828369\n",
      "(64, 33)\n",
      "step 2804, loss is 5.190507888793945\n",
      "(64, 33)\n",
      "step 2805, loss is 5.173325061798096\n",
      "(64, 33)\n",
      "step 2806, loss is 4.986418724060059\n",
      "(64, 33)\n",
      "step 2807, loss is 5.3354315757751465\n",
      "(64, 33)\n",
      "step 2808, loss is 5.311012268066406\n",
      "(64, 33)\n",
      "step 2809, loss is 5.158663749694824\n",
      "(64, 33)\n",
      "step 2810, loss is 5.236939430236816\n",
      "(64, 33)\n",
      "step 2811, loss is 5.175593376159668\n",
      "(64, 33)\n",
      "step 2812, loss is 5.2281174659729\n",
      "(64, 33)\n",
      "step 2813, loss is 5.095987319946289\n",
      "(64, 33)\n",
      "step 2814, loss is 5.462542533874512\n",
      "(64, 33)\n",
      "step 2815, loss is 5.243238925933838\n",
      "(64, 33)\n",
      "step 2816, loss is 5.1589837074279785\n",
      "(64, 33)\n",
      "step 2817, loss is 5.203158378601074\n",
      "(64, 33)\n",
      "step 2818, loss is 5.359281539916992\n",
      "(64, 33)\n",
      "step 2819, loss is 5.047171115875244\n",
      "(64, 33)\n",
      "step 2820, loss is 5.204881191253662\n",
      "(64, 33)\n",
      "step 2821, loss is 5.1298933029174805\n",
      "(64, 33)\n",
      "step 2822, loss is 5.165791034698486\n",
      "(64, 33)\n",
      "step 2823, loss is 5.4079742431640625\n",
      "(64, 33)\n",
      "step 2824, loss is 5.251665115356445\n",
      "(64, 33)\n",
      "step 2825, loss is 5.0154852867126465\n",
      "(64, 33)\n",
      "step 2826, loss is 5.1399407386779785\n",
      "(64, 33)\n",
      "step 2827, loss is 5.360651969909668\n",
      "(64, 33)\n",
      "step 2828, loss is 5.179712772369385\n",
      "(64, 33)\n",
      "step 2829, loss is 5.10691499710083\n",
      "(64, 33)\n",
      "step 2830, loss is 5.041032314300537\n",
      "(64, 33)\n",
      "step 2831, loss is 5.171019077301025\n",
      "(64, 33)\n",
      "step 2832, loss is 5.294347763061523\n",
      "(64, 33)\n",
      "step 2833, loss is 5.202371120452881\n",
      "(64, 33)\n",
      "step 2834, loss is 5.055134296417236\n",
      "(64, 33)\n",
      "step 2835, loss is 5.178313255310059\n",
      "(64, 33)\n",
      "step 2836, loss is 5.332691669464111\n",
      "(64, 33)\n",
      "step 2837, loss is 5.157181739807129\n",
      "(64, 33)\n",
      "step 2838, loss is 5.244007587432861\n",
      "(64, 33)\n",
      "step 2839, loss is 5.124841213226318\n",
      "(64, 33)\n",
      "step 2840, loss is 5.192719459533691\n",
      "(64, 33)\n",
      "step 2841, loss is 5.11073112487793\n",
      "(64, 33)\n",
      "step 2842, loss is 5.118143081665039\n",
      "(64, 33)\n",
      "step 2843, loss is 5.129095077514648\n",
      "(64, 33)\n",
      "step 2844, loss is 4.825015544891357\n",
      "(64, 33)\n",
      "step 2845, loss is 5.1976470947265625\n",
      "(64, 33)\n",
      "step 2846, loss is 4.99113130569458\n",
      "(64, 33)\n",
      "step 2847, loss is 5.299016952514648\n",
      "(64, 33)\n",
      "step 2848, loss is 5.172085285186768\n",
      "(64, 33)\n",
      "step 2849, loss is 5.528085231781006\n",
      "(64, 33)\n",
      "step 2850, loss is 5.154517650604248\n",
      "(64, 33)\n",
      "step 2851, loss is 5.279168605804443\n",
      "(64, 33)\n",
      "step 2852, loss is 5.194212436676025\n",
      "(64, 33)\n",
      "step 2853, loss is 5.151516914367676\n",
      "(64, 33)\n",
      "step 2854, loss is 5.2659010887146\n",
      "(64, 33)\n",
      "step 2855, loss is 5.199037551879883\n",
      "(64, 33)\n",
      "step 2856, loss is 5.304252624511719\n",
      "(64, 33)\n",
      "step 2857, loss is 4.915037155151367\n",
      "(64, 33)\n",
      "step 2858, loss is 5.182760715484619\n",
      "(64, 33)\n",
      "step 2859, loss is 5.272483825683594\n",
      "(64, 33)\n",
      "step 2860, loss is 5.051549434661865\n",
      "(64, 33)\n",
      "step 2861, loss is 5.175489902496338\n",
      "(64, 33)\n",
      "step 2862, loss is 5.276401519775391\n",
      "(64, 33)\n",
      "step 2863, loss is 5.180697441101074\n",
      "(64, 33)\n",
      "step 2864, loss is 5.270270347595215\n",
      "(64, 33)\n",
      "step 2865, loss is 5.178074359893799\n",
      "(64, 33)\n",
      "step 2866, loss is 5.192026138305664\n",
      "(64, 33)\n",
      "step 2867, loss is 5.336925506591797\n",
      "(64, 33)\n",
      "step 2868, loss is 5.275371551513672\n",
      "(64, 33)\n",
      "step 2869, loss is 5.2875657081604\n",
      "(64, 33)\n",
      "step 2870, loss is 5.240235805511475\n",
      "(64, 33)\n",
      "step 2871, loss is 5.192465782165527\n",
      "(64, 33)\n",
      "step 2872, loss is 5.221002578735352\n",
      "(64, 33)\n",
      "step 2873, loss is 5.315793991088867\n",
      "(64, 33)\n",
      "step 2874, loss is 4.99354362487793\n",
      "(64, 33)\n",
      "step 2875, loss is 5.252997875213623\n",
      "(64, 33)\n",
      "step 2876, loss is 5.289735317230225\n",
      "(64, 33)\n",
      "step 2877, loss is 5.268847942352295\n",
      "(64, 33)\n",
      "step 2878, loss is 5.384208679199219\n",
      "(64, 33)\n",
      "step 2879, loss is 5.0049357414245605\n",
      "(64, 33)\n",
      "step 2880, loss is 5.100825309753418\n",
      "(64, 33)\n",
      "step 2881, loss is 5.115947246551514\n",
      "(64, 33)\n",
      "step 2882, loss is 5.144207954406738\n",
      "(64, 33)\n",
      "step 2883, loss is 5.189762592315674\n",
      "(64, 33)\n",
      "step 2884, loss is 5.352636814117432\n",
      "(64, 33)\n",
      "step 2885, loss is 5.041796684265137\n",
      "(64, 33)\n",
      "step 2886, loss is 5.214908599853516\n",
      "(64, 33)\n",
      "step 2887, loss is 5.141356468200684\n",
      "(64, 33)\n",
      "step 2888, loss is 5.04292631149292\n",
      "(64, 33)\n",
      "step 2889, loss is 5.089853763580322\n",
      "(64, 33)\n",
      "step 2890, loss is 5.187066555023193\n",
      "(64, 33)\n",
      "step 2891, loss is 5.357054710388184\n",
      "(64, 33)\n",
      "step 2892, loss is 5.199902057647705\n",
      "(64, 33)\n",
      "step 2893, loss is 4.998760223388672\n",
      "(64, 33)\n",
      "step 2894, loss is 5.25691032409668\n",
      "(64, 33)\n",
      "step 2895, loss is 4.931031703948975\n",
      "(64, 33)\n",
      "step 2896, loss is 5.229420185089111\n",
      "(64, 33)\n",
      "step 2897, loss is 5.129677772521973\n",
      "(64, 33)\n",
      "step 2898, loss is 5.1143999099731445\n",
      "(64, 33)\n",
      "step 2899, loss is 4.940685749053955\n",
      "(64, 33)\n",
      "step 2900, loss is 5.095592498779297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 2901, loss is 5.137090682983398\n",
      "(64, 33)\n",
      "step 2902, loss is 5.211148738861084\n",
      "(64, 33)\n",
      "step 2903, loss is 4.985485076904297\n",
      "(64, 33)\n",
      "step 2904, loss is 5.183376789093018\n",
      "(64, 33)\n",
      "step 2905, loss is 5.152876853942871\n",
      "(64, 33)\n",
      "step 2906, loss is 5.070557594299316\n",
      "(64, 33)\n",
      "step 2907, loss is 5.212616920471191\n",
      "(64, 33)\n",
      "step 2908, loss is 5.222817897796631\n",
      "(64, 33)\n",
      "step 2909, loss is 5.007511615753174\n",
      "(64, 33)\n",
      "step 2910, loss is 5.1382551193237305\n",
      "(64, 33)\n",
      "step 2911, loss is 5.0565056800842285\n",
      "(64, 33)\n",
      "step 2912, loss is 5.290273189544678\n",
      "(64, 33)\n",
      "step 2913, loss is 5.209712505340576\n",
      "(64, 33)\n",
      "step 2914, loss is 5.261645317077637\n",
      "(64, 33)\n",
      "step 2915, loss is 5.108767986297607\n",
      "(64, 33)\n",
      "step 2916, loss is 5.01223611831665\n",
      "(64, 33)\n",
      "step 2917, loss is 5.337465763092041\n",
      "(64, 33)\n",
      "step 2918, loss is 5.240321636199951\n",
      "(64, 33)\n",
      "step 2919, loss is 5.040587425231934\n",
      "(64, 33)\n",
      "step 2920, loss is 5.08156156539917\n",
      "(64, 33)\n",
      "step 2921, loss is 5.278462886810303\n",
      "(64, 33)\n",
      "step 2922, loss is 5.314512729644775\n",
      "(64, 33)\n",
      "step 2923, loss is 5.253686904907227\n",
      "(64, 33)\n",
      "step 2924, loss is 5.167039394378662\n",
      "(64, 33)\n",
      "step 2925, loss is 5.134973526000977\n",
      "(64, 33)\n",
      "step 2926, loss is 4.987955570220947\n",
      "(64, 33)\n",
      "step 2927, loss is 5.270740985870361\n",
      "(64, 33)\n",
      "step 2928, loss is 5.229500770568848\n",
      "(64, 33)\n",
      "step 2929, loss is 5.205240726470947\n",
      "(64, 33)\n",
      "step 2930, loss is 5.3298420906066895\n",
      "(64, 33)\n",
      "step 2931, loss is 5.0893330574035645\n",
      "(64, 33)\n",
      "step 2932, loss is 5.13164758682251\n",
      "(64, 33)\n",
      "step 2933, loss is 5.193964958190918\n",
      "(64, 33)\n",
      "step 2934, loss is 5.05879020690918\n",
      "(64, 33)\n",
      "step 2935, loss is 5.174819469451904\n",
      "(64, 33)\n",
      "step 2936, loss is 5.163991451263428\n",
      "(64, 33)\n",
      "step 2937, loss is 5.079716205596924\n",
      "(64, 33)\n",
      "step 2938, loss is 5.084489822387695\n",
      "(64, 33)\n",
      "step 2939, loss is 5.437195301055908\n",
      "(64, 33)\n",
      "step 2940, loss is 5.194250583648682\n",
      "(64, 33)\n",
      "step 2941, loss is 5.0928192138671875\n",
      "(64, 33)\n",
      "step 2942, loss is 5.173213958740234\n",
      "(64, 33)\n",
      "step 2943, loss is 5.22959041595459\n",
      "(64, 33)\n",
      "step 2944, loss is 5.227240085601807\n",
      "(64, 33)\n",
      "step 2945, loss is 5.271398544311523\n",
      "(64, 33)\n",
      "step 2946, loss is 5.000357151031494\n",
      "(64, 33)\n",
      "step 2947, loss is 5.110793113708496\n",
      "(64, 33)\n",
      "step 2948, loss is 5.347747802734375\n",
      "(64, 33)\n",
      "step 2949, loss is 5.201783657073975\n",
      "(64, 33)\n",
      "step 2950, loss is 5.249775409698486\n",
      "(64, 33)\n",
      "step 2951, loss is 5.323702812194824\n",
      "(64, 33)\n",
      "step 2952, loss is 5.073631286621094\n",
      "(64, 33)\n",
      "step 2953, loss is 5.089930534362793\n",
      "(64, 33)\n",
      "step 2954, loss is 5.380873203277588\n",
      "(64, 33)\n",
      "step 2955, loss is 5.387564659118652\n",
      "(64, 33)\n",
      "step 2956, loss is 5.126272678375244\n",
      "(64, 33)\n",
      "step 2957, loss is 5.108948230743408\n",
      "(64, 33)\n",
      "step 2958, loss is 5.195860862731934\n",
      "(64, 33)\n",
      "step 2959, loss is 5.061323642730713\n",
      "(64, 33)\n",
      "step 2960, loss is 5.449830532073975\n",
      "(64, 33)\n",
      "step 2961, loss is 5.229076385498047\n",
      "(64, 33)\n",
      "step 2962, loss is 5.358115196228027\n",
      "(64, 33)\n",
      "step 2963, loss is 4.923568248748779\n",
      "(64, 33)\n",
      "step 2964, loss is 5.174616813659668\n",
      "(64, 33)\n",
      "step 2965, loss is 5.090750217437744\n",
      "(64, 33)\n",
      "step 2966, loss is 5.260362148284912\n",
      "(64, 33)\n",
      "step 2967, loss is 5.086249351501465\n",
      "(64, 33)\n",
      "step 2968, loss is 5.077213287353516\n",
      "(64, 33)\n",
      "step 2969, loss is 5.047701358795166\n",
      "(64, 33)\n",
      "step 2970, loss is 5.299161434173584\n",
      "(64, 33)\n",
      "step 2971, loss is 5.381874084472656\n",
      "(64, 33)\n",
      "step 2972, loss is 5.092904567718506\n",
      "(64, 33)\n",
      "step 2973, loss is 5.088597774505615\n",
      "(64, 33)\n",
      "step 2974, loss is 5.15579080581665\n",
      "(64, 33)\n",
      "step 2975, loss is 5.143124580383301\n",
      "(64, 33)\n",
      "step 2976, loss is 5.301836013793945\n",
      "(64, 33)\n",
      "step 2977, loss is 5.133710861206055\n",
      "(64, 33)\n",
      "step 2978, loss is 5.16129732131958\n",
      "(64, 33)\n",
      "step 2979, loss is 5.196667194366455\n",
      "(64, 33)\n",
      "step 2980, loss is 5.136936187744141\n",
      "(64, 33)\n",
      "step 2981, loss is 5.363633155822754\n",
      "(64, 33)\n",
      "step 2982, loss is 5.041924476623535\n",
      "(64, 33)\n",
      "step 2983, loss is 5.259764671325684\n",
      "(64, 33)\n",
      "step 2984, loss is 4.993076324462891\n",
      "(64, 33)\n",
      "step 2985, loss is 5.1143879890441895\n",
      "(64, 33)\n",
      "step 2986, loss is 5.064835548400879\n",
      "(64, 33)\n",
      "step 2987, loss is 5.1618146896362305\n",
      "(64, 33)\n",
      "step 2988, loss is 5.146494388580322\n",
      "(64, 33)\n",
      "step 2989, loss is 5.120792388916016\n",
      "(64, 33)\n",
      "step 2990, loss is 5.259344577789307\n",
      "(64, 33)\n",
      "step 2991, loss is 4.9828057289123535\n",
      "(64, 33)\n",
      "step 2992, loss is 5.2497100830078125\n",
      "(64, 33)\n",
      "step 2993, loss is 4.992122650146484\n",
      "(64, 33)\n",
      "step 2994, loss is 4.961495876312256\n",
      "(64, 33)\n",
      "step 2995, loss is 5.209489822387695\n",
      "(64, 33)\n",
      "step 2996, loss is 5.12546968460083\n",
      "(64, 33)\n",
      "step 2997, loss is 5.175268173217773\n",
      "(64, 33)\n",
      "step 2998, loss is 5.063216686248779\n",
      "(64, 33)\n",
      "step 2999, loss is 5.254793167114258\n",
      "(64, 33)\n",
      "step 3000, loss is 5.212455749511719\n",
      "(64, 33)\n",
      "step 3001, loss is 5.259743690490723\n",
      "(64, 33)\n",
      "step 3002, loss is 5.149807929992676\n",
      "(64, 33)\n",
      "step 3003, loss is 4.949408531188965\n",
      "(64, 33)\n",
      "step 3004, loss is 4.911532402038574\n",
      "(64, 33)\n",
      "step 3005, loss is 5.350020408630371\n",
      "(64, 33)\n",
      "step 3006, loss is 5.2624640464782715\n",
      "(64, 33)\n",
      "step 3007, loss is 5.306252956390381\n",
      "(64, 33)\n",
      "step 3008, loss is 5.113447666168213\n",
      "(64, 33)\n",
      "step 3009, loss is 5.0432047843933105\n",
      "(64, 33)\n",
      "step 3010, loss is 5.026254653930664\n",
      "(64, 33)\n",
      "step 3011, loss is 5.119268417358398\n",
      "(64, 33)\n",
      "step 3012, loss is 5.144341945648193\n",
      "(64, 33)\n",
      "step 3013, loss is 5.139827728271484\n",
      "(64, 33)\n",
      "step 3014, loss is 5.277732849121094\n",
      "(64, 33)\n",
      "step 3015, loss is 5.277822017669678\n",
      "(64, 33)\n",
      "step 3016, loss is 4.937795639038086\n",
      "(64, 33)\n",
      "step 3017, loss is 5.2154059410095215\n",
      "(64, 33)\n",
      "step 3018, loss is 5.268287181854248\n",
      "(64, 33)\n",
      "step 3019, loss is 4.966641426086426\n",
      "(64, 33)\n",
      "step 3020, loss is 5.37326192855835\n",
      "(64, 33)\n",
      "step 3021, loss is 5.154994487762451\n",
      "(64, 33)\n",
      "step 3022, loss is 5.06829309463501\n",
      "(64, 33)\n",
      "step 3023, loss is 5.267594814300537\n",
      "(64, 33)\n",
      "step 3024, loss is 5.408205986022949\n",
      "(64, 33)\n",
      "step 3025, loss is 5.137931823730469\n",
      "(64, 33)\n",
      "step 3026, loss is 5.296268939971924\n",
      "(64, 33)\n",
      "step 3027, loss is 5.19130277633667\n",
      "(64, 33)\n",
      "step 3028, loss is 5.222057342529297\n",
      "(64, 33)\n",
      "step 3029, loss is 5.120305061340332\n",
      "(64, 33)\n",
      "step 3030, loss is 5.192997932434082\n",
      "(64, 33)\n",
      "step 3031, loss is 5.070774555206299\n",
      "(64, 33)\n",
      "step 3032, loss is 5.022290229797363\n",
      "(64, 33)\n",
      "step 3033, loss is 5.2367448806762695\n",
      "(64, 33)\n",
      "step 3034, loss is 5.357906818389893\n",
      "(64, 33)\n",
      "step 3035, loss is 5.0767717361450195\n",
      "(64, 33)\n",
      "step 3036, loss is 4.94287109375\n",
      "(64, 33)\n",
      "step 3037, loss is 5.095681667327881\n",
      "(64, 33)\n",
      "step 3038, loss is 5.001008033752441\n",
      "(64, 33)\n",
      "step 3039, loss is 5.123542785644531\n",
      "(64, 33)\n",
      "step 3040, loss is 5.272759437561035\n",
      "(64, 33)\n",
      "step 3041, loss is 5.277673244476318\n",
      "(64, 33)\n",
      "step 3042, loss is 5.3287034034729\n",
      "(64, 33)\n",
      "step 3043, loss is 5.022250175476074\n",
      "(64, 33)\n",
      "step 3044, loss is 5.2282586097717285\n",
      "(64, 33)\n",
      "step 3045, loss is 5.031533241271973\n",
      "(64, 33)\n",
      "step 3046, loss is 5.086766242980957\n",
      "(64, 33)\n",
      "step 3047, loss is 5.260616779327393\n",
      "(64, 33)\n",
      "step 3048, loss is 5.112135887145996\n",
      "(64, 33)\n",
      "step 3049, loss is 5.226904392242432\n",
      "(64, 33)\n",
      "step 3050, loss is 5.281822681427002\n",
      "(64, 33)\n",
      "step 3051, loss is 5.133192539215088\n",
      "(64, 33)\n",
      "step 3052, loss is 5.140276908874512\n",
      "(64, 33)\n",
      "step 3053, loss is 5.159099578857422\n",
      "(64, 33)\n",
      "step 3054, loss is 5.092833518981934\n",
      "(64, 33)\n",
      "step 3055, loss is 5.056783199310303\n",
      "(64, 33)\n",
      "step 3056, loss is 5.273811340332031\n",
      "(64, 33)\n",
      "step 3057, loss is 5.0254292488098145\n",
      "(64, 33)\n",
      "step 3058, loss is 5.2627482414245605\n",
      "(64, 33)\n",
      "step 3059, loss is 5.142209529876709\n",
      "(64, 33)\n",
      "step 3060, loss is 5.030447006225586\n",
      "(64, 33)\n",
      "step 3061, loss is 5.134034156799316\n",
      "(64, 33)\n",
      "step 3062, loss is 5.21735143661499\n",
      "(64, 33)\n",
      "step 3063, loss is 5.101112365722656\n",
      "(64, 33)\n",
      "step 3064, loss is 4.928263187408447\n",
      "(64, 33)\n",
      "step 3065, loss is 5.078514575958252\n",
      "(64, 33)\n",
      "step 3066, loss is 5.245023727416992\n",
      "(64, 33)\n",
      "step 3067, loss is 5.029928684234619\n",
      "(64, 33)\n",
      "step 3068, loss is 5.138284206390381\n",
      "(64, 33)\n",
      "step 3069, loss is 4.9556565284729\n",
      "(64, 33)\n",
      "step 3070, loss is 5.11214017868042\n",
      "(64, 33)\n",
      "step 3071, loss is 5.149929523468018\n",
      "(64, 33)\n",
      "step 3072, loss is 5.216952800750732\n",
      "(64, 33)\n",
      "step 3073, loss is 5.367279052734375\n",
      "(64, 33)\n",
      "step 3074, loss is 5.261430740356445\n",
      "(64, 33)\n",
      "step 3075, loss is 5.163930416107178\n",
      "(64, 33)\n",
      "step 3076, loss is 5.176094055175781\n",
      "(64, 33)\n",
      "step 3077, loss is 5.010073661804199\n",
      "(64, 33)\n",
      "step 3078, loss is 4.9327497482299805\n",
      "(64, 33)\n",
      "step 3079, loss is 5.248985290527344\n",
      "(64, 33)\n",
      "step 3080, loss is 4.984809875488281\n",
      "(64, 33)\n",
      "step 3081, loss is 5.154486179351807\n",
      "(64, 33)\n",
      "step 3082, loss is 5.086791038513184\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3083, loss is 5.004391193389893\n",
      "(64, 33)\n",
      "step 3084, loss is 5.0588297843933105\n",
      "(64, 33)\n",
      "step 3085, loss is 5.165119647979736\n",
      "(64, 33)\n",
      "step 3086, loss is 5.230823516845703\n",
      "(64, 33)\n",
      "step 3087, loss is 5.25567102432251\n",
      "(64, 33)\n",
      "step 3088, loss is 5.208151817321777\n",
      "(64, 33)\n",
      "step 3089, loss is 4.990882873535156\n",
      "(64, 33)\n",
      "step 3090, loss is 5.085249423980713\n",
      "(64, 33)\n",
      "step 3091, loss is 5.048555850982666\n",
      "(64, 33)\n",
      "step 3092, loss is 5.2697577476501465\n",
      "(64, 33)\n",
      "step 3093, loss is 5.194120407104492\n",
      "(64, 33)\n",
      "step 3094, loss is 5.066251277923584\n",
      "(64, 33)\n",
      "step 3095, loss is 5.200742244720459\n",
      "(64, 33)\n",
      "step 3096, loss is 5.153486728668213\n",
      "(64, 33)\n",
      "step 3097, loss is 5.061495780944824\n",
      "(64, 33)\n",
      "step 3098, loss is 5.1450982093811035\n",
      "(64, 33)\n",
      "step 3099, loss is 5.031988143920898\n",
      "(64, 33)\n",
      "step 3100, loss is 4.986215114593506\n",
      "(64, 33)\n",
      "step 3101, loss is 5.202586650848389\n",
      "(64, 33)\n",
      "step 3102, loss is 5.159521579742432\n",
      "(64, 33)\n",
      "step 3103, loss is 5.224212646484375\n",
      "(64, 33)\n",
      "step 3104, loss is 5.280880451202393\n",
      "(64, 33)\n",
      "step 3105, loss is 5.0449442863464355\n",
      "(64, 33)\n",
      "step 3106, loss is 5.185736179351807\n",
      "(64, 33)\n",
      "step 3107, loss is 5.135810852050781\n",
      "(64, 33)\n",
      "step 3108, loss is 5.303149223327637\n",
      "(64, 33)\n",
      "step 3109, loss is 5.250815391540527\n",
      "(64, 33)\n",
      "step 3110, loss is 5.205885887145996\n",
      "(64, 33)\n",
      "step 3111, loss is 5.076412200927734\n",
      "(64, 33)\n",
      "step 3112, loss is 5.388225078582764\n",
      "(64, 33)\n",
      "step 3113, loss is 5.046724796295166\n",
      "(64, 33)\n",
      "step 3114, loss is 5.236647605895996\n",
      "(64, 33)\n",
      "step 3115, loss is 5.121385097503662\n",
      "(64, 33)\n",
      "step 3116, loss is 5.264721393585205\n",
      "(64, 33)\n",
      "step 3117, loss is 5.093984603881836\n",
      "(64, 33)\n",
      "step 3118, loss is 5.198390960693359\n",
      "(64, 33)\n",
      "step 3119, loss is 5.133003234863281\n",
      "(64, 33)\n",
      "step 3120, loss is 5.196690559387207\n",
      "(64, 33)\n",
      "step 3121, loss is 5.117534637451172\n",
      "(64, 33)\n",
      "step 3122, loss is 5.054591655731201\n",
      "(64, 33)\n",
      "step 3123, loss is 4.954952239990234\n",
      "(64, 33)\n",
      "step 3124, loss is 5.090241432189941\n",
      "(64, 33)\n",
      "step 3125, loss is 5.014126777648926\n",
      "(64, 33)\n",
      "step 3126, loss is 5.103857040405273\n",
      "(64, 33)\n",
      "step 3127, loss is 5.106110572814941\n",
      "(64, 33)\n",
      "step 3128, loss is 5.174786567687988\n",
      "(64, 33)\n",
      "step 3129, loss is 5.299511909484863\n",
      "(64, 33)\n",
      "step 3130, loss is 5.335123062133789\n",
      "(64, 33)\n",
      "step 3131, loss is 4.973062038421631\n",
      "(64, 33)\n",
      "step 3132, loss is 5.134505271911621\n",
      "(64, 33)\n",
      "step 3133, loss is 4.97810173034668\n",
      "(64, 33)\n",
      "step 3134, loss is 4.997105598449707\n",
      "(64, 33)\n",
      "step 3135, loss is 5.316810131072998\n",
      "(64, 33)\n",
      "step 3136, loss is 4.861300945281982\n",
      "(64, 33)\n",
      "step 3137, loss is 4.928603172302246\n",
      "(64, 33)\n",
      "step 3138, loss is 5.244787216186523\n",
      "(64, 33)\n",
      "step 3139, loss is 4.9659295082092285\n",
      "(64, 33)\n",
      "step 3140, loss is 5.095690727233887\n",
      "(64, 33)\n",
      "step 3141, loss is 5.058276653289795\n",
      "(64, 33)\n",
      "step 3142, loss is 5.043025493621826\n",
      "(64, 33)\n",
      "step 3143, loss is 5.180657386779785\n",
      "(64, 33)\n",
      "step 3144, loss is 5.067608833312988\n",
      "(64, 33)\n",
      "step 3145, loss is 4.938602447509766\n",
      "(64, 33)\n",
      "step 3146, loss is 5.256544589996338\n",
      "(64, 33)\n",
      "step 3147, loss is 5.135312557220459\n",
      "(64, 33)\n",
      "step 3148, loss is 5.257519721984863\n",
      "(64, 33)\n",
      "step 3149, loss is 5.303905963897705\n",
      "(64, 33)\n",
      "step 3150, loss is 5.1914496421813965\n",
      "(64, 33)\n",
      "step 3151, loss is 5.094559669494629\n",
      "(64, 33)\n",
      "step 3152, loss is 5.256219387054443\n",
      "(64, 33)\n",
      "step 3153, loss is 5.269801139831543\n",
      "(64, 33)\n",
      "step 3154, loss is 5.027836799621582\n",
      "(64, 33)\n",
      "step 3155, loss is 4.998669147491455\n",
      "(64, 33)\n",
      "step 3156, loss is 5.138552188873291\n",
      "(64, 33)\n",
      "step 3157, loss is 5.317789077758789\n",
      "(64, 33)\n",
      "step 3158, loss is 5.06749963760376\n",
      "(64, 33)\n",
      "step 3159, loss is 5.092769145965576\n",
      "(64, 33)\n",
      "step 3160, loss is 5.161430358886719\n",
      "(64, 33)\n",
      "step 3161, loss is 5.174519062042236\n",
      "(64, 33)\n",
      "step 3162, loss is 5.22609281539917\n",
      "(64, 33)\n",
      "step 3163, loss is 4.9923200607299805\n",
      "(64, 33)\n",
      "step 3164, loss is 5.1191606521606445\n",
      "(64, 33)\n",
      "step 3165, loss is 5.234413146972656\n",
      "(64, 33)\n",
      "step 3166, loss is 5.239446640014648\n",
      "(64, 33)\n",
      "step 3167, loss is 4.9502787590026855\n",
      "(64, 33)\n",
      "step 3168, loss is 5.167198657989502\n",
      "(64, 33)\n",
      "step 3169, loss is 5.3018670082092285\n",
      "(64, 33)\n",
      "step 3170, loss is 5.235586643218994\n",
      "(64, 33)\n",
      "step 3171, loss is 5.178071022033691\n",
      "(64, 33)\n",
      "step 3172, loss is 5.147938251495361\n",
      "(64, 33)\n",
      "step 3173, loss is 5.145088195800781\n",
      "(64, 33)\n",
      "step 3174, loss is 5.082351207733154\n",
      "(64, 33)\n",
      "step 3175, loss is 5.100862503051758\n",
      "(64, 33)\n",
      "step 3176, loss is 5.186768531799316\n",
      "(64, 33)\n",
      "step 3177, loss is 5.216777801513672\n",
      "(64, 33)\n",
      "step 3178, loss is 4.928056240081787\n",
      "(64, 33)\n",
      "step 3179, loss is 5.055034160614014\n",
      "(64, 33)\n",
      "step 3180, loss is 5.283819198608398\n",
      "(64, 33)\n",
      "step 3181, loss is 5.202064037322998\n",
      "(64, 33)\n",
      "step 3182, loss is 4.9412713050842285\n",
      "(64, 33)\n",
      "step 3183, loss is 5.219329833984375\n",
      "(64, 33)\n",
      "step 3184, loss is 5.23073148727417\n",
      "(64, 33)\n",
      "step 3185, loss is 5.062276363372803\n",
      "(64, 33)\n",
      "step 3186, loss is 5.252976894378662\n",
      "(64, 33)\n",
      "step 3187, loss is 5.07963228225708\n",
      "(64, 33)\n",
      "step 3188, loss is 5.030938625335693\n",
      "(64, 33)\n",
      "step 3189, loss is 5.018141269683838\n",
      "(64, 33)\n",
      "step 3190, loss is 5.231545448303223\n",
      "(64, 33)\n",
      "step 3191, loss is 5.067052364349365\n",
      "(64, 33)\n",
      "step 3192, loss is 5.221014499664307\n",
      "(64, 33)\n",
      "step 3193, loss is 5.115098476409912\n",
      "(64, 33)\n",
      "step 3194, loss is 5.2292094230651855\n",
      "(64, 33)\n",
      "step 3195, loss is 5.1592888832092285\n",
      "(64, 33)\n",
      "step 3196, loss is 5.178049564361572\n",
      "(64, 33)\n",
      "step 3197, loss is 5.185039043426514\n",
      "(64, 33)\n",
      "step 3198, loss is 4.994239330291748\n",
      "(64, 33)\n",
      "step 3199, loss is 5.102468013763428\n",
      "(64, 33)\n",
      "step 3200, loss is 5.0025105476379395\n",
      "(64, 33)\n",
      "step 3201, loss is 5.1408185958862305\n",
      "(64, 33)\n",
      "step 3202, loss is 5.197483539581299\n",
      "(64, 33)\n",
      "step 3203, loss is 5.257168769836426\n",
      "(64, 33)\n",
      "step 3204, loss is 5.141908645629883\n",
      "(64, 33)\n",
      "step 3205, loss is 5.137629985809326\n",
      "(64, 33)\n",
      "step 3206, loss is 5.119795322418213\n",
      "(64, 33)\n",
      "step 3207, loss is 5.234644412994385\n",
      "(64, 33)\n",
      "step 3208, loss is 5.15645694732666\n",
      "(64, 33)\n",
      "step 3209, loss is 5.056291103363037\n",
      "(64, 33)\n",
      "step 3210, loss is 4.9931559562683105\n",
      "(64, 33)\n",
      "step 3211, loss is 5.227800369262695\n",
      "(64, 33)\n",
      "step 3212, loss is 5.261420249938965\n",
      "(64, 33)\n",
      "step 3213, loss is 5.112912654876709\n",
      "(64, 33)\n",
      "step 3214, loss is 5.052440643310547\n",
      "(64, 33)\n",
      "step 3215, loss is 5.153054714202881\n",
      "(64, 33)\n",
      "step 3216, loss is 5.098423004150391\n",
      "(64, 33)\n",
      "step 3217, loss is 4.927468776702881\n",
      "(64, 33)\n",
      "step 3218, loss is 5.255701065063477\n",
      "(64, 33)\n",
      "step 3219, loss is 5.1488823890686035\n",
      "(64, 33)\n",
      "step 3220, loss is 5.156148433685303\n",
      "(64, 33)\n",
      "step 3221, loss is 5.129571437835693\n",
      "(64, 33)\n",
      "step 3222, loss is 5.034195899963379\n",
      "(64, 33)\n",
      "step 3223, loss is 5.334202766418457\n",
      "(64, 33)\n",
      "step 3224, loss is 5.272745132446289\n",
      "(64, 33)\n",
      "step 3225, loss is 5.0610880851745605\n",
      "(64, 33)\n",
      "step 3226, loss is 5.09196662902832\n",
      "(64, 33)\n",
      "step 3227, loss is 5.229450702667236\n",
      "(64, 33)\n",
      "step 3228, loss is 5.075686931610107\n",
      "(64, 33)\n",
      "step 3229, loss is 5.191124439239502\n",
      "(64, 33)\n",
      "step 3230, loss is 5.3069987297058105\n",
      "(64, 33)\n",
      "step 3231, loss is 5.14030122756958\n",
      "(64, 33)\n",
      "step 3232, loss is 5.0772624015808105\n",
      "(64, 33)\n",
      "step 3233, loss is 5.175553798675537\n",
      "(64, 33)\n",
      "step 3234, loss is 5.151082515716553\n",
      "(64, 33)\n",
      "step 3235, loss is 5.145957946777344\n",
      "(64, 33)\n",
      "step 3236, loss is 5.268577575683594\n",
      "(64, 33)\n",
      "step 3237, loss is 5.234501361846924\n",
      "(64, 33)\n",
      "step 3238, loss is 4.983527660369873\n",
      "(64, 33)\n",
      "step 3239, loss is 4.937063217163086\n",
      "(64, 33)\n",
      "step 3240, loss is 5.216376304626465\n",
      "(64, 33)\n",
      "step 3241, loss is 5.291075706481934\n",
      "(64, 33)\n",
      "step 3242, loss is 5.192558765411377\n",
      "(64, 33)\n",
      "step 3243, loss is 5.318290710449219\n",
      "(64, 33)\n",
      "step 3244, loss is 5.135776042938232\n",
      "(64, 33)\n",
      "step 3245, loss is 5.004964351654053\n",
      "(64, 33)\n",
      "step 3246, loss is 5.151745319366455\n",
      "(64, 33)\n",
      "step 3247, loss is 5.273827075958252\n",
      "(64, 33)\n",
      "step 3248, loss is 5.18646240234375\n",
      "(64, 33)\n",
      "step 3249, loss is 5.0710344314575195\n",
      "(64, 33)\n",
      "step 3250, loss is 5.141391277313232\n",
      "(64, 33)\n",
      "step 3251, loss is 5.108282566070557\n",
      "(64, 33)\n",
      "step 3252, loss is 5.128792762756348\n",
      "(64, 33)\n",
      "step 3253, loss is 5.221554279327393\n",
      "(64, 33)\n",
      "step 3254, loss is 5.242807865142822\n",
      "(64, 33)\n",
      "step 3255, loss is 5.0697712898254395\n",
      "(64, 33)\n",
      "step 3256, loss is 5.136571407318115\n",
      "(64, 33)\n",
      "step 3257, loss is 5.06381368637085\n",
      "(64, 33)\n",
      "step 3258, loss is 4.999273300170898\n",
      "(64, 33)\n",
      "step 3259, loss is 5.137950420379639\n",
      "(64, 33)\n",
      "step 3260, loss is 5.010670185089111\n",
      "(64, 33)\n",
      "step 3261, loss is 5.069857597351074\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3262, loss is 5.056685924530029\n",
      "(64, 33)\n",
      "step 3263, loss is 5.200806617736816\n",
      "(64, 33)\n",
      "step 3264, loss is 5.361939907073975\n",
      "(64, 33)\n",
      "step 3265, loss is 5.194767951965332\n",
      "(64, 33)\n",
      "step 3266, loss is 5.265400409698486\n",
      "(64, 33)\n",
      "step 3267, loss is 5.184977054595947\n",
      "(64, 33)\n",
      "step 3268, loss is 4.993966579437256\n",
      "(64, 33)\n",
      "step 3269, loss is 5.238385200500488\n",
      "(64, 33)\n",
      "step 3270, loss is 5.094566345214844\n",
      "(64, 33)\n",
      "step 3271, loss is 5.0957417488098145\n",
      "(64, 33)\n",
      "step 3272, loss is 5.258090496063232\n",
      "(64, 33)\n",
      "step 3273, loss is 5.155817985534668\n",
      "(64, 33)\n",
      "step 3274, loss is 5.136474609375\n",
      "(64, 33)\n",
      "step 3275, loss is 5.234849452972412\n",
      "(64, 33)\n",
      "step 3276, loss is 5.117377281188965\n",
      "(64, 33)\n",
      "step 3277, loss is 5.047561168670654\n",
      "(64, 33)\n",
      "step 3278, loss is 5.136618614196777\n",
      "(64, 33)\n",
      "step 3279, loss is 5.132959842681885\n",
      "(64, 33)\n",
      "step 3280, loss is 5.143132209777832\n",
      "(64, 33)\n",
      "step 3281, loss is 5.243160247802734\n",
      "(64, 33)\n",
      "step 3282, loss is 5.275357723236084\n",
      "(64, 33)\n",
      "step 3283, loss is 5.120702743530273\n",
      "(64, 33)\n",
      "step 3284, loss is 5.224453449249268\n",
      "(64, 33)\n",
      "step 3285, loss is 4.827969551086426\n",
      "(64, 33)\n",
      "step 3286, loss is 5.0493669509887695\n",
      "(64, 33)\n",
      "step 3287, loss is 5.034280300140381\n",
      "(64, 33)\n",
      "step 3288, loss is 5.088271141052246\n",
      "(64, 33)\n",
      "step 3289, loss is 5.259013652801514\n",
      "(64, 33)\n",
      "step 3290, loss is 5.244718551635742\n",
      "(64, 33)\n",
      "step 3291, loss is 5.303106307983398\n",
      "(64, 33)\n",
      "step 3292, loss is 5.157716274261475\n",
      "(64, 33)\n",
      "step 3293, loss is 5.1262617111206055\n",
      "(64, 33)\n",
      "step 3294, loss is 5.151679992675781\n",
      "(64, 33)\n",
      "step 3295, loss is 5.265333652496338\n",
      "(64, 33)\n",
      "step 3296, loss is 5.022725582122803\n",
      "(64, 33)\n",
      "step 3297, loss is 5.167076110839844\n",
      "(64, 33)\n",
      "step 3298, loss is 5.293577194213867\n",
      "(64, 33)\n",
      "step 3299, loss is 5.189694404602051\n",
      "(64, 33)\n",
      "step 3300, loss is 4.978447437286377\n",
      "(64, 33)\n",
      "step 3301, loss is 5.03668737411499\n",
      "(64, 33)\n",
      "step 3302, loss is 5.17617130279541\n",
      "(64, 33)\n",
      "step 3303, loss is 5.013737201690674\n",
      "(64, 33)\n",
      "step 3304, loss is 5.1078996658325195\n",
      "(64, 33)\n",
      "step 3305, loss is 4.988950252532959\n",
      "(64, 33)\n",
      "step 3306, loss is 5.12073278427124\n",
      "(64, 33)\n",
      "step 3307, loss is 5.018828392028809\n",
      "(64, 33)\n",
      "step 3308, loss is 5.136393070220947\n",
      "(64, 33)\n",
      "step 3309, loss is 5.0255889892578125\n",
      "(64, 33)\n",
      "step 3310, loss is 5.080225467681885\n",
      "(64, 33)\n",
      "step 3311, loss is 5.005913257598877\n",
      "(64, 33)\n",
      "step 3312, loss is 5.182295799255371\n",
      "(64, 33)\n",
      "step 3313, loss is 5.259345054626465\n",
      "(64, 33)\n",
      "step 3314, loss is 5.1590752601623535\n",
      "(64, 33)\n",
      "step 3315, loss is 5.066545009613037\n",
      "(64, 33)\n",
      "step 3316, loss is 4.9858927726745605\n",
      "(64, 33)\n",
      "step 3317, loss is 5.23090934753418\n",
      "(64, 33)\n",
      "step 3318, loss is 5.054859638214111\n",
      "(64, 33)\n",
      "step 3319, loss is 5.128484725952148\n",
      "(64, 33)\n",
      "step 3320, loss is 4.9557061195373535\n",
      "(64, 33)\n",
      "step 3321, loss is 5.313229560852051\n",
      "(64, 33)\n",
      "step 3322, loss is 5.290283203125\n",
      "(64, 33)\n",
      "step 3323, loss is 5.325425624847412\n",
      "(64, 33)\n",
      "step 3324, loss is 5.0673909187316895\n",
      "(64, 33)\n",
      "step 3325, loss is 5.261318206787109\n",
      "(64, 33)\n",
      "step 3326, loss is 5.0168938636779785\n",
      "(64, 33)\n",
      "step 3327, loss is 5.1766815185546875\n",
      "(64, 33)\n",
      "step 3328, loss is 5.3641438484191895\n",
      "(64, 33)\n",
      "step 3329, loss is 5.035043239593506\n",
      "(64, 33)\n",
      "step 3330, loss is 5.094666957855225\n",
      "(64, 33)\n",
      "step 3331, loss is 5.2483930587768555\n",
      "(64, 33)\n",
      "step 3332, loss is 5.050039291381836\n",
      "(64, 33)\n",
      "step 3333, loss is 5.15186071395874\n",
      "(64, 33)\n",
      "step 3334, loss is 5.013609886169434\n",
      "(64, 33)\n",
      "step 3335, loss is 5.10715389251709\n",
      "(64, 33)\n",
      "step 3336, loss is 5.196849822998047\n",
      "(64, 33)\n",
      "step 3337, loss is 5.340848445892334\n",
      "(64, 33)\n",
      "step 3338, loss is 5.202225208282471\n",
      "(64, 33)\n",
      "step 3339, loss is 5.265329837799072\n",
      "(64, 33)\n",
      "step 3340, loss is 5.200113773345947\n",
      "(64, 33)\n",
      "step 3341, loss is 5.203108787536621\n",
      "(64, 33)\n",
      "step 3342, loss is 5.293168544769287\n",
      "(64, 33)\n",
      "step 3343, loss is 5.168567180633545\n",
      "(64, 33)\n",
      "step 3344, loss is 5.091297149658203\n",
      "(64, 33)\n",
      "step 3345, loss is 5.122061252593994\n",
      "(64, 33)\n",
      "step 3346, loss is 5.124410152435303\n",
      "(64, 33)\n",
      "step 3347, loss is 5.25198221206665\n",
      "(64, 33)\n",
      "step 3348, loss is 5.138352394104004\n",
      "(64, 33)\n",
      "step 3349, loss is 5.238434314727783\n",
      "(64, 33)\n",
      "step 3350, loss is 5.159901142120361\n",
      "(64, 33)\n",
      "step 3351, loss is 5.116902828216553\n",
      "(64, 33)\n",
      "step 3352, loss is 5.220518589019775\n",
      "(64, 33)\n",
      "step 3353, loss is 5.232471942901611\n",
      "(64, 33)\n",
      "step 3354, loss is 5.010745525360107\n",
      "(64, 33)\n",
      "step 3355, loss is 5.171854496002197\n",
      "(64, 33)\n",
      "step 3356, loss is 5.015113830566406\n",
      "(64, 33)\n",
      "step 3357, loss is 5.219436168670654\n",
      "(64, 33)\n",
      "step 3358, loss is 5.064846515655518\n",
      "(64, 33)\n",
      "step 3359, loss is 5.107664585113525\n",
      "(64, 33)\n",
      "step 3360, loss is 5.010909557342529\n",
      "(64, 33)\n",
      "step 3361, loss is 5.130936622619629\n",
      "(64, 33)\n",
      "step 3362, loss is 4.972702980041504\n",
      "(64, 33)\n",
      "step 3363, loss is 5.245356559753418\n",
      "(64, 33)\n",
      "step 3364, loss is 5.046528339385986\n",
      "(64, 33)\n",
      "step 3365, loss is 4.997964382171631\n",
      "(64, 33)\n",
      "step 3366, loss is 5.197044849395752\n",
      "(64, 33)\n",
      "step 3367, loss is 4.866247653961182\n",
      "(64, 33)\n",
      "step 3368, loss is 4.993933200836182\n",
      "(64, 33)\n",
      "step 3369, loss is 5.066452503204346\n",
      "(64, 33)\n",
      "step 3370, loss is 5.184669017791748\n",
      "(64, 33)\n",
      "step 3371, loss is 5.090843677520752\n",
      "(64, 33)\n",
      "step 3372, loss is 5.0801544189453125\n",
      "(64, 33)\n",
      "step 3373, loss is 4.917500972747803\n",
      "(64, 33)\n",
      "step 3374, loss is 5.042499542236328\n",
      "(64, 33)\n",
      "step 3375, loss is 5.21659517288208\n",
      "(64, 33)\n",
      "step 3376, loss is 5.0400872230529785\n",
      "(64, 33)\n",
      "step 3377, loss is 5.15103816986084\n",
      "(64, 33)\n",
      "step 3378, loss is 4.982825756072998\n",
      "(64, 33)\n",
      "step 3379, loss is 4.983832836151123\n",
      "(64, 33)\n",
      "step 3380, loss is 5.369496822357178\n",
      "(64, 33)\n",
      "step 3381, loss is 5.142804145812988\n",
      "(64, 33)\n",
      "step 3382, loss is 5.214914321899414\n",
      "(64, 33)\n",
      "step 3383, loss is 5.0873308181762695\n",
      "(64, 33)\n",
      "step 3384, loss is 5.095529079437256\n",
      "(64, 33)\n",
      "step 3385, loss is 5.212827205657959\n",
      "(64, 33)\n",
      "step 3386, loss is 5.1768646240234375\n",
      "(64, 33)\n",
      "step 3387, loss is 5.205228805541992\n",
      "(64, 33)\n",
      "step 3388, loss is 5.117863655090332\n",
      "(64, 33)\n",
      "step 3389, loss is 5.124874114990234\n",
      "(64, 33)\n",
      "step 3390, loss is 5.110285758972168\n",
      "(64, 33)\n",
      "step 3391, loss is 5.210470676422119\n",
      "(64, 33)\n",
      "step 3392, loss is 5.283842086791992\n",
      "(64, 33)\n",
      "step 3393, loss is 5.195668697357178\n",
      "(64, 33)\n",
      "step 3394, loss is 5.086284637451172\n",
      "(64, 33)\n",
      "step 3395, loss is 5.1879754066467285\n",
      "(64, 33)\n",
      "step 3396, loss is 5.30533504486084\n",
      "(64, 33)\n",
      "step 3397, loss is 5.320775032043457\n",
      "(64, 33)\n",
      "step 3398, loss is 5.179508686065674\n",
      "(64, 33)\n",
      "step 3399, loss is 5.201423645019531\n",
      "(64, 33)\n",
      "step 3400, loss is 5.0806884765625\n",
      "(64, 33)\n",
      "step 3401, loss is 5.101130485534668\n",
      "(64, 33)\n",
      "step 3402, loss is 5.1607160568237305\n",
      "(64, 33)\n",
      "step 3403, loss is 5.103513717651367\n",
      "(64, 33)\n",
      "step 3404, loss is 5.315876007080078\n",
      "(64, 33)\n",
      "step 3405, loss is 4.979574680328369\n",
      "(64, 33)\n",
      "step 3406, loss is 5.316217422485352\n",
      "(64, 33)\n",
      "step 3407, loss is 4.990698337554932\n",
      "(64, 33)\n",
      "step 3408, loss is 5.249986171722412\n",
      "(64, 33)\n",
      "step 3409, loss is 4.986929416656494\n",
      "(64, 33)\n",
      "step 3410, loss is 5.027839660644531\n",
      "(64, 33)\n",
      "step 3411, loss is 5.132503986358643\n",
      "(64, 33)\n",
      "step 3412, loss is 5.119340896606445\n",
      "(64, 33)\n",
      "step 3413, loss is 5.256036281585693\n",
      "(64, 33)\n",
      "step 3414, loss is 5.20051908493042\n",
      "(64, 33)\n",
      "step 3415, loss is 5.288114547729492\n",
      "(64, 33)\n",
      "step 3416, loss is 5.071076393127441\n",
      "(64, 33)\n",
      "step 3417, loss is 5.21239709854126\n",
      "(64, 33)\n",
      "step 3418, loss is 5.052628517150879\n",
      "(64, 33)\n",
      "step 3419, loss is 5.093733310699463\n",
      "(64, 33)\n",
      "step 3420, loss is 5.160774230957031\n",
      "(64, 33)\n",
      "step 3421, loss is 5.163204193115234\n",
      "(64, 33)\n",
      "step 3422, loss is 5.287193298339844\n",
      "(64, 33)\n",
      "step 3423, loss is 5.241495609283447\n",
      "(64, 33)\n",
      "step 3424, loss is 5.021084785461426\n",
      "(64, 33)\n",
      "step 3425, loss is 5.26464319229126\n",
      "(64, 33)\n",
      "step 3426, loss is 5.161757469177246\n",
      "(64, 33)\n",
      "step 3427, loss is 5.133794784545898\n",
      "(64, 33)\n",
      "step 3428, loss is 5.052056312561035\n",
      "(64, 33)\n",
      "step 3429, loss is 5.258862018585205\n",
      "(64, 33)\n",
      "step 3430, loss is 5.272794246673584\n",
      "(64, 33)\n",
      "step 3431, loss is 5.307404041290283\n",
      "(64, 33)\n",
      "step 3432, loss is 5.125443458557129\n",
      "(64, 33)\n",
      "step 3433, loss is 5.024364471435547\n",
      "(64, 33)\n",
      "step 3434, loss is 5.054397106170654\n",
      "(64, 33)\n",
      "step 3435, loss is 5.186465263366699\n",
      "(64, 33)\n",
      "step 3436, loss is 5.339130401611328\n",
      "(64, 33)\n",
      "step 3437, loss is 4.925469875335693\n",
      "(64, 33)\n",
      "step 3438, loss is 5.192821979522705\n",
      "(64, 33)\n",
      "step 3439, loss is 5.2345290184021\n",
      "(64, 33)\n",
      "step 3440, loss is 5.21750545501709\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3441, loss is 4.993902206420898\n",
      "(64, 33)\n",
      "step 3442, loss is 5.287646293640137\n",
      "(64, 33)\n",
      "step 3443, loss is 5.376854419708252\n",
      "(64, 33)\n",
      "step 3444, loss is 5.315829277038574\n",
      "(64, 33)\n",
      "step 3445, loss is 5.251870632171631\n",
      "(64, 33)\n",
      "step 3446, loss is 5.128911018371582\n",
      "(64, 33)\n",
      "step 3447, loss is 5.250771999359131\n",
      "(64, 33)\n",
      "step 3448, loss is 5.218415260314941\n",
      "(64, 33)\n",
      "step 3449, loss is 5.104486465454102\n",
      "(64, 33)\n",
      "step 3450, loss is 4.7472662925720215\n",
      "(64, 33)\n",
      "step 3451, loss is 5.089770793914795\n",
      "(64, 33)\n",
      "step 3452, loss is 5.145985126495361\n",
      "(64, 33)\n",
      "step 3453, loss is 5.166323184967041\n",
      "(64, 33)\n",
      "step 3454, loss is 4.898614406585693\n",
      "(64, 33)\n",
      "step 3455, loss is 5.067924499511719\n",
      "(64, 33)\n",
      "step 3456, loss is 5.145412921905518\n",
      "(64, 33)\n",
      "step 3457, loss is 5.328410625457764\n",
      "(64, 33)\n",
      "step 3458, loss is 5.1120781898498535\n",
      "(64, 33)\n",
      "step 3459, loss is 5.173495292663574\n",
      "(64, 33)\n",
      "step 3460, loss is 5.045575141906738\n",
      "(64, 33)\n",
      "step 3461, loss is 5.0275678634643555\n",
      "(64, 33)\n",
      "step 3462, loss is 5.140438079833984\n",
      "(64, 33)\n",
      "step 3463, loss is 5.072070121765137\n",
      "(64, 33)\n",
      "step 3464, loss is 5.018521785736084\n",
      "(64, 33)\n",
      "step 3465, loss is 5.179940700531006\n",
      "(64, 33)\n",
      "step 3466, loss is 5.044774055480957\n",
      "(64, 33)\n",
      "step 3467, loss is 5.130259037017822\n",
      "(64, 33)\n",
      "step 3468, loss is 4.987303733825684\n",
      "(64, 33)\n",
      "step 3469, loss is 5.158624649047852\n",
      "(64, 33)\n",
      "step 3470, loss is 4.916821002960205\n",
      "(64, 33)\n",
      "step 3471, loss is 5.168747425079346\n",
      "(64, 33)\n",
      "step 3472, loss is 5.052755832672119\n",
      "(64, 33)\n",
      "step 3473, loss is 5.29525089263916\n",
      "(64, 33)\n",
      "step 3474, loss is 4.980733394622803\n",
      "(64, 33)\n",
      "step 3475, loss is 5.009049892425537\n",
      "(64, 33)\n",
      "step 3476, loss is 5.1947550773620605\n",
      "(64, 33)\n",
      "step 3477, loss is 5.221599102020264\n",
      "(64, 33)\n",
      "step 3478, loss is 4.974739074707031\n",
      "(64, 33)\n",
      "step 3479, loss is 5.026093006134033\n",
      "(64, 33)\n",
      "step 3480, loss is 5.179671764373779\n",
      "(64, 33)\n",
      "step 3481, loss is 5.211797714233398\n",
      "(64, 33)\n",
      "step 3482, loss is 5.066986560821533\n",
      "(64, 33)\n",
      "step 3483, loss is 5.202517986297607\n",
      "(64, 33)\n",
      "step 3484, loss is 5.1889119148254395\n",
      "(64, 33)\n",
      "step 3485, loss is 5.138634204864502\n",
      "(64, 33)\n",
      "step 3486, loss is 5.078101634979248\n",
      "(64, 33)\n",
      "step 3487, loss is 5.165122985839844\n",
      "(64, 33)\n",
      "step 3488, loss is 5.214262008666992\n",
      "(64, 33)\n",
      "step 3489, loss is 5.093439102172852\n",
      "(64, 33)\n",
      "step 3490, loss is 5.1270833015441895\n",
      "(64, 33)\n",
      "step 3491, loss is 5.0916972160339355\n",
      "(64, 33)\n",
      "step 3492, loss is 5.239490985870361\n",
      "(64, 33)\n",
      "step 3493, loss is 5.157131671905518\n",
      "(64, 33)\n",
      "step 3494, loss is 4.945086479187012\n",
      "(64, 33)\n",
      "step 3495, loss is 5.1054863929748535\n",
      "(64, 33)\n",
      "step 3496, loss is 5.424795150756836\n",
      "(64, 33)\n",
      "step 3497, loss is 5.160425662994385\n",
      "(64, 33)\n",
      "step 3498, loss is 5.132661819458008\n",
      "(64, 33)\n",
      "step 3499, loss is 5.129803657531738\n",
      "(64, 33)\n",
      "step 3500, loss is 5.042048454284668\n",
      "(64, 33)\n",
      "step 3501, loss is 5.194336891174316\n",
      "(64, 33)\n",
      "step 3502, loss is 5.139391899108887\n",
      "(64, 33)\n",
      "step 3503, loss is 5.042436122894287\n",
      "(64, 33)\n",
      "step 3504, loss is 5.021140098571777\n",
      "(64, 33)\n",
      "step 3505, loss is 5.1860504150390625\n",
      "(64, 33)\n",
      "step 3506, loss is 5.058234214782715\n",
      "(64, 33)\n",
      "step 3507, loss is 5.090600490570068\n",
      "(64, 33)\n",
      "step 3508, loss is 5.088858127593994\n",
      "(64, 33)\n",
      "step 3509, loss is 5.019905090332031\n",
      "(64, 33)\n",
      "step 3510, loss is 5.082636833190918\n",
      "(64, 33)\n",
      "step 3511, loss is 5.151430130004883\n",
      "(64, 33)\n",
      "step 3512, loss is 5.180984020233154\n",
      "(64, 33)\n",
      "step 3513, loss is 5.1359405517578125\n",
      "(64, 33)\n",
      "step 3514, loss is 5.026805400848389\n",
      "(64, 33)\n",
      "step 3515, loss is 5.1518330574035645\n",
      "(64, 33)\n",
      "step 3516, loss is 5.154582500457764\n",
      "(64, 33)\n",
      "step 3517, loss is 5.175553798675537\n",
      "(64, 33)\n",
      "step 3518, loss is 4.989760398864746\n",
      "(64, 33)\n",
      "step 3519, loss is 5.188486099243164\n",
      "(64, 33)\n",
      "step 3520, loss is 5.069097518920898\n",
      "(64, 33)\n",
      "step 3521, loss is 5.163825988769531\n",
      "(64, 33)\n",
      "step 3522, loss is 5.314925670623779\n",
      "(64, 33)\n",
      "step 3523, loss is 5.048656940460205\n",
      "(64, 33)\n",
      "step 3524, loss is 5.246504306793213\n",
      "(64, 33)\n",
      "step 3525, loss is 5.137822151184082\n",
      "(64, 33)\n",
      "step 3526, loss is 4.8814921379089355\n",
      "(64, 33)\n",
      "step 3527, loss is 5.097335338592529\n",
      "(64, 33)\n",
      "step 3528, loss is 5.244937419891357\n",
      "(64, 33)\n",
      "step 3529, loss is 5.057213306427002\n",
      "(64, 33)\n",
      "step 3530, loss is 5.018670558929443\n",
      "(64, 33)\n",
      "step 3531, loss is 5.15415620803833\n",
      "(64, 33)\n",
      "step 3532, loss is 5.129603385925293\n",
      "(64, 33)\n",
      "step 3533, loss is 5.233978748321533\n",
      "(64, 33)\n",
      "step 3534, loss is 4.950054168701172\n",
      "(64, 33)\n",
      "step 3535, loss is 5.220924377441406\n",
      "(64, 33)\n",
      "step 3536, loss is 5.020541667938232\n",
      "(64, 33)\n",
      "step 3537, loss is 5.117782115936279\n",
      "(64, 33)\n",
      "step 3538, loss is 5.218564987182617\n",
      "(64, 33)\n",
      "step 3539, loss is 5.1461567878723145\n",
      "(64, 33)\n",
      "step 3540, loss is 5.11131477355957\n",
      "(64, 33)\n",
      "step 3541, loss is 5.093320846557617\n",
      "(64, 33)\n",
      "step 3542, loss is 4.9181694984436035\n",
      "(64, 33)\n",
      "step 3543, loss is 5.021526336669922\n",
      "(64, 33)\n",
      "step 3544, loss is 5.212825298309326\n",
      "(64, 33)\n",
      "step 3545, loss is 5.1961846351623535\n",
      "(64, 33)\n",
      "step 3546, loss is 5.0937275886535645\n",
      "(64, 33)\n",
      "step 3547, loss is 5.052034378051758\n",
      "(64, 33)\n",
      "step 3548, loss is 5.115634441375732\n",
      "(64, 33)\n",
      "step 3549, loss is 5.1777448654174805\n",
      "(64, 33)\n",
      "step 3550, loss is 5.139366149902344\n",
      "(64, 33)\n",
      "step 3551, loss is 4.981714248657227\n",
      "(64, 33)\n",
      "step 3552, loss is 5.107822895050049\n",
      "(64, 33)\n",
      "step 3553, loss is 5.2161641120910645\n",
      "(64, 33)\n",
      "step 3554, loss is 5.062266826629639\n",
      "(64, 33)\n",
      "step 3555, loss is 5.164937973022461\n",
      "(64, 33)\n",
      "step 3556, loss is 4.918793678283691\n",
      "(64, 33)\n",
      "step 3557, loss is 5.068029880523682\n",
      "(64, 33)\n",
      "step 3558, loss is 5.134477138519287\n",
      "(64, 33)\n",
      "step 3559, loss is 5.201914310455322\n",
      "(64, 33)\n",
      "step 3560, loss is 5.082510948181152\n",
      "(64, 33)\n",
      "step 3561, loss is 5.173028945922852\n",
      "(64, 33)\n",
      "step 3562, loss is 5.253514289855957\n",
      "(64, 33)\n",
      "step 3563, loss is 5.339749336242676\n",
      "(64, 33)\n",
      "step 3564, loss is 5.2180328369140625\n",
      "(64, 33)\n",
      "step 3565, loss is 5.129738807678223\n",
      "(64, 33)\n",
      "step 3566, loss is 5.053241729736328\n",
      "(64, 33)\n",
      "step 3567, loss is 5.239706993103027\n",
      "(64, 33)\n",
      "step 3568, loss is 5.111822605133057\n",
      "(64, 33)\n",
      "step 3569, loss is 4.906684398651123\n",
      "(64, 33)\n",
      "step 3570, loss is 5.192989349365234\n",
      "(64, 33)\n",
      "step 3571, loss is 5.215206146240234\n",
      "(64, 33)\n",
      "step 3572, loss is 4.901871204376221\n",
      "(64, 33)\n",
      "step 3573, loss is 5.325350284576416\n",
      "(64, 33)\n",
      "step 3574, loss is 5.126485824584961\n",
      "(64, 33)\n",
      "step 3575, loss is 5.428106307983398\n",
      "(64, 33)\n",
      "step 3576, loss is 4.951459884643555\n",
      "(64, 33)\n",
      "step 3577, loss is 5.106095314025879\n",
      "(64, 33)\n",
      "step 3578, loss is 5.169297218322754\n",
      "(64, 33)\n",
      "step 3579, loss is 5.105238437652588\n",
      "(64, 33)\n",
      "step 3580, loss is 5.149503707885742\n",
      "(64, 33)\n",
      "step 3581, loss is 5.251242160797119\n",
      "(64, 33)\n",
      "step 3582, loss is 5.106105804443359\n",
      "(64, 33)\n",
      "step 3583, loss is 5.342229843139648\n",
      "(64, 33)\n",
      "step 3584, loss is 5.069535732269287\n",
      "(64, 33)\n",
      "step 3585, loss is 5.10066032409668\n",
      "(64, 33)\n",
      "step 3586, loss is 5.017601013183594\n",
      "(64, 33)\n",
      "step 3587, loss is 5.053481578826904\n",
      "(64, 33)\n",
      "step 3588, loss is 5.29762601852417\n",
      "(64, 33)\n",
      "step 3589, loss is 5.117334842681885\n",
      "(64, 33)\n",
      "step 3590, loss is 5.271023750305176\n",
      "(64, 33)\n",
      "step 3591, loss is 4.9632368087768555\n",
      "(64, 33)\n",
      "step 3592, loss is 5.189834117889404\n",
      "(64, 33)\n",
      "step 3593, loss is 5.120993614196777\n",
      "(64, 33)\n",
      "step 3594, loss is 5.071112632751465\n",
      "(64, 33)\n",
      "step 3595, loss is 5.208834171295166\n",
      "(64, 33)\n",
      "step 3596, loss is 5.18581485748291\n",
      "(64, 33)\n",
      "step 3597, loss is 5.3941850662231445\n",
      "(64, 33)\n",
      "step 3598, loss is 5.197142601013184\n",
      "(64, 33)\n",
      "step 3599, loss is 5.302504062652588\n",
      "(64, 33)\n",
      "step 3600, loss is 5.033870697021484\n",
      "(64, 33)\n",
      "step 3601, loss is 5.038506984710693\n",
      "(64, 33)\n",
      "step 3602, loss is 5.079813480377197\n",
      "(64, 33)\n",
      "step 3603, loss is 5.160724639892578\n",
      "(64, 33)\n",
      "step 3604, loss is 5.081537246704102\n",
      "(64, 33)\n",
      "step 3605, loss is 5.117863655090332\n",
      "(64, 33)\n",
      "step 3606, loss is 5.212068557739258\n",
      "(64, 33)\n",
      "step 3607, loss is 4.920862197875977\n",
      "(64, 33)\n",
      "step 3608, loss is 5.0653839111328125\n",
      "(64, 33)\n",
      "step 3609, loss is 5.256031513214111\n",
      "(64, 33)\n",
      "step 3610, loss is 5.057149887084961\n",
      "(64, 33)\n",
      "step 3611, loss is 5.1288065910339355\n",
      "(64, 33)\n",
      "step 3612, loss is 5.081490516662598\n",
      "(64, 33)\n",
      "step 3613, loss is 5.310351848602295\n",
      "(64, 33)\n",
      "step 3614, loss is 5.127053260803223\n",
      "(64, 33)\n",
      "step 3615, loss is 5.048248767852783\n",
      "(64, 33)\n",
      "step 3616, loss is 5.252974033355713\n",
      "(64, 33)\n",
      "step 3617, loss is 5.120660781860352\n",
      "(64, 33)\n",
      "step 3618, loss is 5.071756839752197\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3619, loss is 5.167394161224365\n",
      "(64, 33)\n",
      "step 3620, loss is 5.18550443649292\n",
      "(64, 33)\n",
      "step 3621, loss is 5.202569484710693\n",
      "(64, 33)\n",
      "step 3622, loss is 5.239670276641846\n",
      "(64, 33)\n",
      "step 3623, loss is 5.188804626464844\n",
      "(64, 33)\n",
      "step 3624, loss is 5.21330451965332\n",
      "(64, 33)\n",
      "step 3625, loss is 5.080264568328857\n",
      "(64, 33)\n",
      "step 3626, loss is 5.058226108551025\n",
      "(64, 33)\n",
      "step 3627, loss is 5.089431285858154\n",
      "(64, 33)\n",
      "step 3628, loss is 5.025821685791016\n",
      "(64, 33)\n",
      "step 3629, loss is 5.004519462585449\n",
      "(64, 33)\n",
      "step 3630, loss is 5.0347065925598145\n",
      "(64, 33)\n",
      "step 3631, loss is 5.211129188537598\n",
      "(64, 33)\n",
      "step 3632, loss is 5.048280239105225\n",
      "(64, 33)\n",
      "step 3633, loss is 5.085395812988281\n",
      "(64, 33)\n",
      "step 3634, loss is 5.27044153213501\n",
      "(64, 33)\n",
      "step 3635, loss is 5.074056148529053\n",
      "(64, 33)\n",
      "step 3636, loss is 5.192085266113281\n",
      "(64, 33)\n",
      "step 3637, loss is 5.150818824768066\n",
      "(64, 33)\n",
      "step 3638, loss is 5.35862922668457\n",
      "(64, 33)\n",
      "step 3639, loss is 5.070289611816406\n",
      "(64, 33)\n",
      "step 3640, loss is 5.072393417358398\n",
      "(64, 33)\n",
      "step 3641, loss is 4.982619762420654\n",
      "(64, 33)\n",
      "step 3642, loss is 4.986690521240234\n",
      "(64, 33)\n",
      "step 3643, loss is 5.12007474899292\n",
      "(64, 33)\n",
      "step 3644, loss is 5.061402320861816\n",
      "(64, 33)\n",
      "step 3645, loss is 5.01892614364624\n",
      "(64, 33)\n",
      "step 3646, loss is 5.087145805358887\n",
      "(64, 33)\n",
      "step 3647, loss is 5.1084794998168945\n",
      "(64, 33)\n",
      "step 3648, loss is 5.130715370178223\n",
      "(64, 33)\n",
      "step 3649, loss is 5.215272426605225\n",
      "(64, 33)\n",
      "step 3650, loss is 5.134596347808838\n",
      "(64, 33)\n",
      "step 3651, loss is 5.009763717651367\n",
      "(64, 33)\n",
      "step 3652, loss is 5.079788684844971\n",
      "(64, 33)\n",
      "step 3653, loss is 5.128955841064453\n",
      "(64, 33)\n",
      "step 3654, loss is 5.010613918304443\n",
      "(64, 33)\n",
      "step 3655, loss is 5.252873420715332\n",
      "(64, 33)\n",
      "step 3656, loss is 5.26324462890625\n",
      "(64, 33)\n",
      "step 3657, loss is 5.048503875732422\n",
      "(64, 33)\n",
      "step 3658, loss is 4.932565689086914\n",
      "(64, 33)\n",
      "step 3659, loss is 5.3053507804870605\n",
      "(64, 33)\n",
      "step 3660, loss is 5.110103607177734\n",
      "(64, 33)\n",
      "step 3661, loss is 5.238053321838379\n",
      "(64, 33)\n",
      "step 3662, loss is 5.0244832038879395\n",
      "(64, 33)\n",
      "step 3663, loss is 5.2069292068481445\n",
      "(64, 33)\n",
      "step 3664, loss is 5.097673416137695\n",
      "(64, 33)\n",
      "step 3665, loss is 5.028522968292236\n",
      "(64, 33)\n",
      "step 3666, loss is 4.895195960998535\n",
      "(64, 33)\n",
      "step 3667, loss is 5.238690376281738\n",
      "(64, 33)\n",
      "step 3668, loss is 5.033873558044434\n",
      "(64, 33)\n",
      "step 3669, loss is 5.33627986907959\n",
      "(64, 33)\n",
      "step 3670, loss is 5.135857105255127\n",
      "(64, 33)\n",
      "step 3671, loss is 5.163387775421143\n",
      "(64, 33)\n",
      "step 3672, loss is 5.055252552032471\n",
      "(64, 33)\n",
      "step 3673, loss is 5.186791896820068\n",
      "(64, 33)\n",
      "step 3674, loss is 5.075756072998047\n",
      "(64, 33)\n",
      "step 3675, loss is 5.058408737182617\n",
      "(64, 33)\n",
      "step 3676, loss is 5.369778156280518\n",
      "(64, 33)\n",
      "step 3677, loss is 4.999001979827881\n",
      "(64, 33)\n",
      "step 3678, loss is 5.123291015625\n",
      "(64, 33)\n",
      "step 3679, loss is 5.078102111816406\n",
      "(64, 33)\n",
      "step 3680, loss is 5.06043815612793\n",
      "(64, 33)\n",
      "step 3681, loss is 4.947403430938721\n",
      "(64, 33)\n",
      "step 3682, loss is 5.01440954208374\n",
      "(64, 33)\n",
      "step 3683, loss is 5.252482891082764\n",
      "(64, 33)\n",
      "step 3684, loss is 5.003297805786133\n",
      "(64, 33)\n",
      "step 3685, loss is 5.060801982879639\n",
      "(64, 33)\n",
      "step 3686, loss is 5.113453388214111\n",
      "(64, 33)\n",
      "step 3687, loss is 4.588672161102295\n",
      "(64, 33)\n",
      "step 3688, loss is 4.925656795501709\n",
      "(64, 33)\n",
      "step 3689, loss is 4.974076747894287\n",
      "(64, 33)\n",
      "step 3690, loss is 5.278972148895264\n",
      "(64, 33)\n",
      "step 3691, loss is 4.933130264282227\n",
      "(64, 33)\n",
      "step 3692, loss is 5.1084370613098145\n",
      "(64, 33)\n",
      "step 3693, loss is 5.011298656463623\n",
      "(64, 33)\n",
      "step 3694, loss is 4.999863624572754\n",
      "(64, 33)\n",
      "step 3695, loss is 5.183263778686523\n",
      "(64, 33)\n",
      "step 3696, loss is 5.122968673706055\n",
      "(64, 33)\n",
      "step 3697, loss is 5.126729965209961\n",
      "(64, 33)\n",
      "step 3698, loss is 5.2001423835754395\n",
      "(64, 33)\n",
      "step 3699, loss is 5.255111217498779\n",
      "(64, 33)\n",
      "step 3700, loss is 5.165972709655762\n",
      "(64, 33)\n",
      "step 3701, loss is 5.20200777053833\n",
      "(64, 33)\n",
      "step 3702, loss is 4.968421936035156\n",
      "(64, 33)\n",
      "step 3703, loss is 5.047884941101074\n",
      "(64, 33)\n",
      "step 3704, loss is 5.181038856506348\n",
      "(64, 33)\n",
      "step 3705, loss is 5.316037654876709\n",
      "(64, 33)\n",
      "step 3706, loss is 5.261038303375244\n",
      "(64, 33)\n",
      "step 3707, loss is 4.971803665161133\n",
      "(64, 33)\n",
      "step 3708, loss is 5.059840202331543\n",
      "(64, 33)\n",
      "step 3709, loss is 5.12899112701416\n",
      "(64, 33)\n",
      "step 3710, loss is 5.1334004402160645\n",
      "(64, 33)\n",
      "step 3711, loss is 5.184735298156738\n",
      "(64, 33)\n",
      "step 3712, loss is 4.814121246337891\n",
      "(64, 33)\n",
      "step 3713, loss is 5.11669921875\n",
      "(64, 33)\n",
      "step 3714, loss is 4.982158184051514\n",
      "(64, 33)\n",
      "step 3715, loss is 4.993937015533447\n",
      "(64, 33)\n",
      "step 3716, loss is 5.230081081390381\n",
      "(64, 33)\n",
      "step 3717, loss is 5.238370895385742\n",
      "(64, 33)\n",
      "step 3718, loss is 5.15902042388916\n",
      "(64, 33)\n",
      "step 3719, loss is 5.127447605133057\n",
      "(64, 33)\n",
      "step 3720, loss is 5.037304878234863\n",
      "(64, 33)\n",
      "step 3721, loss is 4.9567131996154785\n",
      "(64, 33)\n",
      "step 3722, loss is 5.120113372802734\n",
      "(64, 33)\n",
      "step 3723, loss is 5.19882869720459\n",
      "(64, 33)\n",
      "step 3724, loss is 5.143635272979736\n",
      "(64, 33)\n",
      "step 3725, loss is 5.19325590133667\n",
      "(64, 33)\n",
      "step 3726, loss is 5.235721588134766\n",
      "(64, 33)\n",
      "step 3727, loss is 5.231443881988525\n",
      "(64, 33)\n",
      "step 3728, loss is 5.110488414764404\n",
      "(64, 33)\n",
      "step 3729, loss is 5.0569376945495605\n",
      "(64, 33)\n",
      "step 3730, loss is 4.9900898933410645\n",
      "(64, 33)\n",
      "step 3731, loss is 5.1200337409973145\n",
      "(64, 33)\n",
      "step 3732, loss is 4.9113640785217285\n",
      "(64, 33)\n",
      "step 3733, loss is 5.169137477874756\n",
      "(64, 33)\n",
      "step 3734, loss is 5.167544841766357\n",
      "(64, 33)\n",
      "step 3735, loss is 5.081706523895264\n",
      "(64, 33)\n",
      "step 3736, loss is 5.25889778137207\n",
      "(64, 33)\n",
      "step 3737, loss is 5.028767108917236\n",
      "(64, 33)\n",
      "step 3738, loss is 5.020421504974365\n",
      "(64, 33)\n",
      "step 3739, loss is 5.1202778816223145\n",
      "(64, 33)\n",
      "step 3740, loss is 5.037868499755859\n",
      "(64, 33)\n",
      "step 3741, loss is 4.981472015380859\n",
      "(64, 33)\n",
      "step 3742, loss is 5.093977928161621\n",
      "(64, 33)\n",
      "step 3743, loss is 4.97239875793457\n",
      "(64, 33)\n",
      "step 3744, loss is 5.053094387054443\n",
      "(64, 33)\n",
      "step 3745, loss is 4.9707441329956055\n",
      "(64, 33)\n",
      "step 3746, loss is 5.093084812164307\n",
      "(64, 33)\n",
      "step 3747, loss is 5.446911334991455\n",
      "(64, 33)\n",
      "step 3748, loss is 5.3641839027404785\n",
      "(64, 33)\n",
      "step 3749, loss is 4.9776225090026855\n",
      "(64, 33)\n",
      "step 3750, loss is 4.908713340759277\n",
      "(64, 33)\n",
      "step 3751, loss is 5.208878040313721\n",
      "(64, 33)\n",
      "step 3752, loss is 5.047770023345947\n",
      "(64, 33)\n",
      "step 3753, loss is 5.341851711273193\n",
      "(64, 33)\n",
      "step 3754, loss is 4.934534549713135\n",
      "(64, 33)\n",
      "step 3755, loss is 5.130948066711426\n",
      "(64, 33)\n",
      "step 3756, loss is 5.080571174621582\n",
      "(64, 33)\n",
      "step 3757, loss is 5.107602119445801\n",
      "(64, 33)\n",
      "step 3758, loss is 5.143801689147949\n",
      "(64, 33)\n",
      "step 3759, loss is 5.252064228057861\n",
      "(64, 33)\n",
      "step 3760, loss is 4.995362281799316\n",
      "(64, 33)\n",
      "step 3761, loss is 4.832869529724121\n",
      "(64, 33)\n",
      "step 3762, loss is 5.0380120277404785\n",
      "(64, 33)\n",
      "step 3763, loss is 5.073672771453857\n",
      "(64, 33)\n",
      "step 3764, loss is 5.216212272644043\n",
      "(64, 33)\n",
      "step 3765, loss is 4.730736255645752\n",
      "(64, 33)\n",
      "step 3766, loss is 4.9848713874816895\n",
      "(64, 33)\n",
      "step 3767, loss is 5.208767414093018\n",
      "(64, 33)\n",
      "step 3768, loss is 5.034307956695557\n",
      "(64, 33)\n",
      "step 3769, loss is 5.024219512939453\n",
      "(64, 33)\n",
      "step 3770, loss is 5.101611614227295\n",
      "(64, 33)\n",
      "step 3771, loss is 5.091115474700928\n",
      "(64, 33)\n",
      "step 3772, loss is 4.832374572753906\n",
      "(64, 33)\n",
      "step 3773, loss is 5.068507194519043\n",
      "(64, 33)\n",
      "step 3774, loss is 5.226308345794678\n",
      "(64, 33)\n",
      "step 3775, loss is 5.135193824768066\n",
      "(64, 33)\n",
      "step 3776, loss is 5.156664848327637\n",
      "(64, 33)\n",
      "step 3777, loss is 5.030707836151123\n",
      "(64, 33)\n",
      "step 3778, loss is 5.011034965515137\n",
      "(64, 33)\n",
      "step 3779, loss is 5.02729606628418\n",
      "(64, 33)\n",
      "step 3780, loss is 5.1066765785217285\n",
      "(64, 33)\n",
      "step 3781, loss is 5.048882961273193\n",
      "(64, 33)\n",
      "step 3782, loss is 5.219343662261963\n",
      "(64, 33)\n",
      "step 3783, loss is 5.076991081237793\n",
      "(64, 33)\n",
      "step 3784, loss is 5.0438385009765625\n",
      "(64, 33)\n",
      "step 3785, loss is 5.1635661125183105\n",
      "(64, 33)\n",
      "step 3786, loss is 5.093260765075684\n",
      "(64, 33)\n",
      "step 3787, loss is 4.923274040222168\n",
      "(64, 33)\n",
      "step 3788, loss is 4.980056285858154\n",
      "(64, 33)\n",
      "step 3789, loss is 5.186038017272949\n",
      "(64, 33)\n",
      "step 3790, loss is 5.027215003967285\n",
      "(64, 33)\n",
      "step 3791, loss is 5.039637088775635\n",
      "(64, 33)\n",
      "step 3792, loss is 4.933586597442627\n",
      "(64, 33)\n",
      "step 3793, loss is 5.022575378417969\n",
      "(64, 33)\n",
      "step 3794, loss is 5.064297676086426\n",
      "(64, 33)\n",
      "step 3795, loss is 4.961285591125488\n",
      "(64, 33)\n",
      "step 3796, loss is 5.166099548339844\n",
      "(64, 33)\n",
      "step 3797, loss is 5.047401428222656\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3798, loss is 5.0862836837768555\n",
      "(64, 33)\n",
      "step 3799, loss is 5.157550811767578\n",
      "(64, 33)\n",
      "step 3800, loss is 5.07972526550293\n",
      "(64, 33)\n",
      "step 3801, loss is 4.95379638671875\n",
      "(64, 33)\n",
      "step 3802, loss is 5.136404037475586\n",
      "(64, 33)\n",
      "step 3803, loss is 5.170053005218506\n",
      "(64, 33)\n",
      "step 3804, loss is 5.128756999969482\n",
      "(64, 33)\n",
      "step 3805, loss is 4.951596736907959\n",
      "(64, 33)\n",
      "step 3806, loss is 5.063567638397217\n",
      "(64, 33)\n",
      "step 3807, loss is 4.965732574462891\n",
      "(64, 33)\n",
      "step 3808, loss is 4.9535346031188965\n",
      "(64, 33)\n",
      "step 3809, loss is 5.105973720550537\n",
      "(64, 33)\n",
      "step 3810, loss is 5.030752658843994\n",
      "(64, 33)\n",
      "step 3811, loss is 5.137257099151611\n",
      "(64, 33)\n",
      "step 3812, loss is 4.976548671722412\n",
      "(64, 33)\n",
      "step 3813, loss is 5.06394100189209\n",
      "(64, 33)\n",
      "step 3814, loss is 5.302093029022217\n",
      "(64, 33)\n",
      "step 3815, loss is 5.083994388580322\n",
      "(64, 33)\n",
      "step 3816, loss is 4.919073581695557\n",
      "(64, 33)\n",
      "step 3817, loss is 5.079317569732666\n",
      "(64, 33)\n",
      "step 3818, loss is 5.242247104644775\n",
      "(64, 33)\n",
      "step 3819, loss is 4.908515453338623\n",
      "(64, 33)\n",
      "step 3820, loss is 5.156035423278809\n",
      "(64, 33)\n",
      "step 3821, loss is 4.981509685516357\n",
      "(64, 33)\n",
      "step 3822, loss is 5.315742492675781\n",
      "(64, 33)\n",
      "step 3823, loss is 5.178138732910156\n",
      "(64, 33)\n",
      "step 3824, loss is 4.881839275360107\n",
      "(64, 33)\n",
      "step 3825, loss is 5.161005020141602\n",
      "(64, 33)\n",
      "step 3826, loss is 4.955494403839111\n",
      "(64, 33)\n",
      "step 3827, loss is 4.9435954093933105\n",
      "(64, 33)\n",
      "step 3828, loss is 5.311875820159912\n",
      "(64, 33)\n",
      "step 3829, loss is 5.109927177429199\n",
      "(64, 33)\n",
      "step 3830, loss is 5.028141975402832\n",
      "(64, 33)\n",
      "step 3831, loss is 5.1008195877075195\n",
      "(64, 33)\n",
      "step 3832, loss is 5.376509189605713\n",
      "(64, 33)\n",
      "step 3833, loss is 4.979766368865967\n",
      "(64, 33)\n",
      "step 3834, loss is 4.829748153686523\n",
      "(64, 33)\n",
      "step 3835, loss is 5.242254734039307\n",
      "(64, 33)\n",
      "step 3836, loss is 5.102444648742676\n",
      "(64, 33)\n",
      "step 3837, loss is 4.9642863273620605\n",
      "(64, 33)\n",
      "step 3838, loss is 5.159654140472412\n",
      "(64, 33)\n",
      "step 3839, loss is 5.039583206176758\n",
      "(64, 33)\n",
      "step 3840, loss is 4.906246185302734\n",
      "(64, 33)\n",
      "step 3841, loss is 5.006619453430176\n",
      "(64, 33)\n",
      "step 3842, loss is 4.930820941925049\n",
      "(64, 33)\n",
      "step 3843, loss is 5.215614318847656\n",
      "(64, 33)\n",
      "step 3844, loss is 5.197206020355225\n",
      "(64, 33)\n",
      "step 3845, loss is 4.958583831787109\n",
      "(64, 33)\n",
      "step 3846, loss is 5.223815441131592\n",
      "(64, 33)\n",
      "step 3847, loss is 5.138280391693115\n",
      "(64, 33)\n",
      "step 3848, loss is 5.061567783355713\n",
      "(64, 33)\n",
      "step 3849, loss is 5.168156623840332\n",
      "(64, 33)\n",
      "step 3850, loss is 5.175791263580322\n",
      "(64, 33)\n",
      "step 3851, loss is 5.188522815704346\n",
      "(64, 33)\n",
      "step 3852, loss is 5.149168968200684\n",
      "(64, 33)\n",
      "step 3853, loss is 5.333399772644043\n",
      "(64, 33)\n",
      "step 3854, loss is 5.0723443031311035\n",
      "(64, 33)\n",
      "step 3855, loss is 5.1366376876831055\n",
      "(64, 33)\n",
      "step 3856, loss is 5.064999580383301\n",
      "(64, 33)\n",
      "step 3857, loss is 5.134877681732178\n",
      "(64, 33)\n",
      "step 3858, loss is 4.917916297912598\n",
      "(64, 33)\n",
      "step 3859, loss is 4.9292731285095215\n",
      "(64, 33)\n",
      "step 3860, loss is 5.154411792755127\n",
      "(64, 33)\n",
      "step 3861, loss is 5.016773223876953\n",
      "(64, 33)\n",
      "step 3862, loss is 5.4452643394470215\n",
      "(64, 33)\n",
      "step 3863, loss is 4.975327968597412\n",
      "(64, 33)\n",
      "step 3864, loss is 4.999095916748047\n",
      "(64, 33)\n",
      "step 3865, loss is 5.09413480758667\n",
      "(64, 33)\n",
      "step 3866, loss is 4.917123317718506\n",
      "(64, 33)\n",
      "step 3867, loss is 4.917289733886719\n",
      "(64, 33)\n",
      "step 3868, loss is 5.124856472015381\n",
      "(64, 33)\n",
      "step 3869, loss is 5.177757263183594\n",
      "(64, 33)\n",
      "step 3870, loss is 5.007004261016846\n",
      "(64, 33)\n",
      "step 3871, loss is 5.302155494689941\n",
      "(64, 33)\n",
      "step 3872, loss is 4.991782188415527\n",
      "(64, 33)\n",
      "step 3873, loss is 5.1462721824646\n",
      "(64, 33)\n",
      "step 3874, loss is 5.224674224853516\n",
      "(64, 33)\n",
      "step 3875, loss is 5.112985610961914\n",
      "(64, 33)\n",
      "step 3876, loss is 5.151394844055176\n",
      "(64, 33)\n",
      "step 3877, loss is 4.967323303222656\n",
      "(64, 33)\n",
      "step 3878, loss is 5.135748863220215\n",
      "(64, 33)\n",
      "step 3879, loss is 5.1208720207214355\n",
      "(64, 33)\n",
      "step 3880, loss is 5.108704090118408\n",
      "(64, 33)\n",
      "step 3881, loss is 5.0835771560668945\n",
      "(64, 33)\n",
      "step 3882, loss is 5.209860324859619\n",
      "(64, 33)\n",
      "step 3883, loss is 4.830163955688477\n",
      "(64, 33)\n",
      "step 3884, loss is 5.070477485656738\n",
      "(64, 33)\n",
      "step 3885, loss is 5.080346584320068\n",
      "(64, 33)\n",
      "step 3886, loss is 5.192965030670166\n",
      "(64, 33)\n",
      "step 3887, loss is 5.05362606048584\n",
      "(64, 33)\n",
      "step 3888, loss is 4.847009658813477\n",
      "(64, 33)\n",
      "step 3889, loss is 5.073359489440918\n",
      "(64, 33)\n",
      "step 3890, loss is 5.063358306884766\n",
      "(64, 33)\n",
      "step 3891, loss is 5.211679935455322\n",
      "(64, 33)\n",
      "step 3892, loss is 4.813234806060791\n",
      "(64, 33)\n",
      "step 3893, loss is 5.117526054382324\n",
      "(64, 33)\n",
      "step 3894, loss is 5.061873435974121\n",
      "(64, 33)\n",
      "step 3895, loss is 5.214540958404541\n",
      "(64, 33)\n",
      "step 3896, loss is 5.125509738922119\n",
      "(64, 33)\n",
      "step 3897, loss is 4.9822893142700195\n",
      "(64, 33)\n",
      "step 3898, loss is 4.939260005950928\n",
      "(64, 33)\n",
      "step 3899, loss is 4.990201473236084\n",
      "(64, 33)\n",
      "step 3900, loss is 4.845698356628418\n",
      "(64, 33)\n",
      "step 3901, loss is 5.218114376068115\n",
      "(64, 33)\n",
      "step 3902, loss is 5.188304424285889\n",
      "(64, 33)\n",
      "step 3903, loss is 5.003840923309326\n",
      "(64, 33)\n",
      "step 3904, loss is 5.028468608856201\n",
      "(64, 33)\n",
      "step 3905, loss is 4.921763896942139\n",
      "(64, 33)\n",
      "step 3906, loss is 5.143575191497803\n",
      "(64, 33)\n",
      "step 3907, loss is 5.272560119628906\n",
      "(64, 33)\n",
      "step 3908, loss is 5.187744140625\n",
      "(64, 33)\n",
      "step 3909, loss is 5.0618510246276855\n",
      "(64, 33)\n",
      "step 3910, loss is 5.221389293670654\n",
      "(64, 33)\n",
      "step 3911, loss is 5.002969741821289\n",
      "(64, 33)\n",
      "step 3912, loss is 5.212297439575195\n",
      "(64, 33)\n",
      "step 3913, loss is 5.290336608886719\n",
      "(64, 33)\n",
      "step 3914, loss is 5.11221981048584\n",
      "(64, 33)\n",
      "step 3915, loss is 5.303038597106934\n",
      "(64, 33)\n",
      "step 3916, loss is 5.166749000549316\n",
      "(64, 33)\n",
      "step 3917, loss is 5.278647422790527\n",
      "(64, 33)\n",
      "step 3918, loss is 5.06393575668335\n",
      "(64, 33)\n",
      "step 3919, loss is 5.15541410446167\n",
      "(64, 33)\n",
      "step 3920, loss is 5.165511608123779\n",
      "(64, 33)\n",
      "step 3921, loss is 4.955193996429443\n",
      "(64, 33)\n",
      "step 3922, loss is 5.181865692138672\n",
      "(64, 33)\n",
      "step 3923, loss is 5.211922645568848\n",
      "(64, 33)\n",
      "step 3924, loss is 5.149482727050781\n",
      "(64, 33)\n",
      "step 3925, loss is 5.0880279541015625\n",
      "(64, 33)\n",
      "step 3926, loss is 4.967327117919922\n",
      "(64, 33)\n",
      "step 3927, loss is 4.987386703491211\n",
      "(64, 33)\n",
      "step 3928, loss is 4.971260070800781\n",
      "(64, 33)\n",
      "step 3929, loss is 5.093684196472168\n",
      "(64, 33)\n",
      "step 3930, loss is 4.940332412719727\n",
      "(64, 33)\n",
      "step 3931, loss is 5.0787482261657715\n",
      "(64, 33)\n",
      "step 3932, loss is 5.212526321411133\n",
      "(64, 33)\n",
      "step 3933, loss is 5.259503364562988\n",
      "(64, 33)\n",
      "step 3934, loss is 5.255593776702881\n",
      "(64, 33)\n",
      "step 3935, loss is 5.117786884307861\n",
      "(64, 33)\n",
      "step 3936, loss is 4.989789962768555\n",
      "(64, 33)\n",
      "step 3937, loss is 4.969718933105469\n",
      "(64, 33)\n",
      "step 3938, loss is 5.050866603851318\n",
      "(64, 33)\n",
      "step 3939, loss is 4.983561038970947\n",
      "(64, 33)\n",
      "step 3940, loss is 5.334466934204102\n",
      "(64, 33)\n",
      "step 3941, loss is 5.008417129516602\n",
      "(64, 33)\n",
      "step 3942, loss is 5.2779221534729\n",
      "(64, 33)\n",
      "step 3943, loss is 5.085110187530518\n",
      "(64, 33)\n",
      "step 3944, loss is 5.032115459442139\n",
      "(64, 33)\n",
      "step 3945, loss is 5.071381568908691\n",
      "(64, 33)\n",
      "step 3946, loss is 5.003424167633057\n",
      "(64, 33)\n",
      "step 3947, loss is 5.144773483276367\n",
      "(64, 33)\n",
      "step 3948, loss is 5.026845932006836\n",
      "(64, 33)\n",
      "step 3949, loss is 4.980723857879639\n",
      "(64, 33)\n",
      "step 3950, loss is 4.939574718475342\n",
      "(64, 33)\n",
      "step 3951, loss is 5.218024253845215\n",
      "(64, 33)\n",
      "step 3952, loss is 5.258535385131836\n",
      "(64, 33)\n",
      "step 3953, loss is 5.030792236328125\n",
      "(64, 33)\n",
      "step 3954, loss is 5.143723011016846\n",
      "(64, 33)\n",
      "step 3955, loss is 5.075604438781738\n",
      "(64, 33)\n",
      "step 3956, loss is 4.999805927276611\n",
      "(64, 33)\n",
      "step 3957, loss is 5.145781517028809\n",
      "(64, 33)\n",
      "step 3958, loss is 5.230180740356445\n",
      "(64, 33)\n",
      "step 3959, loss is 5.291608810424805\n",
      "(64, 33)\n",
      "step 3960, loss is 4.932912826538086\n",
      "(64, 33)\n",
      "step 3961, loss is 5.0373854637146\n",
      "(64, 33)\n",
      "step 3962, loss is 4.981710910797119\n",
      "(64, 33)\n",
      "step 3963, loss is 4.78737211227417\n",
      "(64, 33)\n",
      "step 3964, loss is 5.082645416259766\n",
      "(64, 33)\n",
      "step 3965, loss is 5.085653781890869\n",
      "(64, 33)\n",
      "step 3966, loss is 5.168117523193359\n",
      "(64, 33)\n",
      "step 3967, loss is 5.052334785461426\n",
      "(64, 33)\n",
      "step 3968, loss is 5.082542419433594\n",
      "(64, 33)\n",
      "step 3969, loss is 5.370788097381592\n",
      "(64, 33)\n",
      "step 3970, loss is 5.298221111297607\n",
      "(64, 33)\n",
      "step 3971, loss is 4.9853363037109375\n",
      "(64, 33)\n",
      "step 3972, loss is 5.148301124572754\n",
      "(64, 33)\n",
      "step 3973, loss is 4.975249290466309\n",
      "(64, 33)\n",
      "step 3974, loss is 5.119561672210693\n",
      "(64, 33)\n",
      "step 3975, loss is 5.111491680145264\n",
      "(64, 33)\n",
      "step 3976, loss is 5.15535306930542\n",
      "(64, 33)\n",
      "step 3977, loss is 5.029370307922363\n",
      "(64, 33)\n",
      "step 3978, loss is 5.161637783050537\n",
      "(64, 33)\n",
      "step 3979, loss is 5.06339693069458\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3980, loss is 5.036190032958984\n",
      "(64, 33)\n",
      "step 3981, loss is 5.109135627746582\n",
      "(64, 33)\n",
      "step 3982, loss is 5.029867649078369\n",
      "(64, 33)\n",
      "step 3983, loss is 5.204463958740234\n",
      "(64, 33)\n",
      "step 3984, loss is 5.192028045654297\n",
      "(64, 33)\n",
      "step 3985, loss is 5.060187339782715\n",
      "(64, 33)\n",
      "step 3986, loss is 5.056970119476318\n",
      "(64, 33)\n",
      "step 3987, loss is 5.029234409332275\n",
      "(64, 33)\n",
      "step 3988, loss is 5.047018051147461\n",
      "(64, 33)\n",
      "step 3989, loss is 5.178328514099121\n",
      "(64, 33)\n",
      "step 3990, loss is 5.032851219177246\n",
      "(64, 33)\n",
      "step 3991, loss is 5.169682025909424\n",
      "(64, 33)\n",
      "step 3992, loss is 5.081897735595703\n",
      "(64, 33)\n",
      "step 3993, loss is 4.952965259552002\n",
      "(64, 33)\n",
      "step 3994, loss is 4.812654495239258\n",
      "(64, 33)\n",
      "step 3995, loss is 5.057758808135986\n",
      "(64, 33)\n",
      "step 3996, loss is 5.125190734863281\n",
      "(64, 33)\n",
      "step 3997, loss is 5.18199348449707\n",
      "(64, 33)\n",
      "step 3998, loss is 5.070051193237305\n",
      "(64, 33)\n",
      "step 3999, loss is 4.896185398101807\n",
      "(64, 33)\n",
      "step 4000, loss is 5.031249523162842\n",
      "(64, 33)\n",
      "step 4001, loss is 5.121214389801025\n",
      "(64, 33)\n",
      "step 4002, loss is 5.02306604385376\n",
      "(64, 33)\n",
      "step 4003, loss is 5.209018707275391\n",
      "(64, 33)\n",
      "step 4004, loss is 5.101834774017334\n",
      "(64, 33)\n",
      "step 4005, loss is 5.07600736618042\n",
      "(64, 33)\n",
      "step 4006, loss is 5.18306303024292\n",
      "(64, 33)\n",
      "step 4007, loss is 5.294850826263428\n",
      "(64, 33)\n",
      "step 4008, loss is 5.171554088592529\n",
      "(64, 33)\n",
      "step 4009, loss is 5.320010662078857\n",
      "(64, 33)\n",
      "step 4010, loss is 5.032952785491943\n",
      "(64, 33)\n",
      "step 4011, loss is 5.111591815948486\n",
      "(64, 33)\n",
      "step 4012, loss is 5.058101654052734\n",
      "(64, 33)\n",
      "step 4013, loss is 5.013123512268066\n",
      "(64, 33)\n",
      "step 4014, loss is 5.105072021484375\n",
      "(64, 33)\n",
      "step 4015, loss is 5.057792663574219\n",
      "(64, 33)\n",
      "step 4016, loss is 5.274669647216797\n",
      "(64, 33)\n",
      "step 4017, loss is 4.937886714935303\n",
      "(64, 33)\n",
      "step 4018, loss is 4.929533958435059\n",
      "(64, 33)\n",
      "step 4019, loss is 5.159528732299805\n",
      "(64, 33)\n",
      "step 4020, loss is 5.132053375244141\n",
      "(64, 33)\n",
      "step 4021, loss is 4.99583101272583\n",
      "(64, 33)\n",
      "step 4022, loss is 5.185729026794434\n",
      "(64, 33)\n",
      "step 4023, loss is 5.0396599769592285\n",
      "(64, 33)\n",
      "step 4024, loss is 5.181243896484375\n",
      "(64, 33)\n",
      "step 4025, loss is 5.241900444030762\n",
      "(64, 33)\n",
      "step 4026, loss is 5.025731086730957\n",
      "(64, 33)\n",
      "step 4027, loss is 5.260585308074951\n",
      "(64, 33)\n",
      "step 4028, loss is 4.992824077606201\n",
      "(64, 33)\n",
      "step 4029, loss is 5.142010688781738\n",
      "(64, 33)\n",
      "step 4030, loss is 5.07310152053833\n",
      "(64, 33)\n",
      "step 4031, loss is 5.01765775680542\n",
      "(64, 33)\n",
      "step 4032, loss is 5.132979869842529\n",
      "(64, 33)\n",
      "step 4033, loss is 5.151275157928467\n",
      "(64, 33)\n",
      "step 4034, loss is 5.105644702911377\n",
      "(64, 33)\n",
      "step 4035, loss is 5.086280822753906\n",
      "(64, 33)\n",
      "step 4036, loss is 5.018857955932617\n",
      "(64, 33)\n",
      "step 4037, loss is 4.9946064949035645\n",
      "(64, 33)\n",
      "step 4038, loss is 5.315165996551514\n",
      "(64, 33)\n",
      "step 4039, loss is 5.097409248352051\n",
      "(64, 33)\n",
      "step 4040, loss is 5.197948932647705\n",
      "(64, 33)\n",
      "step 4041, loss is 5.045289516448975\n",
      "(64, 33)\n",
      "step 4042, loss is 5.1899027824401855\n",
      "(64, 33)\n",
      "step 4043, loss is 5.103653430938721\n",
      "(64, 33)\n",
      "step 4044, loss is 5.210376262664795\n",
      "(64, 33)\n",
      "step 4045, loss is 5.31520414352417\n",
      "(64, 33)\n",
      "step 4046, loss is 5.193549156188965\n",
      "(64, 33)\n",
      "step 4047, loss is 5.039360046386719\n",
      "(64, 33)\n",
      "step 4048, loss is 5.348137378692627\n",
      "(64, 33)\n",
      "step 4049, loss is 4.936578273773193\n",
      "(64, 33)\n",
      "step 4050, loss is 5.00014066696167\n",
      "(64, 33)\n",
      "step 4051, loss is 4.8785786628723145\n",
      "(64, 33)\n",
      "step 4052, loss is 5.307159900665283\n",
      "(64, 33)\n",
      "step 4053, loss is 5.160333633422852\n",
      "(64, 33)\n",
      "step 4054, loss is 5.047795295715332\n",
      "(64, 33)\n",
      "step 4055, loss is 5.186509609222412\n",
      "(64, 33)\n",
      "step 4056, loss is 5.131718158721924\n",
      "(64, 33)\n",
      "step 4057, loss is 5.2777557373046875\n",
      "(64, 33)\n",
      "step 4058, loss is 4.972158432006836\n",
      "(64, 33)\n",
      "step 4059, loss is 5.245559215545654\n",
      "(64, 33)\n",
      "step 4060, loss is 5.169893741607666\n",
      "(64, 33)\n",
      "step 4061, loss is 5.3046770095825195\n",
      "(64, 33)\n",
      "step 4062, loss is 5.112852096557617\n",
      "(64, 33)\n",
      "step 4063, loss is 4.9173712730407715\n",
      "(64, 33)\n",
      "step 4064, loss is 5.245946884155273\n",
      "(64, 33)\n",
      "step 4065, loss is 5.171214580535889\n",
      "(64, 33)\n",
      "step 4066, loss is 5.039059162139893\n",
      "(64, 33)\n",
      "step 4067, loss is 5.068472385406494\n",
      "(64, 33)\n",
      "step 4068, loss is 5.084347248077393\n",
      "(64, 33)\n",
      "step 4069, loss is 4.89146089553833\n",
      "(64, 33)\n",
      "step 4070, loss is 5.231865882873535\n",
      "(64, 33)\n",
      "step 4071, loss is 5.020817756652832\n",
      "(64, 33)\n",
      "step 4072, loss is 4.965588092803955\n",
      "(64, 33)\n",
      "step 4073, loss is 5.086203575134277\n",
      "(64, 33)\n",
      "step 4074, loss is 4.936333179473877\n",
      "(64, 33)\n",
      "step 4075, loss is 4.972229480743408\n",
      "(64, 33)\n",
      "step 4076, loss is 5.15492057800293\n",
      "(64, 33)\n",
      "step 4077, loss is 5.006401538848877\n",
      "(64, 33)\n",
      "step 4078, loss is 5.100574970245361\n",
      "(64, 33)\n",
      "step 4079, loss is 5.337736129760742\n",
      "(64, 33)\n",
      "step 4080, loss is 4.998542785644531\n",
      "(64, 33)\n",
      "step 4081, loss is 5.068867206573486\n",
      "(64, 33)\n",
      "step 4082, loss is 5.023942470550537\n",
      "(64, 33)\n",
      "step 4083, loss is 4.905228614807129\n",
      "(64, 33)\n",
      "step 4084, loss is 5.129188537597656\n",
      "(64, 33)\n",
      "step 4085, loss is 5.0429792404174805\n",
      "(64, 33)\n",
      "step 4086, loss is 5.202061653137207\n",
      "(64, 33)\n",
      "step 4087, loss is 5.083281993865967\n",
      "(64, 33)\n",
      "step 4088, loss is 4.904173851013184\n",
      "(64, 33)\n",
      "step 4089, loss is 5.0973734855651855\n",
      "(64, 33)\n",
      "step 4090, loss is 4.932981014251709\n",
      "(64, 33)\n",
      "step 4091, loss is 5.073004722595215\n",
      "(64, 33)\n",
      "step 4092, loss is 5.125831604003906\n",
      "(64, 33)\n",
      "step 4093, loss is 4.994388103485107\n",
      "(64, 33)\n",
      "step 4094, loss is 5.2954535484313965\n",
      "(64, 33)\n",
      "step 4095, loss is 5.022162437438965\n",
      "(64, 33)\n",
      "step 4096, loss is 5.075129508972168\n",
      "(64, 33)\n",
      "step 4097, loss is 5.010773658752441\n",
      "(64, 33)\n",
      "step 4098, loss is 4.892908096313477\n",
      "(64, 33)\n",
      "step 4099, loss is 4.893028259277344\n",
      "(64, 33)\n",
      "step 4100, loss is 5.108333587646484\n",
      "(64, 33)\n",
      "step 4101, loss is 5.247825622558594\n",
      "(64, 33)\n",
      "step 4102, loss is 5.197951316833496\n",
      "(64, 33)\n",
      "step 4103, loss is 4.950654983520508\n",
      "(64, 33)\n",
      "step 4104, loss is 5.267448902130127\n",
      "(64, 33)\n",
      "step 4105, loss is 5.340980529785156\n",
      "(64, 33)\n",
      "step 4106, loss is 5.204933166503906\n",
      "(64, 33)\n",
      "step 4107, loss is 5.103978633880615\n",
      "(64, 33)\n",
      "step 4108, loss is 5.119449615478516\n",
      "(64, 33)\n",
      "step 4109, loss is 5.068243026733398\n",
      "(64, 33)\n",
      "step 4110, loss is 4.841989040374756\n",
      "(64, 33)\n",
      "step 4111, loss is 5.057969570159912\n",
      "(64, 33)\n",
      "step 4112, loss is 5.053657054901123\n",
      "(64, 33)\n",
      "step 4113, loss is 5.088768005371094\n",
      "(64, 33)\n",
      "step 4114, loss is 5.22148323059082\n",
      "(64, 33)\n",
      "step 4115, loss is 5.107369899749756\n",
      "(64, 33)\n",
      "step 4116, loss is 5.148098945617676\n",
      "(64, 33)\n",
      "step 4117, loss is 4.962137222290039\n",
      "(64, 33)\n",
      "step 4118, loss is 5.016993522644043\n",
      "(64, 33)\n",
      "step 4119, loss is 4.952323913574219\n",
      "(64, 33)\n",
      "step 4120, loss is 5.108494758605957\n",
      "(64, 33)\n",
      "step 4121, loss is 5.084148406982422\n",
      "(64, 33)\n",
      "step 4122, loss is 4.857184410095215\n",
      "(64, 33)\n",
      "step 4123, loss is 5.170082092285156\n",
      "(64, 33)\n",
      "step 4124, loss is 5.13590669631958\n",
      "(64, 33)\n",
      "step 4125, loss is 5.032798767089844\n",
      "(64, 33)\n",
      "step 4126, loss is 5.183235168457031\n",
      "(64, 33)\n",
      "step 4127, loss is 5.036084175109863\n",
      "(64, 33)\n",
      "step 4128, loss is 5.1735453605651855\n",
      "(64, 33)\n",
      "step 4129, loss is 5.175422668457031\n",
      "(64, 33)\n",
      "step 4130, loss is 4.968955993652344\n",
      "(64, 33)\n",
      "step 4131, loss is 5.2079267501831055\n",
      "(64, 33)\n",
      "step 4132, loss is 4.886855125427246\n",
      "(64, 33)\n",
      "step 4133, loss is 5.149075984954834\n",
      "(64, 33)\n",
      "step 4134, loss is 4.955643653869629\n",
      "(64, 33)\n",
      "step 4135, loss is 5.08499002456665\n",
      "(64, 33)\n",
      "step 4136, loss is 5.068885803222656\n",
      "(64, 33)\n",
      "step 4137, loss is 4.962924957275391\n",
      "(64, 33)\n",
      "step 4138, loss is 5.080859661102295\n",
      "(64, 33)\n",
      "step 4139, loss is 5.170628070831299\n",
      "(64, 33)\n",
      "step 4140, loss is 4.958381175994873\n",
      "(64, 33)\n",
      "step 4141, loss is 4.924770355224609\n",
      "(64, 33)\n",
      "step 4142, loss is 4.84134578704834\n",
      "(64, 33)\n",
      "step 4143, loss is 5.088449954986572\n",
      "(64, 33)\n",
      "step 4144, loss is 5.085174083709717\n",
      "(64, 33)\n",
      "step 4145, loss is 5.1341166496276855\n",
      "(64, 33)\n",
      "step 4146, loss is 5.036235809326172\n",
      "(64, 33)\n",
      "step 4147, loss is 5.104641437530518\n",
      "(64, 33)\n",
      "step 4148, loss is 5.014903545379639\n",
      "(64, 33)\n",
      "step 4149, loss is 4.821557998657227\n",
      "(64, 33)\n",
      "step 4150, loss is 4.97828483581543\n",
      "(64, 33)\n",
      "step 4151, loss is 5.112061500549316\n",
      "(64, 33)\n",
      "step 4152, loss is 5.205320358276367\n",
      "(64, 33)\n",
      "step 4153, loss is 5.02318000793457\n",
      "(64, 33)\n",
      "step 4154, loss is 4.976794719696045\n",
      "(64, 33)\n",
      "step 4155, loss is 5.1640167236328125\n",
      "(64, 33)\n",
      "step 4156, loss is 4.965355396270752\n",
      "(64, 33)\n",
      "step 4157, loss is 4.974173069000244\n",
      "(64, 33)\n",
      "step 4158, loss is 5.001736164093018\n",
      "(64, 33)\n",
      "step 4159, loss is 5.007336616516113\n",
      "(64, 33)\n",
      "step 4160, loss is 4.996833801269531\n",
      "(64, 33)\n",
      "step 4161, loss is 5.199015140533447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 4162, loss is 5.1705098152160645\n",
      "(64, 33)\n",
      "step 4163, loss is 5.071642875671387\n",
      "(64, 33)\n",
      "step 4164, loss is 5.162108421325684\n",
      "(64, 33)\n",
      "step 4165, loss is 5.225528240203857\n",
      "(64, 33)\n",
      "step 4166, loss is 5.137155055999756\n",
      "(64, 33)\n",
      "step 4167, loss is 5.131280899047852\n",
      "(64, 33)\n",
      "step 4168, loss is 5.291325569152832\n",
      "(64, 33)\n",
      "step 4169, loss is 5.106745719909668\n",
      "(64, 33)\n",
      "step 4170, loss is 5.095512390136719\n",
      "(64, 33)\n",
      "step 4171, loss is 5.106855869293213\n",
      "(64, 33)\n",
      "step 4172, loss is 5.047930717468262\n",
      "(64, 33)\n",
      "step 4173, loss is 5.266107082366943\n",
      "(64, 33)\n",
      "step 4174, loss is 5.253750801086426\n",
      "(64, 33)\n",
      "step 4175, loss is 5.305832862854004\n",
      "(64, 33)\n",
      "step 4176, loss is 5.206974029541016\n",
      "(64, 33)\n",
      "step 4177, loss is 5.182710647583008\n",
      "(64, 33)\n",
      "step 4178, loss is 5.1291890144348145\n",
      "(64, 33)\n",
      "step 4179, loss is 5.172804832458496\n",
      "(64, 33)\n",
      "step 4180, loss is 5.075347900390625\n",
      "(64, 33)\n",
      "step 4181, loss is 5.164522171020508\n",
      "(64, 33)\n",
      "step 4182, loss is 5.139524459838867\n",
      "(64, 33)\n",
      "step 4183, loss is 5.141508102416992\n",
      "(64, 33)\n",
      "step 4184, loss is 5.161574840545654\n",
      "(64, 33)\n",
      "step 4185, loss is 5.044149398803711\n",
      "(64, 33)\n",
      "step 4186, loss is 4.864388942718506\n",
      "(64, 33)\n",
      "step 4187, loss is 5.073729991912842\n",
      "(64, 33)\n",
      "step 4188, loss is 5.056258201599121\n",
      "(64, 33)\n",
      "step 4189, loss is 4.9832377433776855\n",
      "(64, 33)\n",
      "step 4190, loss is 5.128603458404541\n",
      "(64, 33)\n",
      "step 4191, loss is 5.014908313751221\n",
      "(64, 33)\n",
      "step 4192, loss is 5.167255401611328\n",
      "(64, 33)\n",
      "step 4193, loss is 4.90470027923584\n",
      "(64, 33)\n",
      "step 4194, loss is 4.965607643127441\n",
      "(64, 33)\n",
      "step 4195, loss is 4.8699212074279785\n",
      "(64, 33)\n",
      "step 4196, loss is 5.1717963218688965\n",
      "(64, 33)\n",
      "step 4197, loss is 4.894208908081055\n",
      "(64, 33)\n",
      "step 4198, loss is 5.026804447174072\n",
      "(64, 33)\n",
      "step 4199, loss is 5.231338024139404\n",
      "(64, 33)\n",
      "step 4200, loss is 5.1127190589904785\n",
      "(64, 33)\n",
      "step 4201, loss is 5.120686054229736\n",
      "(64, 33)\n",
      "step 4202, loss is 4.938880443572998\n",
      "(64, 33)\n",
      "step 4203, loss is 5.212273120880127\n",
      "(64, 33)\n",
      "step 4204, loss is 5.102558135986328\n",
      "(64, 33)\n",
      "step 4205, loss is 5.098354339599609\n",
      "(64, 33)\n",
      "step 4206, loss is 5.109837532043457\n",
      "(64, 33)\n",
      "step 4207, loss is 5.2080559730529785\n",
      "(64, 33)\n",
      "step 4208, loss is 5.137143135070801\n",
      "(64, 33)\n",
      "step 4209, loss is 5.0532426834106445\n",
      "(64, 33)\n",
      "step 4210, loss is 5.255190849304199\n",
      "(64, 33)\n",
      "step 4211, loss is 4.915309429168701\n",
      "(64, 33)\n",
      "step 4212, loss is 5.09994649887085\n",
      "(64, 33)\n",
      "step 4213, loss is 5.053749084472656\n",
      "(64, 33)\n",
      "step 4214, loss is 5.0226640701293945\n",
      "(64, 33)\n",
      "step 4215, loss is 5.032327175140381\n",
      "(64, 33)\n",
      "step 4216, loss is 5.108433723449707\n",
      "(64, 33)\n",
      "step 4217, loss is 5.204011917114258\n",
      "(64, 33)\n",
      "step 4218, loss is 5.186153411865234\n",
      "(64, 33)\n",
      "step 4219, loss is 5.182523250579834\n",
      "(64, 33)\n",
      "step 4220, loss is 5.072196960449219\n",
      "(64, 33)\n",
      "step 4221, loss is 5.086155414581299\n",
      "(64, 33)\n",
      "step 4222, loss is 5.218919277191162\n",
      "(64, 33)\n",
      "step 4223, loss is 5.184110164642334\n",
      "(64, 33)\n",
      "step 4224, loss is 4.873862266540527\n",
      "(64, 33)\n",
      "step 4225, loss is 5.117974758148193\n",
      "(64, 33)\n",
      "step 4226, loss is 5.000561714172363\n",
      "(64, 33)\n",
      "step 4227, loss is 5.220130920410156\n",
      "(64, 33)\n",
      "step 4228, loss is 4.832794666290283\n",
      "(64, 33)\n",
      "step 4229, loss is 5.207211017608643\n",
      "(64, 33)\n",
      "step 4230, loss is 4.934597969055176\n",
      "(64, 33)\n",
      "step 4231, loss is 5.219282150268555\n",
      "(64, 33)\n",
      "step 4232, loss is 5.118112087249756\n",
      "(64, 33)\n",
      "step 4233, loss is 4.893915176391602\n",
      "(64, 33)\n",
      "step 4234, loss is 4.923999309539795\n",
      "(64, 33)\n",
      "step 4235, loss is 5.0623297691345215\n",
      "(64, 33)\n",
      "step 4236, loss is 5.146275520324707\n",
      "(64, 33)\n",
      "step 4237, loss is 5.039436340332031\n",
      "(64, 33)\n",
      "step 4238, loss is 5.096718788146973\n",
      "(64, 33)\n",
      "step 4239, loss is 5.0378618240356445\n",
      "(64, 33)\n",
      "step 4240, loss is 4.796169281005859\n",
      "(64, 33)\n",
      "step 4241, loss is 5.09879207611084\n",
      "(64, 33)\n",
      "step 4242, loss is 5.170395851135254\n",
      "(64, 33)\n",
      "step 4243, loss is 4.921447277069092\n",
      "(64, 33)\n",
      "step 4244, loss is 5.065400123596191\n",
      "(64, 33)\n",
      "step 4245, loss is 5.085120677947998\n",
      "(64, 33)\n",
      "step 4246, loss is 5.104681015014648\n",
      "(64, 33)\n",
      "step 4247, loss is 5.013315200805664\n",
      "(64, 33)\n",
      "step 4248, loss is 5.26727294921875\n",
      "(64, 33)\n",
      "step 4249, loss is 4.895225524902344\n",
      "(64, 33)\n",
      "step 4250, loss is 5.154476642608643\n",
      "(64, 33)\n",
      "step 4251, loss is 4.997768402099609\n",
      "(64, 33)\n",
      "step 4252, loss is 5.144248008728027\n",
      "(64, 33)\n",
      "step 4253, loss is 5.031925201416016\n",
      "(64, 33)\n",
      "step 4254, loss is 5.1059770584106445\n",
      "(64, 33)\n",
      "step 4255, loss is 4.989840030670166\n",
      "(64, 33)\n",
      "step 4256, loss is 5.017430305480957\n",
      "(64, 33)\n",
      "step 4257, loss is 5.236671447753906\n",
      "(64, 33)\n",
      "step 4258, loss is 5.195359230041504\n",
      "(64, 33)\n",
      "step 4259, loss is 4.996721267700195\n",
      "(64, 33)\n",
      "step 4260, loss is 5.237605094909668\n",
      "(64, 33)\n",
      "step 4261, loss is 4.969176292419434\n",
      "(64, 33)\n",
      "step 4262, loss is 5.036517143249512\n",
      "(64, 33)\n",
      "step 4263, loss is 5.155703544616699\n",
      "(64, 33)\n",
      "step 4264, loss is 5.113077163696289\n",
      "(64, 33)\n",
      "step 4265, loss is 5.166981220245361\n",
      "(64, 33)\n",
      "step 4266, loss is 5.136538982391357\n",
      "(64, 33)\n",
      "step 4267, loss is 5.129541873931885\n",
      "(64, 33)\n",
      "step 4268, loss is 5.048044681549072\n",
      "(64, 33)\n",
      "step 4269, loss is 5.116506099700928\n",
      "(64, 33)\n",
      "step 4270, loss is 4.926866054534912\n",
      "(64, 33)\n",
      "step 4271, loss is 5.19738245010376\n",
      "(64, 33)\n",
      "step 4272, loss is 5.154840469360352\n",
      "(64, 33)\n",
      "step 4273, loss is 5.05963134765625\n",
      "(64, 33)\n",
      "step 4274, loss is 5.001684665679932\n",
      "(64, 33)\n",
      "step 4275, loss is 5.015428066253662\n",
      "(64, 33)\n",
      "step 4276, loss is 5.126736640930176\n",
      "(64, 33)\n",
      "step 4277, loss is 4.945901393890381\n",
      "(64, 33)\n",
      "step 4278, loss is 5.181267738342285\n",
      "(64, 33)\n",
      "step 4279, loss is 4.982016563415527\n",
      "(64, 33)\n",
      "step 4280, loss is 5.208683967590332\n",
      "(64, 33)\n",
      "step 4281, loss is 5.166367053985596\n",
      "(64, 33)\n",
      "step 4282, loss is 4.98549222946167\n",
      "(64, 33)\n",
      "step 4283, loss is 5.062164783477783\n",
      "(64, 33)\n",
      "step 4284, loss is 5.030998229980469\n",
      "(64, 33)\n",
      "step 4285, loss is 5.047147274017334\n",
      "(64, 33)\n",
      "step 4286, loss is 4.795748710632324\n",
      "(64, 33)\n",
      "step 4287, loss is 4.940761566162109\n",
      "(64, 33)\n",
      "step 4288, loss is 5.14161491394043\n",
      "(64, 33)\n",
      "step 4289, loss is 5.063183307647705\n",
      "(64, 33)\n",
      "step 4290, loss is 4.943967342376709\n",
      "(64, 33)\n",
      "step 4291, loss is 4.97683048248291\n",
      "(64, 33)\n",
      "step 4292, loss is 4.938708782196045\n",
      "(64, 33)\n",
      "step 4293, loss is 4.984400272369385\n",
      "(64, 33)\n",
      "step 4294, loss is 5.145625114440918\n",
      "(64, 33)\n",
      "step 4295, loss is 5.206442832946777\n",
      "(64, 33)\n",
      "step 4296, loss is 5.062231063842773\n",
      "(64, 33)\n",
      "step 4297, loss is 5.078779220581055\n",
      "(64, 33)\n",
      "step 4298, loss is 4.8795390129089355\n",
      "(64, 33)\n",
      "step 4299, loss is 4.970007419586182\n",
      "(64, 33)\n",
      "step 4300, loss is 5.0674052238464355\n",
      "(64, 33)\n",
      "step 4301, loss is 5.087578773498535\n",
      "(64, 33)\n",
      "step 4302, loss is 5.031214714050293\n",
      "(64, 33)\n",
      "step 4303, loss is 5.184789657592773\n",
      "(64, 33)\n",
      "step 4304, loss is 5.170160293579102\n",
      "(64, 33)\n",
      "step 4305, loss is 5.119168758392334\n",
      "(64, 33)\n",
      "step 4306, loss is 5.144062519073486\n",
      "(64, 33)\n",
      "step 4307, loss is 4.894386291503906\n",
      "(64, 33)\n",
      "step 4308, loss is 5.016557693481445\n",
      "(64, 33)\n",
      "step 4309, loss is 4.896967887878418\n",
      "(64, 33)\n",
      "step 4310, loss is 5.1822357177734375\n",
      "(64, 33)\n",
      "step 4311, loss is 5.167272567749023\n",
      "(64, 33)\n",
      "step 4312, loss is 5.137355804443359\n",
      "(64, 33)\n",
      "step 4313, loss is 5.040955066680908\n",
      "(64, 33)\n",
      "step 4314, loss is 5.075113773345947\n",
      "(64, 33)\n",
      "step 4315, loss is 5.1074957847595215\n",
      "(64, 33)\n",
      "step 4316, loss is 5.078604221343994\n",
      "(64, 33)\n",
      "step 4317, loss is 5.001736164093018\n",
      "(64, 33)\n",
      "step 4318, loss is 4.92613410949707\n",
      "(64, 33)\n",
      "step 4319, loss is 5.055558681488037\n",
      "(64, 33)\n",
      "step 4320, loss is 4.990835666656494\n",
      "(64, 33)\n",
      "step 4321, loss is 5.072747707366943\n",
      "(64, 33)\n",
      "step 4322, loss is 5.014091968536377\n",
      "(64, 33)\n",
      "step 4323, loss is 4.900996685028076\n",
      "(64, 33)\n",
      "step 4324, loss is 5.0485429763793945\n",
      "(64, 33)\n",
      "step 4325, loss is 4.964608192443848\n",
      "(64, 33)\n",
      "step 4326, loss is 5.14107608795166\n",
      "(64, 33)\n",
      "step 4327, loss is 5.117876052856445\n",
      "(64, 33)\n",
      "step 4328, loss is 5.071154594421387\n",
      "(64, 33)\n",
      "step 4329, loss is 4.959278583526611\n",
      "(64, 33)\n",
      "step 4330, loss is 4.9518303871154785\n",
      "(64, 33)\n",
      "step 4331, loss is 4.909721374511719\n",
      "(64, 33)\n",
      "step 4332, loss is 5.018377304077148\n",
      "(64, 33)\n",
      "step 4333, loss is 5.100780963897705\n",
      "(64, 33)\n",
      "step 4334, loss is 4.921203136444092\n",
      "(64, 33)\n",
      "step 4335, loss is 5.047656059265137\n",
      "(64, 33)\n",
      "step 4336, loss is 5.018447399139404\n",
      "(64, 33)\n",
      "step 4337, loss is 5.010566711425781\n",
      "(64, 33)\n",
      "step 4338, loss is 4.972117900848389\n",
      "(64, 33)\n",
      "step 4339, loss is 4.890830993652344\n",
      "(64, 33)\n",
      "step 4340, loss is 5.258552074432373\n",
      "(64, 33)\n",
      "step 4341, loss is 5.046426296234131\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4342, loss is 4.928685188293457\n",
      "(64, 33)\n",
      "step 4343, loss is 4.907927513122559\n",
      "(64, 33)\n",
      "step 4344, loss is 4.966547012329102\n",
      "(64, 33)\n",
      "step 4345, loss is 4.955244064331055\n",
      "(64, 33)\n",
      "step 4346, loss is 4.980350494384766\n",
      "(64, 33)\n",
      "step 4347, loss is 5.0395188331604\n",
      "(64, 33)\n",
      "step 4348, loss is 5.0667619705200195\n",
      "(64, 33)\n",
      "step 4349, loss is 5.070628643035889\n",
      "(64, 33)\n",
      "step 4350, loss is 4.987188816070557\n",
      "(64, 33)\n",
      "step 4351, loss is 4.946871757507324\n",
      "(64, 33)\n",
      "step 4352, loss is 5.20744514465332\n",
      "(64, 33)\n",
      "step 4353, loss is 5.045505523681641\n",
      "(64, 33)\n",
      "step 4354, loss is 5.145846843719482\n",
      "(64, 33)\n",
      "step 4355, loss is 5.011372089385986\n",
      "(64, 33)\n",
      "step 4356, loss is 5.029407978057861\n",
      "(64, 33)\n",
      "step 4357, loss is 4.938331604003906\n",
      "(64, 33)\n",
      "step 4358, loss is 4.978890895843506\n",
      "(64, 33)\n",
      "step 4359, loss is 5.055393695831299\n",
      "(64, 33)\n",
      "step 4360, loss is 5.039272308349609\n",
      "(64, 33)\n",
      "step 4361, loss is 5.142282962799072\n",
      "(64, 33)\n",
      "step 4362, loss is 4.9513115882873535\n",
      "(64, 33)\n",
      "step 4363, loss is 4.927788734436035\n",
      "(64, 33)\n",
      "step 4364, loss is 5.142873287200928\n",
      "(64, 33)\n",
      "step 4365, loss is 5.074446678161621\n",
      "(64, 33)\n",
      "step 4366, loss is 4.92477560043335\n",
      "(64, 33)\n",
      "step 4367, loss is 5.0100932121276855\n",
      "(64, 33)\n",
      "step 4368, loss is 4.954991817474365\n",
      "(64, 33)\n",
      "step 4369, loss is 5.021597862243652\n",
      "(64, 33)\n",
      "step 4370, loss is 5.093869209289551\n",
      "(64, 33)\n",
      "step 4371, loss is 4.931476593017578\n",
      "(64, 33)\n",
      "step 4372, loss is 4.979917049407959\n",
      "(64, 33)\n",
      "step 4373, loss is 4.973544120788574\n",
      "(64, 33)\n",
      "step 4374, loss is 5.06373405456543\n",
      "(64, 33)\n",
      "step 4375, loss is 5.237596035003662\n",
      "(64, 33)\n",
      "step 4376, loss is 5.030867099761963\n",
      "(64, 33)\n",
      "step 4377, loss is 5.142634868621826\n",
      "(64, 33)\n",
      "step 4378, loss is 4.9708333015441895\n",
      "(64, 33)\n",
      "step 4379, loss is 5.244546890258789\n",
      "(64, 33)\n",
      "step 4380, loss is 5.0828728675842285\n",
      "(64, 33)\n",
      "step 4381, loss is 4.878839492797852\n",
      "(64, 33)\n",
      "step 4382, loss is 5.114602088928223\n",
      "(64, 33)\n",
      "step 4383, loss is 5.039164066314697\n",
      "(64, 33)\n",
      "step 4384, loss is 5.153111457824707\n",
      "(64, 33)\n",
      "step 4385, loss is 5.017416477203369\n",
      "(64, 33)\n",
      "step 4386, loss is 4.749814987182617\n",
      "(64, 33)\n",
      "step 4387, loss is 4.965653896331787\n",
      "(64, 33)\n",
      "step 4388, loss is 4.975790500640869\n",
      "(64, 33)\n",
      "step 4389, loss is 5.035820484161377\n",
      "(64, 33)\n",
      "step 4390, loss is 4.8927459716796875\n",
      "(64, 33)\n",
      "step 4391, loss is 5.077516078948975\n",
      "(64, 33)\n",
      "step 4392, loss is 4.735034942626953\n",
      "(64, 33)\n",
      "step 4393, loss is 5.035895824432373\n",
      "(64, 33)\n",
      "step 4394, loss is 5.018304824829102\n",
      "(64, 33)\n",
      "step 4395, loss is 4.983589172363281\n",
      "(64, 33)\n",
      "step 4396, loss is 4.976360321044922\n",
      "(64, 33)\n",
      "step 4397, loss is 5.001471519470215\n",
      "(64, 33)\n",
      "step 4398, loss is 5.172277450561523\n",
      "(64, 33)\n",
      "step 4399, loss is 5.039527416229248\n",
      "(64, 33)\n",
      "step 4400, loss is 4.884803771972656\n",
      "(64, 33)\n",
      "step 4401, loss is 5.0242486000061035\n",
      "(64, 33)\n",
      "step 4402, loss is 5.163473606109619\n",
      "(64, 33)\n",
      "step 4403, loss is 5.036062240600586\n",
      "(64, 33)\n",
      "step 4404, loss is 5.085953712463379\n",
      "(64, 33)\n",
      "step 4405, loss is 4.986124038696289\n",
      "(64, 33)\n",
      "step 4406, loss is 5.0638508796691895\n",
      "(64, 33)\n",
      "step 4407, loss is 5.04675817489624\n",
      "(64, 33)\n",
      "step 4408, loss is 5.012400150299072\n",
      "(64, 33)\n",
      "step 4409, loss is 5.090134143829346\n",
      "(64, 33)\n",
      "step 4410, loss is 5.0882182121276855\n",
      "(64, 33)\n",
      "step 4411, loss is 5.223483562469482\n",
      "(64, 33)\n",
      "step 4412, loss is 5.095574378967285\n",
      "(64, 33)\n",
      "step 4413, loss is 4.982538223266602\n",
      "(64, 33)\n",
      "step 4414, loss is 5.047196865081787\n",
      "(64, 33)\n",
      "step 4415, loss is 5.047089099884033\n",
      "(64, 33)\n",
      "step 4416, loss is 5.013099193572998\n",
      "(64, 33)\n",
      "step 4417, loss is 5.091215133666992\n",
      "(64, 33)\n",
      "step 4418, loss is 4.947287559509277\n",
      "(64, 33)\n",
      "step 4419, loss is 4.910778522491455\n",
      "(64, 33)\n",
      "step 4420, loss is 5.08341121673584\n",
      "(64, 33)\n",
      "step 4421, loss is 5.155559539794922\n",
      "(64, 33)\n",
      "step 4422, loss is 5.0732831954956055\n",
      "(64, 33)\n",
      "step 4423, loss is 5.16664981842041\n",
      "(64, 33)\n",
      "step 4424, loss is 5.083775043487549\n",
      "(64, 33)\n",
      "step 4425, loss is 4.920223712921143\n",
      "(64, 33)\n",
      "step 4426, loss is 5.091400623321533\n",
      "(64, 33)\n",
      "step 4427, loss is 4.991257667541504\n",
      "(64, 33)\n",
      "step 4428, loss is 5.206444263458252\n",
      "(64, 33)\n",
      "step 4429, loss is 4.899991512298584\n",
      "(64, 33)\n",
      "step 4430, loss is 5.086463451385498\n",
      "(64, 33)\n",
      "step 4431, loss is 5.058342456817627\n",
      "(64, 33)\n",
      "step 4432, loss is 5.253804683685303\n",
      "(64, 33)\n",
      "step 4433, loss is 5.130316734313965\n",
      "(64, 33)\n",
      "step 4434, loss is 4.845044136047363\n",
      "(64, 33)\n",
      "step 4435, loss is 5.1089277267456055\n",
      "(64, 33)\n",
      "step 4436, loss is 5.0084309577941895\n",
      "(64, 33)\n",
      "step 4437, loss is 5.138671875\n",
      "(64, 33)\n",
      "step 4438, loss is 4.967187881469727\n",
      "(64, 33)\n",
      "step 4439, loss is 5.169637203216553\n",
      "(64, 33)\n",
      "step 4440, loss is 5.109250068664551\n",
      "(64, 33)\n",
      "step 4441, loss is 5.144186973571777\n",
      "(64, 33)\n",
      "step 4442, loss is 5.079555988311768\n",
      "(64, 33)\n",
      "step 4443, loss is 5.020390033721924\n",
      "(64, 33)\n",
      "step 4444, loss is 5.0833024978637695\n",
      "(64, 33)\n",
      "step 4445, loss is 5.124382972717285\n",
      "(64, 33)\n",
      "step 4446, loss is 5.1582865715026855\n",
      "(64, 33)\n",
      "step 4447, loss is 5.012134552001953\n",
      "(64, 33)\n",
      "step 4448, loss is 4.886964797973633\n",
      "(64, 33)\n",
      "step 4449, loss is 5.073008060455322\n",
      "(64, 33)\n",
      "step 4450, loss is 5.081135272979736\n",
      "(64, 33)\n",
      "step 4451, loss is 5.182775497436523\n",
      "(64, 33)\n",
      "step 4452, loss is 4.965217590332031\n",
      "(64, 33)\n",
      "step 4453, loss is 4.888459205627441\n",
      "(64, 33)\n",
      "step 4454, loss is 5.160768508911133\n",
      "(64, 33)\n",
      "step 4455, loss is 4.959855556488037\n",
      "(64, 33)\n",
      "step 4456, loss is 5.182086944580078\n",
      "(64, 33)\n",
      "step 4457, loss is 5.098741054534912\n",
      "(64, 33)\n",
      "step 4458, loss is 5.024986743927002\n",
      "(64, 33)\n",
      "step 4459, loss is 4.9929070472717285\n",
      "(64, 33)\n",
      "step 4460, loss is 5.133559703826904\n",
      "(64, 33)\n",
      "step 4461, loss is 4.87345027923584\n",
      "(64, 33)\n",
      "step 4462, loss is 4.901216506958008\n",
      "(64, 33)\n",
      "step 4463, loss is 5.1299004554748535\n",
      "(64, 33)\n",
      "step 4464, loss is 4.8979573249816895\n",
      "(64, 33)\n",
      "step 4465, loss is 5.094001770019531\n",
      "(64, 33)\n",
      "step 4466, loss is 5.213100910186768\n",
      "(64, 33)\n",
      "step 4467, loss is 4.955076217651367\n",
      "(64, 33)\n",
      "step 4468, loss is 4.952615737915039\n",
      "(64, 33)\n",
      "step 4469, loss is 5.175993919372559\n",
      "(64, 33)\n",
      "step 4470, loss is 5.062066555023193\n",
      "(64, 33)\n",
      "step 4471, loss is 5.228233814239502\n",
      "(64, 33)\n",
      "step 4472, loss is 4.94814920425415\n",
      "(64, 33)\n",
      "step 4473, loss is 5.048739910125732\n",
      "(64, 33)\n",
      "step 4474, loss is 5.0118842124938965\n",
      "(64, 33)\n",
      "step 4475, loss is 4.923464775085449\n",
      "(64, 33)\n",
      "step 4476, loss is 5.115190505981445\n",
      "(64, 33)\n",
      "step 4477, loss is 5.061677932739258\n",
      "(64, 33)\n",
      "step 4478, loss is 5.100828170776367\n",
      "(64, 33)\n",
      "step 4479, loss is 5.146924018859863\n",
      "(64, 33)\n",
      "step 4480, loss is 4.9019246101379395\n",
      "(64, 33)\n",
      "step 4481, loss is 5.0782575607299805\n",
      "(64, 33)\n",
      "step 4482, loss is 5.039519309997559\n",
      "(64, 33)\n",
      "step 4483, loss is 5.170051574707031\n",
      "(64, 33)\n",
      "step 4484, loss is 5.078143119812012\n",
      "(64, 33)\n",
      "step 4485, loss is 5.0565900802612305\n",
      "(64, 33)\n",
      "step 4486, loss is 5.050840377807617\n",
      "(64, 33)\n",
      "step 4487, loss is 5.112895965576172\n",
      "(64, 33)\n",
      "step 4488, loss is 5.09948205947876\n",
      "(64, 33)\n",
      "step 4489, loss is 4.961233139038086\n",
      "(64, 33)\n",
      "step 4490, loss is 5.077592849731445\n",
      "(64, 33)\n",
      "step 4491, loss is 5.040084362030029\n",
      "(64, 33)\n",
      "step 4492, loss is 5.075908184051514\n",
      "(64, 33)\n",
      "step 4493, loss is 5.142001628875732\n",
      "(64, 33)\n",
      "step 4494, loss is 5.09397554397583\n",
      "(64, 33)\n",
      "step 4495, loss is 4.927349090576172\n",
      "(64, 33)\n",
      "step 4496, loss is 5.238289833068848\n",
      "(64, 33)\n",
      "step 4497, loss is 5.246496200561523\n",
      "(64, 33)\n",
      "step 4498, loss is 4.872840881347656\n",
      "(64, 33)\n",
      "step 4499, loss is 5.026124477386475\n",
      "(64, 33)\n",
      "step 4500, loss is 5.169162273406982\n",
      "(64, 33)\n",
      "step 4501, loss is 4.921474456787109\n",
      "(64, 33)\n",
      "step 4502, loss is 5.158248424530029\n",
      "(64, 33)\n",
      "step 4503, loss is 5.191067695617676\n",
      "(64, 33)\n",
      "step 4504, loss is 5.106401443481445\n",
      "(64, 33)\n",
      "step 4505, loss is 5.128807067871094\n",
      "(64, 33)\n",
      "step 4506, loss is 5.007321357727051\n",
      "(64, 33)\n",
      "step 4507, loss is 4.951220989227295\n",
      "(64, 33)\n",
      "step 4508, loss is 4.828368186950684\n",
      "(64, 33)\n",
      "step 4509, loss is 4.893093109130859\n",
      "(64, 33)\n",
      "step 4510, loss is 5.196120262145996\n",
      "(64, 33)\n",
      "step 4511, loss is 5.056020259857178\n",
      "(64, 33)\n",
      "step 4512, loss is 5.009859561920166\n",
      "(64, 33)\n",
      "step 4513, loss is 5.02769136428833\n",
      "(64, 33)\n",
      "step 4514, loss is 5.2023749351501465\n",
      "(64, 33)\n",
      "step 4515, loss is 5.201972961425781\n",
      "(64, 33)\n",
      "step 4516, loss is 5.121133804321289\n",
      "(64, 33)\n",
      "step 4517, loss is 5.046830177307129\n",
      "(64, 33)\n",
      "step 4518, loss is 5.078717231750488\n",
      "(64, 33)\n",
      "step 4519, loss is 5.026294708251953\n",
      "(64, 33)\n",
      "step 4520, loss is 5.099036693572998\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4521, loss is 5.03034782409668\n",
      "(64, 33)\n",
      "step 4522, loss is 5.109214782714844\n",
      "(64, 33)\n",
      "step 4523, loss is 4.836089611053467\n",
      "(64, 33)\n",
      "step 4524, loss is 5.0177693367004395\n",
      "(64, 33)\n",
      "step 4525, loss is 5.017734050750732\n",
      "(64, 33)\n",
      "step 4526, loss is 5.117671966552734\n",
      "(64, 33)\n",
      "step 4527, loss is 4.87723970413208\n",
      "(64, 33)\n",
      "step 4528, loss is 5.107872009277344\n",
      "(64, 33)\n",
      "step 4529, loss is 4.974142074584961\n",
      "(64, 33)\n",
      "step 4530, loss is 5.155250549316406\n",
      "(64, 33)\n",
      "step 4531, loss is 5.125822067260742\n",
      "(64, 33)\n",
      "step 4532, loss is 4.8957390785217285\n",
      "(64, 33)\n",
      "step 4533, loss is 4.893501281738281\n",
      "(64, 33)\n",
      "step 4534, loss is 5.045909404754639\n",
      "(64, 33)\n",
      "step 4535, loss is 4.879258632659912\n",
      "(64, 33)\n",
      "step 4536, loss is 5.078886032104492\n",
      "(64, 33)\n",
      "step 4537, loss is 4.957295894622803\n",
      "(64, 33)\n",
      "step 4538, loss is 5.0018110275268555\n",
      "(64, 33)\n",
      "step 4539, loss is 5.1086039543151855\n",
      "(64, 33)\n",
      "step 4540, loss is 5.159560680389404\n",
      "(64, 33)\n",
      "step 4541, loss is 5.182102203369141\n",
      "(64, 33)\n",
      "step 4542, loss is 5.107907772064209\n",
      "(64, 33)\n",
      "step 4543, loss is 5.317441940307617\n",
      "(64, 33)\n",
      "step 4544, loss is 5.050748348236084\n",
      "(64, 33)\n",
      "step 4545, loss is 5.187727451324463\n",
      "(64, 33)\n",
      "step 4546, loss is 5.2839741706848145\n",
      "(64, 33)\n",
      "step 4547, loss is 5.126528263092041\n",
      "(64, 33)\n",
      "step 4548, loss is 4.937386512756348\n",
      "(64, 33)\n",
      "step 4549, loss is 4.951576232910156\n",
      "(64, 33)\n",
      "step 4550, loss is 4.812909126281738\n",
      "(64, 33)\n",
      "step 4551, loss is 5.060481071472168\n",
      "(64, 33)\n",
      "step 4552, loss is 4.967686176300049\n",
      "(64, 33)\n",
      "step 4553, loss is 5.24653434753418\n",
      "(64, 33)\n",
      "step 4554, loss is 5.025352954864502\n",
      "(64, 33)\n",
      "step 4555, loss is 4.710827827453613\n",
      "(64, 33)\n",
      "step 4556, loss is 4.991495132446289\n",
      "(64, 33)\n",
      "step 4557, loss is 4.7925496101379395\n",
      "(64, 33)\n",
      "step 4558, loss is 4.936828136444092\n",
      "(64, 33)\n",
      "step 4559, loss is 4.90588903427124\n",
      "(64, 33)\n",
      "step 4560, loss is 5.050901412963867\n",
      "(64, 33)\n",
      "step 4561, loss is 5.054581642150879\n",
      "(64, 33)\n",
      "step 4562, loss is 4.843470096588135\n",
      "(64, 33)\n",
      "step 4563, loss is 5.17519474029541\n",
      "(64, 33)\n",
      "step 4564, loss is 5.10699462890625\n",
      "(64, 33)\n",
      "step 4565, loss is 4.9371161460876465\n",
      "(64, 33)\n",
      "step 4566, loss is 5.07991361618042\n",
      "(64, 33)\n",
      "step 4567, loss is 5.094231605529785\n",
      "(64, 33)\n",
      "step 4568, loss is 5.111985683441162\n",
      "(64, 33)\n",
      "step 4569, loss is 5.028864860534668\n",
      "(64, 33)\n",
      "step 4570, loss is 4.984952926635742\n",
      "(64, 33)\n",
      "step 4571, loss is 5.181162357330322\n",
      "(64, 33)\n",
      "step 4572, loss is 5.011137962341309\n",
      "(64, 33)\n",
      "step 4573, loss is 5.090221405029297\n",
      "(64, 33)\n",
      "step 4574, loss is 5.099102973937988\n",
      "(64, 33)\n",
      "step 4575, loss is 4.998556613922119\n",
      "(64, 33)\n",
      "step 4576, loss is 5.1458868980407715\n",
      "(64, 33)\n",
      "step 4577, loss is 5.293312072753906\n",
      "(64, 33)\n",
      "step 4578, loss is 4.945534706115723\n",
      "(64, 33)\n",
      "step 4579, loss is 4.905142307281494\n",
      "(64, 33)\n",
      "step 4580, loss is 5.025952339172363\n",
      "(64, 33)\n",
      "step 4581, loss is 4.974437713623047\n",
      "(64, 33)\n",
      "step 4582, loss is 4.9168620109558105\n",
      "(64, 33)\n",
      "step 4583, loss is 4.885419845581055\n",
      "(64, 33)\n",
      "step 4584, loss is 4.803488254547119\n",
      "(64, 33)\n",
      "step 4585, loss is 4.878891944885254\n",
      "(64, 33)\n",
      "step 4586, loss is 5.030176639556885\n",
      "(64, 33)\n",
      "step 4587, loss is 4.845585823059082\n",
      "(64, 33)\n",
      "step 4588, loss is 4.957765102386475\n",
      "(64, 33)\n",
      "step 4589, loss is 4.9086689949035645\n",
      "(64, 33)\n",
      "step 4590, loss is 5.085890293121338\n",
      "(64, 33)\n",
      "step 4591, loss is 5.0151495933532715\n",
      "(64, 33)\n",
      "step 4592, loss is 4.916768550872803\n",
      "(64, 33)\n",
      "step 4593, loss is 5.156921863555908\n",
      "(64, 33)\n",
      "step 4594, loss is 5.044839382171631\n",
      "(64, 33)\n",
      "step 4595, loss is 5.170778274536133\n",
      "(64, 33)\n",
      "step 4596, loss is 5.099226951599121\n",
      "(64, 33)\n",
      "step 4597, loss is 5.115512847900391\n",
      "(64, 33)\n",
      "step 4598, loss is 5.068942546844482\n",
      "(64, 33)\n",
      "step 4599, loss is 5.060986042022705\n",
      "(64, 33)\n",
      "step 4600, loss is 5.007549285888672\n",
      "(64, 33)\n",
      "step 4601, loss is 4.957908630371094\n",
      "(64, 33)\n",
      "step 4602, loss is 4.948184967041016\n",
      "(64, 33)\n",
      "step 4603, loss is 4.8538289070129395\n",
      "(64, 33)\n",
      "step 4604, loss is 4.708286762237549\n",
      "(64, 33)\n",
      "step 4605, loss is 5.049919128417969\n",
      "(64, 33)\n",
      "step 4606, loss is 4.901039123535156\n",
      "(64, 33)\n",
      "step 4607, loss is 5.043431282043457\n",
      "(64, 33)\n",
      "step 4608, loss is 5.107452869415283\n",
      "(64, 33)\n",
      "step 4609, loss is 4.992076396942139\n",
      "(64, 33)\n",
      "step 4610, loss is 4.776837348937988\n",
      "(64, 33)\n",
      "step 4611, loss is 4.937099933624268\n",
      "(64, 33)\n",
      "step 4612, loss is 5.078824996948242\n",
      "(64, 33)\n",
      "step 4613, loss is 4.8813910484313965\n",
      "(64, 33)\n",
      "step 4614, loss is 5.080310344696045\n",
      "(64, 33)\n",
      "step 4615, loss is 4.896252632141113\n",
      "(64, 33)\n",
      "step 4616, loss is 5.036123275756836\n",
      "(64, 33)\n",
      "step 4617, loss is 5.157049655914307\n",
      "(64, 33)\n",
      "step 4618, loss is 5.105081081390381\n",
      "(64, 33)\n",
      "step 4619, loss is 5.087989807128906\n",
      "(64, 33)\n",
      "step 4620, loss is 4.964343547821045\n",
      "(64, 33)\n",
      "step 4621, loss is 4.905765056610107\n",
      "(64, 33)\n",
      "step 4622, loss is 5.061190605163574\n",
      "(64, 33)\n",
      "step 4623, loss is 4.853328704833984\n",
      "(64, 33)\n",
      "step 4624, loss is 5.062274932861328\n",
      "(64, 33)\n",
      "step 4625, loss is 4.871738433837891\n",
      "(64, 33)\n",
      "step 4626, loss is 5.080715179443359\n",
      "(64, 33)\n",
      "step 4627, loss is 4.927175521850586\n",
      "(64, 33)\n",
      "step 4628, loss is 5.0388383865356445\n",
      "(64, 33)\n",
      "step 4629, loss is 5.1344685554504395\n",
      "(64, 33)\n",
      "step 4630, loss is 4.968352317810059\n",
      "(64, 33)\n",
      "step 4631, loss is 4.946156978607178\n",
      "(64, 33)\n",
      "step 4632, loss is 5.0019001960754395\n",
      "(64, 33)\n",
      "step 4633, loss is 5.088834285736084\n",
      "(64, 33)\n",
      "step 4634, loss is 5.105677604675293\n",
      "(64, 33)\n",
      "step 4635, loss is 5.015888690948486\n",
      "(64, 33)\n",
      "step 4636, loss is 4.900790691375732\n",
      "(64, 33)\n",
      "step 4637, loss is 4.722131729125977\n",
      "(64, 33)\n",
      "step 4638, loss is 5.26896858215332\n",
      "(64, 33)\n",
      "step 4639, loss is 5.129203796386719\n",
      "(64, 33)\n",
      "step 4640, loss is 4.848968982696533\n",
      "(64, 33)\n",
      "step 4641, loss is 5.106647968292236\n",
      "(64, 33)\n",
      "step 4642, loss is 4.8594536781311035\n",
      "(64, 33)\n",
      "step 4643, loss is 5.092271327972412\n",
      "(64, 33)\n",
      "step 4644, loss is 4.954453945159912\n",
      "(64, 33)\n",
      "step 4645, loss is 4.99213981628418\n",
      "(64, 33)\n",
      "step 4646, loss is 4.925240516662598\n",
      "(64, 33)\n",
      "step 4647, loss is 5.128422260284424\n",
      "(64, 33)\n",
      "step 4648, loss is 5.062103271484375\n",
      "(64, 33)\n",
      "step 4649, loss is 5.154001712799072\n",
      "(64, 33)\n",
      "step 4650, loss is 4.972114562988281\n",
      "(64, 33)\n",
      "step 4651, loss is 5.030730247497559\n",
      "(64, 33)\n",
      "step 4652, loss is 4.931117534637451\n",
      "(64, 33)\n",
      "step 4653, loss is 4.932348728179932\n",
      "(64, 33)\n",
      "step 4654, loss is 5.082151889801025\n",
      "(64, 33)\n",
      "step 4655, loss is 5.058361530303955\n",
      "(64, 33)\n",
      "step 4656, loss is 4.9234938621521\n",
      "(64, 33)\n",
      "step 4657, loss is 5.085118293762207\n",
      "(64, 33)\n",
      "step 4658, loss is 4.859368801116943\n",
      "(64, 33)\n",
      "step 4659, loss is 4.762935161590576\n",
      "(64, 33)\n",
      "step 4660, loss is 5.049809455871582\n",
      "(64, 33)\n",
      "step 4661, loss is 5.01731014251709\n",
      "(64, 33)\n",
      "step 4662, loss is 5.061897277832031\n",
      "(64, 33)\n",
      "step 4663, loss is 4.8373284339904785\n",
      "(64, 33)\n",
      "step 4664, loss is 5.003391265869141\n",
      "(64, 33)\n",
      "step 4665, loss is 4.941368579864502\n",
      "(64, 33)\n",
      "step 4666, loss is 4.816786289215088\n",
      "(64, 33)\n",
      "step 4667, loss is 4.906364917755127\n",
      "(64, 33)\n",
      "step 4668, loss is 5.037569046020508\n",
      "(64, 33)\n",
      "step 4669, loss is 5.035668849945068\n",
      "(64, 33)\n",
      "step 4670, loss is 5.006957054138184\n",
      "(64, 33)\n",
      "step 4671, loss is 4.896861553192139\n",
      "(64, 33)\n",
      "step 4672, loss is 4.9501051902771\n",
      "(64, 33)\n",
      "step 4673, loss is 5.081957817077637\n",
      "(64, 33)\n",
      "step 4674, loss is 5.00527811050415\n",
      "(64, 33)\n",
      "step 4675, loss is 4.965290546417236\n",
      "(64, 33)\n",
      "step 4676, loss is 4.916021347045898\n",
      "(64, 33)\n",
      "step 4677, loss is 5.032600402832031\n",
      "(64, 33)\n",
      "step 4678, loss is 4.945617198944092\n",
      "(64, 33)\n",
      "step 4679, loss is 4.9330878257751465\n",
      "(64, 33)\n",
      "step 4680, loss is 4.948368549346924\n",
      "(64, 33)\n",
      "step 4681, loss is 4.9336724281311035\n",
      "(64, 33)\n",
      "step 4682, loss is 5.107690811157227\n",
      "(64, 33)\n",
      "step 4683, loss is 4.879653453826904\n",
      "(64, 33)\n",
      "step 4684, loss is 5.082027435302734\n",
      "(64, 33)\n",
      "step 4685, loss is 5.043891906738281\n",
      "(64, 33)\n",
      "step 4686, loss is 5.013028144836426\n",
      "(64, 33)\n",
      "step 4687, loss is 5.004444599151611\n",
      "(64, 33)\n",
      "step 4688, loss is 4.89761209487915\n",
      "(64, 33)\n",
      "step 4689, loss is 5.070054054260254\n",
      "(64, 33)\n",
      "step 4690, loss is 5.077418327331543\n",
      "(64, 33)\n",
      "step 4691, loss is 5.041769504547119\n",
      "(64, 33)\n",
      "step 4692, loss is 5.006393909454346\n",
      "(64, 33)\n",
      "step 4693, loss is 5.205031871795654\n",
      "(64, 33)\n",
      "step 4694, loss is 5.071040153503418\n",
      "(64, 33)\n",
      "step 4695, loss is 5.017988204956055\n",
      "(64, 33)\n",
      "step 4696, loss is 4.95725679397583\n",
      "(64, 33)\n",
      "step 4697, loss is 5.013756275177002\n",
      "(64, 33)\n",
      "step 4698, loss is 4.872045993804932\n",
      "(64, 33)\n",
      "step 4699, loss is 5.133157253265381\n",
      "(64, 33)\n",
      "step 4700, loss is 4.875852584838867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 4701, loss is 5.067786693572998\n",
      "(64, 33)\n",
      "step 4702, loss is 5.099691867828369\n",
      "(64, 33)\n",
      "step 4703, loss is 5.180342674255371\n",
      "(64, 33)\n",
      "step 4704, loss is 4.979053020477295\n",
      "(64, 33)\n",
      "step 4705, loss is 4.994431495666504\n",
      "(64, 33)\n",
      "step 4706, loss is 4.961948394775391\n",
      "(64, 33)\n",
      "step 4707, loss is 5.182365894317627\n",
      "(64, 33)\n",
      "step 4708, loss is 4.975432872772217\n",
      "(64, 33)\n",
      "step 4709, loss is 5.068984508514404\n",
      "(64, 33)\n",
      "step 4710, loss is 5.049276351928711\n",
      "(64, 33)\n",
      "step 4711, loss is 5.150604724884033\n",
      "(64, 33)\n",
      "step 4712, loss is 5.065868854522705\n",
      "(64, 33)\n",
      "step 4713, loss is 5.0537285804748535\n",
      "(64, 33)\n",
      "step 4714, loss is 4.971174716949463\n",
      "(64, 33)\n",
      "step 4715, loss is 5.064454555511475\n",
      "(64, 33)\n",
      "step 4716, loss is 5.018131256103516\n",
      "(64, 33)\n",
      "step 4717, loss is 5.0643510818481445\n",
      "(64, 33)\n",
      "step 4718, loss is 5.098757743835449\n",
      "(64, 33)\n",
      "step 4719, loss is 4.989900588989258\n",
      "(64, 33)\n",
      "step 4720, loss is 4.868878364562988\n",
      "(64, 33)\n",
      "step 4721, loss is 5.078856945037842\n",
      "(64, 33)\n",
      "step 4722, loss is 4.828619480133057\n",
      "(64, 33)\n",
      "step 4723, loss is 5.13260555267334\n",
      "(64, 33)\n",
      "step 4724, loss is 5.013583660125732\n",
      "(64, 33)\n",
      "step 4725, loss is 5.215163707733154\n",
      "(64, 33)\n",
      "step 4726, loss is 5.104329586029053\n",
      "(64, 33)\n",
      "step 4727, loss is 5.1413960456848145\n",
      "(64, 33)\n",
      "step 4728, loss is 5.091498851776123\n",
      "(64, 33)\n",
      "step 4729, loss is 4.906434535980225\n",
      "(64, 33)\n",
      "step 4730, loss is 4.910569667816162\n",
      "(64, 33)\n",
      "step 4731, loss is 5.126479625701904\n",
      "(64, 33)\n",
      "step 4732, loss is 5.032817840576172\n",
      "(64, 33)\n",
      "step 4733, loss is 4.999223232269287\n",
      "(64, 33)\n",
      "step 4734, loss is 4.791203498840332\n",
      "(64, 33)\n",
      "step 4735, loss is 5.108094215393066\n",
      "(64, 33)\n",
      "step 4736, loss is 4.867154121398926\n",
      "(64, 33)\n",
      "step 4737, loss is 4.727789878845215\n",
      "(64, 33)\n",
      "step 4738, loss is 5.014657974243164\n",
      "(64, 33)\n",
      "step 4739, loss is 4.994318008422852\n",
      "(64, 33)\n",
      "step 4740, loss is 5.042375564575195\n",
      "(64, 33)\n",
      "step 4741, loss is 5.019899845123291\n",
      "(64, 33)\n",
      "step 4742, loss is 5.152237415313721\n",
      "(64, 33)\n",
      "step 4743, loss is 5.215087413787842\n",
      "(64, 33)\n",
      "step 4744, loss is 5.106040954589844\n",
      "(64, 33)\n",
      "step 4745, loss is 4.980207443237305\n",
      "(64, 33)\n",
      "step 4746, loss is 5.011819839477539\n",
      "(64, 33)\n",
      "step 4747, loss is 4.916332721710205\n",
      "(64, 33)\n",
      "step 4748, loss is 4.947099208831787\n",
      "(64, 33)\n",
      "step 4749, loss is 5.154115200042725\n",
      "(64, 33)\n",
      "step 4750, loss is 5.1649088859558105\n",
      "(64, 33)\n",
      "step 4751, loss is 4.8602118492126465\n",
      "(64, 33)\n",
      "step 4752, loss is 4.861715316772461\n",
      "(64, 33)\n",
      "step 4753, loss is 5.058098793029785\n",
      "(64, 33)\n",
      "step 4754, loss is 5.025704860687256\n",
      "(64, 33)\n",
      "step 4755, loss is 5.156613826751709\n",
      "(64, 33)\n",
      "step 4756, loss is 4.960592269897461\n",
      "(64, 33)\n",
      "step 4757, loss is 4.992166996002197\n",
      "(64, 33)\n",
      "step 4758, loss is 4.9533867835998535\n",
      "(64, 33)\n",
      "step 4759, loss is 4.815377235412598\n",
      "(64, 33)\n",
      "step 4760, loss is 5.127176761627197\n",
      "(64, 33)\n",
      "step 4761, loss is 4.846511363983154\n",
      "(64, 33)\n",
      "step 4762, loss is 5.060365200042725\n",
      "(64, 33)\n",
      "step 4763, loss is 5.053234100341797\n",
      "(64, 33)\n",
      "step 4764, loss is 5.053158283233643\n",
      "(64, 33)\n",
      "step 4765, loss is 4.8884663581848145\n",
      "(64, 33)\n",
      "step 4766, loss is 4.989199638366699\n",
      "(64, 33)\n",
      "step 4767, loss is 4.938494682312012\n",
      "(64, 33)\n",
      "step 4768, loss is 4.926490783691406\n",
      "(64, 33)\n",
      "step 4769, loss is 4.969069957733154\n",
      "(64, 33)\n",
      "step 4770, loss is 5.023808479309082\n",
      "(64, 33)\n",
      "step 4771, loss is 5.160651683807373\n",
      "(64, 33)\n",
      "step 4772, loss is 5.0277838706970215\n",
      "(64, 33)\n",
      "step 4773, loss is 5.12310791015625\n",
      "(64, 33)\n",
      "step 4774, loss is 4.661024570465088\n",
      "(64, 33)\n",
      "step 4775, loss is 5.056552410125732\n",
      "(64, 33)\n",
      "step 4776, loss is 4.949726581573486\n",
      "(64, 33)\n",
      "step 4777, loss is 4.971748352050781\n",
      "(64, 33)\n",
      "step 4778, loss is 4.901521682739258\n",
      "(64, 33)\n",
      "step 4779, loss is 4.913695335388184\n",
      "(64, 33)\n",
      "step 4780, loss is 4.94005012512207\n",
      "(64, 33)\n",
      "step 4781, loss is 5.067521572113037\n",
      "(64, 33)\n",
      "step 4782, loss is 5.097108364105225\n",
      "(64, 33)\n",
      "step 4783, loss is 4.983458042144775\n",
      "(64, 33)\n",
      "step 4784, loss is 5.0144572257995605\n",
      "(64, 33)\n",
      "step 4785, loss is 4.893641948699951\n",
      "(64, 33)\n",
      "step 4786, loss is 5.003920078277588\n",
      "(64, 33)\n",
      "step 4787, loss is 5.268401622772217\n",
      "(64, 33)\n",
      "step 4788, loss is 4.938401699066162\n",
      "(64, 33)\n",
      "step 4789, loss is 5.076195240020752\n",
      "(64, 33)\n",
      "step 4790, loss is 5.14048957824707\n",
      "(64, 33)\n",
      "step 4791, loss is 4.911148548126221\n",
      "(64, 33)\n",
      "step 4792, loss is 5.115068435668945\n",
      "(64, 33)\n",
      "step 4793, loss is 4.9950103759765625\n",
      "(64, 33)\n",
      "step 4794, loss is 5.165286064147949\n",
      "(64, 33)\n",
      "step 4795, loss is 5.053487300872803\n",
      "(64, 33)\n",
      "step 4796, loss is 5.020427227020264\n",
      "(64, 33)\n",
      "step 4797, loss is 5.0120463371276855\n",
      "(64, 33)\n",
      "step 4798, loss is 4.960177898406982\n",
      "(64, 33)\n",
      "step 4799, loss is 5.009210109710693\n",
      "(64, 33)\n",
      "step 4800, loss is 4.930764675140381\n",
      "(64, 33)\n",
      "step 4801, loss is 4.978849411010742\n",
      "(64, 33)\n",
      "step 4802, loss is 4.995163440704346\n",
      "(64, 33)\n",
      "step 4803, loss is 5.131892204284668\n",
      "(64, 33)\n",
      "step 4804, loss is 5.034482955932617\n",
      "(64, 33)\n",
      "step 4805, loss is 4.835403919219971\n",
      "(64, 33)\n",
      "step 4806, loss is 5.034068584442139\n",
      "(64, 33)\n",
      "step 4807, loss is 5.073067665100098\n",
      "(64, 33)\n",
      "step 4808, loss is 4.924625873565674\n",
      "(64, 33)\n",
      "step 4809, loss is 4.925905227661133\n",
      "(64, 33)\n",
      "step 4810, loss is 4.87554931640625\n",
      "(64, 33)\n",
      "step 4811, loss is 4.9319682121276855\n",
      "(64, 33)\n",
      "step 4812, loss is 4.921590805053711\n",
      "(64, 33)\n",
      "step 4813, loss is 5.0795063972473145\n",
      "(64, 33)\n",
      "step 4814, loss is 4.894226551055908\n",
      "(64, 33)\n",
      "step 4815, loss is 4.803887367248535\n",
      "(64, 33)\n",
      "step 4816, loss is 4.993880748748779\n",
      "(64, 33)\n",
      "step 4817, loss is 5.068162441253662\n",
      "(64, 33)\n",
      "step 4818, loss is 5.037388801574707\n",
      "(64, 33)\n",
      "step 4819, loss is 4.899365425109863\n",
      "(64, 33)\n",
      "step 4820, loss is 5.081727981567383\n",
      "(64, 33)\n",
      "step 4821, loss is 5.034372806549072\n",
      "(64, 33)\n",
      "step 4822, loss is 4.781692981719971\n",
      "(64, 33)\n",
      "step 4823, loss is 4.923654079437256\n",
      "(64, 33)\n",
      "step 4824, loss is 5.134276866912842\n",
      "(64, 33)\n",
      "step 4825, loss is 5.135410308837891\n",
      "(64, 33)\n",
      "step 4826, loss is 5.0748419761657715\n",
      "(64, 33)\n",
      "step 4827, loss is 4.914432048797607\n",
      "(64, 33)\n",
      "step 4828, loss is 5.183989524841309\n",
      "(64, 33)\n",
      "step 4829, loss is 4.994225025177002\n",
      "(64, 33)\n",
      "step 4830, loss is 5.161462306976318\n",
      "(64, 33)\n",
      "step 4831, loss is 5.062641143798828\n",
      "(64, 33)\n",
      "step 4832, loss is 4.9345927238464355\n",
      "(64, 33)\n",
      "step 4833, loss is 5.049104690551758\n",
      "(64, 33)\n",
      "step 4834, loss is 4.943177700042725\n",
      "(64, 33)\n",
      "step 4835, loss is 5.063667297363281\n",
      "(64, 33)\n",
      "step 4836, loss is 5.120237827301025\n",
      "(64, 33)\n",
      "step 4837, loss is 4.9745306968688965\n",
      "(64, 33)\n",
      "step 4838, loss is 4.992581844329834\n",
      "(64, 33)\n",
      "step 4839, loss is 5.0108466148376465\n",
      "(64, 33)\n",
      "step 4840, loss is 5.057688236236572\n",
      "(64, 33)\n",
      "step 4841, loss is 5.118946075439453\n",
      "(64, 33)\n",
      "step 4842, loss is 5.048852443695068\n",
      "(64, 33)\n",
      "step 4843, loss is 5.0243916511535645\n",
      "(64, 33)\n",
      "step 4844, loss is 4.978413105010986\n",
      "(64, 33)\n",
      "step 4845, loss is 5.187496185302734\n",
      "(64, 33)\n",
      "step 4846, loss is 5.021862030029297\n",
      "(64, 33)\n",
      "step 4847, loss is 4.955639362335205\n",
      "(64, 33)\n",
      "step 4848, loss is 5.064403533935547\n",
      "(64, 33)\n",
      "step 4849, loss is 5.075324058532715\n",
      "(64, 33)\n",
      "step 4850, loss is 4.919682025909424\n",
      "(64, 33)\n",
      "step 4851, loss is 5.000244617462158\n",
      "(64, 33)\n",
      "step 4852, loss is 4.910645961761475\n",
      "(64, 33)\n",
      "step 4853, loss is 5.137198448181152\n",
      "(64, 33)\n",
      "step 4854, loss is 4.935962200164795\n",
      "(64, 33)\n",
      "step 4855, loss is 5.017030715942383\n",
      "(64, 33)\n",
      "step 4856, loss is 4.917029857635498\n",
      "(64, 33)\n",
      "step 4857, loss is 4.77689790725708\n",
      "(64, 33)\n",
      "step 4858, loss is 5.076100826263428\n",
      "(64, 33)\n",
      "step 4859, loss is 4.845070838928223\n",
      "(64, 33)\n",
      "step 4860, loss is 4.994845390319824\n",
      "(64, 33)\n",
      "step 4861, loss is 4.823302268981934\n",
      "(64, 33)\n",
      "step 4862, loss is 5.194865703582764\n",
      "(64, 33)\n",
      "step 4863, loss is 4.937128067016602\n",
      "(64, 33)\n",
      "step 4864, loss is 5.1201300621032715\n",
      "(64, 33)\n",
      "step 4865, loss is 4.836306095123291\n",
      "(64, 33)\n",
      "step 4866, loss is 4.964698791503906\n",
      "(64, 33)\n",
      "step 4867, loss is 4.990091323852539\n",
      "(64, 33)\n",
      "step 4868, loss is 4.960249423980713\n",
      "(64, 33)\n",
      "step 4869, loss is 4.923020839691162\n",
      "(64, 33)\n",
      "step 4870, loss is 5.020633220672607\n",
      "(64, 33)\n",
      "step 4871, loss is 5.074553489685059\n",
      "(64, 33)\n",
      "step 4872, loss is 4.854856014251709\n",
      "(64, 33)\n",
      "step 4873, loss is 5.144384860992432\n",
      "(64, 33)\n",
      "step 4874, loss is 4.940515518188477\n",
      "(64, 33)\n",
      "step 4875, loss is 4.91500186920166\n",
      "(64, 33)\n",
      "step 4876, loss is 5.165866374969482\n",
      "(64, 33)\n",
      "step 4877, loss is 5.040153503417969\n",
      "(64, 33)\n",
      "step 4878, loss is 5.001871109008789\n",
      "(64, 33)\n",
      "step 4879, loss is 4.866262912750244\n",
      "(64, 33)\n",
      "step 4880, loss is 5.078455448150635\n",
      "(64, 33)\n",
      "step 4881, loss is 5.055596351623535\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4882, loss is 5.058189392089844\n",
      "(64, 33)\n",
      "step 4883, loss is 4.93356466293335\n",
      "(64, 33)\n",
      "step 4884, loss is 4.951706409454346\n",
      "(64, 33)\n",
      "step 4885, loss is 4.8524169921875\n",
      "(64, 33)\n",
      "step 4886, loss is 4.98301362991333\n",
      "(64, 33)\n",
      "step 4887, loss is 4.997750759124756\n",
      "(64, 33)\n",
      "step 4888, loss is 5.172019958496094\n",
      "(64, 33)\n",
      "step 4889, loss is 4.998170375823975\n",
      "(64, 33)\n",
      "step 4890, loss is 5.050448894500732\n",
      "(64, 33)\n",
      "step 4891, loss is 4.874670505523682\n",
      "(64, 33)\n",
      "step 4892, loss is 5.092404842376709\n",
      "(64, 33)\n",
      "step 4893, loss is 4.9711809158325195\n",
      "(64, 33)\n",
      "step 4894, loss is 5.108225345611572\n",
      "(64, 33)\n",
      "step 4895, loss is 5.025517463684082\n",
      "(64, 33)\n",
      "step 4896, loss is 4.955928802490234\n",
      "(64, 33)\n",
      "step 4897, loss is 5.026497840881348\n",
      "(64, 33)\n",
      "step 4898, loss is 4.72563362121582\n",
      "(64, 33)\n",
      "step 4899, loss is 5.117288589477539\n",
      "(64, 33)\n",
      "step 4900, loss is 4.952423095703125\n",
      "(64, 33)\n",
      "step 4901, loss is 5.076327800750732\n",
      "(64, 33)\n",
      "step 4902, loss is 4.837738990783691\n",
      "(64, 33)\n",
      "step 4903, loss is 5.119264602661133\n",
      "(64, 33)\n",
      "step 4904, loss is 4.888889312744141\n",
      "(64, 33)\n",
      "step 4905, loss is 5.150367259979248\n",
      "(64, 33)\n",
      "step 4906, loss is 4.870851039886475\n",
      "(64, 33)\n",
      "step 4907, loss is 5.055057525634766\n",
      "(64, 33)\n",
      "step 4908, loss is 4.781731605529785\n",
      "(64, 33)\n",
      "step 4909, loss is 4.9701313972473145\n",
      "(64, 33)\n",
      "step 4910, loss is 5.0393290519714355\n",
      "(64, 33)\n",
      "step 4911, loss is 5.076095104217529\n",
      "(64, 33)\n",
      "step 4912, loss is 4.995898723602295\n",
      "(64, 33)\n",
      "step 4913, loss is 5.0456976890563965\n",
      "(64, 33)\n",
      "step 4914, loss is 5.019161701202393\n",
      "(64, 33)\n",
      "step 4915, loss is 4.991396903991699\n",
      "(64, 33)\n",
      "step 4916, loss is 5.018466949462891\n",
      "(64, 33)\n",
      "step 4917, loss is 4.780585765838623\n",
      "(64, 33)\n",
      "step 4918, loss is 4.986011505126953\n",
      "(64, 33)\n",
      "step 4919, loss is 5.029058456420898\n",
      "(64, 33)\n",
      "step 4920, loss is 4.942209720611572\n",
      "(64, 33)\n",
      "step 4921, loss is 5.053173542022705\n",
      "(64, 33)\n",
      "step 4922, loss is 4.904755592346191\n",
      "(64, 33)\n",
      "step 4923, loss is 4.801540851593018\n",
      "(64, 33)\n",
      "step 4924, loss is 4.962860107421875\n",
      "(64, 33)\n",
      "step 4925, loss is 5.292839050292969\n",
      "(64, 33)\n",
      "step 4926, loss is 5.016311168670654\n",
      "(64, 33)\n",
      "step 4927, loss is 5.021485328674316\n",
      "(64, 33)\n",
      "step 4928, loss is 4.71730899810791\n",
      "(64, 33)\n",
      "step 4929, loss is 5.273164749145508\n",
      "(64, 33)\n",
      "step 4930, loss is 4.9112958908081055\n",
      "(64, 33)\n",
      "step 4931, loss is 5.137236595153809\n",
      "(64, 33)\n",
      "step 4932, loss is 4.999750137329102\n",
      "(64, 33)\n",
      "step 4933, loss is 5.212653160095215\n",
      "(64, 33)\n",
      "step 4934, loss is 5.0424370765686035\n",
      "(64, 33)\n",
      "step 4935, loss is 4.994575023651123\n",
      "(64, 33)\n",
      "step 4936, loss is 5.066677093505859\n",
      "(64, 33)\n",
      "step 4937, loss is 4.721229076385498\n",
      "(64, 33)\n",
      "step 4938, loss is 4.8689422607421875\n",
      "(64, 33)\n",
      "step 4939, loss is 4.966567516326904\n",
      "(64, 33)\n",
      "step 4940, loss is 4.979652404785156\n",
      "(64, 33)\n",
      "step 4941, loss is 4.900731563568115\n",
      "(64, 33)\n",
      "step 4942, loss is 4.933116436004639\n",
      "(64, 33)\n",
      "step 4943, loss is 5.065161228179932\n",
      "(64, 33)\n",
      "step 4944, loss is 4.95739221572876\n",
      "(64, 33)\n",
      "step 4945, loss is 4.987638473510742\n",
      "(64, 33)\n",
      "step 4946, loss is 4.873638153076172\n",
      "(64, 33)\n",
      "step 4947, loss is 5.000509738922119\n",
      "(64, 33)\n",
      "step 4948, loss is 4.974916458129883\n",
      "(64, 33)\n",
      "step 4949, loss is 4.908420562744141\n",
      "(64, 33)\n",
      "step 4950, loss is 4.790345668792725\n",
      "(64, 33)\n",
      "step 4951, loss is 5.074619770050049\n",
      "(64, 33)\n",
      "step 4952, loss is 5.104952335357666\n",
      "(64, 33)\n",
      "step 4953, loss is 4.978495121002197\n",
      "(64, 33)\n",
      "step 4954, loss is 5.0463080406188965\n",
      "(64, 33)\n",
      "step 4955, loss is 4.9971723556518555\n",
      "(64, 33)\n",
      "step 4956, loss is 4.970682144165039\n",
      "(64, 33)\n",
      "step 4957, loss is 5.020518779754639\n",
      "(64, 33)\n",
      "step 4958, loss is 5.0662689208984375\n",
      "(64, 33)\n",
      "step 4959, loss is 5.093588829040527\n",
      "(64, 33)\n",
      "step 4960, loss is 4.9865264892578125\n",
      "(64, 33)\n",
      "step 4961, loss is 5.180983066558838\n",
      "(64, 33)\n",
      "step 4962, loss is 5.039823055267334\n",
      "(64, 33)\n",
      "step 4963, loss is 4.759474754333496\n",
      "(64, 33)\n",
      "step 4964, loss is 5.046592712402344\n",
      "(64, 33)\n",
      "step 4965, loss is 5.145150661468506\n",
      "(64, 33)\n",
      "step 4966, loss is 4.8915934562683105\n",
      "(64, 33)\n",
      "step 4967, loss is 5.020095348358154\n",
      "(64, 33)\n",
      "step 4968, loss is 5.011351108551025\n",
      "(64, 33)\n",
      "step 4969, loss is 5.003301620483398\n",
      "(64, 33)\n",
      "step 4970, loss is 4.771103382110596\n",
      "(64, 33)\n",
      "step 4971, loss is 5.089216709136963\n",
      "(64, 33)\n",
      "step 4972, loss is 5.018569469451904\n",
      "(64, 33)\n",
      "step 4973, loss is 4.974715232849121\n",
      "(64, 33)\n",
      "step 4974, loss is 4.886283874511719\n",
      "(64, 33)\n",
      "step 4975, loss is 4.960109710693359\n",
      "(64, 33)\n",
      "step 4976, loss is 4.940262317657471\n",
      "(64, 33)\n",
      "step 4977, loss is 5.054158687591553\n",
      "(64, 33)\n",
      "step 4978, loss is 4.806031703948975\n",
      "(64, 33)\n",
      "step 4979, loss is 5.197262763977051\n",
      "(64, 33)\n",
      "step 4980, loss is 4.938553333282471\n",
      "(64, 33)\n",
      "step 4981, loss is 5.185838222503662\n",
      "(64, 33)\n",
      "step 4982, loss is 5.106322765350342\n",
      "(64, 33)\n",
      "step 4983, loss is 5.0039448738098145\n",
      "(64, 33)\n",
      "step 4984, loss is 4.996555328369141\n",
      "(64, 33)\n",
      "step 4985, loss is 5.092397212982178\n",
      "(64, 33)\n",
      "step 4986, loss is 4.858741760253906\n",
      "(64, 33)\n",
      "step 4987, loss is 5.108582973480225\n",
      "(64, 33)\n",
      "step 4988, loss is 5.027831077575684\n",
      "(64, 33)\n",
      "step 4989, loss is 5.209595680236816\n",
      "(64, 33)\n",
      "step 4990, loss is 4.894670486450195\n",
      "(64, 33)\n",
      "step 4991, loss is 4.9955010414123535\n",
      "(64, 33)\n",
      "step 4992, loss is 5.005771636962891\n",
      "(64, 33)\n",
      "step 4993, loss is 5.13936710357666\n",
      "(64, 33)\n",
      "step 4994, loss is 5.070234775543213\n",
      "(64, 33)\n",
      "step 4995, loss is 4.929758071899414\n",
      "(64, 33)\n",
      "step 4996, loss is 4.948954105377197\n",
      "(64, 33)\n",
      "step 4997, loss is 5.034106731414795\n",
      "(64, 33)\n",
      "step 4998, loss is 4.923149108886719\n",
      "(64, 33)\n",
      "step 4999, loss is 5.015574932098389\n",
      "(64, 33)\n",
      "step 5000, loss is 5.072438716888428\n",
      "(64, 33)\n",
      "step 5001, loss is 4.958464622497559\n",
      "(64, 33)\n",
      "step 5002, loss is 4.882777214050293\n",
      "(64, 33)\n",
      "step 5003, loss is 5.175183296203613\n",
      "(64, 33)\n",
      "step 5004, loss is 5.254947662353516\n",
      "(64, 33)\n",
      "step 5005, loss is 4.873985290527344\n",
      "(64, 33)\n",
      "step 5006, loss is 5.035656929016113\n",
      "(64, 33)\n",
      "step 5007, loss is 4.895927906036377\n",
      "(64, 33)\n",
      "step 5008, loss is 5.032925605773926\n",
      "(64, 33)\n",
      "step 5009, loss is 5.144028186798096\n",
      "(64, 33)\n",
      "step 5010, loss is 5.019580364227295\n",
      "(64, 33)\n",
      "step 5011, loss is 4.922983646392822\n",
      "(64, 33)\n",
      "step 5012, loss is 4.828169345855713\n",
      "(64, 33)\n",
      "step 5013, loss is 4.886962890625\n",
      "(64, 33)\n",
      "step 5014, loss is 4.962523937225342\n",
      "(64, 33)\n",
      "step 5015, loss is 4.8862690925598145\n",
      "(64, 33)\n",
      "step 5016, loss is 5.1460041999816895\n",
      "(64, 33)\n",
      "step 5017, loss is 4.994622230529785\n",
      "(64, 33)\n",
      "step 5018, loss is 5.052907466888428\n",
      "(64, 33)\n",
      "step 5019, loss is 4.992669582366943\n",
      "(64, 33)\n",
      "step 5020, loss is 5.019314289093018\n",
      "(64, 33)\n",
      "step 5021, loss is 5.048399925231934\n",
      "(64, 33)\n",
      "step 5022, loss is 4.925037384033203\n",
      "(64, 33)\n",
      "step 5023, loss is 4.973928451538086\n",
      "(64, 33)\n",
      "step 5024, loss is 5.1031084060668945\n",
      "(64, 33)\n",
      "step 5025, loss is 5.045797824859619\n",
      "(64, 33)\n",
      "step 5026, loss is 4.895561695098877\n",
      "(64, 33)\n",
      "step 5027, loss is 4.899785995483398\n",
      "(64, 33)\n",
      "step 5028, loss is 4.9599785804748535\n",
      "(64, 33)\n",
      "step 5029, loss is 4.978147983551025\n",
      "(64, 33)\n",
      "step 5030, loss is 4.90262508392334\n",
      "(64, 33)\n",
      "step 5031, loss is 5.037026405334473\n",
      "(64, 33)\n",
      "step 5032, loss is 5.080451965332031\n",
      "(64, 33)\n",
      "step 5033, loss is 4.9759440422058105\n",
      "(64, 33)\n",
      "step 5034, loss is 5.090099811553955\n",
      "(64, 33)\n",
      "step 5035, loss is 4.955417156219482\n",
      "(64, 33)\n",
      "step 5036, loss is 4.670044898986816\n",
      "(64, 33)\n",
      "step 5037, loss is 5.035017013549805\n",
      "(64, 33)\n",
      "step 5038, loss is 5.021071434020996\n",
      "(64, 33)\n",
      "step 5039, loss is 4.981930255889893\n",
      "(64, 33)\n",
      "step 5040, loss is 4.923702239990234\n",
      "(64, 33)\n",
      "step 5041, loss is 4.990499973297119\n",
      "(64, 33)\n",
      "step 5042, loss is 4.875701904296875\n",
      "(64, 33)\n",
      "step 5043, loss is 4.88356876373291\n",
      "(64, 33)\n",
      "step 5044, loss is 5.1289286613464355\n",
      "(64, 33)\n",
      "step 5045, loss is 4.969625949859619\n",
      "(64, 33)\n",
      "step 5046, loss is 4.988494396209717\n",
      "(64, 33)\n",
      "step 5047, loss is 4.923079490661621\n",
      "(64, 33)\n",
      "step 5048, loss is 4.989229679107666\n",
      "(64, 33)\n",
      "step 5049, loss is 5.086596488952637\n",
      "(64, 33)\n",
      "step 5050, loss is 5.006361484527588\n",
      "(64, 33)\n",
      "step 5051, loss is 4.791080474853516\n",
      "(64, 33)\n",
      "step 5052, loss is 5.0244927406311035\n",
      "(64, 33)\n",
      "step 5053, loss is 5.081768035888672\n",
      "(64, 33)\n",
      "step 5054, loss is 5.148040771484375\n",
      "(64, 33)\n",
      "step 5055, loss is 5.019805431365967\n",
      "(64, 33)\n",
      "step 5056, loss is 5.021635055541992\n",
      "(64, 33)\n",
      "step 5057, loss is 4.934787750244141\n",
      "(64, 33)\n",
      "step 5058, loss is 5.067070007324219\n",
      "(64, 33)\n",
      "step 5059, loss is 5.020211696624756\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5060, loss is 5.122377395629883\n",
      "(64, 33)\n",
      "step 5061, loss is 4.984767436981201\n",
      "(64, 33)\n",
      "step 5062, loss is 4.927012920379639\n",
      "(64, 33)\n",
      "step 5063, loss is 4.921994209289551\n",
      "(64, 33)\n",
      "step 5064, loss is 4.988269329071045\n",
      "(64, 33)\n",
      "step 5065, loss is 5.0022687911987305\n",
      "(64, 33)\n",
      "step 5066, loss is 4.928706169128418\n",
      "(64, 33)\n",
      "step 5067, loss is 4.825097560882568\n",
      "(64, 33)\n",
      "step 5068, loss is 4.875431060791016\n",
      "(64, 33)\n",
      "step 5069, loss is 4.913740634918213\n",
      "(64, 33)\n",
      "step 5070, loss is 4.976647853851318\n",
      "(64, 33)\n",
      "step 5071, loss is 4.788407325744629\n",
      "(64, 33)\n",
      "step 5072, loss is 5.097702503204346\n",
      "(64, 33)\n",
      "step 5073, loss is 5.083419322967529\n",
      "(64, 33)\n",
      "step 5074, loss is 5.2201032638549805\n",
      "(64, 33)\n",
      "step 5075, loss is 4.994509696960449\n",
      "(64, 33)\n",
      "step 5076, loss is 5.0061259269714355\n",
      "(64, 33)\n",
      "step 5077, loss is 5.022946834564209\n",
      "(64, 33)\n",
      "step 5078, loss is 5.088848114013672\n",
      "(64, 33)\n",
      "step 5079, loss is 5.2232747077941895\n",
      "(64, 33)\n",
      "step 5080, loss is 4.883358478546143\n",
      "(64, 33)\n",
      "step 5081, loss is 4.965335845947266\n",
      "(64, 33)\n",
      "step 5082, loss is 4.993773460388184\n",
      "(64, 33)\n",
      "step 5083, loss is 5.069131374359131\n",
      "(64, 33)\n",
      "step 5084, loss is 4.945688724517822\n",
      "(64, 33)\n",
      "step 5085, loss is 4.89840030670166\n",
      "(64, 33)\n",
      "step 5086, loss is 4.943973064422607\n",
      "(64, 33)\n",
      "step 5087, loss is 5.271687984466553\n",
      "(64, 33)\n",
      "step 5088, loss is 4.91743803024292\n",
      "(64, 33)\n",
      "step 5089, loss is 4.989925861358643\n",
      "(64, 33)\n",
      "step 5090, loss is 4.996903896331787\n",
      "(64, 33)\n",
      "step 5091, loss is 5.114006042480469\n",
      "(64, 33)\n",
      "step 5092, loss is 4.9401373863220215\n",
      "(64, 33)\n",
      "step 5093, loss is 4.798055648803711\n",
      "(64, 33)\n",
      "step 5094, loss is 5.064624309539795\n",
      "(64, 33)\n",
      "step 5095, loss is 4.962101459503174\n",
      "(64, 33)\n",
      "step 5096, loss is 5.020241737365723\n",
      "(64, 33)\n",
      "step 5097, loss is 5.297720432281494\n",
      "(64, 33)\n",
      "step 5098, loss is 4.884194374084473\n",
      "(64, 33)\n",
      "step 5099, loss is 4.926918029785156\n",
      "(64, 33)\n",
      "step 5100, loss is 4.977870464324951\n",
      "(64, 33)\n",
      "step 5101, loss is 4.999340057373047\n",
      "(64, 33)\n",
      "step 5102, loss is 5.200376510620117\n",
      "(64, 33)\n",
      "step 5103, loss is 4.9387640953063965\n",
      "(64, 33)\n",
      "step 5104, loss is 4.940298080444336\n",
      "(64, 33)\n",
      "step 5105, loss is 4.90408992767334\n",
      "(64, 33)\n",
      "step 5106, loss is 4.899837017059326\n",
      "(64, 33)\n",
      "step 5107, loss is 4.76264762878418\n",
      "(64, 33)\n",
      "step 5108, loss is 5.0919952392578125\n",
      "(64, 33)\n",
      "step 5109, loss is 4.903390407562256\n",
      "(64, 33)\n",
      "step 5110, loss is 5.03078556060791\n",
      "(64, 33)\n",
      "step 5111, loss is 5.007009983062744\n",
      "(64, 33)\n",
      "step 5112, loss is 4.974559783935547\n",
      "(64, 33)\n",
      "step 5113, loss is 4.883017063140869\n",
      "(64, 33)\n",
      "step 5114, loss is 5.026880741119385\n",
      "(64, 33)\n",
      "step 5115, loss is 4.964427471160889\n",
      "(64, 33)\n",
      "step 5116, loss is 5.021302223205566\n",
      "(64, 33)\n",
      "step 5117, loss is 5.081943511962891\n",
      "(64, 33)\n",
      "step 5118, loss is 5.206003189086914\n",
      "(64, 33)\n",
      "step 5119, loss is 5.076974391937256\n",
      "(64, 33)\n",
      "step 5120, loss is 5.089668273925781\n",
      "(64, 33)\n",
      "step 5121, loss is 5.019784450531006\n",
      "(64, 33)\n",
      "step 5122, loss is 4.959475040435791\n",
      "(64, 33)\n",
      "step 5123, loss is 4.773131847381592\n",
      "(64, 33)\n",
      "step 5124, loss is 4.997700214385986\n",
      "(64, 33)\n",
      "step 5125, loss is 5.006857395172119\n",
      "(64, 33)\n",
      "step 5126, loss is 4.811533451080322\n",
      "(64, 33)\n",
      "step 5127, loss is 4.952305316925049\n",
      "(64, 33)\n",
      "step 5128, loss is 5.1163153648376465\n",
      "(64, 33)\n",
      "step 5129, loss is 5.084095478057861\n",
      "(64, 33)\n",
      "step 5130, loss is 4.907702922821045\n",
      "(64, 33)\n",
      "step 5131, loss is 4.836355209350586\n",
      "(64, 33)\n",
      "step 5132, loss is 4.935041427612305\n",
      "(64, 33)\n",
      "step 5133, loss is 4.883577346801758\n",
      "(64, 33)\n",
      "step 5134, loss is 4.9819793701171875\n",
      "(64, 33)\n",
      "step 5135, loss is 5.005077838897705\n",
      "(64, 33)\n",
      "step 5136, loss is 5.068147659301758\n",
      "(64, 33)\n",
      "step 5137, loss is 4.923502445220947\n",
      "(64, 33)\n",
      "step 5138, loss is 5.046395778656006\n",
      "(64, 33)\n",
      "step 5139, loss is 4.945189476013184\n",
      "(64, 33)\n",
      "step 5140, loss is 5.067581653594971\n",
      "(64, 33)\n",
      "step 5141, loss is 5.025228500366211\n",
      "(64, 33)\n",
      "step 5142, loss is 4.945835113525391\n",
      "(64, 33)\n",
      "step 5143, loss is 5.072982311248779\n",
      "(64, 33)\n",
      "step 5144, loss is 5.117424011230469\n",
      "(64, 33)\n",
      "step 5145, loss is 5.0380730628967285\n",
      "(64, 33)\n",
      "step 5146, loss is 4.9522528648376465\n",
      "(64, 33)\n",
      "step 5147, loss is 5.067814826965332\n",
      "(64, 33)\n",
      "step 5148, loss is 5.06258487701416\n",
      "(64, 33)\n",
      "step 5149, loss is 4.882054328918457\n",
      "(64, 33)\n",
      "step 5150, loss is 5.118978500366211\n",
      "(64, 33)\n",
      "step 5151, loss is 5.141117095947266\n",
      "(64, 33)\n",
      "step 5152, loss is 5.0159430503845215\n",
      "(64, 33)\n",
      "step 5153, loss is 4.9614577293396\n",
      "(64, 33)\n",
      "step 5154, loss is 4.995839595794678\n",
      "(64, 33)\n",
      "step 5155, loss is 4.808624267578125\n",
      "(64, 33)\n",
      "step 5156, loss is 5.095008373260498\n",
      "(64, 33)\n",
      "step 5157, loss is 4.9201860427856445\n",
      "(64, 33)\n",
      "step 5158, loss is 4.927559852600098\n",
      "(64, 33)\n",
      "step 5159, loss is 5.18787956237793\n",
      "(64, 33)\n",
      "step 5160, loss is 4.899070739746094\n",
      "(64, 33)\n",
      "step 5161, loss is 4.981071472167969\n",
      "(64, 33)\n",
      "step 5162, loss is 4.954039096832275\n",
      "(64, 33)\n",
      "step 5163, loss is 4.90653657913208\n",
      "(64, 33)\n",
      "step 5164, loss is 4.937324523925781\n",
      "(64, 33)\n",
      "step 5165, loss is 5.219898700714111\n",
      "(64, 33)\n",
      "step 5166, loss is 5.024387359619141\n",
      "(64, 33)\n",
      "step 5167, loss is 4.942119598388672\n",
      "(64, 33)\n",
      "step 5168, loss is 4.942123889923096\n",
      "(64, 33)\n",
      "step 5169, loss is 5.056399345397949\n",
      "(64, 33)\n",
      "step 5170, loss is 4.72161865234375\n",
      "(64, 33)\n",
      "step 5171, loss is 4.775404453277588\n",
      "(64, 33)\n",
      "step 5172, loss is 5.206060886383057\n",
      "(64, 33)\n",
      "step 5173, loss is 4.914737701416016\n",
      "(64, 33)\n",
      "step 5174, loss is 5.0287346839904785\n",
      "(64, 33)\n",
      "step 5175, loss is 4.942103385925293\n",
      "(64, 33)\n",
      "step 5176, loss is 5.086201190948486\n",
      "(64, 33)\n",
      "step 5177, loss is 4.677485942840576\n",
      "(64, 33)\n",
      "step 5178, loss is 4.99373722076416\n",
      "(64, 33)\n",
      "step 5179, loss is 5.238851547241211\n",
      "(64, 33)\n",
      "step 5180, loss is 4.90576696395874\n",
      "(64, 33)\n",
      "step 5181, loss is 5.235728740692139\n",
      "(64, 33)\n",
      "step 5182, loss is 4.980346202850342\n",
      "(64, 33)\n",
      "step 5183, loss is 5.027237415313721\n",
      "(64, 33)\n",
      "step 5184, loss is 4.878066539764404\n",
      "(64, 33)\n",
      "step 5185, loss is 4.797074794769287\n",
      "(64, 33)\n",
      "step 5186, loss is 5.134407043457031\n",
      "(64, 33)\n",
      "step 5187, loss is 5.067041397094727\n",
      "(64, 33)\n",
      "step 5188, loss is 4.944474697113037\n",
      "(64, 33)\n",
      "step 5189, loss is 5.069987773895264\n",
      "(64, 33)\n",
      "step 5190, loss is 4.924280166625977\n",
      "(64, 33)\n",
      "step 5191, loss is 5.011755466461182\n",
      "(64, 33)\n",
      "step 5192, loss is 4.738917350769043\n",
      "(64, 33)\n",
      "step 5193, loss is 5.158302307128906\n",
      "(64, 33)\n",
      "step 5194, loss is 5.06482458114624\n",
      "(64, 33)\n",
      "step 5195, loss is 4.9294586181640625\n",
      "(64, 33)\n",
      "step 5196, loss is 4.788157939910889\n",
      "(64, 33)\n",
      "step 5197, loss is 5.265327453613281\n",
      "(64, 33)\n",
      "step 5198, loss is 4.766140937805176\n",
      "(64, 33)\n",
      "step 5199, loss is 4.993568420410156\n",
      "(64, 33)\n",
      "step 5200, loss is 4.886143684387207\n",
      "(64, 33)\n",
      "step 5201, loss is 5.066181182861328\n",
      "(64, 33)\n",
      "step 5202, loss is 5.064104080200195\n",
      "(64, 33)\n",
      "step 5203, loss is 5.097009658813477\n",
      "(64, 33)\n",
      "step 5204, loss is 4.946083068847656\n",
      "(64, 33)\n",
      "step 5205, loss is 4.9035539627075195\n",
      "(64, 33)\n",
      "step 5206, loss is 4.892788887023926\n",
      "(64, 33)\n",
      "step 5207, loss is 5.0393853187561035\n",
      "(64, 33)\n",
      "step 5208, loss is 4.9327778816223145\n",
      "(64, 33)\n",
      "step 5209, loss is 5.1098246574401855\n",
      "(64, 33)\n",
      "step 5210, loss is 5.021275520324707\n",
      "(64, 33)\n",
      "step 5211, loss is 5.1398701667785645\n",
      "(64, 33)\n",
      "step 5212, loss is 5.059373378753662\n",
      "(64, 33)\n",
      "step 5213, loss is 4.866543292999268\n",
      "(64, 33)\n",
      "step 5214, loss is 5.123116493225098\n",
      "(64, 33)\n",
      "step 5215, loss is 5.139484405517578\n",
      "(64, 33)\n",
      "step 5216, loss is 4.987026691436768\n",
      "(64, 33)\n",
      "step 5217, loss is 4.834565162658691\n",
      "(64, 33)\n",
      "step 5218, loss is 4.915075302124023\n",
      "(64, 33)\n",
      "step 5219, loss is 4.927220344543457\n",
      "(64, 33)\n",
      "step 5220, loss is 4.90024995803833\n",
      "(64, 33)\n",
      "step 5221, loss is 4.971750736236572\n",
      "(64, 33)\n",
      "step 5222, loss is 4.891330718994141\n",
      "(64, 33)\n",
      "step 5223, loss is 4.911555290222168\n",
      "(64, 33)\n",
      "step 5224, loss is 4.899825096130371\n",
      "(64, 33)\n",
      "step 5225, loss is 4.864471912384033\n",
      "(64, 33)\n",
      "step 5226, loss is 4.943211555480957\n",
      "(64, 33)\n",
      "step 5227, loss is 4.977662086486816\n",
      "(64, 33)\n",
      "step 5228, loss is 5.07523775100708\n",
      "(64, 33)\n",
      "step 5229, loss is 4.80158805847168\n",
      "(64, 33)\n",
      "step 5230, loss is 4.837852478027344\n",
      "(64, 33)\n",
      "step 5231, loss is 5.147823810577393\n",
      "(64, 33)\n",
      "step 5232, loss is 5.176498889923096\n",
      "(64, 33)\n",
      "step 5233, loss is 5.020503520965576\n",
      "(64, 33)\n",
      "step 5234, loss is 5.035993576049805\n",
      "(64, 33)\n",
      "step 5235, loss is 4.900618076324463\n",
      "(64, 33)\n",
      "step 5236, loss is 5.004964828491211\n",
      "(64, 33)\n",
      "step 5237, loss is 4.912723541259766\n",
      "(64, 33)\n",
      "step 5238, loss is 5.015444755554199\n",
      "(64, 33)\n",
      "step 5239, loss is 4.964310169219971\n",
      "(64, 33)\n",
      "step 5240, loss is 5.138869762420654\n",
      "(64, 33)\n",
      "step 5241, loss is 4.801528453826904\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5242, loss is 4.968687057495117\n",
      "(64, 33)\n",
      "step 5243, loss is 4.872252941131592\n",
      "(64, 33)\n",
      "step 5244, loss is 5.113219738006592\n",
      "(64, 33)\n",
      "step 5245, loss is 4.905605316162109\n",
      "(64, 33)\n",
      "step 5246, loss is 4.913058280944824\n",
      "(64, 33)\n",
      "step 5247, loss is 4.990533828735352\n",
      "(64, 33)\n",
      "step 5248, loss is 4.845924377441406\n",
      "(64, 33)\n",
      "step 5249, loss is 5.135546684265137\n",
      "(64, 33)\n",
      "step 5250, loss is 5.070436000823975\n",
      "(64, 33)\n",
      "step 5251, loss is 5.102170944213867\n",
      "(64, 33)\n",
      "step 5252, loss is 4.892616271972656\n",
      "(64, 33)\n",
      "step 5253, loss is 5.112340450286865\n",
      "(64, 33)\n",
      "step 5254, loss is 4.966488361358643\n",
      "(64, 33)\n",
      "step 5255, loss is 4.951394557952881\n",
      "(64, 33)\n",
      "step 5256, loss is 4.965242862701416\n",
      "(64, 33)\n",
      "step 5257, loss is 5.119582653045654\n",
      "(64, 33)\n",
      "step 5258, loss is 4.950644016265869\n",
      "(64, 33)\n",
      "step 5259, loss is 4.922958850860596\n",
      "(64, 33)\n",
      "step 5260, loss is 5.136455535888672\n",
      "(64, 33)\n",
      "step 5261, loss is 5.003681659698486\n",
      "(64, 33)\n",
      "step 5262, loss is 5.024801254272461\n",
      "(64, 33)\n",
      "step 5263, loss is 5.050528526306152\n",
      "(64, 33)\n",
      "step 5264, loss is 4.934260368347168\n",
      "(64, 33)\n",
      "step 5265, loss is 5.071287155151367\n",
      "(64, 33)\n",
      "step 5266, loss is 5.191444396972656\n",
      "(64, 33)\n",
      "step 5267, loss is 4.9574174880981445\n",
      "(64, 33)\n",
      "step 5268, loss is 5.018733024597168\n",
      "(64, 33)\n",
      "step 5269, loss is 4.862963676452637\n",
      "(64, 33)\n",
      "step 5270, loss is 5.056178092956543\n",
      "(64, 33)\n",
      "step 5271, loss is 5.176248073577881\n",
      "(64, 33)\n",
      "step 5272, loss is 4.84576940536499\n",
      "(64, 33)\n",
      "step 5273, loss is 4.978874206542969\n",
      "(64, 33)\n",
      "step 5274, loss is 4.917201995849609\n",
      "(64, 33)\n",
      "step 5275, loss is 5.053828239440918\n",
      "(64, 33)\n",
      "step 5276, loss is 5.247706413269043\n",
      "(64, 33)\n",
      "step 5277, loss is 4.830541610717773\n",
      "(64, 33)\n",
      "step 5278, loss is 5.149085998535156\n",
      "(64, 33)\n",
      "step 5279, loss is 5.122447490692139\n",
      "(64, 33)\n",
      "step 5280, loss is 5.072023391723633\n",
      "(64, 33)\n",
      "step 5281, loss is 5.063042163848877\n",
      "(64, 33)\n",
      "step 5282, loss is 4.881947040557861\n",
      "(64, 33)\n",
      "step 5283, loss is 4.9486870765686035\n",
      "(64, 33)\n",
      "step 5284, loss is 4.797352313995361\n",
      "(64, 33)\n",
      "step 5285, loss is 4.767428874969482\n",
      "(64, 33)\n",
      "step 5286, loss is 5.219365119934082\n",
      "(64, 33)\n",
      "step 5287, loss is 5.136229038238525\n",
      "(64, 33)\n",
      "step 5288, loss is 4.97212028503418\n",
      "(64, 33)\n",
      "step 5289, loss is 4.942833423614502\n",
      "(64, 33)\n",
      "step 5290, loss is 5.081474781036377\n",
      "(64, 33)\n",
      "step 5291, loss is 4.928520202636719\n",
      "(64, 33)\n",
      "step 5292, loss is 4.987486362457275\n",
      "(64, 33)\n",
      "step 5293, loss is 4.976875305175781\n",
      "(64, 33)\n",
      "step 5294, loss is 5.049654960632324\n",
      "(64, 33)\n",
      "step 5295, loss is 4.920370578765869\n",
      "(64, 33)\n",
      "step 5296, loss is 4.79625940322876\n",
      "(64, 33)\n",
      "step 5297, loss is 5.093814373016357\n",
      "(64, 33)\n",
      "step 5298, loss is 4.869597911834717\n",
      "(64, 33)\n",
      "step 5299, loss is 4.928618431091309\n",
      "(64, 33)\n",
      "step 5300, loss is 5.035353183746338\n",
      "(64, 33)\n",
      "step 5301, loss is 4.960181713104248\n",
      "(64, 33)\n",
      "step 5302, loss is 4.9511847496032715\n",
      "(64, 33)\n",
      "step 5303, loss is 4.958766460418701\n",
      "(64, 33)\n",
      "step 5304, loss is 5.204478740692139\n",
      "(64, 33)\n",
      "step 5305, loss is 4.9302544593811035\n",
      "(64, 33)\n",
      "step 5306, loss is 4.954038619995117\n",
      "(64, 33)\n",
      "step 5307, loss is 4.847129821777344\n",
      "(64, 33)\n",
      "step 5308, loss is 4.9285807609558105\n",
      "(64, 33)\n",
      "step 5309, loss is 4.9593729972839355\n",
      "(64, 33)\n",
      "step 5310, loss is 4.910058498382568\n",
      "(64, 33)\n",
      "step 5311, loss is 4.913149833679199\n",
      "(64, 33)\n",
      "step 5312, loss is 5.022049427032471\n",
      "(64, 33)\n",
      "step 5313, loss is 5.046009540557861\n",
      "(64, 33)\n",
      "step 5314, loss is 4.8573760986328125\n",
      "(64, 33)\n",
      "step 5315, loss is 5.035749912261963\n",
      "(64, 33)\n",
      "step 5316, loss is 4.774049282073975\n",
      "(64, 33)\n",
      "step 5317, loss is 4.993175983428955\n",
      "(64, 33)\n",
      "step 5318, loss is 5.061648368835449\n",
      "(64, 33)\n",
      "step 5319, loss is 5.047480583190918\n",
      "(64, 33)\n",
      "step 5320, loss is 4.911873817443848\n",
      "(64, 33)\n",
      "step 5321, loss is 5.016917705535889\n",
      "(64, 33)\n",
      "step 5322, loss is 5.044883728027344\n",
      "(64, 33)\n",
      "step 5323, loss is 5.012158393859863\n",
      "(64, 33)\n",
      "step 5324, loss is 5.027183532714844\n",
      "(64, 33)\n",
      "step 5325, loss is 5.072103500366211\n",
      "(64, 33)\n",
      "step 5326, loss is 4.973462104797363\n",
      "(64, 33)\n",
      "step 5327, loss is 5.159745693206787\n",
      "(64, 33)\n",
      "step 5328, loss is 5.074130058288574\n",
      "(64, 33)\n",
      "step 5329, loss is 5.109241962432861\n",
      "(64, 33)\n",
      "step 5330, loss is 4.951864242553711\n",
      "(64, 33)\n",
      "step 5331, loss is 4.84909200668335\n",
      "(64, 33)\n",
      "step 5332, loss is 5.067404747009277\n",
      "(64, 33)\n",
      "step 5333, loss is 5.070559978485107\n",
      "(64, 33)\n",
      "step 5334, loss is 4.837066650390625\n",
      "(64, 33)\n",
      "step 5335, loss is 4.879173278808594\n",
      "(64, 33)\n",
      "step 5336, loss is 4.904338836669922\n",
      "(64, 33)\n",
      "step 5337, loss is 5.093388557434082\n",
      "(64, 33)\n",
      "step 5338, loss is 5.147876262664795\n",
      "(64, 33)\n",
      "step 5339, loss is 4.975559234619141\n",
      "(64, 33)\n",
      "step 5340, loss is 4.926558017730713\n",
      "(64, 33)\n",
      "step 5341, loss is 4.995095729827881\n",
      "(64, 33)\n",
      "step 5342, loss is 4.969725608825684\n",
      "(64, 33)\n",
      "step 5343, loss is 5.034082412719727\n",
      "(64, 33)\n",
      "step 5344, loss is 4.943975448608398\n",
      "(64, 33)\n",
      "step 5345, loss is 5.096916198730469\n",
      "(64, 33)\n",
      "step 5346, loss is 5.03711462020874\n",
      "(64, 33)\n",
      "step 5347, loss is 4.779952526092529\n",
      "(64, 33)\n",
      "step 5348, loss is 5.09475040435791\n",
      "(64, 33)\n",
      "step 5349, loss is 4.838892936706543\n",
      "(64, 33)\n",
      "step 5350, loss is 5.057879447937012\n",
      "(64, 33)\n",
      "step 5351, loss is 5.108743190765381\n",
      "(64, 33)\n",
      "step 5352, loss is 5.050652027130127\n",
      "(64, 33)\n",
      "step 5353, loss is 4.955902099609375\n",
      "(64, 33)\n",
      "step 5354, loss is 4.953053951263428\n",
      "(64, 33)\n",
      "step 5355, loss is 4.999113082885742\n",
      "(64, 33)\n",
      "step 5356, loss is 5.079679012298584\n",
      "(64, 33)\n",
      "step 5357, loss is 4.791804313659668\n",
      "(64, 33)\n",
      "step 5358, loss is 4.933804512023926\n",
      "(64, 33)\n",
      "step 5359, loss is 5.207020282745361\n",
      "(64, 33)\n",
      "step 5360, loss is 5.048726558685303\n",
      "(64, 33)\n",
      "step 5361, loss is 5.041112899780273\n",
      "(64, 33)\n",
      "step 5362, loss is 4.961008548736572\n",
      "(64, 33)\n",
      "step 5363, loss is 4.831103801727295\n",
      "(64, 33)\n",
      "step 5364, loss is 4.838877201080322\n",
      "(64, 33)\n",
      "step 5365, loss is 5.250988483428955\n",
      "(64, 33)\n",
      "step 5366, loss is 4.98885440826416\n",
      "(64, 33)\n",
      "step 5367, loss is 4.96687126159668\n",
      "(64, 33)\n",
      "step 5368, loss is 4.9578375816345215\n",
      "(64, 33)\n",
      "step 5369, loss is 5.06889009475708\n",
      "(64, 33)\n",
      "step 5370, loss is 5.0790605545043945\n",
      "(64, 33)\n",
      "step 5371, loss is 5.022283554077148\n",
      "(64, 33)\n",
      "step 5372, loss is 5.098954677581787\n",
      "(64, 33)\n",
      "step 5373, loss is 4.9227447509765625\n",
      "(64, 33)\n",
      "step 5374, loss is 4.6803483963012695\n",
      "(64, 33)\n",
      "step 5375, loss is 5.01560115814209\n",
      "(64, 33)\n",
      "step 5376, loss is 5.020257949829102\n",
      "(64, 33)\n",
      "step 5377, loss is 5.117991924285889\n",
      "(64, 33)\n",
      "step 5378, loss is 5.052538871765137\n",
      "(64, 33)\n",
      "step 5379, loss is 4.807188034057617\n",
      "(64, 33)\n",
      "step 5380, loss is 5.0945634841918945\n",
      "(64, 33)\n",
      "step 5381, loss is 4.891312122344971\n",
      "(64, 33)\n",
      "step 5382, loss is 4.898174285888672\n",
      "(64, 33)\n",
      "step 5383, loss is 5.015467643737793\n",
      "(64, 33)\n",
      "step 5384, loss is 4.950730323791504\n",
      "(64, 33)\n",
      "step 5385, loss is 4.9023308753967285\n",
      "(64, 33)\n",
      "step 5386, loss is 4.881561279296875\n",
      "(64, 33)\n",
      "step 5387, loss is 5.198774814605713\n",
      "(64, 33)\n",
      "step 5388, loss is 5.013792037963867\n",
      "(64, 33)\n",
      "step 5389, loss is 5.0046257972717285\n",
      "(64, 33)\n",
      "step 5390, loss is 5.052745342254639\n",
      "(64, 33)\n",
      "step 5391, loss is 4.983179092407227\n",
      "(64, 33)\n",
      "step 5392, loss is 4.994287967681885\n",
      "(64, 33)\n",
      "step 5393, loss is 5.121726989746094\n",
      "(64, 33)\n",
      "step 5394, loss is 5.009892463684082\n",
      "(64, 33)\n",
      "step 5395, loss is 5.066267490386963\n",
      "(64, 33)\n",
      "step 5396, loss is 4.983436584472656\n",
      "(64, 33)\n",
      "step 5397, loss is 4.969083309173584\n",
      "(64, 33)\n",
      "step 5398, loss is 4.856511116027832\n",
      "(64, 33)\n",
      "step 5399, loss is 4.945899486541748\n",
      "(64, 33)\n",
      "step 5400, loss is 4.910242080688477\n",
      "(64, 33)\n",
      "step 5401, loss is 4.922444820404053\n",
      "(64, 33)\n",
      "step 5402, loss is 4.990594863891602\n",
      "(64, 33)\n",
      "step 5403, loss is 5.0135579109191895\n",
      "(64, 33)\n",
      "step 5404, loss is 5.113516330718994\n",
      "(64, 33)\n",
      "step 5405, loss is 4.69810676574707\n",
      "(64, 33)\n",
      "step 5406, loss is 4.816493988037109\n",
      "(64, 33)\n",
      "step 5407, loss is 4.977616310119629\n",
      "(64, 33)\n",
      "step 5408, loss is 4.965775489807129\n",
      "(64, 33)\n",
      "step 5409, loss is 5.096731662750244\n",
      "(64, 33)\n",
      "step 5410, loss is 4.878872394561768\n",
      "(64, 33)\n",
      "step 5411, loss is 5.149999618530273\n",
      "(64, 33)\n",
      "step 5412, loss is 4.86698055267334\n",
      "(64, 33)\n",
      "step 5413, loss is 4.770799160003662\n",
      "(64, 33)\n",
      "step 5414, loss is 4.934982776641846\n",
      "(64, 33)\n",
      "step 5415, loss is 4.964193820953369\n",
      "(64, 33)\n",
      "step 5416, loss is 4.915901184082031\n",
      "(64, 33)\n",
      "step 5417, loss is 5.0181074142456055\n",
      "(64, 33)\n",
      "step 5418, loss is 5.021275520324707\n",
      "(64, 33)\n",
      "step 5419, loss is 5.195087432861328\n",
      "(64, 33)\n",
      "step 5420, loss is 4.991120338439941\n",
      "(64, 33)\n",
      "step 5421, loss is 4.967947483062744\n",
      "(64, 33)\n",
      "step 5422, loss is 4.935989856719971\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5423, loss is 4.892387866973877\n",
      "(64, 33)\n",
      "step 5424, loss is 4.799799919128418\n",
      "(64, 33)\n",
      "step 5425, loss is 4.957737445831299\n",
      "(64, 33)\n",
      "step 5426, loss is 4.8026957511901855\n",
      "(64, 33)\n",
      "step 5427, loss is 4.981874942779541\n",
      "(64, 33)\n",
      "step 5428, loss is 4.992571830749512\n",
      "(64, 33)\n",
      "step 5429, loss is 5.022408962249756\n",
      "(64, 33)\n",
      "step 5430, loss is 5.085847854614258\n",
      "(64, 33)\n",
      "step 5431, loss is 5.068748950958252\n",
      "(64, 33)\n",
      "step 5432, loss is 4.952024459838867\n",
      "(64, 33)\n",
      "step 5433, loss is 5.044003486633301\n",
      "(64, 33)\n",
      "step 5434, loss is 4.8884148597717285\n",
      "(64, 33)\n",
      "step 5435, loss is 5.109151840209961\n",
      "(64, 33)\n",
      "step 5436, loss is 5.0178327560424805\n",
      "(64, 33)\n",
      "step 5437, loss is 4.8782958984375\n",
      "(64, 33)\n",
      "step 5438, loss is 5.011682987213135\n",
      "(64, 33)\n",
      "step 5439, loss is 5.1412529945373535\n",
      "(64, 33)\n",
      "step 5440, loss is 5.114165306091309\n",
      "(64, 33)\n",
      "step 5441, loss is 4.946529865264893\n",
      "(64, 33)\n",
      "step 5442, loss is 4.900988578796387\n",
      "(64, 33)\n",
      "step 5443, loss is 4.9163594245910645\n",
      "(64, 33)\n",
      "step 5444, loss is 5.0807647705078125\n",
      "(64, 33)\n",
      "step 5445, loss is 4.930587291717529\n",
      "(64, 33)\n",
      "step 5446, loss is 4.8750786781311035\n",
      "(64, 33)\n",
      "step 5447, loss is 4.881962299346924\n",
      "(64, 33)\n",
      "step 5448, loss is 5.006149768829346\n",
      "(64, 33)\n",
      "step 5449, loss is 5.2691473960876465\n",
      "(64, 33)\n",
      "step 5450, loss is 4.912829875946045\n",
      "(64, 33)\n",
      "step 5451, loss is 5.055882453918457\n",
      "(64, 33)\n",
      "step 5452, loss is 4.8575215339660645\n",
      "(64, 33)\n",
      "step 5453, loss is 4.972084999084473\n",
      "(64, 33)\n",
      "step 5454, loss is 4.911623001098633\n",
      "(64, 33)\n",
      "step 5455, loss is 4.846920490264893\n",
      "(64, 33)\n",
      "step 5456, loss is 4.893492698669434\n",
      "(64, 33)\n",
      "step 5457, loss is 4.9739813804626465\n",
      "(64, 33)\n",
      "step 5458, loss is 5.007359981536865\n",
      "(64, 33)\n",
      "step 5459, loss is 5.086304664611816\n",
      "(64, 33)\n",
      "step 5460, loss is 4.930520057678223\n",
      "(64, 33)\n",
      "step 5461, loss is 4.962376117706299\n",
      "(64, 33)\n",
      "step 5462, loss is 4.951176166534424\n",
      "(64, 33)\n",
      "step 5463, loss is 4.908661365509033\n",
      "(64, 33)\n",
      "step 5464, loss is 4.867724418640137\n",
      "(64, 33)\n",
      "step 5465, loss is 4.874069690704346\n",
      "(64, 33)\n",
      "step 5466, loss is 4.8629913330078125\n",
      "(64, 33)\n",
      "step 5467, loss is 5.080379962921143\n",
      "(64, 33)\n",
      "step 5468, loss is 5.005923271179199\n",
      "(64, 33)\n",
      "step 5469, loss is 4.8364715576171875\n",
      "(64, 33)\n",
      "step 5470, loss is 5.0807785987854\n",
      "(64, 33)\n",
      "step 5471, loss is 4.855498313903809\n",
      "(64, 33)\n",
      "step 5472, loss is 4.875223159790039\n",
      "(64, 33)\n",
      "step 5473, loss is 5.017157554626465\n",
      "(64, 33)\n",
      "step 5474, loss is 4.923720836639404\n",
      "(64, 33)\n",
      "step 5475, loss is 4.958670616149902\n",
      "(64, 33)\n",
      "step 5476, loss is 5.040779113769531\n",
      "(64, 33)\n",
      "step 5477, loss is 5.027198791503906\n",
      "(64, 33)\n",
      "step 5478, loss is 5.050775527954102\n",
      "(64, 33)\n",
      "step 5479, loss is 4.933038711547852\n",
      "(64, 33)\n",
      "step 5480, loss is 4.819840908050537\n",
      "(64, 33)\n",
      "step 5481, loss is 5.078165054321289\n",
      "(64, 33)\n",
      "step 5482, loss is 4.9306840896606445\n",
      "(64, 33)\n",
      "step 5483, loss is 4.980889797210693\n",
      "(64, 33)\n",
      "step 5484, loss is 4.849765300750732\n",
      "(64, 33)\n",
      "step 5485, loss is 5.07907772064209\n",
      "(64, 33)\n",
      "step 5486, loss is 4.942519187927246\n",
      "(64, 33)\n",
      "step 5487, loss is 5.040000915527344\n",
      "(64, 33)\n",
      "step 5488, loss is 4.9970197677612305\n",
      "(64, 33)\n",
      "step 5489, loss is 5.110535621643066\n",
      "(64, 33)\n",
      "step 5490, loss is 4.839603900909424\n",
      "(64, 33)\n",
      "step 5491, loss is 5.0197577476501465\n",
      "(64, 33)\n",
      "step 5492, loss is 5.010577201843262\n",
      "(64, 33)\n",
      "step 5493, loss is 4.855237007141113\n",
      "(64, 33)\n",
      "step 5494, loss is 5.02009391784668\n",
      "(64, 33)\n",
      "step 5495, loss is 5.210659027099609\n",
      "(64, 33)\n",
      "step 5496, loss is 4.971765518188477\n",
      "(64, 33)\n",
      "step 5497, loss is 4.878669738769531\n",
      "(64, 33)\n",
      "step 5498, loss is 4.939266681671143\n",
      "(64, 33)\n",
      "step 5499, loss is 4.954992294311523\n",
      "(64, 33)\n",
      "step 5500, loss is 4.909140586853027\n",
      "(64, 33)\n",
      "step 5501, loss is 4.805333137512207\n",
      "(64, 33)\n",
      "step 5502, loss is 5.055262088775635\n",
      "(64, 33)\n",
      "step 5503, loss is 4.80986213684082\n",
      "(64, 33)\n",
      "step 5504, loss is 5.068172454833984\n",
      "(64, 33)\n",
      "step 5505, loss is 5.0235209465026855\n",
      "(64, 33)\n",
      "step 5506, loss is 4.942440986633301\n",
      "(64, 33)\n",
      "step 5507, loss is 4.974948406219482\n",
      "(64, 33)\n",
      "step 5508, loss is 4.981813430786133\n",
      "(64, 33)\n",
      "step 5509, loss is 4.902617454528809\n",
      "(64, 33)\n",
      "step 5510, loss is 4.902194976806641\n",
      "(64, 33)\n",
      "step 5511, loss is 4.96803617477417\n",
      "(64, 33)\n",
      "step 5512, loss is 5.153534889221191\n",
      "(64, 33)\n",
      "step 5513, loss is 5.096567153930664\n",
      "(64, 33)\n",
      "step 5514, loss is 4.7533698081970215\n",
      "(64, 33)\n",
      "step 5515, loss is 5.010371208190918\n",
      "(64, 33)\n",
      "step 5516, loss is 5.119431018829346\n",
      "(64, 33)\n",
      "step 5517, loss is 4.887650012969971\n",
      "(64, 33)\n",
      "step 5518, loss is 4.9170379638671875\n",
      "(64, 33)\n",
      "step 5519, loss is 5.0561017990112305\n",
      "(64, 33)\n",
      "step 5520, loss is 5.0712738037109375\n",
      "(64, 33)\n",
      "step 5521, loss is 5.066118240356445\n",
      "(64, 33)\n",
      "step 5522, loss is 5.176170825958252\n",
      "(64, 33)\n",
      "step 5523, loss is 5.1294074058532715\n",
      "(64, 33)\n",
      "step 5524, loss is 5.037681579589844\n",
      "(64, 33)\n",
      "step 5525, loss is 4.9615159034729\n",
      "(64, 33)\n",
      "step 5526, loss is 4.986989974975586\n",
      "(64, 33)\n",
      "step 5527, loss is 4.994764804840088\n",
      "(64, 33)\n",
      "step 5528, loss is 4.935973644256592\n",
      "(64, 33)\n",
      "step 5529, loss is 5.091305255889893\n",
      "(64, 33)\n",
      "step 5530, loss is 4.985714435577393\n",
      "(64, 33)\n",
      "step 5531, loss is 4.969588756561279\n",
      "(64, 33)\n",
      "step 5532, loss is 5.096135139465332\n",
      "(64, 33)\n",
      "step 5533, loss is 4.830987453460693\n",
      "(64, 33)\n",
      "step 5534, loss is 5.034228801727295\n",
      "(64, 33)\n",
      "step 5535, loss is 4.939537525177002\n",
      "(64, 33)\n",
      "step 5536, loss is 4.946420192718506\n",
      "(64, 33)\n",
      "step 5537, loss is 4.919118404388428\n",
      "(64, 33)\n",
      "step 5538, loss is 4.848121643066406\n",
      "(64, 33)\n",
      "step 5539, loss is 4.967827320098877\n",
      "(64, 33)\n",
      "step 5540, loss is 4.924831867218018\n",
      "(64, 33)\n",
      "step 5541, loss is 5.073930263519287\n",
      "(64, 33)\n",
      "step 5542, loss is 5.133547306060791\n",
      "(64, 33)\n",
      "step 5543, loss is 4.900213718414307\n",
      "(64, 33)\n",
      "step 5544, loss is 4.952037334442139\n",
      "(64, 33)\n",
      "step 5545, loss is 5.189034461975098\n",
      "(64, 33)\n",
      "step 5546, loss is 4.933804035186768\n",
      "(64, 33)\n",
      "step 5547, loss is 4.988178730010986\n",
      "(64, 33)\n",
      "step 5548, loss is 4.8108439445495605\n",
      "(64, 33)\n",
      "step 5549, loss is 4.82850980758667\n",
      "(64, 33)\n",
      "step 5550, loss is 4.883349418640137\n",
      "(64, 33)\n",
      "step 5551, loss is 4.925475120544434\n",
      "(64, 33)\n",
      "step 5552, loss is 5.093345642089844\n",
      "(64, 33)\n",
      "step 5553, loss is 4.933502197265625\n",
      "(64, 33)\n",
      "step 5554, loss is 5.082365989685059\n",
      "(64, 33)\n",
      "step 5555, loss is 4.836915969848633\n",
      "(64, 33)\n",
      "step 5556, loss is 5.1148152351379395\n",
      "(64, 33)\n",
      "step 5557, loss is 5.103784561157227\n",
      "(64, 33)\n",
      "step 5558, loss is 5.170637607574463\n",
      "(64, 33)\n",
      "step 5559, loss is 4.880910396575928\n",
      "(64, 33)\n",
      "step 5560, loss is 5.032435417175293\n",
      "(64, 33)\n",
      "step 5561, loss is 5.0092244148254395\n",
      "(64, 33)\n",
      "step 5562, loss is 5.0355048179626465\n",
      "(64, 33)\n",
      "step 5563, loss is 4.964005947113037\n",
      "(64, 33)\n",
      "step 5564, loss is 4.937834739685059\n",
      "(64, 33)\n",
      "step 5565, loss is 4.955323219299316\n",
      "(64, 33)\n",
      "step 5566, loss is 4.93358850479126\n",
      "(64, 33)\n",
      "step 5567, loss is 5.013347148895264\n",
      "(64, 33)\n",
      "step 5568, loss is 4.950084686279297\n",
      "(64, 33)\n",
      "step 5569, loss is 4.964616775512695\n",
      "(64, 33)\n",
      "step 5570, loss is 4.989662170410156\n",
      "(64, 33)\n",
      "step 5571, loss is 4.920619010925293\n",
      "(64, 33)\n",
      "step 5572, loss is 4.906907558441162\n",
      "(64, 33)\n",
      "step 5573, loss is 4.822010040283203\n",
      "(64, 33)\n",
      "step 5574, loss is 4.99028205871582\n",
      "(64, 33)\n",
      "step 5575, loss is 4.985631465911865\n",
      "(64, 33)\n",
      "step 5576, loss is 4.951042175292969\n",
      "(64, 33)\n",
      "step 5577, loss is 4.79922342300415\n",
      "(64, 33)\n",
      "step 5578, loss is 4.813683986663818\n",
      "(64, 33)\n",
      "step 5579, loss is 4.929213523864746\n",
      "(64, 33)\n",
      "step 5580, loss is 5.009054183959961\n",
      "(64, 33)\n",
      "step 5581, loss is 5.0176496505737305\n",
      "(64, 33)\n",
      "step 5582, loss is 4.754410743713379\n",
      "(64, 33)\n",
      "step 5583, loss is 4.93878173828125\n",
      "(64, 33)\n",
      "step 5584, loss is 5.109889030456543\n",
      "(64, 33)\n",
      "step 5585, loss is 4.870002269744873\n",
      "(64, 33)\n",
      "step 5586, loss is 5.2844038009643555\n",
      "(64, 33)\n",
      "step 5587, loss is 4.894750595092773\n",
      "(64, 33)\n",
      "step 5588, loss is 4.821704387664795\n",
      "(64, 33)\n",
      "step 5589, loss is 5.036008834838867\n",
      "(64, 33)\n",
      "step 5590, loss is 4.776994228363037\n",
      "(64, 33)\n",
      "step 5591, loss is 5.265033721923828\n",
      "(64, 33)\n",
      "step 5592, loss is 4.9589362144470215\n",
      "(64, 33)\n",
      "step 5593, loss is 4.978507995605469\n",
      "(64, 33)\n",
      "step 5594, loss is 5.157223224639893\n",
      "(64, 33)\n",
      "step 5595, loss is 5.175334930419922\n",
      "(64, 33)\n",
      "step 5596, loss is 4.907282829284668\n",
      "(64, 33)\n",
      "step 5597, loss is 4.970183849334717\n",
      "(64, 33)\n",
      "step 5598, loss is 5.009393215179443\n",
      "(64, 33)\n",
      "step 5599, loss is 4.683759689331055\n",
      "(64, 33)\n",
      "step 5600, loss is 5.162378311157227\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5601, loss is 4.982948303222656\n",
      "(64, 33)\n",
      "step 5602, loss is 5.1997971534729\n",
      "(64, 33)\n",
      "step 5603, loss is 5.032296657562256\n",
      "(64, 33)\n",
      "step 5604, loss is 4.8290629386901855\n",
      "(64, 33)\n",
      "step 5605, loss is 4.924767971038818\n",
      "(64, 33)\n",
      "step 5606, loss is 4.934867858886719\n",
      "(64, 33)\n",
      "step 5607, loss is 4.970515251159668\n",
      "(64, 33)\n",
      "step 5608, loss is 4.900458335876465\n",
      "(64, 33)\n",
      "step 5609, loss is 5.158823490142822\n",
      "(64, 33)\n",
      "step 5610, loss is 4.755031108856201\n",
      "(64, 33)\n",
      "step 5611, loss is 5.046506404876709\n",
      "(64, 33)\n",
      "step 5612, loss is 5.061940670013428\n",
      "(64, 33)\n",
      "step 5613, loss is 4.925507545471191\n",
      "(64, 33)\n",
      "step 5614, loss is 4.761476993560791\n",
      "(64, 33)\n",
      "step 5615, loss is 5.121269226074219\n",
      "(64, 33)\n",
      "step 5616, loss is 4.941646575927734\n",
      "(64, 33)\n",
      "step 5617, loss is 4.704400539398193\n",
      "(64, 33)\n",
      "step 5618, loss is 4.784133434295654\n",
      "(64, 33)\n",
      "step 5619, loss is 4.823676109313965\n",
      "(64, 33)\n",
      "step 5620, loss is 4.930489540100098\n",
      "(64, 33)\n",
      "step 5621, loss is 4.959503173828125\n",
      "(64, 33)\n",
      "step 5622, loss is 4.883603572845459\n",
      "(64, 33)\n",
      "step 5623, loss is 5.053489685058594\n",
      "(64, 33)\n",
      "step 5624, loss is 4.960407733917236\n",
      "(64, 33)\n",
      "step 5625, loss is 5.048407554626465\n",
      "(64, 33)\n",
      "step 5626, loss is 4.9900946617126465\n",
      "(64, 33)\n",
      "step 5627, loss is 5.110845565795898\n",
      "(64, 33)\n",
      "step 5628, loss is 5.033419609069824\n",
      "(64, 33)\n",
      "step 5629, loss is 4.874906063079834\n",
      "(64, 33)\n",
      "step 5630, loss is 4.878135681152344\n",
      "(64, 33)\n",
      "step 5631, loss is 4.784674644470215\n",
      "(64, 33)\n",
      "step 5632, loss is 4.996525287628174\n",
      "(64, 33)\n",
      "step 5633, loss is 4.856647968292236\n",
      "(64, 33)\n",
      "step 5634, loss is 4.878740310668945\n",
      "(64, 33)\n",
      "step 5635, loss is 4.904500961303711\n",
      "(64, 33)\n",
      "step 5636, loss is 5.240827560424805\n",
      "(64, 33)\n",
      "step 5637, loss is 4.9667439460754395\n",
      "(64, 33)\n",
      "step 5638, loss is 5.035155296325684\n",
      "(64, 33)\n",
      "step 5639, loss is 4.963189125061035\n",
      "(64, 33)\n",
      "step 5640, loss is 4.76035213470459\n",
      "(64, 33)\n",
      "step 5641, loss is 4.990823268890381\n",
      "(64, 33)\n",
      "step 5642, loss is 5.027198314666748\n",
      "(64, 33)\n",
      "step 5643, loss is 4.792673587799072\n",
      "(64, 33)\n",
      "step 5644, loss is 5.00105619430542\n",
      "(64, 33)\n",
      "step 5645, loss is 4.954950332641602\n",
      "(64, 33)\n",
      "step 5646, loss is 4.968492031097412\n",
      "(64, 33)\n",
      "step 5647, loss is 4.951269626617432\n",
      "(64, 33)\n",
      "step 5648, loss is 5.014132499694824\n",
      "(64, 33)\n",
      "step 5649, loss is 4.944823265075684\n",
      "(64, 33)\n",
      "step 5650, loss is 4.964507102966309\n",
      "(64, 33)\n",
      "step 5651, loss is 5.199049472808838\n",
      "(64, 33)\n",
      "step 5652, loss is 4.898240089416504\n",
      "(64, 33)\n",
      "step 5653, loss is 4.9887871742248535\n",
      "(64, 33)\n",
      "step 5654, loss is 4.902011871337891\n",
      "(64, 33)\n",
      "step 5655, loss is 4.777771949768066\n",
      "(64, 33)\n",
      "step 5656, loss is 4.999253749847412\n",
      "(64, 33)\n",
      "step 5657, loss is 4.933481216430664\n",
      "(64, 33)\n",
      "step 5658, loss is 4.820749759674072\n",
      "(64, 33)\n",
      "step 5659, loss is 5.010042190551758\n",
      "(64, 33)\n",
      "step 5660, loss is 4.778805255889893\n",
      "(64, 33)\n",
      "step 5661, loss is 4.992081642150879\n",
      "(64, 33)\n",
      "step 5662, loss is 4.793233871459961\n",
      "(64, 33)\n",
      "step 5663, loss is 5.1242356300354\n",
      "(64, 33)\n",
      "step 5664, loss is 4.94469690322876\n",
      "(64, 33)\n",
      "step 5665, loss is 5.028463840484619\n",
      "(64, 33)\n",
      "step 5666, loss is 4.81925106048584\n",
      "(64, 33)\n",
      "step 5667, loss is 5.03939962387085\n",
      "(64, 33)\n",
      "step 5668, loss is 5.0931782722473145\n",
      "(64, 33)\n",
      "step 5669, loss is 5.098546028137207\n",
      "(64, 33)\n",
      "step 5670, loss is 4.98103141784668\n",
      "(64, 33)\n",
      "step 5671, loss is 5.030467987060547\n",
      "(64, 33)\n",
      "step 5672, loss is 4.888067722320557\n",
      "(64, 33)\n",
      "step 5673, loss is 4.989264011383057\n",
      "(64, 33)\n",
      "step 5674, loss is 5.066772937774658\n",
      "(64, 33)\n",
      "step 5675, loss is 5.027991771697998\n",
      "(64, 33)\n",
      "step 5676, loss is 5.091865539550781\n",
      "(64, 33)\n",
      "step 5677, loss is 4.826897621154785\n",
      "(64, 33)\n",
      "step 5678, loss is 5.088442802429199\n",
      "(64, 33)\n",
      "step 5679, loss is 4.94556188583374\n",
      "(64, 33)\n",
      "step 5680, loss is 4.855271339416504\n",
      "(64, 33)\n",
      "step 5681, loss is 4.964862823486328\n",
      "(64, 33)\n",
      "step 5682, loss is 4.761695861816406\n",
      "(64, 33)\n",
      "step 5683, loss is 4.826855659484863\n",
      "(64, 33)\n",
      "step 5684, loss is 4.904438018798828\n",
      "(64, 33)\n",
      "step 5685, loss is 4.915167808532715\n",
      "(64, 33)\n",
      "step 5686, loss is 5.087570667266846\n",
      "(64, 33)\n",
      "step 5687, loss is 4.897188186645508\n",
      "(64, 33)\n",
      "step 5688, loss is 4.890334606170654\n",
      "(64, 33)\n",
      "step 5689, loss is 4.812539577484131\n",
      "(64, 33)\n",
      "step 5690, loss is 4.9096503257751465\n",
      "(64, 33)\n",
      "step 5691, loss is 5.084856986999512\n",
      "(64, 33)\n",
      "step 5692, loss is 4.888251304626465\n",
      "(64, 33)\n",
      "step 5693, loss is 4.8829240798950195\n",
      "(64, 33)\n",
      "step 5694, loss is 4.8605427742004395\n",
      "(64, 33)\n",
      "step 5695, loss is 4.778534889221191\n",
      "(64, 33)\n",
      "step 5696, loss is 5.058568954467773\n",
      "(64, 33)\n",
      "step 5697, loss is 4.933440208435059\n",
      "(64, 33)\n",
      "step 5698, loss is 5.244762420654297\n",
      "(64, 33)\n",
      "step 5699, loss is 4.906721115112305\n",
      "(64, 33)\n",
      "step 5700, loss is 5.0682477951049805\n",
      "(64, 33)\n",
      "step 5701, loss is 4.899643421173096\n",
      "(64, 33)\n",
      "step 5702, loss is 4.894121170043945\n",
      "(64, 33)\n",
      "step 5703, loss is 4.851316452026367\n",
      "(64, 33)\n",
      "step 5704, loss is 4.892017841339111\n",
      "(64, 33)\n",
      "step 5705, loss is 5.108885288238525\n",
      "(64, 33)\n",
      "step 5706, loss is 5.008317470550537\n",
      "(64, 33)\n",
      "step 5707, loss is 4.7664875984191895\n",
      "(64, 33)\n",
      "step 5708, loss is 4.952468395233154\n",
      "(64, 33)\n",
      "step 5709, loss is 4.847596168518066\n",
      "(64, 33)\n",
      "step 5710, loss is 4.9789323806762695\n",
      "(64, 33)\n",
      "step 5711, loss is 5.022787570953369\n",
      "(64, 33)\n",
      "step 5712, loss is 4.971400260925293\n",
      "(64, 33)\n",
      "step 5713, loss is 4.964974880218506\n",
      "(64, 33)\n",
      "step 5714, loss is 5.038908958435059\n",
      "(64, 33)\n",
      "step 5715, loss is 4.888378143310547\n",
      "(64, 33)\n",
      "step 5716, loss is 5.047491550445557\n",
      "(64, 33)\n",
      "step 5717, loss is 4.868363857269287\n",
      "(64, 33)\n",
      "step 5718, loss is 4.943682670593262\n",
      "(64, 33)\n",
      "step 5719, loss is 5.165050983428955\n",
      "(64, 33)\n",
      "step 5720, loss is 4.8292341232299805\n",
      "(64, 33)\n",
      "step 5721, loss is 4.950546741485596\n",
      "(64, 33)\n",
      "step 5722, loss is 4.786087512969971\n",
      "(64, 33)\n",
      "step 5723, loss is 4.9267659187316895\n",
      "(64, 33)\n",
      "step 5724, loss is 5.153522968292236\n",
      "(64, 33)\n",
      "step 5725, loss is 5.094014644622803\n",
      "(64, 33)\n",
      "step 5726, loss is 4.763312339782715\n",
      "(64, 33)\n",
      "step 5727, loss is 4.860782146453857\n",
      "(64, 33)\n",
      "step 5728, loss is 5.041759967803955\n",
      "(64, 33)\n",
      "step 5729, loss is 5.016883850097656\n",
      "(64, 33)\n",
      "step 5730, loss is 4.639692783355713\n",
      "(64, 33)\n",
      "step 5731, loss is 5.017125129699707\n",
      "(64, 33)\n",
      "step 5732, loss is 4.92898416519165\n",
      "(64, 33)\n",
      "step 5733, loss is 4.792140007019043\n",
      "(64, 33)\n",
      "step 5734, loss is 4.905350208282471\n",
      "(64, 33)\n",
      "step 5735, loss is 4.868025302886963\n",
      "(64, 33)\n",
      "step 5736, loss is 5.028504371643066\n",
      "(64, 33)\n",
      "step 5737, loss is 5.118512153625488\n",
      "(64, 33)\n",
      "step 5738, loss is 4.892852306365967\n",
      "(64, 33)\n",
      "step 5739, loss is 4.783442974090576\n",
      "(64, 33)\n",
      "step 5740, loss is 4.941606521606445\n",
      "(64, 33)\n",
      "step 5741, loss is 4.964680194854736\n",
      "(64, 33)\n",
      "step 5742, loss is 5.072083950042725\n",
      "(64, 33)\n",
      "step 5743, loss is 4.887135028839111\n",
      "(64, 33)\n",
      "step 5744, loss is 4.932792663574219\n",
      "(64, 33)\n",
      "step 5745, loss is 4.870025157928467\n",
      "(64, 33)\n",
      "step 5746, loss is 5.044170379638672\n",
      "(64, 33)\n",
      "step 5747, loss is 4.8713059425354\n",
      "(64, 33)\n",
      "step 5748, loss is 4.927423477172852\n",
      "(64, 33)\n",
      "step 5749, loss is 5.025295734405518\n",
      "(64, 33)\n",
      "step 5750, loss is 4.929331302642822\n",
      "(64, 33)\n",
      "step 5751, loss is 4.924888610839844\n",
      "(64, 33)\n",
      "step 5752, loss is 4.947604656219482\n",
      "(64, 33)\n",
      "step 5753, loss is 5.006734371185303\n",
      "(64, 33)\n",
      "step 5754, loss is 4.811173915863037\n",
      "(64, 33)\n",
      "step 5755, loss is 5.023098945617676\n",
      "(64, 33)\n",
      "step 5756, loss is 4.962867259979248\n",
      "(64, 33)\n",
      "step 5757, loss is 5.134209632873535\n",
      "(64, 33)\n",
      "step 5758, loss is 4.837648868560791\n",
      "(64, 33)\n",
      "step 5759, loss is 5.131362438201904\n",
      "(64, 33)\n",
      "step 5760, loss is 4.895205497741699\n",
      "(64, 33)\n",
      "step 5761, loss is 5.079092502593994\n",
      "(64, 33)\n",
      "step 5762, loss is 4.902238368988037\n",
      "(64, 33)\n",
      "step 5763, loss is 4.921341419219971\n",
      "(64, 33)\n",
      "step 5764, loss is 4.863430976867676\n",
      "(64, 33)\n",
      "step 5765, loss is 4.905238151550293\n",
      "(64, 33)\n",
      "step 5766, loss is 4.765474319458008\n",
      "(64, 33)\n",
      "step 5767, loss is 4.779939651489258\n",
      "(64, 33)\n",
      "step 5768, loss is 5.148922920227051\n",
      "(64, 33)\n",
      "step 5769, loss is 4.950227737426758\n",
      "(64, 33)\n",
      "step 5770, loss is 4.892932891845703\n",
      "(64, 33)\n",
      "step 5771, loss is 5.105204105377197\n",
      "(64, 33)\n",
      "step 5772, loss is 4.963657855987549\n",
      "(64, 33)\n",
      "step 5773, loss is 4.93168306350708\n",
      "(64, 33)\n",
      "step 5774, loss is 4.820698261260986\n",
      "(64, 33)\n",
      "step 5775, loss is 4.90920352935791\n",
      "(64, 33)\n",
      "step 5776, loss is 4.867782115936279\n",
      "(64, 33)\n",
      "step 5777, loss is 4.958204746246338\n",
      "(64, 33)\n",
      "step 5778, loss is 4.9622483253479\n",
      "(64, 33)\n",
      "step 5779, loss is 5.082327365875244\n",
      "(64, 33)\n",
      "step 5780, loss is 4.818209171295166\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5781, loss is 4.891138076782227\n",
      "(64, 33)\n",
      "step 5782, loss is 4.927073955535889\n",
      "(64, 33)\n",
      "step 5783, loss is 4.8176116943359375\n",
      "(64, 33)\n",
      "step 5784, loss is 5.04042387008667\n",
      "(64, 33)\n",
      "step 5785, loss is 5.002253532409668\n",
      "(64, 33)\n",
      "step 5786, loss is 4.918509006500244\n",
      "(64, 33)\n",
      "step 5787, loss is 5.039895057678223\n",
      "(64, 33)\n",
      "step 5788, loss is 4.891598701477051\n",
      "(64, 33)\n",
      "step 5789, loss is 4.909477710723877\n",
      "(64, 33)\n",
      "step 5790, loss is 4.8652520179748535\n",
      "(64, 33)\n",
      "step 5791, loss is 4.917685031890869\n",
      "(64, 33)\n",
      "step 5792, loss is 4.960661888122559\n",
      "(64, 33)\n",
      "step 5793, loss is 4.799391746520996\n",
      "(64, 33)\n",
      "step 5794, loss is 4.944900989532471\n",
      "(64, 33)\n",
      "step 5795, loss is 4.878896236419678\n",
      "(64, 33)\n",
      "step 5796, loss is 4.893788814544678\n",
      "(64, 33)\n",
      "step 5797, loss is 4.894040584564209\n",
      "(64, 33)\n",
      "step 5798, loss is 4.955100059509277\n",
      "(64, 33)\n",
      "step 5799, loss is 4.9637370109558105\n",
      "(64, 33)\n",
      "step 5800, loss is 4.949599266052246\n",
      "(64, 33)\n",
      "step 5801, loss is 4.897846698760986\n",
      "(64, 33)\n",
      "step 5802, loss is 5.022508144378662\n",
      "(64, 33)\n",
      "step 5803, loss is 5.087702751159668\n",
      "(64, 33)\n",
      "step 5804, loss is 4.826552391052246\n",
      "(64, 33)\n",
      "step 5805, loss is 4.9965033531188965\n",
      "(64, 33)\n",
      "step 5806, loss is 4.708156585693359\n",
      "(64, 33)\n",
      "step 5807, loss is 5.153097629547119\n",
      "(64, 33)\n",
      "step 5808, loss is 4.898219108581543\n",
      "(64, 33)\n",
      "step 5809, loss is 4.841197967529297\n",
      "(64, 33)\n",
      "step 5810, loss is 4.981094837188721\n",
      "(64, 33)\n",
      "step 5811, loss is 4.94660758972168\n",
      "(64, 33)\n",
      "step 5812, loss is 4.9413557052612305\n",
      "(64, 33)\n",
      "step 5813, loss is 5.103835105895996\n",
      "(64, 33)\n",
      "step 5814, loss is 4.8974995613098145\n",
      "(64, 33)\n",
      "step 5815, loss is 5.0048747062683105\n",
      "(64, 33)\n",
      "step 5816, loss is 5.033756732940674\n",
      "(64, 33)\n",
      "step 5817, loss is 4.873111724853516\n",
      "(64, 33)\n",
      "step 5818, loss is 4.919984817504883\n",
      "(64, 33)\n",
      "step 5819, loss is 4.908716678619385\n",
      "(64, 33)\n",
      "step 5820, loss is 5.018625736236572\n",
      "(64, 33)\n",
      "step 5821, loss is 4.866208076477051\n",
      "(64, 33)\n",
      "step 5822, loss is 4.659095764160156\n",
      "(64, 33)\n",
      "step 5823, loss is 4.982985973358154\n",
      "(64, 33)\n",
      "step 5824, loss is 4.935257911682129\n",
      "(64, 33)\n",
      "step 5825, loss is 5.129931926727295\n",
      "(64, 33)\n",
      "step 5826, loss is 5.039370536804199\n",
      "(64, 33)\n",
      "step 5827, loss is 4.971230983734131\n",
      "(64, 33)\n",
      "step 5828, loss is 4.928191184997559\n",
      "(64, 33)\n",
      "step 5829, loss is 4.935312747955322\n",
      "(64, 33)\n",
      "step 5830, loss is 4.875904083251953\n",
      "(64, 33)\n",
      "step 5831, loss is 4.947257041931152\n",
      "(64, 33)\n",
      "step 5832, loss is 4.952490329742432\n",
      "(64, 33)\n",
      "step 5833, loss is 5.050577640533447\n",
      "(64, 33)\n",
      "step 5834, loss is 4.909529209136963\n",
      "(64, 33)\n",
      "step 5835, loss is 4.920069694519043\n",
      "(64, 33)\n",
      "step 5836, loss is 4.8315510749816895\n",
      "(64, 33)\n",
      "step 5837, loss is 5.002742767333984\n",
      "(64, 33)\n",
      "step 5838, loss is 4.792510986328125\n",
      "(64, 33)\n",
      "step 5839, loss is 5.049368381500244\n",
      "(64, 33)\n",
      "step 5840, loss is 4.931985378265381\n",
      "(64, 33)\n",
      "step 5841, loss is 4.852562427520752\n",
      "(64, 33)\n",
      "step 5842, loss is 5.183659553527832\n",
      "(64, 33)\n",
      "step 5843, loss is 4.905185699462891\n",
      "(64, 33)\n",
      "step 5844, loss is 5.130714416503906\n",
      "(64, 33)\n",
      "step 5845, loss is 5.011561393737793\n",
      "(64, 33)\n",
      "step 5846, loss is 5.110073566436768\n",
      "(64, 33)\n",
      "step 5847, loss is 4.797947406768799\n",
      "(64, 33)\n",
      "step 5848, loss is 5.1141133308410645\n",
      "(64, 33)\n",
      "step 5849, loss is 4.79776668548584\n",
      "(64, 33)\n",
      "step 5850, loss is 4.8526153564453125\n",
      "(64, 33)\n",
      "step 5851, loss is 4.923050880432129\n",
      "(64, 33)\n",
      "step 5852, loss is 4.873233795166016\n",
      "(64, 33)\n",
      "step 5853, loss is 5.213276386260986\n",
      "(64, 33)\n",
      "step 5854, loss is 5.138267517089844\n",
      "(64, 33)\n",
      "step 5855, loss is 5.081493854522705\n",
      "(64, 33)\n",
      "step 5856, loss is 4.938077449798584\n",
      "(64, 33)\n",
      "step 5857, loss is 5.03453254699707\n",
      "(64, 33)\n",
      "step 5858, loss is 4.991608142852783\n",
      "(64, 33)\n",
      "step 5859, loss is 4.784439563751221\n",
      "(64, 33)\n",
      "step 5860, loss is 4.7662811279296875\n",
      "(64, 33)\n",
      "step 5861, loss is 4.629964351654053\n",
      "(64, 33)\n",
      "step 5862, loss is 5.070587158203125\n",
      "(64, 33)\n",
      "step 5863, loss is 4.9672393798828125\n",
      "(64, 33)\n",
      "step 5864, loss is 4.9682769775390625\n",
      "(64, 33)\n",
      "step 5865, loss is 5.072691440582275\n",
      "(64, 33)\n",
      "step 5866, loss is 5.012049674987793\n",
      "(64, 33)\n",
      "step 5867, loss is 4.884162425994873\n",
      "(64, 33)\n",
      "step 5868, loss is 4.802225589752197\n",
      "(64, 33)\n",
      "step 5869, loss is 4.905351638793945\n",
      "(64, 33)\n",
      "step 5870, loss is 4.8073859214782715\n",
      "(64, 33)\n",
      "step 5871, loss is 5.030902862548828\n",
      "(64, 33)\n",
      "step 5872, loss is 5.081449031829834\n",
      "(64, 33)\n",
      "step 5873, loss is 4.81887674331665\n",
      "(64, 33)\n",
      "step 5874, loss is 4.921056270599365\n",
      "(64, 33)\n",
      "step 5875, loss is 4.920880317687988\n",
      "(64, 33)\n",
      "step 5876, loss is 4.91987943649292\n",
      "(64, 33)\n",
      "step 5877, loss is 4.948856830596924\n",
      "(64, 33)\n",
      "step 5878, loss is 5.117347717285156\n",
      "(64, 33)\n",
      "step 5879, loss is 4.938564777374268\n",
      "(64, 33)\n",
      "step 5880, loss is 4.970742702484131\n",
      "(64, 33)\n",
      "step 5881, loss is 4.946990966796875\n",
      "(64, 33)\n",
      "step 5882, loss is 4.8608880043029785\n",
      "(64, 33)\n",
      "step 5883, loss is 5.0278191566467285\n",
      "(64, 33)\n",
      "step 5884, loss is 5.030060768127441\n",
      "(64, 33)\n",
      "step 5885, loss is 4.86954927444458\n",
      "(64, 33)\n",
      "step 5886, loss is 4.889334678649902\n",
      "(64, 33)\n",
      "step 5887, loss is 5.045993804931641\n",
      "(64, 33)\n",
      "step 5888, loss is 4.846584796905518\n",
      "(64, 33)\n",
      "step 5889, loss is 5.004055976867676\n",
      "(64, 33)\n",
      "step 5890, loss is 4.961874961853027\n",
      "(64, 33)\n",
      "step 5891, loss is 4.918297290802002\n",
      "(64, 33)\n",
      "step 5892, loss is 4.908984661102295\n",
      "(64, 33)\n",
      "step 5893, loss is 4.841438293457031\n",
      "(64, 33)\n",
      "step 5894, loss is 4.866573810577393\n",
      "(64, 33)\n",
      "step 5895, loss is 5.132561206817627\n",
      "(64, 33)\n",
      "step 5896, loss is 5.02631950378418\n",
      "(64, 33)\n",
      "step 5897, loss is 4.963428497314453\n",
      "(64, 33)\n",
      "step 5898, loss is 5.075521945953369\n",
      "(64, 33)\n",
      "step 5899, loss is 5.063724517822266\n",
      "(64, 33)\n",
      "step 5900, loss is 4.955282211303711\n",
      "(64, 33)\n",
      "step 5901, loss is 4.762639045715332\n",
      "(64, 33)\n",
      "step 5902, loss is 4.793078422546387\n",
      "(64, 33)\n",
      "step 5903, loss is 4.977491855621338\n",
      "(64, 33)\n",
      "step 5904, loss is 5.008495807647705\n",
      "(64, 33)\n",
      "step 5905, loss is 5.010447978973389\n",
      "(64, 33)\n",
      "step 5906, loss is 4.682001113891602\n",
      "(64, 33)\n",
      "step 5907, loss is 5.108181476593018\n",
      "(64, 33)\n",
      "step 5908, loss is 5.005215644836426\n",
      "(64, 33)\n",
      "step 5909, loss is 4.919878959655762\n",
      "(64, 33)\n",
      "step 5910, loss is 5.134029388427734\n",
      "(64, 33)\n",
      "step 5911, loss is 4.932831764221191\n",
      "(64, 33)\n",
      "step 5912, loss is 5.160017967224121\n",
      "(64, 33)\n",
      "step 5913, loss is 4.988945007324219\n",
      "(64, 33)\n",
      "step 5914, loss is 4.96272611618042\n",
      "(64, 33)\n",
      "step 5915, loss is 4.963103294372559\n",
      "(64, 33)\n",
      "step 5916, loss is 4.964018821716309\n",
      "(64, 33)\n",
      "step 5917, loss is 5.015236854553223\n",
      "(64, 33)\n",
      "step 5918, loss is 4.961158752441406\n",
      "(64, 33)\n",
      "step 5919, loss is 5.1935577392578125\n",
      "(64, 33)\n",
      "step 5920, loss is 4.95747709274292\n",
      "(64, 33)\n",
      "step 5921, loss is 4.869795322418213\n",
      "(64, 33)\n",
      "step 5922, loss is 4.944223403930664\n",
      "(64, 33)\n",
      "step 5923, loss is 4.998683929443359\n",
      "(64, 33)\n",
      "step 5924, loss is 4.922306060791016\n",
      "(64, 33)\n",
      "step 5925, loss is 4.972395896911621\n",
      "(64, 33)\n",
      "step 5926, loss is 5.1038594245910645\n",
      "(64, 33)\n",
      "step 5927, loss is 4.875345706939697\n",
      "(64, 33)\n",
      "step 5928, loss is 4.849614143371582\n",
      "(64, 33)\n",
      "step 5929, loss is 4.982404708862305\n",
      "(64, 33)\n",
      "step 5930, loss is 4.831556797027588\n",
      "(64, 33)\n",
      "step 5931, loss is 5.0737833976745605\n",
      "(64, 33)\n",
      "step 5932, loss is 4.926506042480469\n",
      "(64, 33)\n",
      "step 5933, loss is 4.896534442901611\n",
      "(64, 33)\n",
      "step 5934, loss is 4.875019550323486\n",
      "(64, 33)\n",
      "step 5935, loss is 4.817436218261719\n",
      "(64, 33)\n",
      "step 5936, loss is 5.039213180541992\n",
      "(64, 33)\n",
      "step 5937, loss is 4.9860453605651855\n",
      "(64, 33)\n",
      "step 5938, loss is 5.092713832855225\n",
      "(64, 33)\n",
      "step 5939, loss is 4.750454902648926\n",
      "(64, 33)\n",
      "step 5940, loss is 5.00284481048584\n",
      "(64, 33)\n",
      "step 5941, loss is 4.976570129394531\n",
      "(64, 33)\n",
      "step 5942, loss is 4.777488708496094\n",
      "(64, 33)\n",
      "step 5943, loss is 4.757389068603516\n",
      "(64, 33)\n",
      "step 5944, loss is 4.900636196136475\n",
      "(64, 33)\n",
      "step 5945, loss is 4.87200927734375\n",
      "(64, 33)\n",
      "step 5946, loss is 4.859294891357422\n",
      "(64, 33)\n",
      "step 5947, loss is 4.9405388832092285\n",
      "(64, 33)\n",
      "step 5948, loss is 4.8908162117004395\n",
      "(64, 33)\n",
      "step 5949, loss is 4.892017364501953\n",
      "(64, 33)\n",
      "step 5950, loss is 4.976720333099365\n",
      "(64, 33)\n",
      "step 5951, loss is 4.994914531707764\n",
      "(64, 33)\n",
      "step 5952, loss is 5.212701320648193\n",
      "(64, 33)\n",
      "step 5953, loss is 5.059516429901123\n",
      "(64, 33)\n",
      "step 5954, loss is 5.031041145324707\n",
      "(64, 33)\n",
      "step 5955, loss is 4.85833215713501\n",
      "(64, 33)\n",
      "step 5956, loss is 4.897681713104248\n",
      "(64, 33)\n",
      "step 5957, loss is 4.986673355102539\n",
      "(64, 33)\n",
      "step 5958, loss is 5.111124038696289\n",
      "(64, 33)\n",
      "step 5959, loss is 5.070987701416016\n",
      "(64, 33)\n",
      "step 5960, loss is 5.01307487487793\n",
      "(64, 33)\n",
      "step 5961, loss is 5.005823135375977\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5962, loss is 4.963685512542725\n",
      "(64, 33)\n",
      "step 5963, loss is 4.92782735824585\n",
      "(64, 33)\n",
      "step 5964, loss is 4.999494552612305\n",
      "(64, 33)\n",
      "step 5965, loss is 4.88707160949707\n",
      "(64, 33)\n",
      "step 5966, loss is 4.920682430267334\n",
      "(64, 33)\n",
      "step 5967, loss is 4.842353820800781\n",
      "(64, 33)\n",
      "step 5968, loss is 5.104315280914307\n",
      "(64, 33)\n",
      "step 5969, loss is 4.920137405395508\n",
      "(64, 33)\n",
      "step 5970, loss is 5.0317864418029785\n",
      "(64, 33)\n",
      "step 5971, loss is 4.868941783905029\n",
      "(64, 33)\n",
      "step 5972, loss is 4.822917461395264\n",
      "(64, 33)\n",
      "step 5973, loss is 4.949331760406494\n",
      "(64, 33)\n",
      "step 5974, loss is 4.833534240722656\n",
      "(64, 33)\n",
      "step 5975, loss is 5.036001682281494\n",
      "(64, 33)\n",
      "step 5976, loss is 4.866438388824463\n",
      "(64, 33)\n",
      "step 5977, loss is 4.903728008270264\n",
      "(64, 33)\n",
      "step 5978, loss is 4.956655025482178\n",
      "(64, 33)\n",
      "step 5979, loss is 4.955962657928467\n",
      "(64, 33)\n",
      "step 5980, loss is 4.826739311218262\n",
      "(64, 33)\n",
      "step 5981, loss is 5.102998733520508\n",
      "(64, 33)\n",
      "step 5982, loss is 4.914722442626953\n",
      "(64, 33)\n",
      "step 5983, loss is 5.054099082946777\n",
      "(64, 33)\n",
      "step 5984, loss is 4.837191104888916\n",
      "(64, 33)\n",
      "step 5985, loss is 5.001190185546875\n",
      "(64, 33)\n",
      "step 5986, loss is 4.9288506507873535\n",
      "(64, 33)\n",
      "step 5987, loss is 4.898021697998047\n",
      "(64, 33)\n",
      "step 5988, loss is 5.264196395874023\n",
      "(64, 33)\n",
      "step 5989, loss is 4.634762287139893\n",
      "(64, 33)\n",
      "step 5990, loss is 4.896016597747803\n",
      "(64, 33)\n",
      "step 5991, loss is 4.8144989013671875\n",
      "(64, 33)\n",
      "step 5992, loss is 5.036367893218994\n",
      "(64, 33)\n",
      "step 5993, loss is 4.857803821563721\n",
      "(64, 33)\n",
      "step 5994, loss is 4.994043350219727\n",
      "(64, 33)\n",
      "step 5995, loss is 4.822719097137451\n",
      "(64, 33)\n",
      "step 5996, loss is 4.931680679321289\n",
      "(64, 33)\n",
      "step 5997, loss is 4.912162780761719\n",
      "(64, 33)\n",
      "step 5998, loss is 4.915661334991455\n",
      "(64, 33)\n",
      "step 5999, loss is 4.844295501708984\n",
      "(64, 33)\n",
      "step 6000, loss is 4.840667247772217\n",
      "(64, 33)\n",
      "step 6001, loss is 5.099837779998779\n",
      "(64, 33)\n",
      "step 6002, loss is 5.006495952606201\n",
      "(64, 33)\n",
      "step 6003, loss is 5.04732084274292\n",
      "(64, 33)\n",
      "step 6004, loss is 4.87612247467041\n",
      "(64, 33)\n",
      "step 6005, loss is 5.003239631652832\n",
      "(64, 33)\n",
      "step 6006, loss is 5.077461242675781\n",
      "(64, 33)\n",
      "step 6007, loss is 5.005683898925781\n",
      "(64, 33)\n",
      "step 6008, loss is 5.073437213897705\n",
      "(64, 33)\n",
      "step 6009, loss is 4.9014668464660645\n",
      "(64, 33)\n",
      "step 6010, loss is 5.131108283996582\n",
      "(64, 33)\n",
      "step 6011, loss is 4.903260231018066\n",
      "(64, 33)\n",
      "step 6012, loss is 5.076419830322266\n",
      "(64, 33)\n",
      "step 6013, loss is 4.9920454025268555\n",
      "(64, 33)\n",
      "step 6014, loss is 4.841423034667969\n",
      "(64, 33)\n",
      "step 6015, loss is 5.167700290679932\n",
      "(64, 33)\n",
      "step 6016, loss is 4.953361511230469\n",
      "(64, 33)\n",
      "step 6017, loss is 5.036314010620117\n",
      "(64, 33)\n",
      "step 6018, loss is 4.9094624519348145\n",
      "(64, 33)\n",
      "step 6019, loss is 4.9758124351501465\n",
      "(64, 33)\n",
      "step 6020, loss is 5.003940582275391\n",
      "(64, 33)\n",
      "step 6021, loss is 4.992165565490723\n",
      "(64, 33)\n",
      "step 6022, loss is 4.989625930786133\n",
      "(64, 33)\n",
      "step 6023, loss is 4.920161247253418\n",
      "(64, 33)\n",
      "step 6024, loss is 4.833778381347656\n",
      "(64, 33)\n",
      "step 6025, loss is 4.760544776916504\n",
      "(64, 33)\n",
      "step 6026, loss is 4.953116416931152\n",
      "(64, 33)\n",
      "step 6027, loss is 5.229170322418213\n",
      "(64, 33)\n",
      "step 6028, loss is 4.873195171356201\n",
      "(64, 33)\n",
      "step 6029, loss is 4.913646697998047\n",
      "(64, 33)\n",
      "step 6030, loss is 4.967175483703613\n",
      "(64, 33)\n",
      "step 6031, loss is 4.842648029327393\n",
      "(64, 33)\n",
      "step 6032, loss is 4.879890441894531\n",
      "(64, 33)\n",
      "step 6033, loss is 5.041262149810791\n",
      "(64, 33)\n",
      "step 6034, loss is 4.995392799377441\n",
      "(64, 33)\n",
      "step 6035, loss is 4.836840629577637\n",
      "(64, 33)\n",
      "step 6036, loss is 4.789305686950684\n",
      "(64, 33)\n",
      "step 6037, loss is 4.87328577041626\n",
      "(64, 33)\n",
      "step 6038, loss is 5.0617995262146\n",
      "(64, 33)\n",
      "step 6039, loss is 4.8635735511779785\n",
      "(64, 33)\n",
      "step 6040, loss is 4.989902496337891\n",
      "(64, 33)\n",
      "step 6041, loss is 4.9761223793029785\n",
      "(64, 33)\n",
      "step 6042, loss is 4.871119976043701\n",
      "(64, 33)\n",
      "step 6043, loss is 4.87492036819458\n",
      "(64, 33)\n",
      "step 6044, loss is 5.048928737640381\n",
      "(64, 33)\n",
      "step 6045, loss is 4.870615482330322\n",
      "(64, 33)\n",
      "step 6046, loss is 4.851308345794678\n",
      "(64, 33)\n",
      "step 6047, loss is 5.009252548217773\n",
      "(64, 33)\n",
      "step 6048, loss is 5.033237934112549\n",
      "(64, 33)\n",
      "step 6049, loss is 4.911980152130127\n",
      "(64, 33)\n",
      "step 6050, loss is 4.98832368850708\n",
      "(64, 33)\n",
      "step 6051, loss is 4.925086498260498\n",
      "(64, 33)\n",
      "step 6052, loss is 4.979115962982178\n",
      "(64, 33)\n",
      "step 6053, loss is 4.873865127563477\n",
      "(64, 33)\n",
      "step 6054, loss is 5.170080661773682\n",
      "(64, 33)\n",
      "step 6055, loss is 4.928114414215088\n",
      "(64, 33)\n",
      "step 6056, loss is 4.952206611633301\n",
      "(64, 33)\n",
      "step 6057, loss is 4.777994632720947\n",
      "(64, 33)\n",
      "step 6058, loss is 5.103175640106201\n",
      "(64, 33)\n",
      "step 6059, loss is 5.00252103805542\n",
      "(64, 33)\n",
      "step 6060, loss is 4.968112945556641\n",
      "(64, 33)\n",
      "step 6061, loss is 4.852970600128174\n",
      "(64, 33)\n",
      "step 6062, loss is 4.987239360809326\n",
      "(64, 33)\n",
      "step 6063, loss is 5.1258087158203125\n",
      "(64, 33)\n",
      "step 6064, loss is 5.006852626800537\n",
      "(64, 33)\n",
      "step 6065, loss is 4.984902858734131\n",
      "(64, 33)\n",
      "step 6066, loss is 4.933950901031494\n",
      "(64, 33)\n",
      "step 6067, loss is 5.005037307739258\n",
      "(64, 33)\n",
      "step 6068, loss is 4.985994338989258\n",
      "(64, 33)\n",
      "step 6069, loss is 4.937192916870117\n",
      "(64, 33)\n",
      "step 6070, loss is 4.794003963470459\n",
      "(64, 33)\n",
      "step 6071, loss is 4.802543640136719\n",
      "(64, 33)\n",
      "step 6072, loss is 4.8754425048828125\n",
      "(64, 33)\n",
      "step 6073, loss is 4.975331783294678\n",
      "(64, 33)\n",
      "step 6074, loss is 5.142654895782471\n",
      "(64, 33)\n",
      "step 6075, loss is 4.997858047485352\n",
      "(64, 33)\n",
      "step 6076, loss is 5.154599666595459\n",
      "(64, 33)\n",
      "step 6077, loss is 4.933687686920166\n",
      "(64, 33)\n",
      "step 6078, loss is 5.05185079574585\n",
      "(64, 33)\n",
      "step 6079, loss is 4.870543956756592\n",
      "(64, 33)\n",
      "step 6080, loss is 4.971837520599365\n",
      "(64, 33)\n",
      "step 6081, loss is 4.863821506500244\n",
      "(64, 33)\n",
      "step 6082, loss is 4.779753684997559\n",
      "(64, 33)\n",
      "step 6083, loss is 4.914705753326416\n",
      "(64, 33)\n",
      "step 6084, loss is 4.931795120239258\n",
      "(64, 33)\n",
      "step 6085, loss is 4.8423848152160645\n",
      "(64, 33)\n",
      "step 6086, loss is 4.89211368560791\n",
      "(64, 33)\n",
      "step 6087, loss is 4.904393672943115\n",
      "(64, 33)\n",
      "step 6088, loss is 4.882093906402588\n",
      "(64, 33)\n",
      "step 6089, loss is 5.041419982910156\n",
      "(64, 33)\n",
      "step 6090, loss is 4.958094596862793\n",
      "(64, 33)\n",
      "step 6091, loss is 5.018144130706787\n",
      "(64, 33)\n",
      "step 6092, loss is 4.94558048248291\n",
      "(64, 33)\n",
      "step 6093, loss is 4.901913166046143\n",
      "(64, 33)\n",
      "step 6094, loss is 4.917933464050293\n",
      "(64, 33)\n",
      "step 6095, loss is 5.097177028656006\n",
      "(64, 33)\n",
      "step 6096, loss is 5.084967613220215\n",
      "(64, 33)\n",
      "step 6097, loss is 4.842550754547119\n",
      "(64, 33)\n",
      "step 6098, loss is 5.078247547149658\n",
      "(64, 33)\n",
      "step 6099, loss is 4.905529499053955\n",
      "(64, 33)\n",
      "step 6100, loss is 4.8644585609436035\n",
      "(64, 33)\n",
      "step 6101, loss is 4.852474689483643\n",
      "(64, 33)\n",
      "step 6102, loss is 4.788535118103027\n",
      "(64, 33)\n",
      "step 6103, loss is 4.982578754425049\n",
      "(64, 33)\n",
      "step 6104, loss is 5.062221050262451\n",
      "(64, 33)\n",
      "step 6105, loss is 4.700857639312744\n",
      "(64, 33)\n",
      "step 6106, loss is 4.835655689239502\n",
      "(64, 33)\n",
      "step 6107, loss is 5.049516201019287\n",
      "(64, 33)\n",
      "step 6108, loss is 4.948524475097656\n",
      "(64, 33)\n",
      "step 6109, loss is 5.079774856567383\n",
      "(64, 33)\n",
      "step 6110, loss is 5.004253387451172\n",
      "(64, 33)\n",
      "step 6111, loss is 4.835760116577148\n",
      "(64, 33)\n",
      "step 6112, loss is 5.186847686767578\n",
      "(64, 33)\n",
      "step 6113, loss is 5.033420562744141\n",
      "(64, 33)\n",
      "step 6114, loss is 4.891927242279053\n",
      "(64, 33)\n",
      "step 6115, loss is 4.798691749572754\n",
      "(64, 33)\n",
      "step 6116, loss is 4.940027713775635\n",
      "(64, 33)\n",
      "step 6117, loss is 4.786789417266846\n",
      "(64, 33)\n",
      "step 6118, loss is 5.02832555770874\n",
      "(64, 33)\n",
      "step 6119, loss is 4.7870025634765625\n",
      "(64, 33)\n",
      "step 6120, loss is 4.925487041473389\n",
      "(64, 33)\n",
      "step 6121, loss is 4.981409549713135\n",
      "(64, 33)\n",
      "step 6122, loss is 4.980286121368408\n",
      "(64, 33)\n",
      "step 6123, loss is 4.888320446014404\n",
      "(64, 33)\n",
      "step 6124, loss is 4.846421241760254\n",
      "(64, 33)\n",
      "step 6125, loss is 4.915585517883301\n",
      "(64, 33)\n",
      "step 6126, loss is 4.9646992683410645\n",
      "(64, 33)\n",
      "step 6127, loss is 4.939919948577881\n",
      "(64, 33)\n",
      "step 6128, loss is 4.903514385223389\n",
      "(64, 33)\n",
      "step 6129, loss is 5.077944755554199\n",
      "(64, 33)\n",
      "step 6130, loss is 4.830453872680664\n",
      "(64, 33)\n",
      "step 6131, loss is 4.9841532707214355\n",
      "(64, 33)\n",
      "step 6132, loss is 4.921188831329346\n",
      "(64, 33)\n",
      "step 6133, loss is 5.016183376312256\n",
      "(64, 33)\n",
      "step 6134, loss is 4.960872173309326\n",
      "(64, 33)\n",
      "step 6135, loss is 4.956110954284668\n",
      "(64, 33)\n",
      "step 6136, loss is 5.105916500091553\n",
      "(64, 33)\n",
      "step 6137, loss is 4.904048919677734\n",
      "(64, 33)\n",
      "step 6138, loss is 4.913841247558594\n",
      "(64, 33)\n",
      "step 6139, loss is 4.812315940856934\n",
      "(64, 33)\n",
      "step 6140, loss is 4.989109516143799\n",
      "(64, 33)\n",
      "step 6141, loss is 4.7975029945373535\n",
      "(64, 33)\n",
      "step 6142, loss is 4.756736755371094\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6143, loss is 4.848186492919922\n",
      "(64, 33)\n",
      "step 6144, loss is 4.973907470703125\n",
      "(64, 33)\n",
      "step 6145, loss is 4.880210876464844\n",
      "(64, 33)\n",
      "step 6146, loss is 4.9980950355529785\n",
      "(64, 33)\n",
      "step 6147, loss is 5.038440227508545\n",
      "(64, 33)\n",
      "step 6148, loss is 4.9177656173706055\n",
      "(64, 33)\n",
      "step 6149, loss is 4.839175701141357\n",
      "(64, 33)\n",
      "step 6150, loss is 5.010774612426758\n",
      "(64, 33)\n",
      "step 6151, loss is 4.903582572937012\n",
      "(64, 33)\n",
      "step 6152, loss is 4.925847053527832\n",
      "(64, 33)\n",
      "step 6153, loss is 4.824195861816406\n",
      "(64, 33)\n",
      "step 6154, loss is 4.926869869232178\n",
      "(64, 33)\n",
      "step 6155, loss is 4.895197868347168\n",
      "(64, 33)\n",
      "step 6156, loss is 5.009915351867676\n",
      "(64, 33)\n",
      "step 6157, loss is 4.962515830993652\n",
      "(64, 33)\n",
      "step 6158, loss is 5.038153648376465\n",
      "(64, 33)\n",
      "step 6159, loss is 5.004447937011719\n",
      "(64, 33)\n",
      "step 6160, loss is 5.024528503417969\n",
      "(64, 33)\n",
      "step 6161, loss is 5.022280216217041\n",
      "(64, 33)\n",
      "step 6162, loss is 5.008064270019531\n",
      "(64, 33)\n",
      "step 6163, loss is 4.799285888671875\n",
      "(64, 33)\n",
      "step 6164, loss is 4.9844970703125\n",
      "(64, 33)\n",
      "step 6165, loss is 4.870546817779541\n",
      "(64, 33)\n",
      "step 6166, loss is 5.110907554626465\n",
      "(64, 33)\n",
      "step 6167, loss is 4.903405666351318\n",
      "(64, 33)\n",
      "step 6168, loss is 4.944912910461426\n",
      "(64, 33)\n",
      "step 6169, loss is 4.860692024230957\n",
      "(64, 33)\n",
      "step 6170, loss is 4.848477840423584\n",
      "(64, 33)\n",
      "step 6171, loss is 5.039989948272705\n",
      "(64, 33)\n",
      "step 6172, loss is 4.891071319580078\n",
      "(64, 33)\n",
      "step 6173, loss is 4.860899925231934\n",
      "(64, 33)\n",
      "step 6174, loss is 4.848523139953613\n",
      "(64, 33)\n",
      "step 6175, loss is 5.016192436218262\n",
      "(64, 33)\n",
      "step 6176, loss is 4.857154369354248\n",
      "(64, 33)\n",
      "step 6177, loss is 4.997801780700684\n",
      "(64, 33)\n",
      "step 6178, loss is 4.990276336669922\n",
      "(64, 33)\n",
      "step 6179, loss is 4.923655986785889\n",
      "(64, 33)\n",
      "step 6180, loss is 4.898666858673096\n",
      "(64, 33)\n",
      "step 6181, loss is 4.875710487365723\n",
      "(64, 33)\n",
      "step 6182, loss is 5.081684112548828\n",
      "(64, 33)\n",
      "step 6183, loss is 5.091441631317139\n",
      "(64, 33)\n",
      "step 6184, loss is 5.0336222648620605\n",
      "(64, 33)\n",
      "step 6185, loss is 5.063886642456055\n",
      "(64, 33)\n",
      "step 6186, loss is 4.892875671386719\n",
      "(64, 33)\n",
      "step 6187, loss is 4.9474263191223145\n",
      "(64, 33)\n",
      "step 6188, loss is 4.884269714355469\n",
      "(64, 33)\n",
      "step 6189, loss is 5.061161994934082\n",
      "(64, 33)\n",
      "step 6190, loss is 4.984074115753174\n",
      "(64, 33)\n",
      "step 6191, loss is 4.8127031326293945\n",
      "(64, 33)\n",
      "step 6192, loss is 4.835364818572998\n",
      "(64, 33)\n",
      "step 6193, loss is 4.838685989379883\n",
      "(64, 33)\n",
      "step 6194, loss is 4.911708831787109\n",
      "(64, 33)\n",
      "step 6195, loss is 4.940552234649658\n",
      "(64, 33)\n",
      "step 6196, loss is 4.908467769622803\n",
      "(64, 33)\n",
      "step 6197, loss is 5.060302257537842\n",
      "(64, 33)\n",
      "step 6198, loss is 4.788098335266113\n",
      "(64, 33)\n",
      "step 6199, loss is 4.909873008728027\n",
      "(64, 33)\n",
      "step 6200, loss is 4.988950729370117\n",
      "(64, 33)\n",
      "step 6201, loss is 5.063845634460449\n",
      "(64, 33)\n",
      "step 6202, loss is 4.861891269683838\n",
      "(64, 33)\n",
      "step 6203, loss is 4.797801494598389\n",
      "(64, 33)\n",
      "step 6204, loss is 4.926048278808594\n",
      "(64, 33)\n",
      "step 6205, loss is 5.037364482879639\n",
      "(64, 33)\n",
      "step 6206, loss is 5.036209583282471\n",
      "(64, 33)\n",
      "step 6207, loss is 4.86295223236084\n",
      "(64, 33)\n",
      "step 6208, loss is 4.906364917755127\n",
      "(64, 33)\n",
      "step 6209, loss is 4.886934280395508\n",
      "(64, 33)\n",
      "step 6210, loss is 4.944948196411133\n",
      "(64, 33)\n",
      "step 6211, loss is 4.902190208435059\n",
      "(64, 33)\n",
      "step 6212, loss is 4.96956205368042\n",
      "(64, 33)\n",
      "step 6213, loss is 4.747277736663818\n",
      "(64, 33)\n",
      "step 6214, loss is 4.7749128341674805\n",
      "(64, 33)\n",
      "step 6215, loss is 4.766319274902344\n",
      "(64, 33)\n",
      "step 6216, loss is 4.979554653167725\n",
      "(64, 33)\n",
      "step 6217, loss is 4.9795026779174805\n",
      "(64, 33)\n",
      "step 6218, loss is 5.062399387359619\n",
      "(64, 33)\n",
      "step 6219, loss is 4.846153259277344\n",
      "(64, 33)\n",
      "step 6220, loss is 4.995588302612305\n",
      "(64, 33)\n",
      "step 6221, loss is 5.088067054748535\n",
      "(64, 33)\n",
      "step 6222, loss is 4.909687519073486\n",
      "(64, 33)\n",
      "step 6223, loss is 4.7706475257873535\n",
      "(64, 33)\n",
      "step 6224, loss is 5.0169572830200195\n",
      "(64, 33)\n",
      "step 6225, loss is 4.863315105438232\n",
      "(64, 33)\n",
      "step 6226, loss is 4.954538345336914\n",
      "(64, 33)\n",
      "step 6227, loss is 4.999495029449463\n",
      "(64, 33)\n",
      "step 6228, loss is 4.766403675079346\n",
      "(64, 33)\n",
      "step 6229, loss is 5.022848606109619\n",
      "(64, 33)\n",
      "step 6230, loss is 4.9407572746276855\n",
      "(64, 33)\n",
      "step 6231, loss is 5.034568786621094\n",
      "(64, 33)\n",
      "step 6232, loss is 4.672676086425781\n",
      "(64, 33)\n",
      "step 6233, loss is 4.926537036895752\n",
      "(64, 33)\n",
      "step 6234, loss is 5.126167297363281\n",
      "(64, 33)\n",
      "step 6235, loss is 5.181632995605469\n",
      "(64, 33)\n",
      "step 6236, loss is 4.829700946807861\n",
      "(64, 33)\n",
      "step 6237, loss is 4.759298801422119\n",
      "(64, 33)\n",
      "step 6238, loss is 4.679998397827148\n",
      "(64, 33)\n",
      "step 6239, loss is 4.915366172790527\n",
      "(64, 33)\n",
      "step 6240, loss is 4.862679958343506\n",
      "(64, 33)\n",
      "step 6241, loss is 5.011973857879639\n",
      "(64, 33)\n",
      "step 6242, loss is 4.9984612464904785\n",
      "(64, 33)\n",
      "step 6243, loss is 4.853911399841309\n",
      "(64, 33)\n",
      "step 6244, loss is 4.876840114593506\n",
      "(64, 33)\n",
      "step 6245, loss is 4.7286272048950195\n",
      "(64, 33)\n",
      "step 6246, loss is 4.966296672821045\n",
      "(64, 33)\n",
      "step 6247, loss is 5.054842948913574\n",
      "(64, 33)\n",
      "step 6248, loss is 4.998728275299072\n",
      "(64, 33)\n",
      "step 6249, loss is 5.098299026489258\n",
      "(64, 33)\n",
      "step 6250, loss is 4.74591064453125\n",
      "(64, 33)\n",
      "step 6251, loss is 4.831625461578369\n",
      "(64, 33)\n",
      "step 6252, loss is 4.954244613647461\n",
      "(64, 33)\n",
      "step 6253, loss is 4.8488311767578125\n",
      "(64, 33)\n",
      "step 6254, loss is 4.943755626678467\n",
      "(64, 33)\n",
      "step 6255, loss is 4.95559024810791\n",
      "(64, 33)\n",
      "step 6256, loss is 4.9208855628967285\n",
      "(64, 33)\n",
      "step 6257, loss is 5.064183235168457\n",
      "(64, 33)\n",
      "step 6258, loss is 4.865431785583496\n",
      "(64, 33)\n",
      "step 6259, loss is 5.067066192626953\n",
      "(64, 33)\n",
      "step 6260, loss is 4.881916522979736\n",
      "(64, 33)\n",
      "step 6261, loss is 5.01706600189209\n",
      "(64, 33)\n",
      "step 6262, loss is 4.9148268699646\n",
      "(64, 33)\n",
      "step 6263, loss is 5.162230014801025\n",
      "(64, 33)\n",
      "step 6264, loss is 4.847343444824219\n",
      "(64, 33)\n",
      "step 6265, loss is 5.169136047363281\n",
      "(64, 33)\n",
      "step 6266, loss is 4.877111434936523\n",
      "(64, 33)\n",
      "step 6267, loss is 4.8569440841674805\n",
      "(64, 33)\n",
      "step 6268, loss is 4.9776692390441895\n",
      "(64, 33)\n",
      "step 6269, loss is 4.739402770996094\n",
      "(64, 33)\n",
      "step 6270, loss is 5.177768230438232\n",
      "(64, 33)\n",
      "step 6271, loss is 5.018093109130859\n",
      "(64, 33)\n",
      "step 6272, loss is 4.866698265075684\n",
      "(64, 33)\n",
      "step 6273, loss is 4.94639253616333\n",
      "(64, 33)\n",
      "step 6274, loss is 4.948724269866943\n",
      "(64, 33)\n",
      "step 6275, loss is 4.884453296661377\n",
      "(64, 33)\n",
      "step 6276, loss is 4.938500881195068\n",
      "(64, 33)\n",
      "step 6277, loss is 4.875480651855469\n",
      "(64, 33)\n",
      "step 6278, loss is 4.80974006652832\n",
      "(64, 33)\n",
      "step 6279, loss is 4.935459136962891\n",
      "(64, 33)\n",
      "step 6280, loss is 5.0346293449401855\n",
      "(64, 33)\n",
      "step 6281, loss is 4.885931015014648\n",
      "(64, 33)\n",
      "step 6282, loss is 5.125991344451904\n",
      "(64, 33)\n",
      "step 6283, loss is 4.8465752601623535\n",
      "(64, 33)\n",
      "step 6284, loss is 5.024032115936279\n",
      "(64, 33)\n",
      "step 6285, loss is 4.872564315795898\n",
      "(64, 33)\n",
      "step 6286, loss is 4.6142048835754395\n",
      "(64, 33)\n",
      "step 6287, loss is 4.9135355949401855\n",
      "(64, 33)\n",
      "step 6288, loss is 4.906646728515625\n",
      "(64, 33)\n",
      "step 6289, loss is 4.876767635345459\n",
      "(64, 33)\n",
      "step 6290, loss is 4.896849632263184\n",
      "(64, 33)\n",
      "step 6291, loss is 5.0268096923828125\n",
      "(64, 33)\n",
      "step 6292, loss is 5.014062881469727\n",
      "(64, 33)\n",
      "step 6293, loss is 4.8243408203125\n",
      "(64, 33)\n",
      "step 6294, loss is 4.954990386962891\n",
      "(64, 33)\n",
      "step 6295, loss is 4.894096374511719\n",
      "(64, 33)\n",
      "step 6296, loss is 5.084975719451904\n",
      "(64, 33)\n",
      "step 6297, loss is 4.939752578735352\n",
      "(64, 33)\n",
      "step 6298, loss is 5.028012275695801\n",
      "(64, 33)\n",
      "step 6299, loss is 4.803889751434326\n",
      "(64, 33)\n",
      "step 6300, loss is 4.845786094665527\n",
      "(64, 33)\n",
      "step 6301, loss is 4.826838970184326\n",
      "(64, 33)\n",
      "step 6302, loss is 5.011733055114746\n",
      "(64, 33)\n",
      "step 6303, loss is 4.898626804351807\n",
      "(64, 33)\n",
      "step 6304, loss is 4.99997615814209\n",
      "(64, 33)\n",
      "step 6305, loss is 4.802375316619873\n",
      "(64, 33)\n",
      "step 6306, loss is 4.9000372886657715\n",
      "(64, 33)\n",
      "step 6307, loss is 5.0095696449279785\n",
      "(64, 33)\n",
      "step 6308, loss is 4.857138156890869\n",
      "(64, 33)\n",
      "step 6309, loss is 4.920662879943848\n",
      "(64, 33)\n",
      "step 6310, loss is 4.868156433105469\n",
      "(64, 33)\n",
      "step 6311, loss is 5.030089378356934\n",
      "(64, 33)\n",
      "step 6312, loss is 4.855764865875244\n",
      "(64, 33)\n",
      "step 6313, loss is 4.897470951080322\n",
      "(64, 33)\n",
      "step 6314, loss is 5.087113380432129\n",
      "(64, 33)\n",
      "step 6315, loss is 5.0089898109436035\n",
      "(64, 33)\n",
      "step 6316, loss is 4.842836380004883\n",
      "(64, 33)\n",
      "step 6317, loss is 4.958333492279053\n",
      "(64, 33)\n",
      "step 6318, loss is 5.025700092315674\n",
      "(64, 33)\n",
      "step 6319, loss is 4.838526248931885\n",
      "(64, 33)\n",
      "step 6320, loss is 4.870934009552002\n",
      "(64, 33)\n",
      "step 6321, loss is 5.139317512512207\n",
      "(64, 33)\n",
      "step 6322, loss is 5.041014671325684\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6323, loss is 4.989755153656006\n",
      "(64, 33)\n",
      "step 6324, loss is 5.078369140625\n",
      "(64, 33)\n",
      "step 6325, loss is 5.085291862487793\n",
      "(64, 33)\n",
      "step 6326, loss is 4.919027805328369\n",
      "(64, 33)\n",
      "step 6327, loss is 4.717501640319824\n",
      "(64, 33)\n",
      "step 6328, loss is 4.981205940246582\n",
      "(64, 33)\n",
      "step 6329, loss is 4.810653209686279\n",
      "(64, 33)\n",
      "step 6330, loss is 5.065892696380615\n",
      "(64, 33)\n",
      "step 6331, loss is 5.054008483886719\n",
      "(64, 33)\n",
      "step 6332, loss is 5.094444274902344\n",
      "(64, 33)\n",
      "step 6333, loss is 5.003706455230713\n",
      "(64, 33)\n",
      "step 6334, loss is 4.870208263397217\n",
      "(64, 33)\n",
      "step 6335, loss is 4.879920482635498\n",
      "(64, 33)\n",
      "step 6336, loss is 4.994663715362549\n",
      "(64, 33)\n",
      "step 6337, loss is 4.937990665435791\n",
      "(64, 33)\n",
      "step 6338, loss is 4.9279937744140625\n",
      "(64, 33)\n",
      "step 6339, loss is 4.9702839851379395\n",
      "(64, 33)\n",
      "step 6340, loss is 4.7810587882995605\n",
      "(64, 33)\n",
      "step 6341, loss is 4.9402666091918945\n",
      "(64, 33)\n",
      "step 6342, loss is 4.955005168914795\n",
      "(64, 33)\n",
      "step 6343, loss is 4.768146991729736\n",
      "(64, 33)\n",
      "step 6344, loss is 4.945706367492676\n",
      "(64, 33)\n",
      "step 6345, loss is 4.789577484130859\n",
      "(64, 33)\n",
      "step 6346, loss is 4.858087539672852\n",
      "(64, 33)\n",
      "step 6347, loss is 4.797703266143799\n",
      "(64, 33)\n",
      "step 6348, loss is 4.901216506958008\n",
      "(64, 33)\n",
      "step 6349, loss is 5.084192276000977\n",
      "(64, 33)\n",
      "step 6350, loss is 4.932638168334961\n",
      "(64, 33)\n",
      "step 6351, loss is 4.950393199920654\n",
      "(64, 33)\n",
      "step 6352, loss is 4.8338823318481445\n",
      "(64, 33)\n",
      "step 6353, loss is 4.75972843170166\n",
      "(64, 33)\n",
      "step 6354, loss is 4.879371643066406\n",
      "(64, 33)\n",
      "step 6355, loss is 4.932474136352539\n",
      "(64, 33)\n",
      "step 6356, loss is 5.014552116394043\n",
      "(64, 33)\n",
      "step 6357, loss is 4.875576496124268\n",
      "(64, 33)\n",
      "step 6358, loss is 4.871066093444824\n",
      "(64, 33)\n",
      "step 6359, loss is 5.0298542976379395\n",
      "(64, 33)\n",
      "step 6360, loss is 4.849146842956543\n",
      "(64, 33)\n",
      "step 6361, loss is 4.956730842590332\n",
      "(64, 33)\n",
      "step 6362, loss is 4.843066215515137\n",
      "(64, 33)\n",
      "step 6363, loss is 4.895834445953369\n",
      "(64, 33)\n",
      "step 6364, loss is 5.071000576019287\n",
      "(64, 33)\n",
      "step 6365, loss is 4.7578253746032715\n",
      "(64, 33)\n",
      "step 6366, loss is 4.8293681144714355\n",
      "(64, 33)\n",
      "step 6367, loss is 4.906833648681641\n",
      "(64, 33)\n",
      "step 6368, loss is 4.96468448638916\n",
      "(64, 33)\n",
      "step 6369, loss is 4.993529319763184\n",
      "(64, 33)\n",
      "step 6370, loss is 4.952747821807861\n",
      "(64, 33)\n",
      "step 6371, loss is 4.9997148513793945\n",
      "(64, 33)\n",
      "step 6372, loss is 4.744451999664307\n",
      "(64, 33)\n",
      "step 6373, loss is 4.9633355140686035\n",
      "(64, 33)\n",
      "step 6374, loss is 5.029186725616455\n",
      "(64, 33)\n",
      "step 6375, loss is 5.068085670471191\n",
      "(64, 33)\n",
      "step 6376, loss is 4.939905166625977\n",
      "(64, 33)\n",
      "step 6377, loss is 4.891088485717773\n",
      "(64, 33)\n",
      "step 6378, loss is 4.668116569519043\n",
      "(64, 33)\n",
      "step 6379, loss is 4.832422256469727\n",
      "(64, 33)\n",
      "step 6380, loss is 4.831999778747559\n",
      "(64, 33)\n",
      "step 6381, loss is 4.979146480560303\n",
      "(64, 33)\n",
      "step 6382, loss is 5.052327632904053\n",
      "(64, 33)\n",
      "step 6383, loss is 4.845592975616455\n",
      "(64, 33)\n",
      "step 6384, loss is 4.9766845703125\n",
      "(64, 33)\n",
      "step 6385, loss is 4.840376853942871\n",
      "(64, 33)\n",
      "step 6386, loss is 4.854844570159912\n",
      "(64, 33)\n",
      "step 6387, loss is 4.990187644958496\n",
      "(64, 33)\n",
      "step 6388, loss is 5.1368794441223145\n",
      "(64, 33)\n",
      "step 6389, loss is 4.794522762298584\n",
      "(64, 33)\n",
      "step 6390, loss is 4.888862609863281\n",
      "(64, 33)\n",
      "step 6391, loss is 4.9385247230529785\n",
      "(64, 33)\n",
      "step 6392, loss is 5.099404811859131\n",
      "(64, 33)\n",
      "step 6393, loss is 4.868004322052002\n",
      "(64, 33)\n",
      "step 6394, loss is 4.821757793426514\n",
      "(64, 33)\n",
      "step 6395, loss is 4.939921855926514\n",
      "(64, 33)\n",
      "step 6396, loss is 5.136314868927002\n",
      "(64, 33)\n",
      "step 6397, loss is 4.896738529205322\n",
      "(64, 33)\n",
      "step 6398, loss is 5.000211238861084\n",
      "(64, 33)\n",
      "step 6399, loss is 5.047030448913574\n",
      "(64, 33)\n",
      "step 6400, loss is 5.036898612976074\n",
      "(64, 33)\n",
      "step 6401, loss is 5.0211615562438965\n",
      "(64, 33)\n",
      "step 6402, loss is 4.8785176277160645\n",
      "(64, 33)\n",
      "step 6403, loss is 4.732364654541016\n",
      "(64, 33)\n",
      "step 6404, loss is 4.834843635559082\n",
      "(64, 33)\n",
      "step 6405, loss is 4.840722560882568\n",
      "(64, 33)\n",
      "step 6406, loss is 4.986642360687256\n",
      "(64, 33)\n",
      "step 6407, loss is 4.792808532714844\n",
      "(64, 33)\n",
      "step 6408, loss is 4.8546295166015625\n",
      "(64, 33)\n",
      "step 6409, loss is 4.947179317474365\n",
      "(64, 33)\n",
      "step 6410, loss is 5.169699192047119\n",
      "(64, 33)\n",
      "step 6411, loss is 5.025981426239014\n",
      "(64, 33)\n",
      "step 6412, loss is 5.017144203186035\n",
      "(64, 33)\n",
      "step 6413, loss is 4.921426296234131\n",
      "(64, 33)\n",
      "step 6414, loss is 4.923635959625244\n",
      "(64, 33)\n",
      "step 6415, loss is 4.948326110839844\n",
      "(64, 33)\n",
      "step 6416, loss is 5.131872177124023\n",
      "(64, 33)\n",
      "step 6417, loss is 4.979087829589844\n",
      "(64, 33)\n",
      "step 6418, loss is 4.959847927093506\n",
      "(64, 33)\n",
      "step 6419, loss is 5.114701747894287\n",
      "(64, 33)\n",
      "step 6420, loss is 4.942561149597168\n",
      "(64, 33)\n",
      "step 6421, loss is 4.9472880363464355\n",
      "(64, 33)\n",
      "step 6422, loss is 4.997879981994629\n",
      "(64, 33)\n",
      "step 6423, loss is 5.0647406578063965\n",
      "(64, 33)\n",
      "step 6424, loss is 5.055312156677246\n",
      "(64, 33)\n",
      "step 6425, loss is 4.878878593444824\n",
      "(64, 33)\n",
      "step 6426, loss is 4.93510627746582\n",
      "(64, 33)\n",
      "step 6427, loss is 4.89246940612793\n",
      "(64, 33)\n",
      "step 6428, loss is 4.968555927276611\n",
      "(64, 33)\n",
      "step 6429, loss is 5.0576348304748535\n",
      "(64, 33)\n",
      "step 6430, loss is 4.912748336791992\n",
      "(64, 33)\n",
      "step 6431, loss is 5.039266109466553\n",
      "(64, 33)\n",
      "step 6432, loss is 4.946389675140381\n",
      "(64, 33)\n",
      "step 6433, loss is 4.964474201202393\n",
      "(64, 33)\n",
      "step 6434, loss is 5.080291271209717\n",
      "(64, 33)\n",
      "step 6435, loss is 4.993503093719482\n",
      "(64, 33)\n",
      "step 6436, loss is 5.060499668121338\n",
      "(64, 33)\n",
      "step 6437, loss is 4.962676525115967\n",
      "(64, 33)\n",
      "step 6438, loss is 5.022811412811279\n",
      "(64, 33)\n",
      "step 6439, loss is 4.965993404388428\n",
      "(64, 33)\n",
      "step 6440, loss is 4.935134410858154\n",
      "(64, 33)\n",
      "step 6441, loss is 4.76546049118042\n",
      "(64, 33)\n",
      "step 6442, loss is 5.063185214996338\n",
      "(64, 33)\n",
      "step 6443, loss is 5.0482635498046875\n",
      "(64, 33)\n",
      "step 6444, loss is 4.905458450317383\n",
      "(64, 33)\n",
      "step 6445, loss is 4.965268135070801\n",
      "(64, 33)\n",
      "step 6446, loss is 4.905428409576416\n",
      "(64, 33)\n",
      "step 6447, loss is 4.999030113220215\n",
      "(64, 33)\n",
      "step 6448, loss is 4.848846912384033\n",
      "(64, 33)\n",
      "step 6449, loss is 5.1874799728393555\n",
      "(64, 33)\n",
      "step 6450, loss is 4.9776201248168945\n",
      "(64, 33)\n",
      "step 6451, loss is 4.903744220733643\n",
      "(64, 33)\n",
      "step 6452, loss is 4.964274883270264\n",
      "(64, 33)\n",
      "step 6453, loss is 5.119396686553955\n",
      "(64, 33)\n",
      "step 6454, loss is 4.7829365730285645\n",
      "(64, 33)\n",
      "step 6455, loss is 4.965435981750488\n",
      "(64, 33)\n",
      "step 6456, loss is 4.884660720825195\n",
      "(64, 33)\n",
      "step 6457, loss is 4.931002140045166\n",
      "(64, 33)\n",
      "step 6458, loss is 5.152115821838379\n",
      "(64, 33)\n",
      "step 6459, loss is 4.9758195877075195\n",
      "(64, 33)\n",
      "step 6460, loss is 4.762230396270752\n",
      "(64, 33)\n",
      "step 6461, loss is 4.900587558746338\n",
      "(64, 33)\n",
      "step 6462, loss is 5.116130352020264\n",
      "(64, 33)\n",
      "step 6463, loss is 4.9076924324035645\n",
      "(64, 33)\n",
      "step 6464, loss is 4.8767170906066895\n",
      "(64, 33)\n",
      "step 6465, loss is 4.82681131362915\n",
      "(64, 33)\n",
      "step 6466, loss is 4.9250569343566895\n",
      "(64, 33)\n",
      "step 6467, loss is 5.029621601104736\n",
      "(64, 33)\n",
      "step 6468, loss is 4.954605579376221\n",
      "(64, 33)\n",
      "step 6469, loss is 4.796809196472168\n",
      "(64, 33)\n",
      "step 6470, loss is 4.920884132385254\n",
      "(64, 33)\n",
      "step 6471, loss is 5.093415260314941\n",
      "(64, 33)\n",
      "step 6472, loss is 4.893699645996094\n",
      "(64, 33)\n",
      "step 6473, loss is 4.9807610511779785\n",
      "(64, 33)\n",
      "step 6474, loss is 4.861496925354004\n",
      "(64, 33)\n",
      "step 6475, loss is 4.922231197357178\n",
      "(64, 33)\n",
      "step 6476, loss is 4.877631664276123\n",
      "(64, 33)\n",
      "step 6477, loss is 4.857765197753906\n",
      "(64, 33)\n",
      "step 6478, loss is 4.897518157958984\n",
      "(64, 33)\n",
      "step 6479, loss is 4.581758499145508\n",
      "(64, 33)\n",
      "step 6480, loss is 4.958078384399414\n",
      "(64, 33)\n",
      "step 6481, loss is 4.740396499633789\n",
      "(64, 33)\n",
      "step 6482, loss is 5.053944110870361\n",
      "(64, 33)\n",
      "step 6483, loss is 4.9163079261779785\n",
      "(64, 33)\n",
      "step 6484, loss is 5.272459030151367\n",
      "(64, 33)\n",
      "step 6485, loss is 4.91035270690918\n",
      "(64, 33)\n",
      "step 6486, loss is 5.040596961975098\n",
      "(64, 33)\n",
      "step 6487, loss is 4.9611287117004395\n",
      "(64, 33)\n",
      "step 6488, loss is 4.89725399017334\n",
      "(64, 33)\n",
      "step 6489, loss is 4.995254993438721\n",
      "(64, 33)\n",
      "step 6490, loss is 4.9711809158325195\n",
      "(64, 33)\n",
      "step 6491, loss is 5.037959575653076\n",
      "(64, 33)\n",
      "step 6492, loss is 4.647589206695557\n",
      "(64, 33)\n",
      "step 6493, loss is 4.938215732574463\n",
      "(64, 33)\n",
      "step 6494, loss is 5.02396821975708\n",
      "(64, 33)\n",
      "step 6495, loss is 4.801687240600586\n",
      "(64, 33)\n",
      "step 6496, loss is 4.933854579925537\n",
      "(64, 33)\n",
      "step 6497, loss is 5.020371437072754\n",
      "(64, 33)\n",
      "step 6498, loss is 4.9052863121032715\n",
      "(64, 33)\n",
      "step 6499, loss is 4.995448589324951\n",
      "(64, 33)\n",
      "step 6500, loss is 4.923528671264648\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6501, loss is 4.934852600097656\n",
      "(64, 33)\n",
      "step 6502, loss is 5.0818562507629395\n",
      "(64, 33)\n",
      "step 6503, loss is 5.034668445587158\n",
      "(64, 33)\n",
      "step 6504, loss is 5.000263214111328\n",
      "(64, 33)\n",
      "step 6505, loss is 5.001733779907227\n",
      "(64, 33)\n",
      "step 6506, loss is 4.920405864715576\n",
      "(64, 33)\n",
      "step 6507, loss is 4.965865135192871\n",
      "(64, 33)\n",
      "step 6508, loss is 5.0543646812438965\n",
      "(64, 33)\n",
      "step 6509, loss is 4.74458646774292\n",
      "(64, 33)\n",
      "step 6510, loss is 4.990145683288574\n",
      "(64, 33)\n",
      "step 6511, loss is 5.04238224029541\n",
      "(64, 33)\n",
      "step 6512, loss is 5.018077373504639\n",
      "(64, 33)\n",
      "step 6513, loss is 5.145330905914307\n",
      "(64, 33)\n",
      "step 6514, loss is 4.758813381195068\n",
      "(64, 33)\n",
      "step 6515, loss is 4.873910427093506\n",
      "(64, 33)\n",
      "step 6516, loss is 4.864738464355469\n",
      "(64, 33)\n",
      "step 6517, loss is 4.873594284057617\n",
      "(64, 33)\n",
      "step 6518, loss is 4.935269832611084\n",
      "(64, 33)\n",
      "step 6519, loss is 5.092817783355713\n",
      "(64, 33)\n",
      "step 6520, loss is 4.779612064361572\n",
      "(64, 33)\n",
      "step 6521, loss is 4.9753828048706055\n",
      "(64, 33)\n",
      "step 6522, loss is 4.890834808349609\n",
      "(64, 33)\n",
      "step 6523, loss is 4.802450180053711\n",
      "(64, 33)\n",
      "step 6524, loss is 4.833835124969482\n",
      "(64, 33)\n",
      "step 6525, loss is 4.941250801086426\n",
      "(64, 33)\n",
      "step 6526, loss is 5.117118835449219\n",
      "(64, 33)\n",
      "step 6527, loss is 4.928622245788574\n",
      "(64, 33)\n",
      "step 6528, loss is 4.762633323669434\n",
      "(64, 33)\n",
      "step 6529, loss is 4.99587345123291\n",
      "(64, 33)\n",
      "step 6530, loss is 4.68360710144043\n",
      "(64, 33)\n",
      "step 6531, loss is 4.9521355628967285\n",
      "(64, 33)\n",
      "step 6532, loss is 4.8765058517456055\n",
      "(64, 33)\n",
      "step 6533, loss is 4.872729778289795\n",
      "(64, 33)\n",
      "step 6534, loss is 4.692480564117432\n",
      "(64, 33)\n",
      "step 6535, loss is 4.851257801055908\n",
      "(64, 33)\n",
      "step 6536, loss is 4.903796672821045\n",
      "(64, 33)\n",
      "step 6537, loss is 4.951627254486084\n",
      "(64, 33)\n",
      "step 6538, loss is 4.7435221672058105\n",
      "(64, 33)\n",
      "step 6539, loss is 4.971831321716309\n",
      "(64, 33)\n",
      "step 6540, loss is 4.908177375793457\n",
      "(64, 33)\n",
      "step 6541, loss is 4.83635950088501\n",
      "(64, 33)\n",
      "step 6542, loss is 4.9515814781188965\n",
      "(64, 33)\n",
      "step 6543, loss is 4.962719917297363\n",
      "(64, 33)\n",
      "step 6544, loss is 4.778660297393799\n",
      "(64, 33)\n",
      "step 6545, loss is 4.902101039886475\n",
      "(64, 33)\n",
      "step 6546, loss is 4.819491386413574\n",
      "(64, 33)\n",
      "step 6547, loss is 5.029451847076416\n",
      "(64, 33)\n",
      "step 6548, loss is 4.938991069793701\n",
      "(64, 33)\n",
      "step 6549, loss is 4.999732971191406\n",
      "(64, 33)\n",
      "step 6550, loss is 4.861944198608398\n",
      "(64, 33)\n",
      "step 6551, loss is 4.7491021156311035\n",
      "(64, 33)\n",
      "step 6552, loss is 5.062460899353027\n",
      "(64, 33)\n",
      "step 6553, loss is 4.980101585388184\n",
      "(64, 33)\n",
      "step 6554, loss is 4.779967308044434\n",
      "(64, 33)\n",
      "step 6555, loss is 4.854602813720703\n",
      "(64, 33)\n",
      "step 6556, loss is 5.009268283843994\n",
      "(64, 33)\n",
      "step 6557, loss is 5.043739318847656\n",
      "(64, 33)\n",
      "step 6558, loss is 4.973032474517822\n",
      "(64, 33)\n",
      "step 6559, loss is 4.906974792480469\n",
      "(64, 33)\n",
      "step 6560, loss is 4.885047912597656\n",
      "(64, 33)\n",
      "step 6561, loss is 4.761207103729248\n",
      "(64, 33)\n",
      "step 6562, loss is 5.008955478668213\n",
      "(64, 33)\n",
      "step 6563, loss is 4.977056503295898\n",
      "(64, 33)\n",
      "step 6564, loss is 4.976775646209717\n",
      "(64, 33)\n",
      "step 6565, loss is 5.088052272796631\n",
      "(64, 33)\n",
      "step 6566, loss is 4.828342437744141\n",
      "(64, 33)\n",
      "step 6567, loss is 4.914393424987793\n",
      "(64, 33)\n",
      "step 6568, loss is 4.927441120147705\n",
      "(64, 33)\n",
      "step 6569, loss is 4.834012985229492\n",
      "(64, 33)\n",
      "step 6570, loss is 4.9181904792785645\n",
      "(64, 33)\n",
      "step 6571, loss is 4.890435695648193\n",
      "(64, 33)\n",
      "step 6572, loss is 4.829293251037598\n",
      "(64, 33)\n",
      "step 6573, loss is 4.8602986335754395\n",
      "(64, 33)\n",
      "step 6574, loss is 5.141904354095459\n",
      "(64, 33)\n",
      "step 6575, loss is 4.906763553619385\n",
      "(64, 33)\n",
      "step 6576, loss is 4.841569423675537\n",
      "(64, 33)\n",
      "step 6577, loss is 4.916836738586426\n",
      "(64, 33)\n",
      "step 6578, loss is 4.992891311645508\n",
      "(64, 33)\n",
      "step 6579, loss is 4.9640583992004395\n",
      "(64, 33)\n",
      "step 6580, loss is 5.018441677093506\n",
      "(64, 33)\n",
      "step 6581, loss is 4.748380184173584\n",
      "(64, 33)\n",
      "step 6582, loss is 4.830279350280762\n",
      "(64, 33)\n",
      "step 6583, loss is 5.111208915710449\n",
      "(64, 33)\n",
      "step 6584, loss is 4.9560933113098145\n",
      "(64, 33)\n",
      "step 6585, loss is 4.9950175285339355\n",
      "(64, 33)\n",
      "step 6586, loss is 5.09397554397583\n",
      "(64, 33)\n",
      "step 6587, loss is 4.818239212036133\n",
      "(64, 33)\n",
      "step 6588, loss is 4.83107328414917\n",
      "(64, 33)\n",
      "step 6589, loss is 5.114588737487793\n",
      "(64, 33)\n",
      "step 6590, loss is 5.126414775848389\n",
      "(64, 33)\n",
      "step 6591, loss is 4.865201473236084\n",
      "(64, 33)\n",
      "step 6592, loss is 4.8519673347473145\n",
      "(64, 33)\n",
      "step 6593, loss is 4.932240962982178\n",
      "(64, 33)\n",
      "step 6594, loss is 4.801193714141846\n",
      "(64, 33)\n",
      "step 6595, loss is 5.187893867492676\n",
      "(64, 33)\n",
      "step 6596, loss is 4.9670209884643555\n",
      "(64, 33)\n",
      "step 6597, loss is 5.078954696655273\n",
      "(64, 33)\n",
      "step 6598, loss is 4.6595025062561035\n",
      "(64, 33)\n",
      "step 6599, loss is 4.898228645324707\n",
      "(64, 33)\n",
      "step 6600, loss is 4.844442844390869\n",
      "(64, 33)\n",
      "step 6601, loss is 4.993878364562988\n",
      "(64, 33)\n",
      "step 6602, loss is 4.844797134399414\n",
      "(64, 33)\n",
      "step 6603, loss is 4.816812515258789\n",
      "(64, 33)\n",
      "step 6604, loss is 4.784979820251465\n",
      "(64, 33)\n",
      "step 6605, loss is 5.042070388793945\n",
      "(64, 33)\n",
      "step 6606, loss is 5.1245293617248535\n",
      "(64, 33)\n",
      "step 6607, loss is 4.828307628631592\n",
      "(64, 33)\n",
      "step 6608, loss is 4.8148393630981445\n",
      "(64, 33)\n",
      "step 6609, loss is 4.907513618469238\n",
      "(64, 33)\n",
      "step 6610, loss is 4.894084930419922\n",
      "(64, 33)\n",
      "step 6611, loss is 5.03638219833374\n",
      "(64, 33)\n",
      "step 6612, loss is 4.86878776550293\n",
      "(64, 33)\n",
      "step 6613, loss is 4.918771266937256\n",
      "(64, 33)\n",
      "step 6614, loss is 4.960818290710449\n",
      "(64, 33)\n",
      "step 6615, loss is 4.868917465209961\n",
      "(64, 33)\n",
      "step 6616, loss is 5.129456996917725\n",
      "(64, 33)\n",
      "step 6617, loss is 4.795250415802002\n",
      "(64, 33)\n",
      "step 6618, loss is 4.998954772949219\n",
      "(64, 33)\n",
      "step 6619, loss is 4.76625394821167\n",
      "(64, 33)\n",
      "step 6620, loss is 4.894320487976074\n",
      "(64, 33)\n",
      "step 6621, loss is 4.841092109680176\n",
      "(64, 33)\n",
      "step 6622, loss is 4.893505573272705\n",
      "(64, 33)\n",
      "step 6623, loss is 4.88836145401001\n",
      "(64, 33)\n",
      "step 6624, loss is 4.871238708496094\n",
      "(64, 33)\n",
      "step 6625, loss is 5.010621070861816\n",
      "(64, 33)\n",
      "step 6626, loss is 4.715624809265137\n",
      "(64, 33)\n",
      "step 6627, loss is 5.011840343475342\n",
      "(64, 33)\n",
      "step 6628, loss is 4.743274688720703\n",
      "(64, 33)\n",
      "step 6629, loss is 4.698890686035156\n",
      "(64, 33)\n",
      "step 6630, loss is 4.956357955932617\n",
      "(64, 33)\n",
      "step 6631, loss is 4.886600494384766\n",
      "(64, 33)\n",
      "step 6632, loss is 4.926843643188477\n",
      "(64, 33)\n",
      "step 6633, loss is 4.817648410797119\n",
      "(64, 33)\n",
      "step 6634, loss is 4.998751163482666\n",
      "(64, 33)\n",
      "step 6635, loss is 4.969834327697754\n",
      "(64, 33)\n",
      "step 6636, loss is 5.024842262268066\n",
      "(64, 33)\n",
      "step 6637, loss is 4.8863396644592285\n",
      "(64, 33)\n",
      "step 6638, loss is 4.702467918395996\n",
      "(64, 33)\n",
      "step 6639, loss is 4.684162616729736\n",
      "(64, 33)\n",
      "step 6640, loss is 5.085302352905273\n",
      "(64, 33)\n",
      "step 6641, loss is 5.000957012176514\n",
      "(64, 33)\n",
      "step 6642, loss is 5.044210433959961\n",
      "(64, 33)\n",
      "step 6643, loss is 4.872618198394775\n",
      "(64, 33)\n",
      "step 6644, loss is 4.785484313964844\n",
      "(64, 33)\n",
      "step 6645, loss is 4.793372631072998\n",
      "(64, 33)\n",
      "step 6646, loss is 4.874923229217529\n",
      "(64, 33)\n",
      "step 6647, loss is 4.906069278717041\n",
      "(64, 33)\n",
      "step 6648, loss is 4.869515895843506\n",
      "(64, 33)\n",
      "step 6649, loss is 5.019711971282959\n",
      "(64, 33)\n",
      "step 6650, loss is 5.028674602508545\n",
      "(64, 33)\n",
      "step 6651, loss is 4.698606491088867\n",
      "(64, 33)\n",
      "step 6652, loss is 4.974308967590332\n",
      "(64, 33)\n",
      "step 6653, loss is 4.98729133605957\n",
      "(64, 33)\n",
      "step 6654, loss is 4.720710277557373\n",
      "(64, 33)\n",
      "step 6655, loss is 5.102156639099121\n",
      "(64, 33)\n",
      "step 6656, loss is 4.869792461395264\n",
      "(64, 33)\n",
      "step 6657, loss is 4.817695140838623\n",
      "(64, 33)\n",
      "step 6658, loss is 5.003507614135742\n",
      "(64, 33)\n",
      "step 6659, loss is 5.168409824371338\n",
      "(64, 33)\n",
      "step 6660, loss is 4.882663726806641\n",
      "(64, 33)\n",
      "step 6661, loss is 5.028892517089844\n",
      "(64, 33)\n",
      "step 6662, loss is 4.948225021362305\n",
      "(64, 33)\n",
      "step 6663, loss is 4.955408096313477\n",
      "(64, 33)\n",
      "step 6664, loss is 4.880251407623291\n",
      "(64, 33)\n",
      "step 6665, loss is 4.9569478034973145\n",
      "(64, 33)\n",
      "step 6666, loss is 4.817859649658203\n",
      "(64, 33)\n",
      "step 6667, loss is 4.801881790161133\n",
      "(64, 33)\n",
      "step 6668, loss is 4.973438262939453\n",
      "(64, 33)\n",
      "step 6669, loss is 5.109707355499268\n",
      "(64, 33)\n",
      "step 6670, loss is 4.811131477355957\n",
      "(64, 33)\n",
      "step 6671, loss is 4.69354248046875\n",
      "(64, 33)\n",
      "step 6672, loss is 4.862680912017822\n",
      "(64, 33)\n",
      "step 6673, loss is 4.784289836883545\n",
      "(64, 33)\n",
      "step 6674, loss is 4.876742362976074\n",
      "(64, 33)\n",
      "step 6675, loss is 4.992335319519043\n",
      "(64, 33)\n",
      "step 6676, loss is 4.997920036315918\n",
      "(64, 33)\n",
      "step 6677, loss is 5.066350936889648\n",
      "(64, 33)\n",
      "step 6678, loss is 4.77292537689209\n",
      "(64, 33)\n",
      "step 6679, loss is 4.983909606933594\n",
      "(64, 33)\n",
      "step 6680, loss is 4.791918754577637\n",
      "(64, 33)\n",
      "step 6681, loss is 4.859074592590332\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6682, loss is 5.04905366897583\n",
      "(64, 33)\n",
      "step 6683, loss is 4.875600337982178\n",
      "(64, 33)\n",
      "step 6684, loss is 4.9952392578125\n",
      "(64, 33)\n",
      "step 6685, loss is 5.025318145751953\n",
      "(64, 33)\n",
      "step 6686, loss is 4.8736772537231445\n",
      "(64, 33)\n",
      "step 6687, loss is 4.881627082824707\n",
      "(64, 33)\n",
      "step 6688, loss is 4.928529739379883\n",
      "(64, 33)\n",
      "step 6689, loss is 4.8195881843566895\n",
      "(64, 33)\n",
      "step 6690, loss is 4.812839031219482\n",
      "(64, 33)\n",
      "step 6691, loss is 5.03489351272583\n",
      "(64, 33)\n",
      "step 6692, loss is 4.836867332458496\n",
      "(64, 33)\n",
      "step 6693, loss is 5.005488395690918\n",
      "(64, 33)\n",
      "step 6694, loss is 4.907076835632324\n",
      "(64, 33)\n",
      "step 6695, loss is 4.778606414794922\n",
      "(64, 33)\n",
      "step 6696, loss is 4.879627227783203\n",
      "(64, 33)\n",
      "step 6697, loss is 4.966907978057861\n",
      "(64, 33)\n",
      "step 6698, loss is 4.867803573608398\n",
      "(64, 33)\n",
      "step 6699, loss is 4.669675827026367\n",
      "(64, 33)\n",
      "step 6700, loss is 4.844505786895752\n",
      "(64, 33)\n",
      "step 6701, loss is 5.001846790313721\n",
      "(64, 33)\n",
      "step 6702, loss is 4.760654449462891\n",
      "(64, 33)\n",
      "step 6703, loss is 4.901856422424316\n",
      "(64, 33)\n",
      "step 6704, loss is 4.714693546295166\n",
      "(64, 33)\n",
      "step 6705, loss is 4.876606464385986\n",
      "(64, 33)\n",
      "step 6706, loss is 4.904323101043701\n",
      "(64, 33)\n",
      "step 6707, loss is 4.941544055938721\n",
      "(64, 33)\n",
      "step 6708, loss is 5.128282070159912\n",
      "(64, 33)\n",
      "step 6709, loss is 4.984929084777832\n",
      "(64, 33)\n",
      "step 6710, loss is 4.910208225250244\n",
      "(64, 33)\n",
      "step 6711, loss is 4.9238505363464355\n",
      "(64, 33)\n",
      "step 6712, loss is 4.7953386306762695\n",
      "(64, 33)\n",
      "step 6713, loss is 4.694790363311768\n",
      "(64, 33)\n",
      "step 6714, loss is 4.964219570159912\n",
      "(64, 33)\n",
      "step 6715, loss is 4.765172958374023\n",
      "(64, 33)\n",
      "step 6716, loss is 4.896424770355225\n",
      "(64, 33)\n",
      "step 6717, loss is 4.881781101226807\n",
      "(64, 33)\n",
      "step 6718, loss is 4.7779717445373535\n",
      "(64, 33)\n",
      "step 6719, loss is 4.802333354949951\n",
      "(64, 33)\n",
      "step 6720, loss is 4.899528503417969\n",
      "(64, 33)\n",
      "step 6721, loss is 4.961838245391846\n",
      "(64, 33)\n",
      "step 6722, loss is 5.00471830368042\n",
      "(64, 33)\n",
      "step 6723, loss is 4.96331787109375\n",
      "(64, 33)\n",
      "step 6724, loss is 4.7367024421691895\n",
      "(64, 33)\n",
      "step 6725, loss is 4.819726943969727\n",
      "(64, 33)\n",
      "step 6726, loss is 4.773025035858154\n",
      "(64, 33)\n",
      "step 6727, loss is 5.01140832901001\n",
      "(64, 33)\n",
      "step 6728, loss is 4.950499534606934\n",
      "(64, 33)\n",
      "step 6729, loss is 4.826653480529785\n",
      "(64, 33)\n",
      "step 6730, loss is 4.931859493255615\n",
      "(64, 33)\n",
      "step 6731, loss is 4.9013824462890625\n",
      "(64, 33)\n",
      "step 6732, loss is 4.804603576660156\n",
      "(64, 33)\n",
      "step 6733, loss is 4.920698642730713\n",
      "(64, 33)\n",
      "step 6734, loss is 4.814309597015381\n",
      "(64, 33)\n",
      "step 6735, loss is 4.761708736419678\n",
      "(64, 33)\n",
      "step 6736, loss is 4.934232711791992\n",
      "(64, 33)\n",
      "step 6737, loss is 4.888702392578125\n",
      "(64, 33)\n",
      "step 6738, loss is 4.9893975257873535\n",
      "(64, 33)\n",
      "step 6739, loss is 5.021728992462158\n",
      "(64, 33)\n",
      "step 6740, loss is 4.811662197113037\n",
      "(64, 33)\n",
      "step 6741, loss is 4.942465305328369\n",
      "(64, 33)\n",
      "step 6742, loss is 4.9204301834106445\n",
      "(64, 33)\n",
      "step 6743, loss is 5.0249857902526855\n",
      "(64, 33)\n",
      "step 6744, loss is 4.994949817657471\n",
      "(64, 33)\n",
      "step 6745, loss is 4.957774639129639\n",
      "(64, 33)\n",
      "step 6746, loss is 4.830263614654541\n",
      "(64, 33)\n",
      "step 6747, loss is 5.1155900955200195\n",
      "(64, 33)\n",
      "step 6748, loss is 4.8059773445129395\n",
      "(64, 33)\n",
      "step 6749, loss is 4.969333648681641\n",
      "(64, 33)\n",
      "step 6750, loss is 4.903126239776611\n",
      "(64, 33)\n",
      "step 6751, loss is 4.986454486846924\n",
      "(64, 33)\n",
      "step 6752, loss is 4.859060287475586\n",
      "(64, 33)\n",
      "step 6753, loss is 4.941174030303955\n",
      "(64, 33)\n",
      "step 6754, loss is 4.896295070648193\n",
      "(64, 33)\n",
      "step 6755, loss is 4.9922637939453125\n",
      "(64, 33)\n",
      "step 6756, loss is 4.883535861968994\n",
      "(64, 33)\n",
      "step 6757, loss is 4.810105800628662\n",
      "(64, 33)\n",
      "step 6758, loss is 4.715731143951416\n",
      "(64, 33)\n",
      "step 6759, loss is 4.881158828735352\n",
      "(64, 33)\n",
      "step 6760, loss is 4.769248962402344\n",
      "(64, 33)\n",
      "step 6761, loss is 4.8672709465026855\n",
      "(64, 33)\n",
      "step 6762, loss is 4.850336074829102\n",
      "(64, 33)\n",
      "step 6763, loss is 4.940048694610596\n",
      "(64, 33)\n",
      "step 6764, loss is 5.025355815887451\n",
      "(64, 33)\n",
      "step 6765, loss is 5.092842102050781\n",
      "(64, 33)\n",
      "step 6766, loss is 4.695775985717773\n",
      "(64, 33)\n",
      "step 6767, loss is 4.892744541168213\n",
      "(64, 33)\n",
      "step 6768, loss is 4.7636494636535645\n",
      "(64, 33)\n",
      "step 6769, loss is 4.794484615325928\n",
      "(64, 33)\n",
      "step 6770, loss is 5.043283939361572\n",
      "(64, 33)\n",
      "step 6771, loss is 4.627926826477051\n",
      "(64, 33)\n",
      "step 6772, loss is 4.706490993499756\n",
      "(64, 33)\n",
      "step 6773, loss is 5.019866943359375\n",
      "(64, 33)\n",
      "step 6774, loss is 4.708404541015625\n",
      "(64, 33)\n",
      "step 6775, loss is 4.857890605926514\n",
      "(64, 33)\n",
      "step 6776, loss is 4.812934875488281\n",
      "(64, 33)\n",
      "step 6777, loss is 4.779994010925293\n",
      "(64, 33)\n",
      "step 6778, loss is 4.9196367263793945\n",
      "(64, 33)\n",
      "step 6779, loss is 4.810638427734375\n",
      "(64, 33)\n",
      "step 6780, loss is 4.739654541015625\n",
      "(64, 33)\n",
      "step 6781, loss is 5.000023365020752\n",
      "(64, 33)\n",
      "step 6782, loss is 4.899658679962158\n",
      "(64, 33)\n",
      "step 6783, loss is 4.997598171234131\n",
      "(64, 33)\n",
      "step 6784, loss is 5.025589466094971\n",
      "(64, 33)\n",
      "step 6785, loss is 4.939739227294922\n",
      "(64, 33)\n",
      "step 6786, loss is 4.861033916473389\n",
      "(64, 33)\n",
      "step 6787, loss is 5.001101493835449\n",
      "(64, 33)\n",
      "step 6788, loss is 5.039309978485107\n",
      "(64, 33)\n",
      "step 6789, loss is 4.787647247314453\n",
      "(64, 33)\n",
      "step 6790, loss is 4.753419399261475\n",
      "(64, 33)\n",
      "step 6791, loss is 4.886330604553223\n",
      "(64, 33)\n",
      "step 6792, loss is 5.051755428314209\n",
      "(64, 33)\n",
      "step 6793, loss is 4.834245204925537\n",
      "(64, 33)\n",
      "step 6794, loss is 4.85740852355957\n",
      "(64, 33)\n",
      "step 6795, loss is 4.936446666717529\n",
      "(64, 33)\n",
      "step 6796, loss is 4.9482197761535645\n",
      "(64, 33)\n",
      "step 6797, loss is 4.948580265045166\n",
      "(64, 33)\n",
      "step 6798, loss is 4.79145622253418\n",
      "(64, 33)\n",
      "step 6799, loss is 4.8678669929504395\n",
      "(64, 33)\n",
      "step 6800, loss is 4.956188201904297\n",
      "(64, 33)\n",
      "step 6801, loss is 4.989070892333984\n",
      "(64, 33)\n",
      "step 6802, loss is 4.713659763336182\n",
      "(64, 33)\n",
      "step 6803, loss is 4.942173957824707\n",
      "(64, 33)\n",
      "step 6804, loss is 5.0376105308532715\n",
      "(64, 33)\n",
      "step 6805, loss is 4.980413913726807\n",
      "(64, 33)\n",
      "step 6806, loss is 4.892690181732178\n",
      "(64, 33)\n",
      "step 6807, loss is 4.894529342651367\n",
      "(64, 33)\n",
      "step 6808, loss is 4.906852722167969\n",
      "(64, 33)\n",
      "step 6809, loss is 4.84190559387207\n",
      "(64, 33)\n",
      "step 6810, loss is 4.873729228973389\n",
      "(64, 33)\n",
      "step 6811, loss is 4.942502498626709\n",
      "(64, 33)\n",
      "step 6812, loss is 4.97153902053833\n",
      "(64, 33)\n",
      "step 6813, loss is 4.67853307723999\n",
      "(64, 33)\n",
      "step 6814, loss is 4.808207035064697\n",
      "(64, 33)\n",
      "step 6815, loss is 5.052186012268066\n",
      "(64, 33)\n",
      "step 6816, loss is 4.9572930335998535\n",
      "(64, 33)\n",
      "step 6817, loss is 4.737537860870361\n",
      "(64, 33)\n",
      "step 6818, loss is 4.9775519371032715\n",
      "(64, 33)\n",
      "step 6819, loss is 4.973329544067383\n",
      "(64, 33)\n",
      "step 6820, loss is 4.8254194259643555\n",
      "(64, 33)\n",
      "step 6821, loss is 4.997281074523926\n",
      "(64, 33)\n",
      "step 6822, loss is 4.8067193031311035\n",
      "(64, 33)\n",
      "step 6823, loss is 4.794486999511719\n",
      "(64, 33)\n",
      "step 6824, loss is 4.780930995941162\n",
      "(64, 33)\n",
      "step 6825, loss is 4.955974102020264\n",
      "(64, 33)\n",
      "step 6826, loss is 4.827457427978516\n",
      "(64, 33)\n",
      "step 6827, loss is 4.9371018409729\n",
      "(64, 33)\n",
      "step 6828, loss is 4.839737415313721\n",
      "(64, 33)\n",
      "step 6829, loss is 4.978229999542236\n",
      "(64, 33)\n",
      "step 6830, loss is 4.931090354919434\n",
      "(64, 33)\n",
      "step 6831, loss is 4.933673858642578\n",
      "(64, 33)\n",
      "step 6832, loss is 4.93357515335083\n",
      "(64, 33)\n",
      "step 6833, loss is 4.757693767547607\n",
      "(64, 33)\n",
      "step 6834, loss is 4.837388515472412\n",
      "(64, 33)\n",
      "step 6835, loss is 4.771080017089844\n",
      "(64, 33)\n",
      "step 6836, loss is 4.9082255363464355\n",
      "(64, 33)\n",
      "step 6837, loss is 4.940855979919434\n",
      "(64, 33)\n",
      "step 6838, loss is 5.030428409576416\n",
      "(64, 33)\n",
      "step 6839, loss is 4.918680667877197\n",
      "(64, 33)\n",
      "step 6840, loss is 4.898551940917969\n",
      "(64, 33)\n",
      "step 6841, loss is 4.83581018447876\n",
      "(64, 33)\n",
      "step 6842, loss is 4.984960556030273\n",
      "(64, 33)\n",
      "step 6843, loss is 4.911777973175049\n",
      "(64, 33)\n",
      "step 6844, loss is 4.802781581878662\n",
      "(64, 33)\n",
      "step 6845, loss is 4.748157024383545\n",
      "(64, 33)\n",
      "step 6846, loss is 4.990357398986816\n",
      "(64, 33)\n",
      "step 6847, loss is 5.0010457038879395\n",
      "(64, 33)\n",
      "step 6848, loss is 4.9004011154174805\n",
      "(64, 33)\n",
      "step 6849, loss is 4.825204372406006\n",
      "(64, 33)\n",
      "step 6850, loss is 4.899512767791748\n",
      "(64, 33)\n",
      "step 6851, loss is 4.890100002288818\n",
      "(64, 33)\n",
      "step 6852, loss is 4.685883045196533\n",
      "(64, 33)\n",
      "step 6853, loss is 5.003510475158691\n",
      "(64, 33)\n",
      "step 6854, loss is 4.887262344360352\n",
      "(64, 33)\n",
      "step 6855, loss is 4.949084281921387\n",
      "(64, 33)\n",
      "step 6856, loss is 4.874974250793457\n",
      "(64, 33)\n",
      "step 6857, loss is 4.812798976898193\n",
      "(64, 33)\n",
      "step 6858, loss is 5.043029308319092\n",
      "(64, 33)\n",
      "step 6859, loss is 5.000378608703613\n",
      "(64, 33)\n",
      "step 6860, loss is 4.856801509857178\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6861, loss is 4.841124057769775\n",
      "(64, 33)\n",
      "step 6862, loss is 4.9705400466918945\n",
      "(64, 33)\n",
      "step 6863, loss is 4.819695949554443\n",
      "(64, 33)\n",
      "step 6864, loss is 4.921777248382568\n",
      "(64, 33)\n",
      "step 6865, loss is 5.064147472381592\n",
      "(64, 33)\n",
      "step 6866, loss is 4.909515380859375\n",
      "(64, 33)\n",
      "step 6867, loss is 4.842084884643555\n",
      "(64, 33)\n",
      "step 6868, loss is 4.913967132568359\n",
      "(64, 33)\n",
      "step 6869, loss is 4.9023356437683105\n",
      "(64, 33)\n",
      "step 6870, loss is 4.883545875549316\n",
      "(64, 33)\n",
      "step 6871, loss is 5.023836135864258\n",
      "(64, 33)\n",
      "step 6872, loss is 4.9707746505737305\n",
      "(64, 33)\n",
      "step 6873, loss is 4.775722980499268\n",
      "(64, 33)\n",
      "step 6874, loss is 4.702510833740234\n",
      "(64, 33)\n",
      "step 6875, loss is 4.982207775115967\n",
      "(64, 33)\n",
      "step 6876, loss is 5.040062427520752\n",
      "(64, 33)\n",
      "step 6877, loss is 4.936908721923828\n",
      "(64, 33)\n",
      "step 6878, loss is 5.055394172668457\n",
      "(64, 33)\n",
      "step 6879, loss is 4.8901543617248535\n",
      "(64, 33)\n",
      "step 6880, loss is 4.754673004150391\n",
      "(64, 33)\n",
      "step 6881, loss is 4.937201499938965\n",
      "(64, 33)\n",
      "step 6882, loss is 5.00640344619751\n",
      "(64, 33)\n",
      "step 6883, loss is 4.934611797332764\n",
      "(64, 33)\n",
      "step 6884, loss is 4.840068817138672\n",
      "(64, 33)\n",
      "step 6885, loss is 4.897969722747803\n",
      "(64, 33)\n",
      "step 6886, loss is 4.876553535461426\n",
      "(64, 33)\n",
      "step 6887, loss is 4.878688335418701\n",
      "(64, 33)\n",
      "step 6888, loss is 4.956513404846191\n",
      "(64, 33)\n",
      "step 6889, loss is 4.99515962600708\n",
      "(64, 33)\n",
      "step 6890, loss is 4.836545944213867\n",
      "(64, 33)\n",
      "step 6891, loss is 4.932458877563477\n",
      "(64, 33)\n",
      "step 6892, loss is 4.820418834686279\n",
      "(64, 33)\n",
      "step 6893, loss is 4.78513765335083\n",
      "(64, 33)\n",
      "step 6894, loss is 4.9209489822387695\n",
      "(64, 33)\n",
      "step 6895, loss is 4.800041675567627\n",
      "(64, 33)\n",
      "step 6896, loss is 4.836197853088379\n",
      "(64, 33)\n",
      "step 6897, loss is 4.818633079528809\n",
      "(64, 33)\n",
      "step 6898, loss is 4.96751070022583\n",
      "(64, 33)\n",
      "step 6899, loss is 5.1042962074279785\n",
      "(64, 33)\n",
      "step 6900, loss is 4.961018085479736\n",
      "(64, 33)\n",
      "step 6901, loss is 5.007843971252441\n",
      "(64, 33)\n",
      "step 6902, loss is 4.967811584472656\n",
      "(64, 33)\n",
      "step 6903, loss is 4.756954193115234\n",
      "(64, 33)\n",
      "step 6904, loss is 4.99664306640625\n",
      "(64, 33)\n",
      "step 6905, loss is 4.829979419708252\n",
      "(64, 33)\n",
      "step 6906, loss is 4.813873767852783\n",
      "(64, 33)\n",
      "step 6907, loss is 5.02770471572876\n",
      "(64, 33)\n",
      "step 6908, loss is 4.902107238769531\n",
      "(64, 33)\n",
      "step 6909, loss is 4.898751735687256\n",
      "(64, 33)\n",
      "step 6910, loss is 4.985477447509766\n",
      "(64, 33)\n",
      "step 6911, loss is 4.863891124725342\n",
      "(64, 33)\n",
      "step 6912, loss is 4.814733982086182\n",
      "(64, 33)\n",
      "step 6913, loss is 4.893298149108887\n",
      "(64, 33)\n",
      "step 6914, loss is 4.866650104522705\n",
      "(64, 33)\n",
      "step 6915, loss is 4.858302593231201\n",
      "(64, 33)\n",
      "step 6916, loss is 4.9816365242004395\n",
      "(64, 33)\n",
      "step 6917, loss is 5.028849124908447\n",
      "(64, 33)\n",
      "step 6918, loss is 4.902046203613281\n",
      "(64, 33)\n",
      "step 6919, loss is 4.9822516441345215\n",
      "(64, 33)\n",
      "step 6920, loss is 4.606472015380859\n",
      "(64, 33)\n",
      "step 6921, loss is 4.817195415496826\n",
      "(64, 33)\n",
      "step 6922, loss is 4.791911602020264\n",
      "(64, 33)\n",
      "step 6923, loss is 4.839812278747559\n",
      "(64, 33)\n",
      "step 6924, loss is 5.033484935760498\n",
      "(64, 33)\n",
      "step 6925, loss is 5.002007484436035\n",
      "(64, 33)\n",
      "step 6926, loss is 5.058831691741943\n",
      "(64, 33)\n",
      "step 6927, loss is 4.911744594573975\n",
      "(64, 33)\n",
      "step 6928, loss is 4.898216247558594\n",
      "(64, 33)\n",
      "step 6929, loss is 4.888504505157471\n",
      "(64, 33)\n",
      "step 6930, loss is 5.0274128913879395\n",
      "(64, 33)\n",
      "step 6931, loss is 4.7871246337890625\n",
      "(64, 33)\n",
      "step 6932, loss is 4.902103424072266\n",
      "(64, 33)\n",
      "step 6933, loss is 5.02800989151001\n",
      "(64, 33)\n",
      "step 6934, loss is 4.9671196937561035\n",
      "(64, 33)\n",
      "step 6935, loss is 4.755361557006836\n",
      "(64, 33)\n",
      "step 6936, loss is 4.806826591491699\n",
      "(64, 33)\n",
      "step 6937, loss is 4.9403510093688965\n",
      "(64, 33)\n",
      "step 6938, loss is 4.810150623321533\n",
      "(64, 33)\n",
      "step 6939, loss is 4.882774353027344\n",
      "(64, 33)\n",
      "step 6940, loss is 4.75420618057251\n",
      "(64, 33)\n",
      "step 6941, loss is 4.909576416015625\n",
      "(64, 33)\n",
      "step 6942, loss is 4.778777122497559\n",
      "(64, 33)\n",
      "step 6943, loss is 4.8911213874816895\n",
      "(64, 33)\n",
      "step 6944, loss is 4.76833963394165\n",
      "(64, 33)\n",
      "step 6945, loss is 4.847740173339844\n",
      "(64, 33)\n",
      "step 6946, loss is 4.757124423980713\n",
      "(64, 33)\n",
      "step 6947, loss is 4.919825077056885\n",
      "(64, 33)\n",
      "step 6948, loss is 5.011452674865723\n",
      "(64, 33)\n",
      "step 6949, loss is 4.8958024978637695\n",
      "(64, 33)\n",
      "step 6950, loss is 4.8405656814575195\n",
      "(64, 33)\n",
      "step 6951, loss is 4.75553035736084\n",
      "(64, 33)\n",
      "step 6952, loss is 4.985827445983887\n",
      "(64, 33)\n",
      "step 6953, loss is 4.828849792480469\n",
      "(64, 33)\n",
      "step 6954, loss is 4.874919891357422\n",
      "(64, 33)\n",
      "step 6955, loss is 4.75081205368042\n",
      "(64, 33)\n",
      "step 6956, loss is 5.0283894538879395\n",
      "(64, 33)\n",
      "step 6957, loss is 5.043467998504639\n",
      "(64, 33)\n",
      "step 6958, loss is 5.073615074157715\n",
      "(64, 33)\n",
      "step 6959, loss is 4.8502726554870605\n",
      "(64, 33)\n",
      "step 6960, loss is 5.022492408752441\n",
      "(64, 33)\n",
      "step 6961, loss is 4.800872325897217\n",
      "(64, 33)\n",
      "step 6962, loss is 4.93584680557251\n",
      "(64, 33)\n",
      "step 6963, loss is 5.105095386505127\n",
      "(64, 33)\n",
      "step 6964, loss is 4.825740814208984\n",
      "(64, 33)\n",
      "step 6965, loss is 4.848681449890137\n",
      "(64, 33)\n",
      "step 6966, loss is 4.9948410987854\n",
      "(64, 33)\n",
      "step 6967, loss is 4.8208746910095215\n",
      "(64, 33)\n",
      "step 6968, loss is 4.926544189453125\n",
      "(64, 33)\n",
      "step 6969, loss is 4.774118900299072\n",
      "(64, 33)\n",
      "step 6970, loss is 4.850008964538574\n",
      "(64, 33)\n",
      "step 6971, loss is 4.9671220779418945\n",
      "(64, 33)\n",
      "step 6972, loss is 5.087997913360596\n",
      "(64, 33)\n",
      "step 6973, loss is 4.96533203125\n",
      "(64, 33)\n",
      "step 6974, loss is 5.020258903503418\n",
      "(64, 33)\n",
      "step 6975, loss is 4.983612537384033\n",
      "(64, 33)\n",
      "step 6976, loss is 4.967609405517578\n",
      "(64, 33)\n",
      "step 6977, loss is 5.042342185974121\n",
      "(64, 33)\n",
      "step 6978, loss is 4.9130048751831055\n",
      "(64, 33)\n",
      "step 6979, loss is 4.853129863739014\n",
      "(64, 33)\n",
      "step 6980, loss is 4.858847141265869\n",
      "(64, 33)\n",
      "step 6981, loss is 4.877172470092773\n",
      "(64, 33)\n",
      "step 6982, loss is 4.995654582977295\n",
      "(64, 33)\n",
      "step 6983, loss is 4.897957801818848\n",
      "(64, 33)\n",
      "step 6984, loss is 4.970115661621094\n",
      "(64, 33)\n",
      "step 6985, loss is 4.906463623046875\n",
      "(64, 33)\n",
      "step 6986, loss is 4.883190631866455\n",
      "(64, 33)\n",
      "step 6987, loss is 4.972710609436035\n",
      "(64, 33)\n",
      "step 6988, loss is 5.00262451171875\n",
      "(64, 33)\n",
      "step 6989, loss is 4.772563457489014\n",
      "(64, 33)\n",
      "step 6990, loss is 4.951084136962891\n",
      "(64, 33)\n",
      "step 6991, loss is 4.80789852142334\n",
      "(64, 33)\n",
      "step 6992, loss is 4.9969964027404785\n",
      "(64, 33)\n",
      "step 6993, loss is 4.814700603485107\n",
      "(64, 33)\n",
      "step 6994, loss is 4.896867275238037\n",
      "(64, 33)\n",
      "step 6995, loss is 4.774790287017822\n",
      "(64, 33)\n",
      "step 6996, loss is 4.901786804199219\n",
      "(64, 33)\n",
      "step 6997, loss is 4.749043941497803\n",
      "(64, 33)\n",
      "step 6998, loss is 5.010015964508057\n",
      "(64, 33)\n",
      "step 6999, loss is 4.812241554260254\n",
      "(64, 33)\n",
      "step 7000, loss is 4.737695217132568\n",
      "(64, 33)\n",
      "step 7001, loss is 4.993352890014648\n",
      "(64, 33)\n",
      "step 7002, loss is 4.659507751464844\n",
      "(64, 33)\n",
      "step 7003, loss is 4.732321262359619\n",
      "(64, 33)\n",
      "step 7004, loss is 4.806585311889648\n",
      "(64, 33)\n",
      "step 7005, loss is 4.931056022644043\n",
      "(64, 33)\n",
      "step 7006, loss is 4.867260456085205\n",
      "(64, 33)\n",
      "step 7007, loss is 4.8431806564331055\n",
      "(64, 33)\n",
      "step 7008, loss is 4.670114994049072\n",
      "(64, 33)\n",
      "step 7009, loss is 4.795369625091553\n",
      "(64, 33)\n",
      "step 7010, loss is 4.9624481201171875\n",
      "(64, 33)\n",
      "step 7011, loss is 4.8234782218933105\n",
      "(64, 33)\n",
      "step 7012, loss is 4.907007694244385\n",
      "(64, 33)\n",
      "step 7013, loss is 4.776787281036377\n",
      "(64, 33)\n",
      "step 7014, loss is 4.741750717163086\n",
      "(64, 33)\n",
      "step 7015, loss is 5.100711822509766\n",
      "(64, 33)\n",
      "step 7016, loss is 4.898408889770508\n",
      "(64, 33)\n",
      "step 7017, loss is 4.9728522300720215\n",
      "(64, 33)\n",
      "step 7018, loss is 4.861164093017578\n",
      "(64, 33)\n",
      "step 7019, loss is 4.843523979187012\n",
      "(64, 33)\n",
      "step 7020, loss is 4.960485935211182\n",
      "(64, 33)\n",
      "step 7021, loss is 4.9643378257751465\n",
      "(64, 33)\n",
      "step 7022, loss is 4.9384870529174805\n",
      "(64, 33)\n",
      "step 7023, loss is 4.863529682159424\n",
      "(64, 33)\n",
      "step 7024, loss is 4.870993137359619\n",
      "(64, 33)\n",
      "step 7025, loss is 4.887901306152344\n",
      "(64, 33)\n",
      "step 7026, loss is 4.948153018951416\n",
      "(64, 33)\n",
      "step 7027, loss is 5.013225078582764\n",
      "(64, 33)\n",
      "step 7028, loss is 4.943138122558594\n",
      "(64, 33)\n",
      "step 7029, loss is 4.852660179138184\n",
      "(64, 33)\n",
      "step 7030, loss is 4.948662757873535\n",
      "(64, 33)\n",
      "step 7031, loss is 5.040349960327148\n",
      "(64, 33)\n",
      "step 7032, loss is 5.109655380249023\n",
      "(64, 33)\n",
      "step 7033, loss is 4.930903911590576\n",
      "(64, 33)\n",
      "step 7034, loss is 4.973882675170898\n",
      "(64, 33)\n",
      "step 7035, loss is 4.837616443634033\n",
      "(64, 33)\n",
      "step 7036, loss is 4.851045608520508\n",
      "(64, 33)\n",
      "step 7037, loss is 4.906699180603027\n",
      "(64, 33)\n",
      "step 7038, loss is 4.905539035797119\n",
      "(64, 33)\n",
      "step 7039, loss is 5.0444183349609375\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7040, loss is 4.798418998718262\n",
      "(64, 33)\n",
      "step 7041, loss is 5.066390514373779\n",
      "(64, 33)\n",
      "step 7042, loss is 4.786489963531494\n",
      "(64, 33)\n",
      "step 7043, loss is 4.964029312133789\n",
      "(64, 33)\n",
      "step 7044, loss is 4.723278045654297\n",
      "(64, 33)\n",
      "step 7045, loss is 4.7841596603393555\n",
      "(64, 33)\n",
      "step 7046, loss is 4.878310680389404\n",
      "(64, 33)\n",
      "step 7047, loss is 4.886935710906982\n",
      "(64, 33)\n",
      "step 7048, loss is 5.017095565795898\n",
      "(64, 33)\n",
      "step 7049, loss is 4.962226867675781\n",
      "(64, 33)\n",
      "step 7050, loss is 5.040090084075928\n",
      "(64, 33)\n",
      "step 7051, loss is 4.823489665985107\n",
      "(64, 33)\n",
      "step 7052, loss is 4.993476867675781\n",
      "(64, 33)\n",
      "step 7053, loss is 4.796536922454834\n",
      "(64, 33)\n",
      "step 7054, loss is 4.842843055725098\n",
      "(64, 33)\n",
      "step 7055, loss is 4.913276672363281\n",
      "(64, 33)\n",
      "step 7056, loss is 4.911322116851807\n",
      "(64, 33)\n",
      "step 7057, loss is 5.030599594116211\n",
      "(64, 33)\n",
      "step 7058, loss is 5.0125579833984375\n",
      "(64, 33)\n",
      "step 7059, loss is 4.770496845245361\n",
      "(64, 33)\n",
      "step 7060, loss is 5.025296211242676\n",
      "(64, 33)\n",
      "step 7061, loss is 4.937545299530029\n",
      "(64, 33)\n",
      "step 7062, loss is 4.8859944343566895\n",
      "(64, 33)\n",
      "step 7063, loss is 4.811181545257568\n",
      "(64, 33)\n",
      "step 7064, loss is 5.011539459228516\n",
      "(64, 33)\n",
      "step 7065, loss is 5.034985065460205\n",
      "(64, 33)\n",
      "step 7066, loss is 5.058499336242676\n",
      "(64, 33)\n",
      "step 7067, loss is 4.864874839782715\n",
      "(64, 33)\n",
      "step 7068, loss is 4.76542854309082\n",
      "(64, 33)\n",
      "step 7069, loss is 4.821510314941406\n",
      "(64, 33)\n",
      "step 7070, loss is 4.936458587646484\n",
      "(64, 33)\n",
      "step 7071, loss is 5.100669860839844\n",
      "(64, 33)\n",
      "step 7072, loss is 4.730882167816162\n",
      "(64, 33)\n",
      "step 7073, loss is 4.933198928833008\n",
      "(64, 33)\n",
      "step 7074, loss is 4.981788158416748\n",
      "(64, 33)\n",
      "step 7075, loss is 4.971363544464111\n",
      "(64, 33)\n",
      "step 7076, loss is 4.7406086921691895\n",
      "(64, 33)\n",
      "step 7077, loss is 5.053703308105469\n",
      "(64, 33)\n",
      "step 7078, loss is 5.126389503479004\n",
      "(64, 33)\n",
      "step 7079, loss is 5.0477800369262695\n",
      "(64, 33)\n",
      "step 7080, loss is 5.0434465408325195\n",
      "(64, 33)\n",
      "step 7081, loss is 4.904995918273926\n",
      "(64, 33)\n",
      "step 7082, loss is 5.012943744659424\n",
      "(64, 33)\n",
      "step 7083, loss is 4.995206356048584\n",
      "(64, 33)\n",
      "step 7084, loss is 4.859412670135498\n",
      "(64, 33)\n",
      "step 7085, loss is 4.5231404304504395\n",
      "(64, 33)\n",
      "step 7086, loss is 4.877325057983398\n",
      "(64, 33)\n",
      "step 7087, loss is 4.899141311645508\n",
      "(64, 33)\n",
      "step 7088, loss is 4.932450771331787\n",
      "(64, 33)\n",
      "step 7089, loss is 4.7016425132751465\n",
      "(64, 33)\n",
      "step 7090, loss is 4.8363494873046875\n",
      "(64, 33)\n",
      "step 7091, loss is 4.899116039276123\n",
      "(64, 33)\n",
      "step 7092, loss is 5.078399181365967\n",
      "(64, 33)\n",
      "step 7093, loss is 4.872249603271484\n",
      "(64, 33)\n",
      "step 7094, loss is 4.901703834533691\n",
      "(64, 33)\n",
      "step 7095, loss is 4.822388648986816\n",
      "(64, 33)\n",
      "step 7096, loss is 4.833123683929443\n",
      "(64, 33)\n",
      "step 7097, loss is 4.872530937194824\n",
      "(64, 33)\n",
      "step 7098, loss is 4.841468811035156\n",
      "(64, 33)\n",
      "step 7099, loss is 4.783841609954834\n",
      "(64, 33)\n",
      "step 7100, loss is 4.924098014831543\n",
      "(64, 33)\n",
      "step 7101, loss is 4.794621467590332\n",
      "(64, 33)\n",
      "step 7102, loss is 4.880038738250732\n",
      "(64, 33)\n",
      "step 7103, loss is 4.770432472229004\n",
      "(64, 33)\n",
      "step 7104, loss is 4.912867546081543\n",
      "(64, 33)\n",
      "step 7105, loss is 4.68253755569458\n",
      "(64, 33)\n",
      "step 7106, loss is 4.92092752456665\n",
      "(64, 33)\n",
      "step 7107, loss is 4.805544376373291\n",
      "(64, 33)\n",
      "step 7108, loss is 5.065283298492432\n",
      "(64, 33)\n",
      "step 7109, loss is 4.75834321975708\n",
      "(64, 33)\n",
      "step 7110, loss is 4.78785514831543\n",
      "(64, 33)\n",
      "step 7111, loss is 4.965980529785156\n",
      "(64, 33)\n",
      "step 7112, loss is 5.000624656677246\n",
      "(64, 33)\n",
      "step 7113, loss is 4.749868869781494\n",
      "(64, 33)\n",
      "step 7114, loss is 4.799389362335205\n",
      "(64, 33)\n",
      "step 7115, loss is 4.9468159675598145\n",
      "(64, 33)\n",
      "step 7116, loss is 4.965311050415039\n",
      "(64, 33)\n",
      "step 7117, loss is 4.859404563903809\n",
      "(64, 33)\n",
      "step 7118, loss is 4.951671123504639\n",
      "(64, 33)\n",
      "step 7119, loss is 4.973844528198242\n",
      "(64, 33)\n",
      "step 7120, loss is 4.9467997550964355\n",
      "(64, 33)\n",
      "step 7121, loss is 4.815580368041992\n",
      "(64, 33)\n",
      "step 7122, loss is 4.943371772766113\n",
      "(64, 33)\n",
      "step 7123, loss is 4.9586615562438965\n",
      "(64, 33)\n",
      "step 7124, loss is 4.8781938552856445\n",
      "(64, 33)\n",
      "step 7125, loss is 4.8897809982299805\n",
      "(64, 33)\n",
      "step 7126, loss is 4.865194797515869\n",
      "(64, 33)\n",
      "step 7127, loss is 4.998083114624023\n",
      "(64, 33)\n",
      "step 7128, loss is 4.9336628913879395\n",
      "(64, 33)\n",
      "step 7129, loss is 4.708645343780518\n",
      "(64, 33)\n",
      "step 7130, loss is 4.872337818145752\n",
      "(64, 33)\n",
      "step 7131, loss is 5.173782825469971\n",
      "(64, 33)\n",
      "step 7132, loss is 4.907366752624512\n",
      "(64, 33)\n",
      "step 7133, loss is 4.891683578491211\n",
      "(64, 33)\n",
      "step 7134, loss is 4.888694763183594\n",
      "(64, 33)\n",
      "step 7135, loss is 4.802098274230957\n",
      "(64, 33)\n",
      "step 7136, loss is 4.957549095153809\n",
      "(64, 33)\n",
      "step 7137, loss is 4.897970199584961\n",
      "(64, 33)\n",
      "step 7138, loss is 4.797564506530762\n",
      "(64, 33)\n",
      "step 7139, loss is 4.772146701812744\n",
      "(64, 33)\n",
      "step 7140, loss is 4.930782318115234\n",
      "(64, 33)\n",
      "step 7141, loss is 4.811848163604736\n",
      "(64, 33)\n",
      "step 7142, loss is 4.857938766479492\n",
      "(64, 33)\n",
      "step 7143, loss is 4.8548173904418945\n",
      "(64, 33)\n",
      "step 7144, loss is 4.760586738586426\n",
      "(64, 33)\n",
      "step 7145, loss is 4.862778663635254\n",
      "(64, 33)\n",
      "step 7146, loss is 4.917067050933838\n",
      "(64, 33)\n",
      "step 7147, loss is 4.950624942779541\n",
      "(64, 33)\n",
      "step 7148, loss is 4.91919469833374\n",
      "(64, 33)\n",
      "step 7149, loss is 4.7964277267456055\n",
      "(64, 33)\n",
      "step 7150, loss is 4.894229888916016\n",
      "(64, 33)\n",
      "step 7151, loss is 4.914388179779053\n",
      "(64, 33)\n",
      "step 7152, loss is 4.931807994842529\n",
      "(64, 33)\n",
      "step 7153, loss is 4.750614166259766\n",
      "(64, 33)\n",
      "step 7154, loss is 4.965700626373291\n",
      "(64, 33)\n",
      "step 7155, loss is 4.8260393142700195\n",
      "(64, 33)\n",
      "step 7156, loss is 4.930842399597168\n",
      "(64, 33)\n",
      "step 7157, loss is 5.068471908569336\n",
      "(64, 33)\n",
      "step 7158, loss is 4.795293807983398\n",
      "(64, 33)\n",
      "step 7159, loss is 4.982117176055908\n",
      "(64, 33)\n",
      "step 7160, loss is 4.882802963256836\n",
      "(64, 33)\n",
      "step 7161, loss is 4.662466526031494\n",
      "(64, 33)\n",
      "step 7162, loss is 4.867259502410889\n",
      "(64, 33)\n",
      "step 7163, loss is 5.020348072052002\n",
      "(64, 33)\n",
      "step 7164, loss is 4.826062202453613\n",
      "(64, 33)\n",
      "step 7165, loss is 4.786405563354492\n",
      "(64, 33)\n",
      "step 7166, loss is 4.929306507110596\n",
      "(64, 33)\n",
      "step 7167, loss is 4.890939235687256\n",
      "(64, 33)\n",
      "step 7168, loss is 5.013862609863281\n",
      "(64, 33)\n",
      "step 7169, loss is 4.733541011810303\n",
      "(64, 33)\n",
      "step 7170, loss is 4.962168216705322\n",
      "(64, 33)\n",
      "step 7171, loss is 4.79449462890625\n",
      "(64, 33)\n",
      "step 7172, loss is 4.904242515563965\n",
      "(64, 33)\n",
      "step 7173, loss is 4.96873140335083\n",
      "(64, 33)\n",
      "step 7174, loss is 4.899099349975586\n",
      "(64, 33)\n",
      "step 7175, loss is 4.885956764221191\n",
      "(64, 33)\n",
      "step 7176, loss is 4.874637126922607\n",
      "(64, 33)\n",
      "step 7177, loss is 4.665785312652588\n",
      "(64, 33)\n",
      "step 7178, loss is 4.802536487579346\n",
      "(64, 33)\n",
      "step 7179, loss is 4.961250305175781\n",
      "(64, 33)\n",
      "step 7180, loss is 4.949177265167236\n",
      "(64, 33)\n",
      "step 7181, loss is 4.836197376251221\n",
      "(64, 33)\n",
      "step 7182, loss is 4.837043762207031\n",
      "(64, 33)\n",
      "step 7183, loss is 4.904610633850098\n",
      "(64, 33)\n",
      "step 7184, loss is 4.954745769500732\n",
      "(64, 33)\n",
      "step 7185, loss is 4.873629093170166\n",
      "(64, 33)\n",
      "step 7186, loss is 4.7645955085754395\n",
      "(64, 33)\n",
      "step 7187, loss is 4.872826099395752\n",
      "(64, 33)\n",
      "step 7188, loss is 4.9405131340026855\n",
      "(64, 33)\n",
      "step 7189, loss is 4.8424482345581055\n",
      "(64, 33)\n",
      "step 7190, loss is 4.940011024475098\n",
      "(64, 33)\n",
      "step 7191, loss is 4.6966071128845215\n",
      "(64, 33)\n",
      "step 7192, loss is 4.854635715484619\n",
      "(64, 33)\n",
      "step 7193, loss is 4.887622356414795\n",
      "(64, 33)\n",
      "step 7194, loss is 4.9965362548828125\n",
      "(64, 33)\n",
      "step 7195, loss is 4.843707084655762\n",
      "(64, 33)\n",
      "step 7196, loss is 4.909003257751465\n",
      "(64, 33)\n",
      "step 7197, loss is 5.023550510406494\n",
      "(64, 33)\n",
      "step 7198, loss is 5.090768337249756\n",
      "(64, 33)\n",
      "step 7199, loss is 4.983438491821289\n",
      "(64, 33)\n",
      "step 7200, loss is 4.883083820343018\n",
      "(64, 33)\n",
      "step 7201, loss is 4.838598251342773\n",
      "(64, 33)\n",
      "step 7202, loss is 5.000010967254639\n",
      "(64, 33)\n",
      "step 7203, loss is 4.897129058837891\n",
      "(64, 33)\n",
      "step 7204, loss is 4.69424295425415\n",
      "(64, 33)\n",
      "step 7205, loss is 4.97569465637207\n",
      "(64, 33)\n",
      "step 7206, loss is 4.994521141052246\n",
      "(64, 33)\n",
      "step 7207, loss is 4.66694974899292\n",
      "(64, 33)\n",
      "step 7208, loss is 5.0906877517700195\n",
      "(64, 33)\n",
      "step 7209, loss is 4.890476226806641\n",
      "(64, 33)\n",
      "step 7210, loss is 5.170971870422363\n",
      "(64, 33)\n",
      "step 7211, loss is 4.718064785003662\n",
      "(64, 33)\n",
      "step 7212, loss is 4.860423564910889\n",
      "(64, 33)\n",
      "step 7213, loss is 4.937045574188232\n",
      "(64, 33)\n",
      "step 7214, loss is 4.897747039794922\n",
      "(64, 33)\n",
      "step 7215, loss is 4.909979820251465\n",
      "(64, 33)\n",
      "step 7216, loss is 4.995781421661377\n",
      "(64, 33)\n",
      "step 7217, loss is 4.888291835784912\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7218, loss is 5.085655689239502\n",
      "(64, 33)\n",
      "step 7219, loss is 4.872663974761963\n",
      "(64, 33)\n",
      "step 7220, loss is 4.879674434661865\n",
      "(64, 33)\n",
      "step 7221, loss is 4.8053812980651855\n",
      "(64, 33)\n",
      "step 7222, loss is 4.835529327392578\n",
      "(64, 33)\n",
      "step 7223, loss is 5.055734157562256\n",
      "(64, 33)\n",
      "step 7224, loss is 4.909836292266846\n",
      "(64, 33)\n",
      "step 7225, loss is 5.031472206115723\n",
      "(64, 33)\n",
      "step 7226, loss is 4.732182025909424\n",
      "(64, 33)\n",
      "step 7227, loss is 4.937168598175049\n",
      "(64, 33)\n",
      "step 7228, loss is 4.8748674392700195\n",
      "(64, 33)\n",
      "step 7229, loss is 4.841920852661133\n",
      "(64, 33)\n",
      "step 7230, loss is 4.97605562210083\n",
      "(64, 33)\n",
      "step 7231, loss is 4.9504265785217285\n",
      "(64, 33)\n",
      "step 7232, loss is 5.163851261138916\n",
      "(64, 33)\n",
      "step 7233, loss is 4.979559421539307\n",
      "(64, 33)\n",
      "step 7234, loss is 5.041502475738525\n",
      "(64, 33)\n",
      "step 7235, loss is 4.8312458992004395\n",
      "(64, 33)\n",
      "step 7236, loss is 4.80913782119751\n",
      "(64, 33)\n",
      "step 7237, loss is 4.869269371032715\n",
      "(64, 33)\n",
      "step 7238, loss is 4.932322978973389\n",
      "(64, 33)\n",
      "step 7239, loss is 4.869026184082031\n",
      "(64, 33)\n",
      "step 7240, loss is 4.883331298828125\n",
      "(64, 33)\n",
      "step 7241, loss is 4.961785793304443\n",
      "(64, 33)\n",
      "step 7242, loss is 4.674449920654297\n",
      "(64, 33)\n",
      "step 7243, loss is 4.848989963531494\n",
      "(64, 33)\n",
      "step 7244, loss is 5.012867450714111\n",
      "(64, 33)\n",
      "step 7245, loss is 4.8350911140441895\n",
      "(64, 33)\n",
      "step 7246, loss is 4.8922905921936035\n",
      "(64, 33)\n",
      "step 7247, loss is 4.8604888916015625\n",
      "(64, 33)\n",
      "step 7248, loss is 5.079699993133545\n",
      "(64, 33)\n",
      "step 7249, loss is 4.8914570808410645\n",
      "(64, 33)\n",
      "step 7250, loss is 4.819222450256348\n",
      "(64, 33)\n",
      "step 7251, loss is 5.0068817138671875\n",
      "(64, 33)\n",
      "step 7252, loss is 4.9227213859558105\n",
      "(64, 33)\n",
      "step 7253, loss is 4.849881172180176\n",
      "(64, 33)\n",
      "step 7254, loss is 4.933189392089844\n",
      "(64, 33)\n",
      "step 7255, loss is 4.958657741546631\n",
      "(64, 33)\n",
      "step 7256, loss is 4.986173629760742\n",
      "(64, 33)\n",
      "step 7257, loss is 5.018302917480469\n",
      "(64, 33)\n",
      "step 7258, loss is 4.96401309967041\n",
      "(64, 33)\n",
      "step 7259, loss is 4.975090503692627\n",
      "(64, 33)\n",
      "step 7260, loss is 4.869955539703369\n",
      "(64, 33)\n",
      "step 7261, loss is 4.829131126403809\n",
      "(64, 33)\n",
      "step 7262, loss is 4.8597092628479\n",
      "(64, 33)\n",
      "step 7263, loss is 4.786789894104004\n",
      "(64, 33)\n",
      "step 7264, loss is 4.77983283996582\n",
      "(64, 33)\n",
      "step 7265, loss is 4.8366875648498535\n",
      "(64, 33)\n",
      "step 7266, loss is 5.02113151550293\n",
      "(64, 33)\n",
      "step 7267, loss is 4.787445068359375\n",
      "(64, 33)\n",
      "step 7268, loss is 4.86238431930542\n",
      "(64, 33)\n",
      "step 7269, loss is 5.029153347015381\n",
      "(64, 33)\n",
      "step 7270, loss is 4.831849098205566\n",
      "(64, 33)\n",
      "step 7271, loss is 4.968770503997803\n",
      "(64, 33)\n",
      "step 7272, loss is 4.905605316162109\n",
      "(64, 33)\n",
      "step 7273, loss is 5.130063056945801\n",
      "(64, 33)\n",
      "step 7274, loss is 4.808177471160889\n",
      "(64, 33)\n",
      "step 7275, loss is 4.83513069152832\n",
      "(64, 33)\n",
      "step 7276, loss is 4.763396263122559\n",
      "(64, 33)\n",
      "step 7277, loss is 4.748344421386719\n",
      "(64, 33)\n",
      "step 7278, loss is 4.8957037925720215\n",
      "(64, 33)\n",
      "step 7279, loss is 4.836216449737549\n",
      "(64, 33)\n",
      "step 7280, loss is 4.784271240234375\n",
      "(64, 33)\n",
      "step 7281, loss is 4.849298000335693\n",
      "(64, 33)\n",
      "step 7282, loss is 4.880284309387207\n",
      "(64, 33)\n",
      "step 7283, loss is 4.9159932136535645\n",
      "(64, 33)\n",
      "step 7284, loss is 4.975700378417969\n",
      "(64, 33)\n",
      "step 7285, loss is 4.916865348815918\n",
      "(64, 33)\n",
      "step 7286, loss is 4.775075912475586\n",
      "(64, 33)\n",
      "step 7287, loss is 4.86792516708374\n",
      "(64, 33)\n",
      "step 7288, loss is 4.884378910064697\n",
      "(64, 33)\n",
      "step 7289, loss is 4.792039394378662\n",
      "(64, 33)\n",
      "step 7290, loss is 5.032829284667969\n",
      "(64, 33)\n",
      "step 7291, loss is 5.0169572830200195\n",
      "(64, 33)\n",
      "step 7292, loss is 4.810115337371826\n",
      "(64, 33)\n",
      "step 7293, loss is 4.712522983551025\n",
      "(64, 33)\n",
      "step 7294, loss is 5.048285007476807\n",
      "(64, 33)\n",
      "step 7295, loss is 4.905416965484619\n",
      "(64, 33)\n",
      "step 7296, loss is 4.997541904449463\n",
      "(64, 33)\n",
      "step 7297, loss is 4.801843643188477\n",
      "(64, 33)\n",
      "step 7298, loss is 4.972104549407959\n",
      "(64, 33)\n",
      "step 7299, loss is 4.889922142028809\n",
      "(64, 33)\n",
      "step 7300, loss is 4.790980339050293\n",
      "(64, 33)\n",
      "step 7301, loss is 4.6600189208984375\n",
      "(64, 33)\n",
      "step 7302, loss is 4.97376823425293\n",
      "(64, 33)\n",
      "step 7303, loss is 4.798812389373779\n",
      "(64, 33)\n",
      "step 7304, loss is 5.0964274406433105\n",
      "(64, 33)\n",
      "step 7305, loss is 4.912241458892822\n",
      "(64, 33)\n",
      "step 7306, loss is 4.935954570770264\n",
      "(64, 33)\n",
      "step 7307, loss is 4.821365833282471\n",
      "(64, 33)\n",
      "step 7308, loss is 4.966672897338867\n",
      "(64, 33)\n",
      "step 7309, loss is 4.869361877441406\n",
      "(64, 33)\n",
      "step 7310, loss is 4.838250160217285\n",
      "(64, 33)\n",
      "step 7311, loss is 5.129947185516357\n",
      "(64, 33)\n",
      "step 7312, loss is 4.7573771476745605\n",
      "(64, 33)\n",
      "step 7313, loss is 4.895566463470459\n",
      "(64, 33)\n",
      "step 7314, loss is 4.846177577972412\n",
      "(64, 33)\n",
      "step 7315, loss is 4.839337348937988\n",
      "(64, 33)\n",
      "step 7316, loss is 4.7183146476745605\n",
      "(64, 33)\n",
      "step 7317, loss is 4.794374942779541\n",
      "(64, 33)\n",
      "step 7318, loss is 5.03809118270874\n",
      "(64, 33)\n",
      "step 7319, loss is 4.800448417663574\n",
      "(64, 33)\n",
      "step 7320, loss is 4.829299449920654\n",
      "(64, 33)\n",
      "step 7321, loss is 4.882144927978516\n",
      "(64, 33)\n",
      "step 7322, loss is 4.404467582702637\n",
      "(64, 33)\n",
      "step 7323, loss is 4.70696496963501\n",
      "(64, 33)\n",
      "step 7324, loss is 4.735406875610352\n",
      "(64, 33)\n",
      "step 7325, loss is 5.001821994781494\n",
      "(64, 33)\n",
      "step 7326, loss is 4.709536075592041\n",
      "(64, 33)\n",
      "step 7327, loss is 4.913163185119629\n",
      "(64, 33)\n",
      "step 7328, loss is 4.819948673248291\n",
      "(64, 33)\n",
      "step 7329, loss is 4.7688212394714355\n",
      "(64, 33)\n",
      "step 7330, loss is 4.929088115692139\n",
      "(64, 33)\n",
      "step 7331, loss is 4.890768051147461\n",
      "(64, 33)\n",
      "step 7332, loss is 4.895172119140625\n",
      "(64, 33)\n",
      "step 7333, loss is 4.953510761260986\n",
      "(64, 33)\n",
      "step 7334, loss is 5.032384395599365\n",
      "(64, 33)\n",
      "step 7335, loss is 4.9139227867126465\n",
      "(64, 33)\n",
      "step 7336, loss is 4.974365234375\n",
      "(64, 33)\n",
      "step 7337, loss is 4.731428146362305\n",
      "(64, 33)\n",
      "step 7338, loss is 4.837217807769775\n",
      "(64, 33)\n",
      "step 7339, loss is 4.926931858062744\n",
      "(64, 33)\n",
      "step 7340, loss is 5.08707332611084\n",
      "(64, 33)\n",
      "step 7341, loss is 4.995826721191406\n",
      "(64, 33)\n",
      "step 7342, loss is 4.753778457641602\n",
      "(64, 33)\n",
      "step 7343, loss is 4.843354225158691\n",
      "(64, 33)\n",
      "step 7344, loss is 4.911740303039551\n",
      "(64, 33)\n",
      "step 7345, loss is 4.943814754486084\n",
      "(64, 33)\n",
      "step 7346, loss is 4.956389427185059\n",
      "(64, 33)\n",
      "step 7347, loss is 4.602303981781006\n",
      "(64, 33)\n",
      "step 7348, loss is 4.9096598625183105\n",
      "(64, 33)\n",
      "step 7349, loss is 4.7541279792785645\n",
      "(64, 33)\n",
      "step 7350, loss is 4.756376266479492\n",
      "(64, 33)\n",
      "step 7351, loss is 5.016602516174316\n",
      "(64, 33)\n",
      "step 7352, loss is 5.005339622497559\n",
      "(64, 33)\n",
      "step 7353, loss is 4.966314315795898\n",
      "(64, 33)\n",
      "step 7354, loss is 4.925158977508545\n",
      "(64, 33)\n",
      "step 7355, loss is 4.807113170623779\n",
      "(64, 33)\n",
      "step 7356, loss is 4.744165420532227\n",
      "(64, 33)\n",
      "step 7357, loss is 4.8978986740112305\n",
      "(64, 33)\n",
      "step 7358, loss is 4.989266395568848\n",
      "(64, 33)\n",
      "step 7359, loss is 4.944586277008057\n",
      "(64, 33)\n",
      "step 7360, loss is 4.967608451843262\n",
      "(64, 33)\n",
      "step 7361, loss is 5.029768466949463\n",
      "(64, 33)\n",
      "step 7362, loss is 4.996057510375977\n",
      "(64, 33)\n",
      "step 7363, loss is 4.8739447593688965\n",
      "(64, 33)\n",
      "step 7364, loss is 4.864532470703125\n",
      "(64, 33)\n",
      "step 7365, loss is 4.754240989685059\n",
      "(64, 33)\n",
      "step 7366, loss is 4.898138999938965\n",
      "(64, 33)\n",
      "step 7367, loss is 4.691213607788086\n",
      "(64, 33)\n",
      "step 7368, loss is 4.939582824707031\n",
      "(64, 33)\n",
      "step 7369, loss is 4.952678680419922\n",
      "(64, 33)\n",
      "step 7370, loss is 4.845839023590088\n",
      "(64, 33)\n",
      "step 7371, loss is 5.000578880310059\n",
      "(64, 33)\n",
      "step 7372, loss is 4.793651580810547\n",
      "(64, 33)\n",
      "step 7373, loss is 4.786970615386963\n",
      "(64, 33)\n",
      "step 7374, loss is 4.887380599975586\n",
      "(64, 33)\n",
      "step 7375, loss is 4.799647331237793\n",
      "(64, 33)\n",
      "step 7376, loss is 4.77141809463501\n",
      "(64, 33)\n",
      "step 7377, loss is 4.858044624328613\n",
      "(64, 33)\n",
      "step 7378, loss is 4.740912437438965\n",
      "(64, 33)\n",
      "step 7379, loss is 4.79461669921875\n",
      "(64, 33)\n",
      "step 7380, loss is 4.74046516418457\n",
      "(64, 33)\n",
      "step 7381, loss is 4.889656066894531\n",
      "(64, 33)\n",
      "step 7382, loss is 5.19236421585083\n",
      "(64, 33)\n",
      "step 7383, loss is 5.17081880569458\n",
      "(64, 33)\n",
      "step 7384, loss is 4.750069618225098\n",
      "(64, 33)\n",
      "step 7385, loss is 4.671931743621826\n",
      "(64, 33)\n",
      "step 7386, loss is 4.96766996383667\n",
      "(64, 33)\n",
      "step 7387, loss is 4.8168768882751465\n",
      "(64, 33)\n",
      "step 7388, loss is 5.112371444702148\n",
      "(64, 33)\n",
      "step 7389, loss is 4.702564716339111\n",
      "(64, 33)\n",
      "step 7390, loss is 4.9031596183776855\n",
      "(64, 33)\n",
      "step 7391, loss is 4.853221416473389\n",
      "(64, 33)\n",
      "step 7392, loss is 4.890545845031738\n",
      "(64, 33)\n",
      "step 7393, loss is 4.908228397369385\n",
      "(64, 33)\n",
      "step 7394, loss is 5.049472808837891\n",
      "(64, 33)\n",
      "step 7395, loss is 4.789054870605469\n",
      "(64, 33)\n",
      "step 7396, loss is 4.641836643218994\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7397, loss is 4.814043998718262\n",
      "(64, 33)\n",
      "step 7398, loss is 4.863594055175781\n",
      "(64, 33)\n",
      "step 7399, loss is 4.979864597320557\n",
      "(64, 33)\n",
      "step 7400, loss is 4.533649921417236\n",
      "(64, 33)\n",
      "step 7401, loss is 4.759971618652344\n",
      "(64, 33)\n",
      "step 7402, loss is 4.987316131591797\n",
      "(64, 33)\n",
      "step 7403, loss is 4.826599597930908\n",
      "(64, 33)\n",
      "step 7404, loss is 4.798947811126709\n",
      "(64, 33)\n",
      "step 7405, loss is 4.865963935852051\n",
      "(64, 33)\n",
      "step 7406, loss is 4.886471271514893\n",
      "(64, 33)\n",
      "step 7407, loss is 4.618721961975098\n",
      "(64, 33)\n",
      "step 7408, loss is 4.8691253662109375\n",
      "(64, 33)\n",
      "step 7409, loss is 4.984834671020508\n",
      "(64, 33)\n",
      "step 7410, loss is 4.915583610534668\n",
      "(64, 33)\n",
      "step 7411, loss is 4.928583145141602\n",
      "(64, 33)\n",
      "step 7412, loss is 4.816520690917969\n",
      "(64, 33)\n",
      "step 7413, loss is 4.807445526123047\n",
      "(64, 33)\n",
      "step 7414, loss is 4.8114471435546875\n",
      "(64, 33)\n",
      "step 7415, loss is 4.879052639007568\n",
      "(64, 33)\n",
      "step 7416, loss is 4.841753005981445\n",
      "(64, 33)\n",
      "step 7417, loss is 4.977413177490234\n",
      "(64, 33)\n",
      "step 7418, loss is 4.863681793212891\n",
      "(64, 33)\n",
      "step 7419, loss is 4.806991100311279\n",
      "(64, 33)\n",
      "step 7420, loss is 4.9292802810668945\n",
      "(64, 33)\n",
      "step 7421, loss is 4.890528678894043\n",
      "(64, 33)\n",
      "step 7422, loss is 4.741168022155762\n",
      "(64, 33)\n",
      "step 7423, loss is 4.765936374664307\n",
      "(64, 33)\n",
      "step 7424, loss is 4.9445929527282715\n",
      "(64, 33)\n",
      "step 7425, loss is 4.798770904541016\n",
      "(64, 33)\n",
      "step 7426, loss is 4.834486484527588\n",
      "(64, 33)\n",
      "step 7427, loss is 4.726621150970459\n",
      "(64, 33)\n",
      "step 7428, loss is 4.828646659851074\n",
      "(64, 33)\n",
      "step 7429, loss is 4.857032775878906\n",
      "(64, 33)\n",
      "step 7430, loss is 4.7441582679748535\n",
      "(64, 33)\n",
      "step 7431, loss is 4.926383972167969\n",
      "(64, 33)\n",
      "step 7432, loss is 4.8425774574279785\n",
      "(64, 33)\n",
      "step 7433, loss is 4.838441848754883\n",
      "(64, 33)\n",
      "step 7434, loss is 4.9169745445251465\n",
      "(64, 33)\n",
      "step 7435, loss is 4.8481831550598145\n",
      "(64, 33)\n",
      "step 7436, loss is 4.7516961097717285\n",
      "(64, 33)\n",
      "step 7437, loss is 4.914172649383545\n",
      "(64, 33)\n",
      "step 7438, loss is 4.954408645629883\n",
      "(64, 33)\n",
      "step 7439, loss is 4.904640197753906\n",
      "(64, 33)\n",
      "step 7440, loss is 4.749648571014404\n",
      "(64, 33)\n",
      "step 7441, loss is 4.819684982299805\n",
      "(64, 33)\n",
      "step 7442, loss is 4.765402793884277\n",
      "(64, 33)\n",
      "step 7443, loss is 4.738931655883789\n",
      "(64, 33)\n",
      "step 7444, loss is 4.885097503662109\n",
      "(64, 33)\n",
      "step 7445, loss is 4.818686485290527\n",
      "(64, 33)\n",
      "step 7446, loss is 4.909049987792969\n",
      "(64, 33)\n",
      "step 7447, loss is 4.764274597167969\n",
      "(64, 33)\n",
      "step 7448, loss is 4.813525676727295\n",
      "(64, 33)\n",
      "step 7449, loss is 5.05740213394165\n",
      "(64, 33)\n",
      "step 7450, loss is 4.862699031829834\n",
      "(64, 33)\n",
      "step 7451, loss is 4.715320110321045\n",
      "(64, 33)\n",
      "step 7452, loss is 4.842451095581055\n",
      "(64, 33)\n",
      "step 7453, loss is 5.0132012367248535\n",
      "(64, 33)\n",
      "step 7454, loss is 4.690006256103516\n",
      "(64, 33)\n",
      "step 7455, loss is 4.916223049163818\n",
      "(64, 33)\n",
      "step 7456, loss is 4.762507438659668\n",
      "(64, 33)\n",
      "step 7457, loss is 5.096939563751221\n",
      "(64, 33)\n",
      "step 7458, loss is 4.9503493309021\n",
      "(64, 33)\n",
      "step 7459, loss is 4.685969352722168\n",
      "(64, 33)\n",
      "step 7460, loss is 4.951805114746094\n",
      "(64, 33)\n",
      "step 7461, loss is 4.755389213562012\n",
      "(64, 33)\n",
      "step 7462, loss is 4.731232643127441\n",
      "(64, 33)\n",
      "step 7463, loss is 5.073244094848633\n",
      "(64, 33)\n",
      "step 7464, loss is 4.878399848937988\n",
      "(64, 33)\n",
      "step 7465, loss is 4.802396297454834\n",
      "(64, 33)\n",
      "step 7466, loss is 4.8790483474731445\n",
      "(64, 33)\n",
      "step 7467, loss is 5.1319451332092285\n",
      "(64, 33)\n",
      "step 7468, loss is 4.762642860412598\n",
      "(64, 33)\n",
      "step 7469, loss is 4.635410308837891\n",
      "(64, 33)\n",
      "step 7470, loss is 5.02510404586792\n",
      "(64, 33)\n",
      "step 7471, loss is 4.914662837982178\n",
      "(64, 33)\n",
      "step 7472, loss is 4.750574588775635\n",
      "(64, 33)\n",
      "step 7473, loss is 4.921034812927246\n",
      "(64, 33)\n",
      "step 7474, loss is 4.8212056159973145\n",
      "(64, 33)\n",
      "step 7475, loss is 4.671078205108643\n",
      "(64, 33)\n",
      "step 7476, loss is 4.785691261291504\n",
      "(64, 33)\n",
      "step 7477, loss is 4.727926254272461\n",
      "(64, 33)\n",
      "step 7478, loss is 4.983923435211182\n",
      "(64, 33)\n",
      "step 7479, loss is 4.984708786010742\n",
      "(64, 33)\n",
      "step 7480, loss is 4.745530128479004\n",
      "(64, 33)\n",
      "step 7481, loss is 4.988539218902588\n",
      "(64, 33)\n",
      "step 7482, loss is 4.909108638763428\n",
      "(64, 33)\n",
      "step 7483, loss is 4.865262508392334\n",
      "(64, 33)\n",
      "step 7484, loss is 4.931789398193359\n",
      "(64, 33)\n",
      "step 7485, loss is 4.95147705078125\n",
      "(64, 33)\n",
      "step 7486, loss is 4.984177589416504\n",
      "(64, 33)\n",
      "step 7487, loss is 4.931239604949951\n",
      "(64, 33)\n",
      "step 7488, loss is 5.086390018463135\n",
      "(64, 33)\n",
      "step 7489, loss is 4.892208576202393\n",
      "(64, 33)\n",
      "step 7490, loss is 4.933984279632568\n",
      "(64, 33)\n",
      "step 7491, loss is 4.83176851272583\n",
      "(64, 33)\n",
      "step 7492, loss is 4.913051605224609\n",
      "(64, 33)\n",
      "step 7493, loss is 4.71722936630249\n",
      "(64, 33)\n",
      "step 7494, loss is 4.738614082336426\n",
      "(64, 33)\n",
      "step 7495, loss is 4.928013801574707\n",
      "(64, 33)\n",
      "step 7496, loss is 4.797080993652344\n",
      "(64, 33)\n",
      "step 7497, loss is 5.188721179962158\n",
      "(64, 33)\n",
      "step 7498, loss is 4.768547058105469\n",
      "(64, 33)\n",
      "step 7499, loss is 4.774257659912109\n",
      "(64, 33)\n",
      "step 7500, loss is 4.889668941497803\n",
      "(64, 33)\n",
      "step 7501, loss is 4.686098575592041\n",
      "(64, 33)\n",
      "step 7502, loss is 4.694949150085449\n",
      "(64, 33)\n",
      "step 7503, loss is 4.909914970397949\n",
      "(64, 33)\n",
      "step 7504, loss is 4.9508538246154785\n",
      "(64, 33)\n",
      "step 7505, loss is 4.805006504058838\n",
      "(64, 33)\n",
      "step 7506, loss is 5.054541110992432\n",
      "(64, 33)\n",
      "step 7507, loss is 4.799473762512207\n",
      "(64, 33)\n",
      "step 7508, loss is 4.938555717468262\n",
      "(64, 33)\n",
      "step 7509, loss is 5.005641460418701\n",
      "(64, 33)\n",
      "step 7510, loss is 4.881826877593994\n",
      "(64, 33)\n",
      "step 7511, loss is 4.926316261291504\n",
      "(64, 33)\n",
      "step 7512, loss is 4.750781536102295\n",
      "(64, 33)\n",
      "step 7513, loss is 4.917513370513916\n",
      "(64, 33)\n",
      "step 7514, loss is 4.915000915527344\n",
      "(64, 33)\n",
      "step 7515, loss is 4.8984785079956055\n",
      "(64, 33)\n",
      "step 7516, loss is 4.872070789337158\n",
      "(64, 33)\n",
      "step 7517, loss is 5.005708694458008\n",
      "(64, 33)\n",
      "step 7518, loss is 4.615493297576904\n",
      "(64, 33)\n",
      "step 7519, loss is 4.817362308502197\n",
      "(64, 33)\n",
      "step 7520, loss is 4.868789196014404\n",
      "(64, 33)\n",
      "step 7521, loss is 4.976015567779541\n",
      "(64, 33)\n",
      "step 7522, loss is 4.824351787567139\n",
      "(64, 33)\n",
      "step 7523, loss is 4.661965847015381\n",
      "(64, 33)\n",
      "step 7524, loss is 4.82264518737793\n",
      "(64, 33)\n",
      "step 7525, loss is 4.862435817718506\n",
      "(64, 33)\n",
      "step 7526, loss is 5.019872665405273\n",
      "(64, 33)\n",
      "step 7527, loss is 4.628187656402588\n",
      "(64, 33)\n",
      "step 7528, loss is 4.92242956161499\n",
      "(64, 33)\n",
      "step 7529, loss is 4.863707542419434\n",
      "(64, 33)\n",
      "step 7530, loss is 5.018564701080322\n",
      "(64, 33)\n",
      "step 7531, loss is 4.917286396026611\n",
      "(64, 33)\n",
      "step 7532, loss is 4.783856391906738\n",
      "(64, 33)\n",
      "step 7533, loss is 4.771586894989014\n",
      "(64, 33)\n",
      "step 7534, loss is 4.77482271194458\n",
      "(64, 33)\n",
      "step 7535, loss is 4.655333518981934\n",
      "(64, 33)\n",
      "step 7536, loss is 5.0008063316345215\n",
      "(64, 33)\n",
      "step 7537, loss is 4.979630947113037\n",
      "(64, 33)\n",
      "step 7538, loss is 4.789812088012695\n",
      "(64, 33)\n",
      "step 7539, loss is 4.821634769439697\n",
      "(64, 33)\n",
      "step 7540, loss is 4.724664211273193\n",
      "(64, 33)\n",
      "step 7541, loss is 4.9169487953186035\n",
      "(64, 33)\n",
      "step 7542, loss is 5.038814067840576\n",
      "(64, 33)\n",
      "step 7543, loss is 4.977259159088135\n",
      "(64, 33)\n",
      "step 7544, loss is 4.835086345672607\n",
      "(64, 33)\n",
      "step 7545, loss is 4.990511894226074\n",
      "(64, 33)\n",
      "step 7546, loss is 4.782609939575195\n",
      "(64, 33)\n",
      "step 7547, loss is 5.013125419616699\n",
      "(64, 33)\n",
      "step 7548, loss is 5.05927848815918\n",
      "(64, 33)\n",
      "step 7549, loss is 4.880767822265625\n",
      "(64, 33)\n",
      "step 7550, loss is 5.0668816566467285\n",
      "(64, 33)\n",
      "step 7551, loss is 4.926200866699219\n",
      "(64, 33)\n",
      "step 7552, loss is 5.054063320159912\n",
      "(64, 33)\n",
      "step 7553, loss is 4.8564453125\n",
      "(64, 33)\n",
      "step 7554, loss is 4.930872440338135\n",
      "(64, 33)\n",
      "step 7555, loss is 4.951229095458984\n",
      "(64, 33)\n",
      "step 7556, loss is 4.732016563415527\n",
      "(64, 33)\n",
      "step 7557, loss is 4.977729797363281\n",
      "(64, 33)\n",
      "step 7558, loss is 4.990501403808594\n",
      "(64, 33)\n",
      "step 7559, loss is 4.9491400718688965\n",
      "(64, 33)\n",
      "step 7560, loss is 4.8693623542785645\n",
      "(64, 33)\n",
      "step 7561, loss is 4.748106002807617\n",
      "(64, 33)\n",
      "step 7562, loss is 4.7866058349609375\n",
      "(64, 33)\n",
      "step 7563, loss is 4.757752895355225\n",
      "(64, 33)\n",
      "step 7564, loss is 4.8846659660339355\n",
      "(64, 33)\n",
      "step 7565, loss is 4.7312092781066895\n",
      "(64, 33)\n",
      "step 7566, loss is 4.872950553894043\n",
      "(64, 33)\n",
      "step 7567, loss is 4.986615180969238\n",
      "(64, 33)\n",
      "step 7568, loss is 5.046045780181885\n",
      "(64, 33)\n",
      "step 7569, loss is 5.054153919219971\n",
      "(64, 33)\n",
      "step 7570, loss is 4.913259029388428\n",
      "(64, 33)\n",
      "step 7571, loss is 4.797970771789551\n",
      "(64, 33)\n",
      "step 7572, loss is 4.772066116333008\n",
      "(64, 33)\n",
      "step 7573, loss is 4.849303722381592\n",
      "(64, 33)\n",
      "step 7574, loss is 4.808409690856934\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7575, loss is 5.127326488494873\n",
      "(64, 33)\n",
      "step 7576, loss is 4.78599739074707\n",
      "(64, 33)\n",
      "step 7577, loss is 5.072062969207764\n",
      "(64, 33)\n",
      "step 7578, loss is 4.870573997497559\n",
      "(64, 33)\n",
      "step 7579, loss is 4.7908935546875\n",
      "(64, 33)\n",
      "step 7580, loss is 4.862686634063721\n",
      "(64, 33)\n",
      "step 7581, loss is 4.839973449707031\n",
      "(64, 33)\n",
      "step 7582, loss is 4.93776798248291\n",
      "(64, 33)\n",
      "step 7583, loss is 4.8061370849609375\n",
      "(64, 33)\n",
      "step 7584, loss is 4.800473690032959\n",
      "(64, 33)\n",
      "step 7585, loss is 4.740394592285156\n",
      "(64, 33)\n",
      "step 7586, loss is 5.0027995109558105\n",
      "(64, 33)\n",
      "step 7587, loss is 5.022792816162109\n",
      "(64, 33)\n",
      "step 7588, loss is 4.815207004547119\n",
      "(64, 33)\n",
      "step 7589, loss is 4.934049606323242\n",
      "(64, 33)\n",
      "step 7590, loss is 4.874924182891846\n",
      "(64, 33)\n",
      "step 7591, loss is 4.8006205558776855\n",
      "(64, 33)\n",
      "step 7592, loss is 4.91593074798584\n",
      "(64, 33)\n",
      "step 7593, loss is 4.992258071899414\n",
      "(64, 33)\n",
      "step 7594, loss is 5.052523612976074\n",
      "(64, 33)\n",
      "step 7595, loss is 4.728671073913574\n",
      "(64, 33)\n",
      "step 7596, loss is 4.830150604248047\n",
      "(64, 33)\n",
      "step 7597, loss is 4.809440612792969\n",
      "(64, 33)\n",
      "step 7598, loss is 4.590018272399902\n",
      "(64, 33)\n",
      "step 7599, loss is 4.873542308807373\n",
      "(64, 33)\n",
      "step 7600, loss is 4.887259006500244\n",
      "(64, 33)\n",
      "step 7601, loss is 4.975811004638672\n",
      "(64, 33)\n",
      "step 7602, loss is 4.839555740356445\n",
      "(64, 33)\n",
      "step 7603, loss is 4.9019856452941895\n",
      "(64, 33)\n",
      "step 7604, loss is 5.152445316314697\n",
      "(64, 33)\n",
      "step 7605, loss is 5.096585273742676\n",
      "(64, 33)\n",
      "step 7606, loss is 4.77823543548584\n",
      "(64, 33)\n",
      "step 7607, loss is 4.962007999420166\n",
      "(64, 33)\n",
      "step 7608, loss is 4.745151996612549\n",
      "(64, 33)\n",
      "step 7609, loss is 4.908586502075195\n",
      "(64, 33)\n",
      "step 7610, loss is 4.915952682495117\n",
      "(64, 33)\n",
      "step 7611, loss is 4.933055400848389\n",
      "(64, 33)\n",
      "step 7612, loss is 4.809303283691406\n",
      "(64, 33)\n",
      "step 7613, loss is 4.953192710876465\n",
      "(64, 33)\n",
      "step 7614, loss is 4.839328765869141\n",
      "(64, 33)\n",
      "step 7615, loss is 4.826692581176758\n",
      "(64, 33)\n",
      "step 7616, loss is 4.937014579772949\n",
      "(64, 33)\n",
      "step 7617, loss is 4.828524112701416\n",
      "(64, 33)\n",
      "step 7618, loss is 4.994694709777832\n",
      "(64, 33)\n",
      "step 7619, loss is 4.9754719734191895\n",
      "(64, 33)\n",
      "step 7620, loss is 4.868597030639648\n",
      "(64, 33)\n",
      "step 7621, loss is 4.85402774810791\n",
      "(64, 33)\n",
      "step 7622, loss is 4.832574367523193\n",
      "(64, 33)\n",
      "step 7623, loss is 4.8403215408325195\n",
      "(64, 33)\n",
      "step 7624, loss is 4.957465171813965\n",
      "(64, 33)\n",
      "step 7625, loss is 4.822399616241455\n",
      "(64, 33)\n",
      "step 7626, loss is 4.949666976928711\n",
      "(64, 33)\n",
      "step 7627, loss is 4.874020099639893\n",
      "(64, 33)\n",
      "step 7628, loss is 4.764059066772461\n",
      "(64, 33)\n",
      "step 7629, loss is 4.6412272453308105\n",
      "(64, 33)\n",
      "step 7630, loss is 4.867400169372559\n",
      "(64, 33)\n",
      "step 7631, loss is 4.909947872161865\n",
      "(64, 33)\n",
      "step 7632, loss is 4.969970703125\n",
      "(64, 33)\n",
      "step 7633, loss is 4.857723236083984\n",
      "(64, 33)\n",
      "step 7634, loss is 4.697663307189941\n",
      "(64, 33)\n",
      "step 7635, loss is 4.863790988922119\n",
      "(64, 33)\n",
      "step 7636, loss is 4.916515827178955\n",
      "(64, 33)\n",
      "step 7637, loss is 4.827775955200195\n",
      "(64, 33)\n",
      "step 7638, loss is 4.991315841674805\n",
      "(64, 33)\n",
      "step 7639, loss is 4.885777950286865\n",
      "(64, 33)\n",
      "step 7640, loss is 4.874212741851807\n",
      "(64, 33)\n",
      "step 7641, loss is 4.958137512207031\n",
      "(64, 33)\n",
      "step 7642, loss is 5.081594944000244\n",
      "(64, 33)\n",
      "step 7643, loss is 4.968464374542236\n",
      "(64, 33)\n",
      "step 7644, loss is 5.109202861785889\n",
      "(64, 33)\n",
      "step 7645, loss is 4.818359851837158\n",
      "(64, 33)\n",
      "step 7646, loss is 4.914648056030273\n",
      "(64, 33)\n",
      "step 7647, loss is 4.866449356079102\n",
      "(64, 33)\n",
      "step 7648, loss is 4.782658100128174\n",
      "(64, 33)\n",
      "step 7649, loss is 4.881731033325195\n",
      "(64, 33)\n",
      "step 7650, loss is 4.85957670211792\n",
      "(64, 33)\n",
      "step 7651, loss is 5.045328617095947\n",
      "(64, 33)\n",
      "step 7652, loss is 4.742815971374512\n",
      "(64, 33)\n",
      "step 7653, loss is 4.750453472137451\n",
      "(64, 33)\n",
      "step 7654, loss is 4.940483093261719\n",
      "(64, 33)\n",
      "step 7655, loss is 4.931046009063721\n",
      "(64, 33)\n",
      "step 7656, loss is 4.7882819175720215\n",
      "(64, 33)\n",
      "step 7657, loss is 4.989847660064697\n",
      "(64, 33)\n",
      "step 7658, loss is 4.871060371398926\n",
      "(64, 33)\n",
      "step 7659, loss is 4.981513977050781\n",
      "(64, 33)\n",
      "step 7660, loss is 5.014186859130859\n",
      "(64, 33)\n",
      "step 7661, loss is 4.811479568481445\n",
      "(64, 33)\n",
      "step 7662, loss is 5.05364990234375\n",
      "(64, 33)\n",
      "step 7663, loss is 4.801577568054199\n",
      "(64, 33)\n",
      "step 7664, loss is 4.950667381286621\n",
      "(64, 33)\n",
      "step 7665, loss is 4.835862636566162\n",
      "(64, 33)\n",
      "step 7666, loss is 4.808610439300537\n",
      "(64, 33)\n",
      "step 7667, loss is 4.932600021362305\n",
      "(64, 33)\n",
      "step 7668, loss is 4.958817958831787\n",
      "(64, 33)\n",
      "step 7669, loss is 4.91924524307251\n",
      "(64, 33)\n",
      "step 7670, loss is 4.8795976638793945\n",
      "(64, 33)\n",
      "step 7671, loss is 4.803422451019287\n",
      "(64, 33)\n",
      "step 7672, loss is 4.799533367156982\n",
      "(64, 33)\n",
      "step 7673, loss is 5.090402603149414\n",
      "(64, 33)\n",
      "step 7674, loss is 4.896712779998779\n",
      "(64, 33)\n",
      "step 7675, loss is 4.961455345153809\n",
      "(64, 33)\n",
      "step 7676, loss is 4.847048759460449\n",
      "(64, 33)\n",
      "step 7677, loss is 4.99240779876709\n",
      "(64, 33)\n",
      "step 7678, loss is 4.9041900634765625\n",
      "(64, 33)\n",
      "step 7679, loss is 5.011047840118408\n",
      "(64, 33)\n",
      "step 7680, loss is 5.090341091156006\n",
      "(64, 33)\n",
      "step 7681, loss is 4.989887714385986\n",
      "(64, 33)\n",
      "step 7682, loss is 4.852745532989502\n",
      "(64, 33)\n",
      "step 7683, loss is 5.129976749420166\n",
      "(64, 33)\n",
      "step 7684, loss is 4.739511966705322\n",
      "(64, 33)\n",
      "step 7685, loss is 4.798920154571533\n",
      "(64, 33)\n",
      "step 7686, loss is 4.684011936187744\n",
      "(64, 33)\n",
      "step 7687, loss is 5.082353591918945\n",
      "(64, 33)\n",
      "step 7688, loss is 4.944324016571045\n",
      "(64, 33)\n",
      "step 7689, loss is 4.833883762359619\n",
      "(64, 33)\n",
      "step 7690, loss is 4.985188007354736\n",
      "(64, 33)\n",
      "step 7691, loss is 4.942502021789551\n",
      "(64, 33)\n",
      "step 7692, loss is 5.066413402557373\n",
      "(64, 33)\n",
      "step 7693, loss is 4.742994785308838\n",
      "(64, 33)\n",
      "step 7694, loss is 5.027047157287598\n",
      "(64, 33)\n",
      "step 7695, loss is 4.954692363739014\n",
      "(64, 33)\n",
      "step 7696, loss is 5.069230079650879\n",
      "(64, 33)\n",
      "step 7697, loss is 4.893348217010498\n",
      "(64, 33)\n",
      "step 7698, loss is 4.727779865264893\n",
      "(64, 33)\n",
      "step 7699, loss is 5.035593509674072\n",
      "(64, 33)\n",
      "step 7700, loss is 4.968451976776123\n",
      "(64, 33)\n",
      "step 7701, loss is 4.840920925140381\n",
      "(64, 33)\n",
      "step 7702, loss is 4.877486228942871\n",
      "(64, 33)\n",
      "step 7703, loss is 4.877213478088379\n",
      "(64, 33)\n",
      "step 7704, loss is 4.695230960845947\n",
      "(64, 33)\n",
      "step 7705, loss is 5.038081645965576\n",
      "(64, 33)\n",
      "step 7706, loss is 4.827122688293457\n",
      "(64, 33)\n",
      "step 7707, loss is 4.765945911407471\n",
      "(64, 33)\n",
      "step 7708, loss is 4.916910648345947\n",
      "(64, 33)\n",
      "step 7709, loss is 4.742703437805176\n",
      "(64, 33)\n",
      "step 7710, loss is 4.792252540588379\n",
      "(64, 33)\n",
      "step 7711, loss is 4.953887939453125\n",
      "(64, 33)\n",
      "step 7712, loss is 4.7901225090026855\n",
      "(64, 33)\n",
      "step 7713, loss is 4.86685848236084\n",
      "(64, 33)\n",
      "step 7714, loss is 5.1244893074035645\n",
      "(64, 33)\n",
      "step 7715, loss is 4.786240100860596\n",
      "(64, 33)\n",
      "step 7716, loss is 4.882110595703125\n",
      "(64, 33)\n",
      "step 7717, loss is 4.830299377441406\n",
      "(64, 33)\n",
      "step 7718, loss is 4.70367431640625\n",
      "(64, 33)\n",
      "step 7719, loss is 4.925088405609131\n",
      "(64, 33)\n",
      "step 7720, loss is 4.859344959259033\n",
      "(64, 33)\n",
      "step 7721, loss is 5.002647399902344\n",
      "(64, 33)\n",
      "step 7722, loss is 4.892378330230713\n",
      "(64, 33)\n",
      "step 7723, loss is 4.7146806716918945\n",
      "(64, 33)\n",
      "step 7724, loss is 4.903383731842041\n",
      "(64, 33)\n",
      "step 7725, loss is 4.724742412567139\n",
      "(64, 33)\n",
      "step 7726, loss is 4.8785552978515625\n",
      "(64, 33)\n",
      "step 7727, loss is 4.913290977478027\n",
      "(64, 33)\n",
      "step 7728, loss is 4.793666839599609\n",
      "(64, 33)\n",
      "step 7729, loss is 5.083980083465576\n",
      "(64, 33)\n",
      "step 7730, loss is 4.844629287719727\n",
      "(64, 33)\n",
      "step 7731, loss is 4.841679096221924\n",
      "(64, 33)\n",
      "step 7732, loss is 4.804622650146484\n",
      "(64, 33)\n",
      "step 7733, loss is 4.696819305419922\n",
      "(64, 33)\n",
      "step 7734, loss is 4.727816581726074\n",
      "(64, 33)\n",
      "step 7735, loss is 4.891324996948242\n",
      "(64, 33)\n",
      "step 7736, loss is 5.032771587371826\n",
      "(64, 33)\n",
      "step 7737, loss is 4.993965148925781\n",
      "(64, 33)\n",
      "step 7738, loss is 4.76910400390625\n",
      "(64, 33)\n",
      "step 7739, loss is 5.046980381011963\n",
      "(64, 33)\n",
      "step 7740, loss is 5.115203857421875\n",
      "(64, 33)\n",
      "step 7741, loss is 4.974578380584717\n",
      "(64, 33)\n",
      "step 7742, loss is 4.918058395385742\n",
      "(64, 33)\n",
      "step 7743, loss is 4.928860187530518\n",
      "(64, 33)\n",
      "step 7744, loss is 4.850649356842041\n",
      "(64, 33)\n",
      "step 7745, loss is 4.637604713439941\n",
      "(64, 33)\n",
      "step 7746, loss is 4.872165203094482\n",
      "(64, 33)\n",
      "step 7747, loss is 4.863785743713379\n",
      "(64, 33)\n",
      "step 7748, loss is 4.908453941345215\n",
      "(64, 33)\n",
      "step 7749, loss is 4.99157190322876\n",
      "(64, 33)\n",
      "step 7750, loss is 4.930989742279053\n",
      "(64, 33)\n",
      "step 7751, loss is 4.92807149887085\n",
      "(64, 33)\n",
      "step 7752, loss is 4.767756462097168\n",
      "(64, 33)\n",
      "step 7753, loss is 4.831730842590332\n",
      "(64, 33)\n",
      "step 7754, loss is 4.756476402282715\n",
      "(64, 33)\n",
      "step 7755, loss is 4.90816593170166\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7756, loss is 4.865121841430664\n",
      "(64, 33)\n",
      "step 7757, loss is 4.664443016052246\n",
      "(64, 33)\n",
      "step 7758, loss is 4.949634075164795\n",
      "(64, 33)\n",
      "step 7759, loss is 4.939133644104004\n",
      "(64, 33)\n",
      "step 7760, loss is 4.847766876220703\n",
      "(64, 33)\n",
      "step 7761, loss is 4.972280979156494\n",
      "(64, 33)\n",
      "step 7762, loss is 4.836215019226074\n",
      "(64, 33)\n",
      "step 7763, loss is 4.943207263946533\n",
      "(64, 33)\n",
      "step 7764, loss is 5.000053405761719\n",
      "(64, 33)\n",
      "step 7765, loss is 4.794238567352295\n",
      "(64, 33)\n",
      "step 7766, loss is 5.018887519836426\n",
      "(64, 33)\n",
      "step 7767, loss is 4.703037738800049\n",
      "(64, 33)\n",
      "step 7768, loss is 4.943450927734375\n",
      "(64, 33)\n",
      "step 7769, loss is 4.740424633026123\n",
      "(64, 33)\n",
      "step 7770, loss is 4.888373374938965\n",
      "(64, 33)\n",
      "step 7771, loss is 4.854445934295654\n",
      "(64, 33)\n",
      "step 7772, loss is 4.77678918838501\n",
      "(64, 33)\n",
      "step 7773, loss is 4.878931522369385\n",
      "(64, 33)\n",
      "step 7774, loss is 5.003413200378418\n",
      "(64, 33)\n",
      "step 7775, loss is 4.793964862823486\n",
      "(64, 33)\n",
      "step 7776, loss is 4.739013195037842\n",
      "(64, 33)\n",
      "step 7777, loss is 4.657658100128174\n",
      "(64, 33)\n",
      "step 7778, loss is 4.895502090454102\n",
      "(64, 33)\n",
      "step 7779, loss is 4.8985161781311035\n",
      "(64, 33)\n",
      "step 7780, loss is 4.953799724578857\n",
      "(64, 33)\n",
      "step 7781, loss is 4.842281341552734\n",
      "(64, 33)\n",
      "step 7782, loss is 4.893894672393799\n",
      "(64, 33)\n",
      "step 7783, loss is 4.827372074127197\n",
      "(64, 33)\n",
      "step 7784, loss is 4.640530586242676\n",
      "(64, 33)\n",
      "step 7785, loss is 4.786317825317383\n",
      "(64, 33)\n",
      "step 7786, loss is 4.897091388702393\n",
      "(64, 33)\n",
      "step 7787, loss is 5.024434566497803\n",
      "(64, 33)\n",
      "step 7788, loss is 4.828237056732178\n",
      "(64, 33)\n",
      "step 7789, loss is 4.78428316116333\n",
      "(64, 33)\n",
      "step 7790, loss is 4.972835063934326\n",
      "(64, 33)\n",
      "step 7791, loss is 4.766631126403809\n",
      "(64, 33)\n",
      "step 7792, loss is 4.780490875244141\n",
      "(64, 33)\n",
      "step 7793, loss is 4.8495707511901855\n",
      "(64, 33)\n",
      "step 7794, loss is 4.827701568603516\n",
      "(64, 33)\n",
      "step 7795, loss is 4.82464599609375\n",
      "(64, 33)\n",
      "step 7796, loss is 4.989932060241699\n",
      "(64, 33)\n",
      "step 7797, loss is 4.973822116851807\n",
      "(64, 33)\n",
      "step 7798, loss is 4.891401767730713\n",
      "(64, 33)\n",
      "step 7799, loss is 4.959353923797607\n",
      "(64, 33)\n",
      "step 7800, loss is 5.023508071899414\n",
      "(64, 33)\n",
      "step 7801, loss is 4.92700719833374\n",
      "(64, 33)\n",
      "step 7802, loss is 4.934314250946045\n",
      "(64, 33)\n",
      "step 7803, loss is 5.0951642990112305\n",
      "(64, 33)\n",
      "step 7804, loss is 4.913296222686768\n",
      "(64, 33)\n",
      "step 7805, loss is 4.891284465789795\n",
      "(64, 33)\n",
      "step 7806, loss is 4.904904842376709\n",
      "(64, 33)\n",
      "step 7807, loss is 4.854252338409424\n",
      "(64, 33)\n",
      "step 7808, loss is 5.082354545593262\n",
      "(64, 33)\n",
      "step 7809, loss is 5.066144943237305\n",
      "(64, 33)\n",
      "step 7810, loss is 5.085007667541504\n",
      "(64, 33)\n",
      "step 7811, loss is 5.013545989990234\n",
      "(64, 33)\n",
      "step 7812, loss is 4.990516662597656\n",
      "(64, 33)\n",
      "step 7813, loss is 4.92626953125\n",
      "(64, 33)\n",
      "step 7814, loss is 4.993158340454102\n",
      "(64, 33)\n",
      "step 7815, loss is 4.859939098358154\n",
      "(64, 33)\n",
      "step 7816, loss is 4.973704814910889\n",
      "(64, 33)\n",
      "step 7817, loss is 4.906268119812012\n",
      "(64, 33)\n",
      "step 7818, loss is 4.934365749359131\n",
      "(64, 33)\n",
      "step 7819, loss is 4.963684558868408\n",
      "(64, 33)\n",
      "step 7820, loss is 4.873587608337402\n",
      "(64, 33)\n",
      "step 7821, loss is 4.686387538909912\n",
      "(64, 33)\n",
      "step 7822, loss is 4.920570373535156\n",
      "(64, 33)\n",
      "step 7823, loss is 4.847218036651611\n",
      "(64, 33)\n",
      "step 7824, loss is 4.795024871826172\n",
      "(64, 33)\n",
      "step 7825, loss is 4.924856662750244\n",
      "(64, 33)\n",
      "step 7826, loss is 4.812480926513672\n",
      "(64, 33)\n",
      "step 7827, loss is 4.968047618865967\n",
      "(64, 33)\n",
      "step 7828, loss is 4.728149890899658\n",
      "(64, 33)\n",
      "step 7829, loss is 4.776956558227539\n",
      "(64, 33)\n",
      "step 7830, loss is 4.702989101409912\n",
      "(64, 33)\n",
      "step 7831, loss is 4.970832824707031\n",
      "(64, 33)\n",
      "step 7832, loss is 4.715221881866455\n",
      "(64, 33)\n",
      "step 7833, loss is 4.8395280838012695\n",
      "(64, 33)\n",
      "step 7834, loss is 5.048496723175049\n",
      "(64, 33)\n",
      "step 7835, loss is 4.8968186378479\n",
      "(64, 33)\n",
      "step 7836, loss is 4.925648212432861\n",
      "(64, 33)\n",
      "step 7837, loss is 4.756316661834717\n",
      "(64, 33)\n",
      "step 7838, loss is 5.028793811798096\n",
      "(64, 33)\n",
      "step 7839, loss is 4.883387088775635\n",
      "(64, 33)\n",
      "step 7840, loss is 4.926948547363281\n",
      "(64, 33)\n",
      "step 7841, loss is 4.901279926300049\n",
      "(64, 33)\n",
      "step 7842, loss is 5.00016450881958\n",
      "(64, 33)\n",
      "step 7843, loss is 4.946177959442139\n",
      "(64, 33)\n",
      "step 7844, loss is 4.8724188804626465\n",
      "(64, 33)\n",
      "step 7845, loss is 5.0408830642700195\n",
      "(64, 33)\n",
      "step 7846, loss is 4.749770164489746\n",
      "(64, 33)\n",
      "step 7847, loss is 4.90044641494751\n",
      "(64, 33)\n",
      "step 7848, loss is 4.842463493347168\n",
      "(64, 33)\n",
      "step 7849, loss is 4.832346439361572\n",
      "(64, 33)\n",
      "step 7850, loss is 4.836819648742676\n",
      "(64, 33)\n",
      "step 7851, loss is 4.908468246459961\n",
      "(64, 33)\n",
      "step 7852, loss is 5.00126314163208\n",
      "(64, 33)\n",
      "step 7853, loss is 4.963014125823975\n",
      "(64, 33)\n",
      "step 7854, loss is 5.027198791503906\n",
      "(64, 33)\n",
      "step 7855, loss is 4.884572505950928\n",
      "(64, 33)\n",
      "step 7856, loss is 4.895354747772217\n",
      "(64, 33)\n",
      "step 7857, loss is 5.026183605194092\n",
      "(64, 33)\n",
      "step 7858, loss is 4.983086585998535\n",
      "(64, 33)\n",
      "step 7859, loss is 4.676828861236572\n",
      "(64, 33)\n",
      "step 7860, loss is 4.920705318450928\n",
      "(64, 33)\n",
      "step 7861, loss is 4.825857162475586\n",
      "(64, 33)\n",
      "step 7862, loss is 5.021106243133545\n",
      "(64, 33)\n",
      "step 7863, loss is 4.672562122344971\n",
      "(64, 33)\n",
      "step 7864, loss is 5.008047580718994\n",
      "(64, 33)\n",
      "step 7865, loss is 4.7784929275512695\n",
      "(64, 33)\n",
      "step 7866, loss is 5.018258571624756\n",
      "(64, 33)\n",
      "step 7867, loss is 4.944301128387451\n",
      "(64, 33)\n",
      "step 7868, loss is 4.7227654457092285\n",
      "(64, 33)\n",
      "step 7869, loss is 4.746477127075195\n",
      "(64, 33)\n",
      "step 7870, loss is 4.908769607543945\n",
      "(64, 33)\n",
      "step 7871, loss is 4.954680919647217\n",
      "(64, 33)\n",
      "step 7872, loss is 4.836484909057617\n",
      "(64, 33)\n",
      "step 7873, loss is 4.912647247314453\n",
      "(64, 33)\n",
      "step 7874, loss is 4.830729961395264\n",
      "(64, 33)\n",
      "step 7875, loss is 4.608432769775391\n",
      "(64, 33)\n",
      "step 7876, loss is 4.9299421310424805\n",
      "(64, 33)\n",
      "step 7877, loss is 4.993523597717285\n",
      "(64, 33)\n",
      "step 7878, loss is 4.731328010559082\n",
      "(64, 33)\n",
      "step 7879, loss is 4.866085052490234\n",
      "(64, 33)\n",
      "step 7880, loss is 4.8853888511657715\n",
      "(64, 33)\n",
      "step 7881, loss is 4.919701099395752\n",
      "(64, 33)\n",
      "step 7882, loss is 4.819427013397217\n",
      "(64, 33)\n",
      "step 7883, loss is 5.04955530166626\n",
      "(64, 33)\n",
      "step 7884, loss is 4.709214687347412\n",
      "(64, 33)\n",
      "step 7885, loss is 4.959931373596191\n",
      "(64, 33)\n",
      "step 7886, loss is 4.821693420410156\n",
      "(64, 33)\n",
      "step 7887, loss is 4.957729339599609\n",
      "(64, 33)\n",
      "step 7888, loss is 4.864112854003906\n",
      "(64, 33)\n",
      "step 7889, loss is 4.931215763092041\n",
      "(64, 33)\n",
      "step 7890, loss is 4.815860271453857\n",
      "(64, 33)\n",
      "step 7891, loss is 4.844134330749512\n",
      "(64, 33)\n",
      "step 7892, loss is 5.042176246643066\n",
      "(64, 33)\n",
      "step 7893, loss is 5.0288801193237305\n",
      "(64, 33)\n",
      "step 7894, loss is 4.825499534606934\n",
      "(64, 33)\n",
      "step 7895, loss is 5.03131103515625\n",
      "(64, 33)\n",
      "step 7896, loss is 4.7714619636535645\n",
      "(64, 33)\n",
      "step 7897, loss is 4.863764762878418\n",
      "(64, 33)\n",
      "step 7898, loss is 4.9396467208862305\n",
      "(64, 33)\n",
      "step 7899, loss is 4.908369541168213\n",
      "(64, 33)\n",
      "step 7900, loss is 4.977956295013428\n",
      "(64, 33)\n",
      "step 7901, loss is 4.937772750854492\n",
      "(64, 33)\n",
      "step 7902, loss is 4.937743663787842\n",
      "(64, 33)\n",
      "step 7903, loss is 4.870288372039795\n",
      "(64, 33)\n",
      "step 7904, loss is 4.927791595458984\n",
      "(64, 33)\n",
      "step 7905, loss is 4.759678840637207\n",
      "(64, 33)\n",
      "step 7906, loss is 5.00376558303833\n",
      "(64, 33)\n",
      "step 7907, loss is 4.966498374938965\n",
      "(64, 33)\n",
      "step 7908, loss is 4.8917670249938965\n",
      "(64, 33)\n",
      "step 7909, loss is 4.8298211097717285\n",
      "(64, 33)\n",
      "step 7910, loss is 4.839787483215332\n",
      "(64, 33)\n",
      "step 7911, loss is 4.9339165687561035\n",
      "(64, 33)\n",
      "step 7912, loss is 4.794669151306152\n",
      "(64, 33)\n",
      "step 7913, loss is 4.974969387054443\n",
      "(64, 33)\n",
      "step 7914, loss is 4.807295322418213\n",
      "(64, 33)\n",
      "step 7915, loss is 5.013321399688721\n",
      "(64, 33)\n",
      "step 7916, loss is 4.981626033782959\n",
      "(64, 33)\n",
      "step 7917, loss is 4.782626628875732\n",
      "(64, 33)\n",
      "step 7918, loss is 4.87982177734375\n",
      "(64, 33)\n",
      "step 7919, loss is 4.86615514755249\n",
      "(64, 33)\n",
      "step 7920, loss is 4.868089199066162\n",
      "(64, 33)\n",
      "step 7921, loss is 4.616717338562012\n",
      "(64, 33)\n",
      "step 7922, loss is 4.778267860412598\n",
      "(64, 33)\n",
      "step 7923, loss is 4.958600997924805\n",
      "(64, 33)\n",
      "step 7924, loss is 4.864655494689941\n",
      "(64, 33)\n",
      "step 7925, loss is 4.793301105499268\n",
      "(64, 33)\n",
      "step 7926, loss is 4.790142059326172\n",
      "(64, 33)\n",
      "step 7927, loss is 4.761891841888428\n",
      "(64, 33)\n",
      "step 7928, loss is 4.816761016845703\n",
      "(64, 33)\n",
      "step 7929, loss is 4.965823173522949\n",
      "(64, 33)\n",
      "step 7930, loss is 5.016327381134033\n",
      "(64, 33)\n",
      "step 7931, loss is 4.864855766296387\n",
      "(64, 33)\n",
      "step 7932, loss is 4.872619152069092\n",
      "(64, 33)\n",
      "step 7933, loss is 4.690979480743408\n",
      "(64, 33)\n",
      "step 7934, loss is 4.820333480834961\n",
      "(64, 33)\n",
      "step 7935, loss is 4.891029357910156\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7936, loss is 4.895103454589844\n",
      "(64, 33)\n",
      "step 7937, loss is 4.816109657287598\n",
      "(64, 33)\n",
      "step 7938, loss is 4.987117290496826\n",
      "(64, 33)\n",
      "step 7939, loss is 4.960315227508545\n",
      "(64, 33)\n",
      "step 7940, loss is 4.904943943023682\n",
      "(64, 33)\n",
      "step 7941, loss is 4.941600322723389\n",
      "(64, 33)\n",
      "step 7942, loss is 4.692249298095703\n",
      "(64, 33)\n",
      "step 7943, loss is 4.822327136993408\n",
      "(64, 33)\n",
      "step 7944, loss is 4.701393127441406\n",
      "(64, 33)\n",
      "step 7945, loss is 4.992930889129639\n",
      "(64, 33)\n",
      "step 7946, loss is 4.968008518218994\n",
      "(64, 33)\n",
      "step 7947, loss is 4.942998886108398\n",
      "(64, 33)\n",
      "step 7948, loss is 4.810628414154053\n",
      "(64, 33)\n",
      "step 7949, loss is 4.8882222175598145\n",
      "(64, 33)\n",
      "step 7950, loss is 4.890093803405762\n",
      "(64, 33)\n",
      "step 7951, loss is 4.8668036460876465\n",
      "(64, 33)\n",
      "step 7952, loss is 4.845076560974121\n",
      "(64, 33)\n",
      "step 7953, loss is 4.74153470993042\n",
      "(64, 33)\n",
      "step 7954, loss is 4.837697505950928\n",
      "(64, 33)\n",
      "step 7955, loss is 4.8080735206604\n",
      "(64, 33)\n",
      "step 7956, loss is 4.858617782592773\n",
      "(64, 33)\n",
      "step 7957, loss is 4.853184700012207\n",
      "(64, 33)\n",
      "step 7958, loss is 4.710935115814209\n",
      "(64, 33)\n",
      "step 7959, loss is 4.862025260925293\n",
      "(64, 33)\n",
      "step 7960, loss is 4.781905651092529\n",
      "(64, 33)\n",
      "step 7961, loss is 4.985355854034424\n",
      "(64, 33)\n",
      "step 7962, loss is 4.913854598999023\n",
      "(64, 33)\n",
      "step 7963, loss is 4.919760227203369\n",
      "(64, 33)\n",
      "step 7964, loss is 4.772082805633545\n",
      "(64, 33)\n",
      "step 7965, loss is 4.795961856842041\n",
      "(64, 33)\n",
      "step 7966, loss is 4.755136966705322\n",
      "(64, 33)\n",
      "step 7967, loss is 4.844531059265137\n",
      "(64, 33)\n",
      "step 7968, loss is 4.913777828216553\n",
      "(64, 33)\n",
      "step 7969, loss is 4.748040199279785\n",
      "(64, 33)\n",
      "step 7970, loss is 4.859159469604492\n",
      "(64, 33)\n",
      "step 7971, loss is 4.833463668823242\n",
      "(64, 33)\n",
      "step 7972, loss is 4.828161716461182\n",
      "(64, 33)\n",
      "step 7973, loss is 4.763658046722412\n",
      "(64, 33)\n",
      "step 7974, loss is 4.700060844421387\n",
      "(64, 33)\n",
      "step 7975, loss is 5.083451271057129\n",
      "(64, 33)\n",
      "step 7976, loss is 4.869135856628418\n",
      "(64, 33)\n",
      "step 7977, loss is 4.768571376800537\n",
      "(64, 33)\n",
      "step 7978, loss is 4.732682704925537\n",
      "(64, 33)\n",
      "step 7979, loss is 4.777082920074463\n",
      "(64, 33)\n",
      "step 7980, loss is 4.775228977203369\n",
      "(64, 33)\n",
      "step 7981, loss is 4.824985980987549\n",
      "(64, 33)\n",
      "step 7982, loss is 4.880183696746826\n",
      "(64, 33)\n",
      "step 7983, loss is 4.875982761383057\n",
      "(64, 33)\n",
      "step 7984, loss is 4.883602142333984\n",
      "(64, 33)\n",
      "step 7985, loss is 4.793087482452393\n",
      "(64, 33)\n",
      "step 7986, loss is 4.783030986785889\n",
      "(64, 33)\n",
      "step 7987, loss is 5.00353479385376\n",
      "(64, 33)\n",
      "step 7988, loss is 4.852929592132568\n",
      "(64, 33)\n",
      "step 7989, loss is 4.936072826385498\n",
      "(64, 33)\n",
      "step 7990, loss is 4.837819576263428\n",
      "(64, 33)\n",
      "step 7991, loss is 4.860941410064697\n",
      "(64, 33)\n",
      "step 7992, loss is 4.738206386566162\n",
      "(64, 33)\n",
      "step 7993, loss is 4.8001813888549805\n",
      "(64, 33)\n",
      "step 7994, loss is 4.876373291015625\n",
      "(64, 33)\n",
      "step 7995, loss is 4.825414180755615\n",
      "(64, 33)\n",
      "step 7996, loss is 4.978835582733154\n",
      "(64, 33)\n",
      "step 7997, loss is 4.775180816650391\n",
      "(64, 33)\n",
      "step 7998, loss is 4.730799198150635\n",
      "(64, 33)\n",
      "step 7999, loss is 4.940380096435547\n",
      "(64, 33)\n",
      "step 8000, loss is 4.877687454223633\n",
      "(64, 33)\n",
      "step 8001, loss is 4.75081729888916\n",
      "(64, 33)\n",
      "step 8002, loss is 4.821959018707275\n",
      "(64, 33)\n",
      "step 8003, loss is 4.788456916809082\n",
      "(64, 33)\n",
      "step 8004, loss is 4.836701393127441\n",
      "(64, 33)\n",
      "step 8005, loss is 4.897894382476807\n",
      "(64, 33)\n",
      "step 8006, loss is 4.735012054443359\n",
      "(64, 33)\n",
      "step 8007, loss is 4.810166358947754\n",
      "(64, 33)\n",
      "step 8008, loss is 4.822334289550781\n",
      "(64, 33)\n",
      "step 8009, loss is 4.868310928344727\n",
      "(64, 33)\n",
      "step 8010, loss is 5.052609920501709\n",
      "(64, 33)\n",
      "step 8011, loss is 4.828864097595215\n",
      "(64, 33)\n",
      "step 8012, loss is 4.960838317871094\n",
      "(64, 33)\n",
      "step 8013, loss is 4.785562038421631\n",
      "(64, 33)\n",
      "step 8014, loss is 5.065741539001465\n",
      "(64, 33)\n",
      "step 8015, loss is 4.916681289672852\n",
      "(64, 33)\n",
      "step 8016, loss is 4.717310428619385\n",
      "(64, 33)\n",
      "step 8017, loss is 4.926291465759277\n",
      "(64, 33)\n",
      "step 8018, loss is 4.849963665008545\n",
      "(64, 33)\n",
      "step 8019, loss is 4.96745491027832\n",
      "(64, 33)\n",
      "step 8020, loss is 4.870973587036133\n",
      "(64, 33)\n",
      "step 8021, loss is 4.5912089347839355\n",
      "(64, 33)\n",
      "step 8022, loss is 4.7911858558654785\n",
      "(64, 33)\n",
      "step 8023, loss is 4.815809726715088\n",
      "(64, 33)\n",
      "step 8024, loss is 4.845246315002441\n",
      "(64, 33)\n",
      "step 8025, loss is 4.72758150100708\n",
      "(64, 33)\n",
      "step 8026, loss is 4.8998332023620605\n",
      "(64, 33)\n",
      "step 8027, loss is 4.586530685424805\n",
      "(64, 33)\n",
      "step 8028, loss is 4.85809850692749\n",
      "(64, 33)\n",
      "step 8029, loss is 4.84579610824585\n",
      "(64, 33)\n",
      "step 8030, loss is 4.801654815673828\n",
      "(64, 33)\n",
      "step 8031, loss is 4.8117241859436035\n",
      "(64, 33)\n",
      "step 8032, loss is 4.816880702972412\n",
      "(64, 33)\n",
      "step 8033, loss is 4.994292736053467\n",
      "(64, 33)\n",
      "step 8034, loss is 4.855501174926758\n",
      "(64, 33)\n",
      "step 8035, loss is 4.718754768371582\n",
      "(64, 33)\n",
      "step 8036, loss is 4.830333232879639\n",
      "(64, 33)\n",
      "step 8037, loss is 4.990606307983398\n",
      "(64, 33)\n",
      "step 8038, loss is 4.849040985107422\n",
      "(64, 33)\n",
      "step 8039, loss is 4.930306434631348\n",
      "(64, 33)\n",
      "step 8040, loss is 4.827580451965332\n",
      "(64, 33)\n",
      "step 8041, loss is 4.884003162384033\n",
      "(64, 33)\n",
      "step 8042, loss is 4.873374938964844\n",
      "(64, 33)\n",
      "step 8043, loss is 4.8202128410339355\n",
      "(64, 33)\n",
      "step 8044, loss is 4.926477432250977\n",
      "(64, 33)\n",
      "step 8045, loss is 4.926949501037598\n",
      "(64, 33)\n",
      "step 8046, loss is 5.077017784118652\n",
      "(64, 33)\n",
      "step 8047, loss is 4.921416282653809\n",
      "(64, 33)\n",
      "step 8048, loss is 4.773255825042725\n",
      "(64, 33)\n",
      "step 8049, loss is 4.881738662719727\n",
      "(64, 33)\n",
      "step 8050, loss is 4.869141101837158\n",
      "(64, 33)\n",
      "step 8051, loss is 4.828131675720215\n",
      "(64, 33)\n",
      "step 8052, loss is 4.915929794311523\n",
      "(64, 33)\n",
      "step 8053, loss is 4.774229049682617\n",
      "(64, 33)\n",
      "step 8054, loss is 4.761913776397705\n",
      "(64, 33)\n",
      "step 8055, loss is 4.904943943023682\n",
      "(64, 33)\n",
      "step 8056, loss is 4.988747596740723\n",
      "(64, 33)\n",
      "step 8057, loss is 4.892802715301514\n",
      "(64, 33)\n",
      "step 8058, loss is 4.982400894165039\n",
      "(64, 33)\n",
      "step 8059, loss is 4.887044429779053\n",
      "(64, 33)\n",
      "step 8060, loss is 4.755482196807861\n",
      "(64, 33)\n",
      "step 8061, loss is 4.916070461273193\n",
      "(64, 33)\n",
      "step 8062, loss is 4.819656848907471\n",
      "(64, 33)\n",
      "step 8063, loss is 5.035325050354004\n",
      "(64, 33)\n",
      "step 8064, loss is 4.732353210449219\n",
      "(64, 33)\n",
      "step 8065, loss is 4.924110412597656\n",
      "(64, 33)\n",
      "step 8066, loss is 4.874992847442627\n",
      "(64, 33)\n",
      "step 8067, loss is 5.058816432952881\n",
      "(64, 33)\n",
      "step 8068, loss is 4.938148498535156\n",
      "(64, 33)\n",
      "step 8069, loss is 4.685549736022949\n",
      "(64, 33)\n",
      "step 8070, loss is 4.9066081047058105\n",
      "(64, 33)\n",
      "step 8071, loss is 4.8324127197265625\n",
      "(64, 33)\n",
      "step 8072, loss is 4.974707126617432\n",
      "(64, 33)\n",
      "step 8073, loss is 4.796342372894287\n",
      "(64, 33)\n",
      "step 8074, loss is 5.00150728225708\n",
      "(64, 33)\n",
      "step 8075, loss is 4.930522441864014\n",
      "(64, 33)\n",
      "step 8076, loss is 4.958074569702148\n",
      "(64, 33)\n",
      "step 8077, loss is 4.878929138183594\n",
      "(64, 33)\n",
      "step 8078, loss is 4.852166175842285\n",
      "(64, 33)\n",
      "step 8079, loss is 4.893782615661621\n",
      "(64, 33)\n",
      "step 8080, loss is 4.949995040893555\n",
      "(64, 33)\n",
      "step 8081, loss is 4.990494251251221\n",
      "(64, 33)\n",
      "step 8082, loss is 4.836575508117676\n",
      "(64, 33)\n",
      "step 8083, loss is 4.705658435821533\n",
      "(64, 33)\n",
      "step 8084, loss is 4.9108099937438965\n",
      "(64, 33)\n",
      "step 8085, loss is 4.935033321380615\n",
      "(64, 33)\n",
      "step 8086, loss is 4.982658863067627\n",
      "(64, 33)\n",
      "step 8087, loss is 4.781895160675049\n",
      "(64, 33)\n",
      "step 8088, loss is 4.730156898498535\n",
      "(64, 33)\n",
      "step 8089, loss is 4.951182842254639\n",
      "(64, 33)\n",
      "step 8090, loss is 4.786942958831787\n",
      "(64, 33)\n",
      "step 8091, loss is 5.0066351890563965\n",
      "(64, 33)\n",
      "step 8092, loss is 4.930398464202881\n",
      "(64, 33)\n",
      "step 8093, loss is 4.85017204284668\n",
      "(64, 33)\n",
      "step 8094, loss is 4.83823823928833\n",
      "(64, 33)\n",
      "step 8095, loss is 4.9729228019714355\n",
      "(64, 33)\n",
      "step 8096, loss is 4.699788570404053\n",
      "(64, 33)\n",
      "step 8097, loss is 4.741489410400391\n",
      "(64, 33)\n",
      "step 8098, loss is 4.9643754959106445\n",
      "(64, 33)\n",
      "step 8099, loss is 4.72398042678833\n",
      "(64, 33)\n",
      "step 8100, loss is 4.926695823669434\n",
      "(64, 33)\n",
      "step 8101, loss is 5.0180583000183105\n",
      "(64, 33)\n",
      "step 8102, loss is 4.809269428253174\n",
      "(64, 33)\n",
      "step 8103, loss is 4.786520481109619\n",
      "(64, 33)\n",
      "step 8104, loss is 4.983693599700928\n",
      "(64, 33)\n",
      "step 8105, loss is 4.877612590789795\n",
      "(64, 33)\n",
      "step 8106, loss is 5.0464277267456055\n",
      "(64, 33)\n",
      "step 8107, loss is 4.780582427978516\n",
      "(64, 33)\n",
      "step 8108, loss is 4.8675384521484375\n",
      "(64, 33)\n",
      "step 8109, loss is 4.847166061401367\n",
      "(64, 33)\n",
      "step 8110, loss is 4.759742736816406\n",
      "(64, 33)\n",
      "step 8111, loss is 4.932767391204834\n",
      "(64, 33)\n",
      "step 8112, loss is 4.8722615242004395\n",
      "(64, 33)\n",
      "step 8113, loss is 4.928821086883545\n",
      "(64, 33)\n",
      "step 8114, loss is 4.978797435760498\n",
      "(64, 33)\n",
      "step 8115, loss is 4.739067077636719\n",
      "(64, 33)\n",
      "step 8116, loss is 4.900728225708008\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8117, loss is 4.8616108894348145\n",
      "(64, 33)\n",
      "step 8118, loss is 4.993469715118408\n",
      "(64, 33)\n",
      "step 8119, loss is 4.930976867675781\n",
      "(64, 33)\n",
      "step 8120, loss is 4.886209964752197\n",
      "(64, 33)\n",
      "step 8121, loss is 4.887287139892578\n",
      "(64, 33)\n",
      "step 8122, loss is 4.918994903564453\n",
      "(64, 33)\n",
      "step 8123, loss is 4.929978370666504\n",
      "(64, 33)\n",
      "step 8124, loss is 4.789648056030273\n",
      "(64, 33)\n",
      "step 8125, loss is 4.9217610359191895\n",
      "(64, 33)\n",
      "step 8126, loss is 4.892234802246094\n",
      "(64, 33)\n",
      "step 8127, loss is 4.884279251098633\n",
      "(64, 33)\n",
      "step 8128, loss is 4.976381301879883\n",
      "(64, 33)\n",
      "step 8129, loss is 4.930353164672852\n",
      "(64, 33)\n",
      "step 8130, loss is 4.740610122680664\n",
      "(64, 33)\n",
      "step 8131, loss is 5.058088302612305\n",
      "(64, 33)\n",
      "step 8132, loss is 5.065826416015625\n",
      "(64, 33)\n",
      "step 8133, loss is 4.7206807136535645\n",
      "(64, 33)\n",
      "step 8134, loss is 4.866576671600342\n",
      "(64, 33)\n",
      "step 8135, loss is 5.015181064605713\n",
      "(64, 33)\n",
      "step 8136, loss is 4.755974769592285\n",
      "(64, 33)\n",
      "step 8137, loss is 4.990313529968262\n",
      "(64, 33)\n",
      "step 8138, loss is 5.0343780517578125\n",
      "(64, 33)\n",
      "step 8139, loss is 4.91811466217041\n",
      "(64, 33)\n",
      "step 8140, loss is 4.992241859436035\n",
      "(64, 33)\n",
      "step 8141, loss is 4.822227478027344\n",
      "(64, 33)\n",
      "step 8142, loss is 4.788569450378418\n",
      "(64, 33)\n",
      "step 8143, loss is 4.682553291320801\n",
      "(64, 33)\n",
      "step 8144, loss is 4.707646369934082\n",
      "(64, 33)\n",
      "step 8145, loss is 5.035468578338623\n",
      "(64, 33)\n",
      "step 8146, loss is 4.873225212097168\n",
      "(64, 33)\n",
      "step 8147, loss is 4.834325790405273\n",
      "(64, 33)\n",
      "step 8148, loss is 4.85395622253418\n",
      "(64, 33)\n",
      "step 8149, loss is 5.03701114654541\n",
      "(64, 33)\n",
      "step 8150, loss is 5.0226359367370605\n",
      "(64, 33)\n",
      "step 8151, loss is 4.953985214233398\n",
      "(64, 33)\n",
      "step 8152, loss is 4.853017807006836\n",
      "(64, 33)\n",
      "step 8153, loss is 4.900540351867676\n",
      "(64, 33)\n",
      "step 8154, loss is 4.877608776092529\n",
      "(64, 33)\n",
      "step 8155, loss is 4.933191776275635\n",
      "(64, 33)\n",
      "step 8156, loss is 4.869546890258789\n",
      "(64, 33)\n",
      "step 8157, loss is 4.922245025634766\n",
      "(64, 33)\n",
      "step 8158, loss is 4.656447410583496\n",
      "(64, 33)\n",
      "step 8159, loss is 4.824551582336426\n",
      "(64, 33)\n",
      "step 8160, loss is 4.851632595062256\n",
      "(64, 33)\n",
      "step 8161, loss is 4.9252448081970215\n",
      "(64, 33)\n",
      "step 8162, loss is 4.695568561553955\n",
      "(64, 33)\n",
      "step 8163, loss is 4.9696736335754395\n",
      "(64, 33)\n",
      "step 8164, loss is 4.815093517303467\n",
      "(64, 33)\n",
      "step 8165, loss is 4.973864555358887\n",
      "(64, 33)\n",
      "step 8166, loss is 4.927221775054932\n",
      "(64, 33)\n",
      "step 8167, loss is 4.734560966491699\n",
      "(64, 33)\n",
      "step 8168, loss is 4.716340065002441\n",
      "(64, 33)\n",
      "step 8169, loss is 4.858040809631348\n",
      "(64, 33)\n",
      "step 8170, loss is 4.725327491760254\n",
      "(64, 33)\n",
      "step 8171, loss is 4.904869079589844\n",
      "(64, 33)\n",
      "step 8172, loss is 4.808858394622803\n",
      "(64, 33)\n",
      "step 8173, loss is 4.829277515411377\n",
      "(64, 33)\n",
      "step 8174, loss is 4.9416632652282715\n",
      "(64, 33)\n",
      "step 8175, loss is 4.981656551361084\n",
      "(64, 33)\n",
      "step 8176, loss is 5.034815311431885\n",
      "(64, 33)\n",
      "step 8177, loss is 4.9144110679626465\n",
      "(64, 33)\n",
      "step 8178, loss is 5.113685607910156\n",
      "(64, 33)\n",
      "step 8179, loss is 4.87276029586792\n",
      "(64, 33)\n",
      "step 8180, loss is 4.993741512298584\n",
      "(64, 33)\n",
      "step 8181, loss is 5.1345672607421875\n",
      "(64, 33)\n",
      "step 8182, loss is 4.969952583312988\n",
      "(64, 33)\n",
      "step 8183, loss is 4.782599449157715\n",
      "(64, 33)\n",
      "step 8184, loss is 4.766798496246338\n",
      "(64, 33)\n",
      "step 8185, loss is 4.659107208251953\n",
      "(64, 33)\n",
      "step 8186, loss is 4.908228874206543\n",
      "(64, 33)\n",
      "step 8187, loss is 4.820337295532227\n",
      "(64, 33)\n",
      "step 8188, loss is 5.065075397491455\n",
      "(64, 33)\n",
      "step 8189, loss is 4.86622428894043\n",
      "(64, 33)\n",
      "step 8190, loss is 4.552101135253906\n",
      "(64, 33)\n",
      "step 8191, loss is 4.829832077026367\n",
      "(64, 33)\n",
      "step 8192, loss is 4.622180461883545\n",
      "(64, 33)\n",
      "step 8193, loss is 4.77409029006958\n",
      "(64, 33)\n",
      "step 8194, loss is 4.752227783203125\n",
      "(64, 33)\n",
      "step 8195, loss is 4.8900041580200195\n",
      "(64, 33)\n",
      "step 8196, loss is 4.9021782875061035\n",
      "(64, 33)\n",
      "step 8197, loss is 4.702862739562988\n",
      "(64, 33)\n",
      "step 8198, loss is 5.011290073394775\n",
      "(64, 33)\n",
      "step 8199, loss is 4.941072940826416\n",
      "(64, 33)\n",
      "step 8200, loss is 4.773011684417725\n",
      "(64, 33)\n",
      "step 8201, loss is 4.899946689605713\n",
      "(64, 33)\n",
      "step 8202, loss is 4.933811664581299\n",
      "(64, 33)\n",
      "step 8203, loss is 4.961409568786621\n",
      "(64, 33)\n",
      "step 8204, loss is 4.853157043457031\n",
      "(64, 33)\n",
      "step 8205, loss is 4.807456016540527\n",
      "(64, 33)\n",
      "step 8206, loss is 5.017520904541016\n",
      "(64, 33)\n",
      "step 8207, loss is 4.8609619140625\n",
      "(64, 33)\n",
      "step 8208, loss is 4.9300150871276855\n",
      "(64, 33)\n",
      "step 8209, loss is 4.927212715148926\n",
      "(64, 33)\n",
      "step 8210, loss is 4.830798625946045\n",
      "(64, 33)\n",
      "step 8211, loss is 5.000881671905518\n",
      "(64, 33)\n",
      "step 8212, loss is 5.121438980102539\n",
      "(64, 33)\n",
      "step 8213, loss is 4.8084635734558105\n",
      "(64, 33)\n",
      "step 8214, loss is 4.747806549072266\n",
      "(64, 33)\n",
      "step 8215, loss is 4.881761074066162\n",
      "(64, 33)\n",
      "step 8216, loss is 4.802110195159912\n",
      "(64, 33)\n",
      "step 8217, loss is 4.744389057159424\n",
      "(64, 33)\n",
      "step 8218, loss is 4.712721347808838\n",
      "(64, 33)\n",
      "step 8219, loss is 4.631592750549316\n",
      "(64, 33)\n",
      "step 8220, loss is 4.716344356536865\n",
      "(64, 33)\n",
      "step 8221, loss is 4.863838195800781\n",
      "(64, 33)\n",
      "step 8222, loss is 4.702171325683594\n",
      "(64, 33)\n",
      "step 8223, loss is 4.787106513977051\n",
      "(64, 33)\n",
      "step 8224, loss is 4.747517108917236\n",
      "(64, 33)\n",
      "step 8225, loss is 4.909246444702148\n",
      "(64, 33)\n",
      "step 8226, loss is 4.84677791595459\n",
      "(64, 33)\n",
      "step 8227, loss is 4.73511266708374\n",
      "(64, 33)\n",
      "step 8228, loss is 4.996865272521973\n",
      "(64, 33)\n",
      "step 8229, loss is 4.8669233322143555\n",
      "(64, 33)\n",
      "step 8230, loss is 5.032611846923828\n",
      "(64, 33)\n",
      "step 8231, loss is 4.928750514984131\n",
      "(64, 33)\n",
      "step 8232, loss is 4.955526828765869\n",
      "(64, 33)\n",
      "step 8233, loss is 4.869274616241455\n",
      "(64, 33)\n",
      "step 8234, loss is 4.909754753112793\n",
      "(64, 33)\n",
      "step 8235, loss is 4.849826335906982\n",
      "(64, 33)\n",
      "step 8236, loss is 4.806499481201172\n",
      "(64, 33)\n",
      "step 8237, loss is 4.773949146270752\n",
      "(64, 33)\n",
      "step 8238, loss is 4.671291828155518\n",
      "(64, 33)\n",
      "step 8239, loss is 4.559996604919434\n",
      "(64, 33)\n",
      "step 8240, loss is 4.88447904586792\n",
      "(64, 33)\n",
      "step 8241, loss is 4.742575168609619\n",
      "(64, 33)\n",
      "step 8242, loss is 4.885024547576904\n",
      "(64, 33)\n",
      "step 8243, loss is 4.9650750160217285\n",
      "(64, 33)\n",
      "step 8244, loss is 4.844468593597412\n",
      "(64, 33)\n",
      "step 8245, loss is 4.625740051269531\n",
      "(64, 33)\n",
      "step 8246, loss is 4.787674427032471\n",
      "(64, 33)\n",
      "step 8247, loss is 4.915863037109375\n",
      "(64, 33)\n",
      "step 8248, loss is 4.735837936401367\n",
      "(64, 33)\n",
      "step 8249, loss is 4.8863935470581055\n",
      "(64, 33)\n",
      "step 8250, loss is 4.733078956604004\n",
      "(64, 33)\n",
      "step 8251, loss is 4.913219928741455\n",
      "(64, 33)\n",
      "step 8252, loss is 4.976877689361572\n",
      "(64, 33)\n",
      "step 8253, loss is 4.919000625610352\n",
      "(64, 33)\n",
      "step 8254, loss is 4.917817115783691\n",
      "(64, 33)\n",
      "step 8255, loss is 4.802269458770752\n",
      "(64, 33)\n",
      "step 8256, loss is 4.743286609649658\n",
      "(64, 33)\n",
      "step 8257, loss is 4.926541328430176\n",
      "(64, 33)\n",
      "step 8258, loss is 4.702418804168701\n",
      "(64, 33)\n",
      "step 8259, loss is 4.888669967651367\n",
      "(64, 33)\n",
      "step 8260, loss is 4.70996618270874\n",
      "(64, 33)\n",
      "step 8261, loss is 4.9067912101745605\n",
      "(64, 33)\n",
      "step 8262, loss is 4.789740085601807\n",
      "(64, 33)\n",
      "step 8263, loss is 4.900620460510254\n",
      "(64, 33)\n",
      "step 8264, loss is 4.984194278717041\n",
      "(64, 33)\n",
      "step 8265, loss is 4.838570594787598\n",
      "(64, 33)\n",
      "step 8266, loss is 4.7970051765441895\n",
      "(64, 33)\n",
      "step 8267, loss is 4.825796127319336\n",
      "(64, 33)\n",
      "step 8268, loss is 4.936959743499756\n",
      "(64, 33)\n",
      "step 8269, loss is 4.948398113250732\n",
      "(64, 33)\n",
      "step 8270, loss is 4.862138271331787\n",
      "(64, 33)\n",
      "step 8271, loss is 4.75539493560791\n",
      "(64, 33)\n",
      "step 8272, loss is 4.570374488830566\n",
      "(64, 33)\n",
      "step 8273, loss is 5.083522796630859\n",
      "(64, 33)\n",
      "step 8274, loss is 4.954092502593994\n",
      "(64, 33)\n",
      "step 8275, loss is 4.699666976928711\n",
      "(64, 33)\n",
      "step 8276, loss is 4.941728115081787\n",
      "(64, 33)\n",
      "step 8277, loss is 4.711965084075928\n",
      "(64, 33)\n",
      "step 8278, loss is 4.939958095550537\n",
      "(64, 33)\n",
      "step 8279, loss is 4.798319339752197\n",
      "(64, 33)\n",
      "step 8280, loss is 4.852871894836426\n",
      "(64, 33)\n",
      "step 8281, loss is 4.765168190002441\n",
      "(64, 33)\n",
      "step 8282, loss is 4.952674388885498\n",
      "(64, 33)\n",
      "step 8283, loss is 4.8873162269592285\n",
      "(64, 33)\n",
      "step 8284, loss is 4.982938766479492\n",
      "(64, 33)\n",
      "step 8285, loss is 4.8098883628845215\n",
      "(64, 33)\n",
      "step 8286, loss is 4.878233909606934\n",
      "(64, 33)\n",
      "step 8287, loss is 4.7627434730529785\n",
      "(64, 33)\n",
      "step 8288, loss is 4.7850022315979\n",
      "(64, 33)\n",
      "step 8289, loss is 4.914928913116455\n",
      "(64, 33)\n",
      "step 8290, loss is 4.88995361328125\n",
      "(64, 33)\n",
      "step 8291, loss is 4.759876728057861\n",
      "(64, 33)\n",
      "step 8292, loss is 4.917317867279053\n",
      "(64, 33)\n",
      "step 8293, loss is 4.6910881996154785\n",
      "(64, 33)\n",
      "step 8294, loss is 4.619873046875\n",
      "(64, 33)\n",
      "step 8295, loss is 4.881092548370361\n",
      "(64, 33)\n",
      "step 8296, loss is 4.860655307769775\n",
      "(64, 33)\n",
      "step 8297, loss is 4.895795822143555\n",
      "(64, 33)\n",
      "step 8298, loss is 4.698243618011475\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8299, loss is 4.8448357582092285\n",
      "(64, 33)\n",
      "step 8300, loss is 4.778963565826416\n",
      "(64, 33)\n",
      "step 8301, loss is 4.655008792877197\n",
      "(64, 33)\n",
      "step 8302, loss is 4.736410617828369\n",
      "(64, 33)\n",
      "step 8303, loss is 4.89379358291626\n",
      "(64, 33)\n",
      "step 8304, loss is 4.865339756011963\n",
      "(64, 33)\n",
      "step 8305, loss is 4.833071708679199\n",
      "(64, 33)\n",
      "step 8306, loss is 4.737636566162109\n",
      "(64, 33)\n",
      "step 8307, loss is 4.78380012512207\n",
      "(64, 33)\n",
      "step 8308, loss is 4.94059944152832\n",
      "(64, 33)\n",
      "step 8309, loss is 4.839034080505371\n",
      "(64, 33)\n",
      "step 8310, loss is 4.8348541259765625\n",
      "(64, 33)\n",
      "step 8311, loss is 4.7557902336120605\n",
      "(64, 33)\n",
      "step 8312, loss is 4.89708137512207\n",
      "(64, 33)\n",
      "step 8313, loss is 4.7975897789001465\n",
      "(64, 33)\n",
      "step 8314, loss is 4.75937557220459\n",
      "(64, 33)\n",
      "step 8315, loss is 4.8086371421813965\n",
      "(64, 33)\n",
      "step 8316, loss is 4.752203464508057\n",
      "(64, 33)\n",
      "step 8317, loss is 4.953670024871826\n",
      "(64, 33)\n",
      "step 8318, loss is 4.719254970550537\n",
      "(64, 33)\n",
      "step 8319, loss is 4.9326043128967285\n",
      "(64, 33)\n",
      "step 8320, loss is 4.880315780639648\n",
      "(64, 33)\n",
      "step 8321, loss is 4.873623847961426\n",
      "(64, 33)\n",
      "step 8322, loss is 4.877157688140869\n",
      "(64, 33)\n",
      "step 8323, loss is 4.739748001098633\n",
      "(64, 33)\n",
      "step 8324, loss is 4.91829776763916\n",
      "(64, 33)\n",
      "step 8325, loss is 4.903450012207031\n",
      "(64, 33)\n",
      "step 8326, loss is 4.877780914306641\n",
      "(64, 33)\n",
      "step 8327, loss is 4.830474376678467\n",
      "(64, 33)\n",
      "step 8328, loss is 5.039304733276367\n",
      "(64, 33)\n",
      "step 8329, loss is 4.9118876457214355\n",
      "(64, 33)\n",
      "step 8330, loss is 4.847654342651367\n",
      "(64, 33)\n",
      "step 8331, loss is 4.8048529624938965\n",
      "(64, 33)\n",
      "step 8332, loss is 4.846523284912109\n",
      "(64, 33)\n",
      "step 8333, loss is 4.70521879196167\n",
      "(64, 33)\n",
      "step 8334, loss is 4.981654167175293\n",
      "(64, 33)\n",
      "step 8335, loss is 4.702865123748779\n",
      "(64, 33)\n",
      "step 8336, loss is 4.900403022766113\n",
      "(64, 33)\n",
      "step 8337, loss is 4.933751583099365\n",
      "(64, 33)\n",
      "step 8338, loss is 5.025944232940674\n",
      "(64, 33)\n",
      "step 8339, loss is 4.822612285614014\n",
      "(64, 33)\n",
      "step 8340, loss is 4.820267200469971\n",
      "(64, 33)\n",
      "step 8341, loss is 4.803254127502441\n",
      "(64, 33)\n",
      "step 8342, loss is 5.011283874511719\n",
      "(64, 33)\n",
      "step 8343, loss is 4.833607196807861\n",
      "(64, 33)\n",
      "step 8344, loss is 4.925302982330322\n",
      "(64, 33)\n",
      "step 8345, loss is 4.8853325843811035\n",
      "(64, 33)\n",
      "step 8346, loss is 4.980464935302734\n",
      "(64, 33)\n",
      "step 8347, loss is 4.875891208648682\n",
      "(64, 33)\n",
      "step 8348, loss is 4.88993501663208\n",
      "(64, 33)\n",
      "step 8349, loss is 4.792729377746582\n",
      "(64, 33)\n",
      "step 8350, loss is 4.902584075927734\n",
      "(64, 33)\n",
      "step 8351, loss is 4.879068374633789\n",
      "(64, 33)\n",
      "step 8352, loss is 4.90823221206665\n",
      "(64, 33)\n",
      "step 8353, loss is 4.939216613769531\n",
      "(64, 33)\n",
      "step 8354, loss is 4.8326826095581055\n",
      "(64, 33)\n",
      "step 8355, loss is 4.717310428619385\n",
      "(64, 33)\n",
      "step 8356, loss is 4.9110894203186035\n",
      "(64, 33)\n",
      "step 8357, loss is 4.697111129760742\n",
      "(64, 33)\n",
      "step 8358, loss is 4.975669860839844\n",
      "(64, 33)\n",
      "step 8359, loss is 4.87434720993042\n",
      "(64, 33)\n",
      "step 8360, loss is 5.036584377288818\n",
      "(64, 33)\n",
      "step 8361, loss is 4.975519180297852\n",
      "(64, 33)\n",
      "step 8362, loss is 4.985674858093262\n",
      "(64, 33)\n",
      "step 8363, loss is 4.91832971572876\n",
      "(64, 33)\n",
      "step 8364, loss is 4.757201671600342\n",
      "(64, 33)\n",
      "step 8365, loss is 4.7718963623046875\n",
      "(64, 33)\n",
      "step 8366, loss is 4.98488712310791\n",
      "(64, 33)\n",
      "step 8367, loss is 4.872413635253906\n",
      "(64, 33)\n",
      "step 8368, loss is 4.832099914550781\n",
      "(64, 33)\n",
      "step 8369, loss is 4.626431465148926\n",
      "(64, 33)\n",
      "step 8370, loss is 4.9401068687438965\n",
      "(64, 33)\n",
      "step 8371, loss is 4.701393127441406\n",
      "(64, 33)\n",
      "step 8372, loss is 4.561857223510742\n",
      "(64, 33)\n",
      "step 8373, loss is 4.864864349365234\n",
      "(64, 33)\n",
      "step 8374, loss is 4.857298851013184\n",
      "(64, 33)\n",
      "step 8375, loss is 4.8760552406311035\n",
      "(64, 33)\n",
      "step 8376, loss is 4.86707878112793\n",
      "(64, 33)\n",
      "step 8377, loss is 5.007265090942383\n",
      "(64, 33)\n",
      "step 8378, loss is 5.058082580566406\n",
      "(64, 33)\n",
      "step 8379, loss is 4.931189060211182\n",
      "(64, 33)\n",
      "step 8380, loss is 4.828054904937744\n",
      "(64, 33)\n",
      "step 8381, loss is 4.865001201629639\n",
      "(64, 33)\n",
      "step 8382, loss is 4.77607536315918\n",
      "(64, 33)\n",
      "step 8383, loss is 4.796387672424316\n",
      "(64, 33)\n",
      "step 8384, loss is 4.98319149017334\n",
      "(64, 33)\n",
      "step 8385, loss is 4.987951755523682\n",
      "(64, 33)\n",
      "step 8386, loss is 4.697321891784668\n",
      "(64, 33)\n",
      "step 8387, loss is 4.69937801361084\n",
      "(64, 33)\n",
      "step 8388, loss is 4.919493198394775\n",
      "(64, 33)\n",
      "step 8389, loss is 4.892972946166992\n",
      "(64, 33)\n",
      "step 8390, loss is 4.994564056396484\n",
      "(64, 33)\n",
      "step 8391, loss is 4.833157062530518\n",
      "(64, 33)\n",
      "step 8392, loss is 4.830207824707031\n",
      "(64, 33)\n",
      "step 8393, loss is 4.809497356414795\n",
      "(64, 33)\n",
      "step 8394, loss is 4.670933723449707\n",
      "(64, 33)\n",
      "step 8395, loss is 4.967017650604248\n",
      "(64, 33)\n",
      "step 8396, loss is 4.713712215423584\n",
      "(64, 33)\n",
      "step 8397, loss is 4.888397216796875\n",
      "(64, 33)\n",
      "step 8398, loss is 4.875438213348389\n",
      "(64, 33)\n",
      "step 8399, loss is 4.903743743896484\n",
      "(64, 33)\n",
      "step 8400, loss is 4.726090908050537\n",
      "(64, 33)\n",
      "step 8401, loss is 4.838289737701416\n",
      "(64, 33)\n",
      "step 8402, loss is 4.794187068939209\n",
      "(64, 33)\n",
      "step 8403, loss is 4.74435567855835\n",
      "(64, 33)\n",
      "step 8404, loss is 4.855284690856934\n",
      "(64, 33)\n",
      "step 8405, loss is 4.868434906005859\n",
      "(64, 33)\n",
      "step 8406, loss is 4.9959259033203125\n",
      "(64, 33)\n",
      "step 8407, loss is 4.879120826721191\n",
      "(64, 33)\n",
      "step 8408, loss is 4.9662394523620605\n",
      "(64, 33)\n",
      "step 8409, loss is 4.510854244232178\n",
      "(64, 33)\n",
      "step 8410, loss is 4.866002082824707\n",
      "(64, 33)\n",
      "step 8411, loss is 4.806756973266602\n",
      "(64, 33)\n",
      "step 8412, loss is 4.821166038513184\n",
      "(64, 33)\n",
      "step 8413, loss is 4.759998321533203\n",
      "(64, 33)\n",
      "step 8414, loss is 4.766915798187256\n",
      "(64, 33)\n",
      "step 8415, loss is 4.786756992340088\n",
      "(64, 33)\n",
      "step 8416, loss is 4.925602912902832\n",
      "(64, 33)\n",
      "step 8417, loss is 4.939735412597656\n",
      "(64, 33)\n",
      "step 8418, loss is 4.844750881195068\n",
      "(64, 33)\n",
      "step 8419, loss is 4.8427414894104\n",
      "(64, 33)\n",
      "step 8420, loss is 4.747044563293457\n",
      "(64, 33)\n",
      "step 8421, loss is 4.847546100616455\n",
      "(64, 33)\n",
      "step 8422, loss is 5.124444484710693\n",
      "(64, 33)\n",
      "step 8423, loss is 4.782878875732422\n",
      "(64, 33)\n",
      "step 8424, loss is 4.927505016326904\n",
      "(64, 33)\n",
      "step 8425, loss is 4.95621919631958\n",
      "(64, 33)\n",
      "step 8426, loss is 4.754831790924072\n",
      "(64, 33)\n",
      "step 8427, loss is 4.9762372970581055\n",
      "(64, 33)\n",
      "step 8428, loss is 4.862152099609375\n",
      "(64, 33)\n",
      "step 8429, loss is 4.989170074462891\n",
      "(64, 33)\n",
      "step 8430, loss is 4.902620792388916\n",
      "(64, 33)\n",
      "step 8431, loss is 4.852797985076904\n",
      "(64, 33)\n",
      "step 8432, loss is 4.880334377288818\n",
      "(64, 33)\n",
      "step 8433, loss is 4.824428081512451\n",
      "(64, 33)\n",
      "step 8434, loss is 4.844516754150391\n",
      "(64, 33)\n",
      "step 8435, loss is 4.789827346801758\n",
      "(64, 33)\n",
      "step 8436, loss is 4.839510440826416\n",
      "(64, 33)\n",
      "step 8437, loss is 4.850522518157959\n",
      "(64, 33)\n",
      "step 8438, loss is 4.973960876464844\n",
      "(64, 33)\n",
      "step 8439, loss is 4.901952743530273\n",
      "(64, 33)\n",
      "step 8440, loss is 4.693019390106201\n",
      "(64, 33)\n",
      "step 8441, loss is 4.876742839813232\n",
      "(64, 33)\n",
      "step 8442, loss is 4.925335884094238\n",
      "(64, 33)\n",
      "step 8443, loss is 4.7804412841796875\n",
      "(64, 33)\n",
      "step 8444, loss is 4.7843499183654785\n",
      "(64, 33)\n",
      "step 8445, loss is 4.732950687408447\n",
      "(64, 33)\n",
      "step 8446, loss is 4.788609981536865\n",
      "(64, 33)\n",
      "step 8447, loss is 4.781601428985596\n",
      "(64, 33)\n",
      "step 8448, loss is 4.925292015075684\n",
      "(64, 33)\n",
      "step 8449, loss is 4.745399475097656\n",
      "(64, 33)\n",
      "step 8450, loss is 4.690565586090088\n",
      "(64, 33)\n",
      "step 8451, loss is 4.852561950683594\n",
      "(64, 33)\n",
      "step 8452, loss is 4.930907249450684\n",
      "(64, 33)\n",
      "step 8453, loss is 4.894223213195801\n",
      "(64, 33)\n",
      "step 8454, loss is 4.76697301864624\n",
      "(64, 33)\n",
      "step 8455, loss is 4.935464859008789\n",
      "(64, 33)\n",
      "step 8456, loss is 4.874414920806885\n",
      "(64, 33)\n",
      "step 8457, loss is 4.63811731338501\n",
      "(64, 33)\n",
      "step 8458, loss is 4.785853385925293\n",
      "(64, 33)\n",
      "step 8459, loss is 4.983154773712158\n",
      "(64, 33)\n",
      "step 8460, loss is 4.992938995361328\n",
      "(64, 33)\n",
      "step 8461, loss is 4.9206647872924805\n",
      "(64, 33)\n",
      "step 8462, loss is 4.765489101409912\n",
      "(64, 33)\n",
      "step 8463, loss is 5.018261432647705\n",
      "(64, 33)\n",
      "step 8464, loss is 4.829823017120361\n",
      "(64, 33)\n",
      "step 8465, loss is 4.993948936462402\n",
      "(64, 33)\n",
      "step 8466, loss is 4.9185638427734375\n",
      "(64, 33)\n",
      "step 8467, loss is 4.788983345031738\n",
      "(64, 33)\n",
      "step 8468, loss is 4.888981819152832\n",
      "(64, 33)\n",
      "step 8469, loss is 4.792041778564453\n",
      "(64, 33)\n",
      "step 8470, loss is 4.904107093811035\n",
      "(64, 33)\n",
      "step 8471, loss is 4.977766513824463\n",
      "(64, 33)\n",
      "step 8472, loss is 4.836708068847656\n",
      "(64, 33)\n",
      "step 8473, loss is 4.843923568725586\n",
      "(64, 33)\n",
      "step 8474, loss is 4.873602867126465\n",
      "(64, 33)\n",
      "step 8475, loss is 4.896542549133301\n",
      "(64, 33)\n",
      "step 8476, loss is 4.972267150878906\n",
      "(64, 33)\n",
      "step 8477, loss is 4.892695426940918\n",
      "(64, 33)\n",
      "step 8478, loss is 4.868414878845215\n",
      "(64, 33)\n",
      "step 8479, loss is 4.845090389251709\n",
      "(64, 33)\n",
      "step 8480, loss is 5.029456615447998\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8481, loss is 4.874029636383057\n",
      "(64, 33)\n",
      "step 8482, loss is 4.792095184326172\n",
      "(64, 33)\n",
      "step 8483, loss is 4.926353454589844\n",
      "(64, 33)\n",
      "step 8484, loss is 4.925731658935547\n",
      "(64, 33)\n",
      "step 8485, loss is 4.786079406738281\n",
      "(64, 33)\n",
      "step 8486, loss is 4.855113506317139\n",
      "(64, 33)\n",
      "step 8487, loss is 4.7738566398620605\n",
      "(64, 33)\n",
      "step 8488, loss is 4.9859232902526855\n",
      "(64, 33)\n",
      "step 8489, loss is 4.760904312133789\n",
      "(64, 33)\n",
      "step 8490, loss is 4.871291637420654\n",
      "(64, 33)\n",
      "step 8491, loss is 4.7704572677612305\n",
      "(64, 33)\n",
      "step 8492, loss is 4.634617328643799\n",
      "(64, 33)\n",
      "step 8493, loss is 4.918993949890137\n",
      "(64, 33)\n",
      "step 8494, loss is 4.671318531036377\n",
      "(64, 33)\n",
      "step 8495, loss is 4.835119247436523\n",
      "(64, 33)\n",
      "step 8496, loss is 4.691760540008545\n",
      "(64, 33)\n",
      "step 8497, loss is 5.046218395233154\n",
      "(64, 33)\n",
      "step 8498, loss is 4.7784037590026855\n",
      "(64, 33)\n",
      "step 8499, loss is 4.970407009124756\n",
      "(64, 33)\n",
      "step 8500, loss is 4.708279132843018\n",
      "(64, 33)\n",
      "step 8501, loss is 4.8502583503723145\n",
      "(64, 33)\n",
      "step 8502, loss is 4.846962928771973\n",
      "(64, 33)\n",
      "step 8503, loss is 4.812344074249268\n",
      "(64, 33)\n",
      "step 8504, loss is 4.77250337600708\n",
      "(64, 33)\n",
      "step 8505, loss is 4.867750644683838\n",
      "(64, 33)\n",
      "step 8506, loss is 4.940191745758057\n",
      "(64, 33)\n",
      "step 8507, loss is 4.707862854003906\n",
      "(64, 33)\n",
      "step 8508, loss is 4.990084648132324\n",
      "(64, 33)\n",
      "step 8509, loss is 4.795350074768066\n",
      "(64, 33)\n",
      "step 8510, loss is 4.799798965454102\n",
      "(64, 33)\n",
      "step 8511, loss is 5.023521423339844\n",
      "(64, 33)\n",
      "step 8512, loss is 4.894476413726807\n",
      "(64, 33)\n",
      "step 8513, loss is 4.8531718254089355\n",
      "(64, 33)\n",
      "step 8514, loss is 4.676419258117676\n",
      "(64, 33)\n",
      "step 8515, loss is 4.933073997497559\n",
      "(64, 33)\n",
      "step 8516, loss is 4.909910678863525\n",
      "(64, 33)\n",
      "step 8517, loss is 4.900022029876709\n",
      "(64, 33)\n",
      "step 8518, loss is 4.804315090179443\n",
      "(64, 33)\n",
      "step 8519, loss is 4.793652534484863\n",
      "(64, 33)\n",
      "step 8520, loss is 4.6893696784973145\n",
      "(64, 33)\n",
      "step 8521, loss is 4.840636253356934\n",
      "(64, 33)\n",
      "step 8522, loss is 4.86126184463501\n",
      "(64, 33)\n",
      "step 8523, loss is 5.034125328063965\n",
      "(64, 33)\n",
      "step 8524, loss is 4.845562934875488\n",
      "(64, 33)\n",
      "step 8525, loss is 4.900986194610596\n",
      "(64, 33)\n",
      "step 8526, loss is 4.727744102478027\n",
      "(64, 33)\n",
      "step 8527, loss is 4.935253143310547\n",
      "(64, 33)\n",
      "step 8528, loss is 4.839176177978516\n",
      "(64, 33)\n",
      "step 8529, loss is 4.961151123046875\n",
      "(64, 33)\n",
      "step 8530, loss is 4.90426778793335\n",
      "(64, 33)\n",
      "step 8531, loss is 4.820602893829346\n",
      "(64, 33)\n",
      "step 8532, loss is 4.864720821380615\n",
      "(64, 33)\n",
      "step 8533, loss is 4.567215442657471\n",
      "(64, 33)\n",
      "step 8534, loss is 4.953368663787842\n",
      "(64, 33)\n",
      "step 8535, loss is 4.82669734954834\n",
      "(64, 33)\n",
      "step 8536, loss is 4.957617282867432\n",
      "(64, 33)\n",
      "step 8537, loss is 4.677726745605469\n",
      "(64, 33)\n",
      "step 8538, loss is 4.978535175323486\n",
      "(64, 33)\n",
      "step 8539, loss is 4.773062705993652\n",
      "(64, 33)\n",
      "step 8540, loss is 4.991490364074707\n",
      "(64, 33)\n",
      "step 8541, loss is 4.720535755157471\n",
      "(64, 33)\n",
      "step 8542, loss is 4.922064304351807\n",
      "(64, 33)\n",
      "step 8543, loss is 4.621850967407227\n",
      "(64, 33)\n",
      "step 8544, loss is 4.823402404785156\n",
      "(64, 33)\n",
      "step 8545, loss is 4.905577182769775\n",
      "(64, 33)\n",
      "step 8546, loss is 4.938289642333984\n",
      "(64, 33)\n",
      "step 8547, loss is 4.861447334289551\n",
      "(64, 33)\n",
      "step 8548, loss is 4.900668144226074\n",
      "(64, 33)\n",
      "step 8549, loss is 4.868641376495361\n",
      "(64, 33)\n",
      "step 8550, loss is 4.844179153442383\n",
      "(64, 33)\n",
      "step 8551, loss is 4.871315002441406\n",
      "(64, 33)\n",
      "step 8552, loss is 4.639986991882324\n",
      "(64, 33)\n",
      "step 8553, loss is 4.8508381843566895\n",
      "(64, 33)\n",
      "step 8554, loss is 4.8860955238342285\n",
      "(64, 33)\n",
      "step 8555, loss is 4.7985520362854\n",
      "(64, 33)\n",
      "step 8556, loss is 4.919027328491211\n",
      "(64, 33)\n",
      "step 8557, loss is 4.747647285461426\n",
      "(64, 33)\n",
      "step 8558, loss is 4.692124366760254\n",
      "(64, 33)\n",
      "step 8559, loss is 4.795094966888428\n",
      "(64, 33)\n",
      "step 8560, loss is 5.130484104156494\n",
      "(64, 33)\n",
      "step 8561, loss is 4.879467010498047\n",
      "(64, 33)\n",
      "step 8562, loss is 4.878463268280029\n",
      "(64, 33)\n",
      "step 8563, loss is 4.5893425941467285\n",
      "(64, 33)\n",
      "step 8564, loss is 5.117072582244873\n",
      "(64, 33)\n",
      "step 8565, loss is 4.775781154632568\n",
      "(64, 33)\n",
      "step 8566, loss is 5.00774621963501\n",
      "(64, 33)\n",
      "step 8567, loss is 4.840197563171387\n",
      "(64, 33)\n",
      "step 8568, loss is 5.046931266784668\n",
      "(64, 33)\n",
      "step 8569, loss is 4.886146545410156\n",
      "(64, 33)\n",
      "step 8570, loss is 4.82943058013916\n",
      "(64, 33)\n",
      "step 8571, loss is 4.9197001457214355\n",
      "(64, 33)\n",
      "step 8572, loss is 4.600751876831055\n",
      "(64, 33)\n",
      "step 8573, loss is 4.710552215576172\n",
      "(64, 33)\n",
      "step 8574, loss is 4.838268756866455\n",
      "(64, 33)\n",
      "step 8575, loss is 4.846315860748291\n",
      "(64, 33)\n",
      "step 8576, loss is 4.7538604736328125\n",
      "(64, 33)\n",
      "step 8577, loss is 4.784969806671143\n",
      "(64, 33)\n",
      "step 8578, loss is 4.912290573120117\n",
      "(64, 33)\n",
      "step 8579, loss is 4.831299304962158\n",
      "(64, 33)\n",
      "step 8580, loss is 4.815592288970947\n",
      "(64, 33)\n",
      "step 8581, loss is 4.73344087600708\n",
      "(64, 33)\n",
      "step 8582, loss is 4.858432769775391\n",
      "(64, 33)\n",
      "step 8583, loss is 4.8227128982543945\n",
      "(64, 33)\n",
      "step 8584, loss is 4.766383171081543\n",
      "(64, 33)\n",
      "step 8585, loss is 4.655570983886719\n",
      "(64, 33)\n",
      "step 8586, loss is 4.909505367279053\n",
      "(64, 33)\n",
      "step 8587, loss is 4.966090202331543\n",
      "(64, 33)\n",
      "step 8588, loss is 4.843793869018555\n",
      "(64, 33)\n",
      "step 8589, loss is 4.870680332183838\n",
      "(64, 33)\n",
      "step 8590, loss is 4.848666191101074\n",
      "(64, 33)\n",
      "step 8591, loss is 4.846427917480469\n",
      "(64, 33)\n",
      "step 8592, loss is 4.843960762023926\n",
      "(64, 33)\n",
      "step 8593, loss is 4.907070636749268\n",
      "(64, 33)\n",
      "step 8594, loss is 4.945132255554199\n",
      "(64, 33)\n",
      "step 8595, loss is 4.838164806365967\n",
      "(64, 33)\n",
      "step 8596, loss is 5.023837089538574\n",
      "(64, 33)\n",
      "step 8597, loss is 4.889357089996338\n",
      "(64, 33)\n",
      "step 8598, loss is 4.641972541809082\n",
      "(64, 33)\n",
      "step 8599, loss is 4.886729717254639\n",
      "(64, 33)\n",
      "step 8600, loss is 5.0113844871521\n",
      "(64, 33)\n",
      "step 8601, loss is 4.755405426025391\n",
      "(64, 33)\n",
      "step 8602, loss is 4.884627819061279\n",
      "(64, 33)\n",
      "step 8603, loss is 4.875912189483643\n",
      "(64, 33)\n",
      "step 8604, loss is 4.865649223327637\n",
      "(64, 33)\n",
      "step 8605, loss is 4.639764308929443\n",
      "(64, 33)\n",
      "step 8606, loss is 4.934425354003906\n",
      "(64, 33)\n",
      "step 8607, loss is 4.878837585449219\n",
      "(64, 33)\n",
      "step 8608, loss is 4.839940071105957\n",
      "(64, 33)\n",
      "step 8609, loss is 4.758545875549316\n",
      "(64, 33)\n",
      "step 8610, loss is 4.806929111480713\n",
      "(64, 33)\n",
      "step 8611, loss is 4.776089668273926\n",
      "(64, 33)\n",
      "step 8612, loss is 4.9037933349609375\n",
      "(64, 33)\n",
      "step 8613, loss is 4.689833164215088\n",
      "(64, 33)\n",
      "step 8614, loss is 5.0455827713012695\n",
      "(64, 33)\n",
      "step 8615, loss is 4.78737211227417\n",
      "(64, 33)\n",
      "step 8616, loss is 5.012099742889404\n",
      "(64, 33)\n",
      "step 8617, loss is 4.949264049530029\n",
      "(64, 33)\n",
      "step 8618, loss is 4.860709190368652\n",
      "(64, 33)\n",
      "step 8619, loss is 4.863128185272217\n",
      "(64, 33)\n",
      "step 8620, loss is 4.946821212768555\n",
      "(64, 33)\n",
      "step 8621, loss is 4.6969218254089355\n",
      "(64, 33)\n",
      "step 8622, loss is 4.942489147186279\n",
      "(64, 33)\n",
      "step 8623, loss is 4.879708290100098\n",
      "(64, 33)\n",
      "step 8624, loss is 5.041404724121094\n",
      "(64, 33)\n",
      "step 8625, loss is 4.775193691253662\n",
      "(64, 33)\n",
      "step 8626, loss is 4.857550144195557\n",
      "(64, 33)\n",
      "step 8627, loss is 4.883551597595215\n",
      "(64, 33)\n",
      "step 8628, loss is 4.994477272033691\n",
      "(64, 33)\n",
      "step 8629, loss is 4.906032562255859\n",
      "(64, 33)\n",
      "step 8630, loss is 4.787598609924316\n",
      "(64, 33)\n",
      "step 8631, loss is 4.824559211730957\n",
      "(64, 33)\n",
      "step 8632, loss is 4.90317440032959\n",
      "(64, 33)\n",
      "step 8633, loss is 4.781256675720215\n",
      "(64, 33)\n",
      "step 8634, loss is 4.874762535095215\n",
      "(64, 33)\n",
      "step 8635, loss is 4.923707008361816\n",
      "(64, 33)\n",
      "step 8636, loss is 4.826789379119873\n",
      "(64, 33)\n",
      "step 8637, loss is 4.740576267242432\n",
      "(64, 33)\n",
      "step 8638, loss is 5.032164573669434\n",
      "(64, 33)\n",
      "step 8639, loss is 5.093508243560791\n",
      "(64, 33)\n",
      "step 8640, loss is 4.719520568847656\n",
      "(64, 33)\n",
      "step 8641, loss is 4.879487037658691\n",
      "(64, 33)\n",
      "step 8642, loss is 4.750861167907715\n",
      "(64, 33)\n",
      "step 8643, loss is 4.918980121612549\n",
      "(64, 33)\n",
      "step 8644, loss is 5.002419948577881\n",
      "(64, 33)\n",
      "step 8645, loss is 4.887016296386719\n",
      "(64, 33)\n",
      "step 8646, loss is 4.791127681732178\n",
      "(64, 33)\n",
      "step 8647, loss is 4.6678547859191895\n",
      "(64, 33)\n",
      "step 8648, loss is 4.749395370483398\n",
      "(64, 33)\n",
      "step 8649, loss is 4.817195415496826\n",
      "(64, 33)\n",
      "step 8650, loss is 4.725936412811279\n",
      "(64, 33)\n",
      "step 8651, loss is 4.983772277832031\n",
      "(64, 33)\n",
      "step 8652, loss is 4.8327202796936035\n",
      "(64, 33)\n",
      "step 8653, loss is 4.8800272941589355\n",
      "(64, 33)\n",
      "step 8654, loss is 4.851594924926758\n",
      "(64, 33)\n",
      "step 8655, loss is 4.8510355949401855\n",
      "(64, 33)\n",
      "step 8656, loss is 4.904523849487305\n",
      "(64, 33)\n",
      "step 8657, loss is 4.767396926879883\n",
      "(64, 33)\n",
      "step 8658, loss is 4.856523513793945\n",
      "(64, 33)\n",
      "step 8659, loss is 4.950507164001465\n",
      "(64, 33)\n",
      "step 8660, loss is 4.893927574157715\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8661, loss is 4.746482849121094\n",
      "(64, 33)\n",
      "step 8662, loss is 4.767168045043945\n",
      "(64, 33)\n",
      "step 8663, loss is 4.823713779449463\n",
      "(64, 33)\n",
      "step 8664, loss is 4.816159725189209\n",
      "(64, 33)\n",
      "step 8665, loss is 4.743276596069336\n",
      "(64, 33)\n",
      "step 8666, loss is 4.889604091644287\n",
      "(64, 33)\n",
      "step 8667, loss is 4.925302028656006\n",
      "(64, 33)\n",
      "step 8668, loss is 4.834228038787842\n",
      "(64, 33)\n",
      "step 8669, loss is 4.979655742645264\n",
      "(64, 33)\n",
      "step 8670, loss is 4.786590576171875\n",
      "(64, 33)\n",
      "step 8671, loss is 4.545435428619385\n",
      "(64, 33)\n",
      "step 8672, loss is 4.874456882476807\n",
      "(64, 33)\n",
      "step 8673, loss is 4.902727127075195\n",
      "(64, 33)\n",
      "step 8674, loss is 4.843734264373779\n",
      "(64, 33)\n",
      "step 8675, loss is 4.772017955780029\n",
      "(64, 33)\n",
      "step 8676, loss is 4.8456315994262695\n",
      "(64, 33)\n",
      "step 8677, loss is 4.756247520446777\n",
      "(64, 33)\n",
      "step 8678, loss is 4.746379852294922\n",
      "(64, 33)\n",
      "step 8679, loss is 4.965294361114502\n",
      "(64, 33)\n",
      "step 8680, loss is 4.837800025939941\n",
      "(64, 33)\n",
      "step 8681, loss is 4.8498382568359375\n",
      "(64, 33)\n",
      "step 8682, loss is 4.778077125549316\n",
      "(64, 33)\n",
      "step 8683, loss is 4.868481636047363\n",
      "(64, 33)\n",
      "step 8684, loss is 4.924723148345947\n",
      "(64, 33)\n",
      "step 8685, loss is 4.848141193389893\n",
      "(64, 33)\n",
      "step 8686, loss is 4.670168876647949\n",
      "(64, 33)\n",
      "step 8687, loss is 4.872895240783691\n",
      "(64, 33)\n",
      "step 8688, loss is 4.926543235778809\n",
      "(64, 33)\n",
      "step 8689, loss is 4.98960018157959\n",
      "(64, 33)\n",
      "step 8690, loss is 4.876391887664795\n",
      "(64, 33)\n",
      "step 8691, loss is 4.870759010314941\n",
      "(64, 33)\n",
      "step 8692, loss is 4.803840160369873\n",
      "(64, 33)\n",
      "step 8693, loss is 4.943167209625244\n",
      "(64, 33)\n",
      "step 8694, loss is 4.856849670410156\n",
      "(64, 33)\n",
      "step 8695, loss is 4.981736183166504\n",
      "(64, 33)\n",
      "step 8696, loss is 4.858055114746094\n",
      "(64, 33)\n",
      "step 8697, loss is 4.786504745483398\n",
      "(64, 33)\n",
      "step 8698, loss is 4.786964416503906\n",
      "(64, 33)\n",
      "step 8699, loss is 4.85048770904541\n",
      "(64, 33)\n",
      "step 8700, loss is 4.868013381958008\n",
      "(64, 33)\n",
      "step 8701, loss is 4.7939043045043945\n",
      "(64, 33)\n",
      "step 8702, loss is 4.694149494171143\n",
      "(64, 33)\n",
      "step 8703, loss is 4.717286109924316\n",
      "(64, 33)\n",
      "step 8704, loss is 4.752357006072998\n",
      "(64, 33)\n",
      "step 8705, loss is 4.839528560638428\n",
      "(64, 33)\n",
      "step 8706, loss is 4.636480331420898\n",
      "(64, 33)\n",
      "step 8707, loss is 4.920300006866455\n",
      "(64, 33)\n",
      "step 8708, loss is 4.919405937194824\n",
      "(64, 33)\n",
      "step 8709, loss is 5.061014652252197\n",
      "(64, 33)\n",
      "step 8710, loss is 4.869263648986816\n",
      "(64, 33)\n",
      "step 8711, loss is 4.867056369781494\n",
      "(64, 33)\n",
      "step 8712, loss is 4.885479927062988\n",
      "(64, 33)\n",
      "step 8713, loss is 4.962430953979492\n",
      "(64, 33)\n",
      "step 8714, loss is 5.0671772956848145\n",
      "(64, 33)\n",
      "step 8715, loss is 4.747429370880127\n",
      "(64, 33)\n",
      "step 8716, loss is 4.831679821014404\n",
      "(64, 33)\n",
      "step 8717, loss is 4.854031085968018\n",
      "(64, 33)\n",
      "step 8718, loss is 4.9429192543029785\n",
      "(64, 33)\n",
      "step 8719, loss is 4.827224254608154\n",
      "(64, 33)\n",
      "step 8720, loss is 4.788803577423096\n",
      "(64, 33)\n",
      "step 8721, loss is 4.806417942047119\n",
      "(64, 33)\n",
      "step 8722, loss is 5.129037380218506\n",
      "(64, 33)\n",
      "step 8723, loss is 4.788457870483398\n",
      "(64, 33)\n",
      "step 8724, loss is 4.855036735534668\n",
      "(64, 33)\n",
      "step 8725, loss is 4.8532795906066895\n",
      "(64, 33)\n",
      "step 8726, loss is 4.968440532684326\n",
      "(64, 33)\n",
      "step 8727, loss is 4.812970161437988\n",
      "(64, 33)\n",
      "step 8728, loss is 4.679003715515137\n",
      "(64, 33)\n",
      "step 8729, loss is 4.933879375457764\n",
      "(64, 33)\n",
      "step 8730, loss is 4.830491065979004\n",
      "(64, 33)\n",
      "step 8731, loss is 4.863654136657715\n",
      "(64, 33)\n",
      "step 8732, loss is 5.1542134284973145\n",
      "(64, 33)\n",
      "step 8733, loss is 4.7558674812316895\n",
      "(64, 33)\n",
      "step 8734, loss is 4.788618564605713\n",
      "(64, 33)\n",
      "step 8735, loss is 4.8373565673828125\n",
      "(64, 33)\n",
      "step 8736, loss is 4.841287136077881\n",
      "(64, 33)\n",
      "step 8737, loss is 5.050597667694092\n",
      "(64, 33)\n",
      "step 8738, loss is 4.799968719482422\n",
      "(64, 33)\n",
      "step 8739, loss is 4.802101135253906\n",
      "(64, 33)\n",
      "step 8740, loss is 4.7608137130737305\n",
      "(64, 33)\n",
      "step 8741, loss is 4.758640289306641\n",
      "(64, 33)\n",
      "step 8742, loss is 4.642544746398926\n",
      "(64, 33)\n",
      "step 8743, loss is 4.9593634605407715\n",
      "(64, 33)\n",
      "step 8744, loss is 4.776401519775391\n",
      "(64, 33)\n",
      "step 8745, loss is 4.881475448608398\n",
      "(64, 33)\n",
      "step 8746, loss is 4.884486675262451\n",
      "(64, 33)\n",
      "step 8747, loss is 4.860222816467285\n",
      "(64, 33)\n",
      "step 8748, loss is 4.745990753173828\n",
      "(64, 33)\n",
      "step 8749, loss is 4.866996765136719\n",
      "(64, 33)\n",
      "step 8750, loss is 4.819583892822266\n",
      "(64, 33)\n",
      "step 8751, loss is 4.884547233581543\n",
      "(64, 33)\n",
      "step 8752, loss is 4.939420223236084\n",
      "(64, 33)\n",
      "step 8753, loss is 5.067027568817139\n",
      "(64, 33)\n",
      "step 8754, loss is 4.934805393218994\n",
      "(64, 33)\n",
      "step 8755, loss is 4.949126243591309\n",
      "(64, 33)\n",
      "step 8756, loss is 4.856319427490234\n",
      "(64, 33)\n",
      "step 8757, loss is 4.819623947143555\n",
      "(64, 33)\n",
      "step 8758, loss is 4.648857593536377\n",
      "(64, 33)\n",
      "step 8759, loss is 4.867393493652344\n",
      "(64, 33)\n",
      "step 8760, loss is 4.874050140380859\n",
      "(64, 33)\n",
      "step 8761, loss is 4.690730571746826\n",
      "(64, 33)\n",
      "step 8762, loss is 4.80171012878418\n",
      "(64, 33)\n",
      "step 8763, loss is 4.9702982902526855\n",
      "(64, 33)\n",
      "step 8764, loss is 4.950749397277832\n",
      "(64, 33)\n",
      "step 8765, loss is 4.766120910644531\n",
      "(64, 33)\n",
      "step 8766, loss is 4.6820878982543945\n",
      "(64, 33)\n",
      "step 8767, loss is 4.831997871398926\n",
      "(64, 33)\n",
      "step 8768, loss is 4.7615885734558105\n",
      "(64, 33)\n",
      "step 8769, loss is 4.839651107788086\n",
      "(64, 33)\n",
      "step 8770, loss is 4.855484962463379\n",
      "(64, 33)\n",
      "step 8771, loss is 4.957477569580078\n",
      "(64, 33)\n",
      "step 8772, loss is 4.7960944175720215\n",
      "(64, 33)\n",
      "step 8773, loss is 4.9197773933410645\n",
      "(64, 33)\n",
      "step 8774, loss is 4.809003829956055\n",
      "(64, 33)\n",
      "step 8775, loss is 4.936490535736084\n",
      "(64, 33)\n",
      "step 8776, loss is 4.892176628112793\n",
      "(64, 33)\n",
      "step 8777, loss is 4.799083232879639\n",
      "(64, 33)\n",
      "step 8778, loss is 4.910715103149414\n",
      "(64, 33)\n",
      "step 8779, loss is 4.98703670501709\n",
      "(64, 33)\n",
      "step 8780, loss is 4.912468910217285\n",
      "(64, 33)\n",
      "step 8781, loss is 4.810434818267822\n",
      "(64, 33)\n",
      "step 8782, loss is 4.935323238372803\n",
      "(64, 33)\n",
      "step 8783, loss is 4.9219207763671875\n",
      "(64, 33)\n",
      "step 8784, loss is 4.754772663116455\n",
      "(64, 33)\n",
      "step 8785, loss is 4.987204074859619\n",
      "(64, 33)\n",
      "step 8786, loss is 5.00980806350708\n",
      "(64, 33)\n",
      "step 8787, loss is 4.904224872589111\n",
      "(64, 33)\n",
      "step 8788, loss is 4.826254367828369\n",
      "(64, 33)\n",
      "step 8789, loss is 4.855874538421631\n",
      "(64, 33)\n",
      "step 8790, loss is 4.695926666259766\n",
      "(64, 33)\n",
      "step 8791, loss is 4.952223777770996\n",
      "(64, 33)\n",
      "step 8792, loss is 4.787248134613037\n",
      "(64, 33)\n",
      "step 8793, loss is 4.779853820800781\n",
      "(64, 33)\n",
      "step 8794, loss is 5.037027835845947\n",
      "(64, 33)\n",
      "step 8795, loss is 4.735711574554443\n",
      "(64, 33)\n",
      "step 8796, loss is 4.847949504852295\n",
      "(64, 33)\n",
      "step 8797, loss is 4.827535629272461\n",
      "(64, 33)\n",
      "step 8798, loss is 4.777711391448975\n",
      "(64, 33)\n",
      "step 8799, loss is 4.808316230773926\n",
      "(64, 33)\n",
      "step 8800, loss is 5.068521976470947\n",
      "(64, 33)\n",
      "step 8801, loss is 4.878091335296631\n",
      "(64, 33)\n",
      "step 8802, loss is 4.792597770690918\n",
      "(64, 33)\n",
      "step 8803, loss is 4.800084114074707\n",
      "(64, 33)\n",
      "step 8804, loss is 4.8928303718566895\n",
      "(64, 33)\n",
      "step 8805, loss is 4.585319519042969\n",
      "(64, 33)\n",
      "step 8806, loss is 4.633984565734863\n",
      "(64, 33)\n",
      "step 8807, loss is 5.061628818511963\n",
      "(64, 33)\n",
      "step 8808, loss is 4.776909351348877\n",
      "(64, 33)\n",
      "step 8809, loss is 4.889809608459473\n",
      "(64, 33)\n",
      "step 8810, loss is 4.8071489334106445\n",
      "(64, 33)\n",
      "step 8811, loss is 4.94475793838501\n",
      "(64, 33)\n",
      "step 8812, loss is 4.563226222991943\n",
      "(64, 33)\n",
      "step 8813, loss is 4.861248970031738\n",
      "(64, 33)\n",
      "step 8814, loss is 5.101337909698486\n",
      "(64, 33)\n",
      "step 8815, loss is 4.782308101654053\n",
      "(64, 33)\n",
      "step 8816, loss is 5.089656829833984\n",
      "(64, 33)\n",
      "step 8817, loss is 4.837595462799072\n",
      "(64, 33)\n",
      "step 8818, loss is 4.888101100921631\n",
      "(64, 33)\n",
      "step 8819, loss is 4.73053503036499\n",
      "(64, 33)\n",
      "step 8820, loss is 4.67978048324585\n",
      "(64, 33)\n",
      "step 8821, loss is 4.999570846557617\n",
      "(64, 33)\n",
      "step 8822, loss is 4.933955192565918\n",
      "(64, 33)\n",
      "step 8823, loss is 4.79901123046875\n",
      "(64, 33)\n",
      "step 8824, loss is 4.9230756759643555\n",
      "(64, 33)\n",
      "step 8825, loss is 4.79975700378418\n",
      "(64, 33)\n",
      "step 8826, loss is 4.87553596496582\n",
      "(64, 33)\n",
      "step 8827, loss is 4.606813430786133\n",
      "(64, 33)\n",
      "step 8828, loss is 5.012068271636963\n",
      "(64, 33)\n",
      "step 8829, loss is 4.969273090362549\n",
      "(64, 33)\n",
      "step 8830, loss is 4.765983581542969\n",
      "(64, 33)\n",
      "step 8831, loss is 4.671168804168701\n",
      "(64, 33)\n",
      "step 8832, loss is 5.128859519958496\n",
      "(64, 33)\n",
      "step 8833, loss is 4.625554084777832\n",
      "(64, 33)\n",
      "step 8834, loss is 4.857287883758545\n",
      "(64, 33)\n",
      "step 8835, loss is 4.765591621398926\n",
      "(64, 33)\n",
      "step 8836, loss is 4.907803535461426\n",
      "(64, 33)\n",
      "step 8837, loss is 4.917813301086426\n",
      "(64, 33)\n",
      "step 8838, loss is 4.951516151428223\n",
      "(64, 33)\n",
      "step 8839, loss is 4.807472229003906\n",
      "(64, 33)\n",
      "step 8840, loss is 4.788213729858398\n",
      "(64, 33)\n",
      "step 8841, loss is 4.770533084869385\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8842, loss is 4.916675567626953\n",
      "(64, 33)\n",
      "step 8843, loss is 4.806386470794678\n",
      "(64, 33)\n",
      "step 8844, loss is 4.974677085876465\n",
      "(64, 33)\n",
      "step 8845, loss is 4.893170356750488\n",
      "(64, 33)\n",
      "step 8846, loss is 5.011499881744385\n",
      "(64, 33)\n",
      "step 8847, loss is 4.943744659423828\n",
      "(64, 33)\n",
      "step 8848, loss is 4.741706371307373\n",
      "(64, 33)\n",
      "step 8849, loss is 4.999318599700928\n",
      "(64, 33)\n",
      "step 8850, loss is 4.9974188804626465\n",
      "(64, 33)\n",
      "step 8851, loss is 4.832880973815918\n",
      "(64, 33)\n",
      "step 8852, loss is 4.719250202178955\n",
      "(64, 33)\n",
      "step 8853, loss is 4.768922805786133\n",
      "(64, 33)\n",
      "step 8854, loss is 4.815602779388428\n",
      "(64, 33)\n",
      "step 8855, loss is 4.771407127380371\n",
      "(64, 33)\n",
      "step 8856, loss is 4.867620468139648\n",
      "(64, 33)\n",
      "step 8857, loss is 4.745903015136719\n",
      "(64, 33)\n",
      "step 8858, loss is 4.783832550048828\n",
      "(64, 33)\n",
      "step 8859, loss is 4.780445098876953\n",
      "(64, 33)\n",
      "step 8860, loss is 4.737659931182861\n",
      "(64, 33)\n",
      "step 8861, loss is 4.821020126342773\n",
      "(64, 33)\n",
      "step 8862, loss is 4.846767425537109\n",
      "(64, 33)\n",
      "step 8863, loss is 4.950531959533691\n",
      "(64, 33)\n",
      "step 8864, loss is 4.684758186340332\n",
      "(64, 33)\n",
      "step 8865, loss is 4.711872100830078\n",
      "(64, 33)\n",
      "step 8866, loss is 5.019996643066406\n",
      "(64, 33)\n",
      "step 8867, loss is 5.027524471282959\n",
      "(64, 33)\n",
      "step 8868, loss is 4.910269260406494\n",
      "(64, 33)\n",
      "step 8869, loss is 4.907131195068359\n",
      "(64, 33)\n",
      "step 8870, loss is 4.760869026184082\n",
      "(64, 33)\n",
      "step 8871, loss is 4.8932576179504395\n",
      "(64, 33)\n",
      "step 8872, loss is 4.76832914352417\n",
      "(64, 33)\n",
      "step 8873, loss is 4.886794090270996\n",
      "(64, 33)\n",
      "step 8874, loss is 4.8215508460998535\n",
      "(64, 33)\n",
      "step 8875, loss is 4.992737293243408\n",
      "(64, 33)\n",
      "step 8876, loss is 4.680123805999756\n",
      "(64, 33)\n",
      "step 8877, loss is 4.841660022735596\n",
      "(64, 33)\n",
      "step 8878, loss is 4.763007164001465\n",
      "(64, 33)\n",
      "step 8879, loss is 4.957563877105713\n",
      "(64, 33)\n",
      "step 8880, loss is 4.775115966796875\n",
      "(64, 33)\n",
      "step 8881, loss is 4.7742509841918945\n",
      "(64, 33)\n",
      "step 8882, loss is 4.875604629516602\n",
      "(64, 33)\n",
      "step 8883, loss is 4.714901447296143\n",
      "(64, 33)\n",
      "step 8884, loss is 4.9968109130859375\n",
      "(64, 33)\n",
      "step 8885, loss is 4.923862457275391\n",
      "(64, 33)\n",
      "step 8886, loss is 4.964425563812256\n",
      "(64, 33)\n",
      "step 8887, loss is 4.7661871910095215\n",
      "(64, 33)\n",
      "step 8888, loss is 4.977872848510742\n",
      "(64, 33)\n",
      "step 8889, loss is 4.845823287963867\n",
      "(64, 33)\n",
      "step 8890, loss is 4.842765808105469\n",
      "(64, 33)\n",
      "step 8891, loss is 4.847742557525635\n",
      "(64, 33)\n",
      "step 8892, loss is 4.9766669273376465\n",
      "(64, 33)\n",
      "step 8893, loss is 4.795599937438965\n",
      "(64, 33)\n",
      "step 8894, loss is 4.777581214904785\n",
      "(64, 33)\n",
      "step 8895, loss is 4.99173641204834\n",
      "(64, 33)\n",
      "step 8896, loss is 4.8665595054626465\n",
      "(64, 33)\n",
      "step 8897, loss is 4.889225006103516\n",
      "(64, 33)\n",
      "step 8898, loss is 4.897757053375244\n",
      "(64, 33)\n",
      "step 8899, loss is 4.811789035797119\n",
      "(64, 33)\n",
      "step 8900, loss is 4.941269397735596\n",
      "(64, 33)\n",
      "step 8901, loss is 5.0568928718566895\n",
      "(64, 33)\n",
      "step 8902, loss is 4.815843105316162\n",
      "(64, 33)\n",
      "step 8903, loss is 4.900672435760498\n",
      "(64, 33)\n",
      "step 8904, loss is 4.770275115966797\n",
      "(64, 33)\n",
      "step 8905, loss is 4.920215129852295\n",
      "(64, 33)\n",
      "step 8906, loss is 5.061012268066406\n",
      "(64, 33)\n",
      "step 8907, loss is 4.713833808898926\n",
      "(64, 33)\n",
      "step 8908, loss is 4.843807220458984\n",
      "(64, 33)\n",
      "step 8909, loss is 4.783138751983643\n",
      "(64, 33)\n",
      "step 8910, loss is 4.917290210723877\n",
      "(64, 33)\n",
      "step 8911, loss is 5.123087406158447\n",
      "(64, 33)\n",
      "step 8912, loss is 4.711334705352783\n",
      "(64, 33)\n",
      "step 8913, loss is 5.018187522888184\n",
      "(64, 33)\n",
      "step 8914, loss is 4.98153018951416\n",
      "(64, 33)\n",
      "step 8915, loss is 4.923420429229736\n",
      "(64, 33)\n",
      "step 8916, loss is 4.918643474578857\n",
      "(64, 33)\n",
      "step 8917, loss is 4.75812292098999\n",
      "(64, 33)\n",
      "step 8918, loss is 4.798178672790527\n",
      "(64, 33)\n",
      "step 8919, loss is 4.68328857421875\n",
      "(64, 33)\n",
      "step 8920, loss is 4.64339017868042\n",
      "(64, 33)\n",
      "step 8921, loss is 5.098356246948242\n",
      "(64, 33)\n",
      "step 8922, loss is 4.989073753356934\n",
      "(64, 33)\n",
      "step 8923, loss is 4.833773612976074\n",
      "(64, 33)\n",
      "step 8924, loss is 4.816127300262451\n",
      "(64, 33)\n",
      "step 8925, loss is 4.9469499588012695\n",
      "(64, 33)\n",
      "step 8926, loss is 4.809601783752441\n",
      "(64, 33)\n",
      "step 8927, loss is 4.864711284637451\n",
      "(64, 33)\n",
      "step 8928, loss is 4.869234085083008\n",
      "(64, 33)\n",
      "step 8929, loss is 4.914592266082764\n",
      "(64, 33)\n",
      "step 8930, loss is 4.799487590789795\n",
      "(64, 33)\n",
      "step 8931, loss is 4.671546936035156\n",
      "(64, 33)\n",
      "step 8932, loss is 4.991547107696533\n",
      "(64, 33)\n",
      "step 8933, loss is 4.741387844085693\n",
      "(64, 33)\n",
      "step 8934, loss is 4.8003950119018555\n",
      "(64, 33)\n",
      "step 8935, loss is 4.885369300842285\n",
      "(64, 33)\n",
      "step 8936, loss is 4.8212995529174805\n",
      "(64, 33)\n",
      "step 8937, loss is 4.83170747756958\n",
      "(64, 33)\n",
      "step 8938, loss is 4.8347086906433105\n",
      "(64, 33)\n",
      "step 8939, loss is 5.0872416496276855\n",
      "(64, 33)\n",
      "step 8940, loss is 4.797645092010498\n",
      "(64, 33)\n",
      "step 8941, loss is 4.814828872680664\n",
      "(64, 33)\n",
      "step 8942, loss is 4.723274230957031\n",
      "(64, 33)\n",
      "step 8943, loss is 4.806831359863281\n",
      "(64, 33)\n",
      "step 8944, loss is 4.837602138519287\n",
      "(64, 33)\n",
      "step 8945, loss is 4.781809329986572\n",
      "(64, 33)\n",
      "step 8946, loss is 4.782256126403809\n",
      "(64, 33)\n",
      "step 8947, loss is 4.883011341094971\n",
      "(64, 33)\n",
      "step 8948, loss is 4.919389247894287\n",
      "(64, 33)\n",
      "step 8949, loss is 4.734895706176758\n",
      "(64, 33)\n",
      "step 8950, loss is 4.903833866119385\n",
      "(64, 33)\n",
      "step 8951, loss is 4.6392292976379395\n",
      "(64, 33)\n",
      "step 8952, loss is 4.850259304046631\n",
      "(64, 33)\n",
      "step 8953, loss is 4.918939590454102\n",
      "(64, 33)\n",
      "step 8954, loss is 4.918909072875977\n",
      "(64, 33)\n",
      "step 8955, loss is 4.778095245361328\n",
      "(64, 33)\n",
      "step 8956, loss is 4.867700576782227\n",
      "(64, 33)\n",
      "step 8957, loss is 4.904660224914551\n",
      "(64, 33)\n",
      "step 8958, loss is 4.888929843902588\n",
      "(64, 33)\n",
      "step 8959, loss is 4.880321025848389\n",
      "(64, 33)\n",
      "step 8960, loss is 4.951911926269531\n",
      "(64, 33)\n",
      "step 8961, loss is 4.850849628448486\n",
      "(64, 33)\n",
      "step 8962, loss is 5.034873962402344\n",
      "(64, 33)\n",
      "step 8963, loss is 4.942073822021484\n",
      "(64, 33)\n",
      "step 8964, loss is 4.984817028045654\n",
      "(64, 33)\n",
      "step 8965, loss is 4.798521995544434\n",
      "(64, 33)\n",
      "step 8966, loss is 4.723905563354492\n",
      "(64, 33)\n",
      "step 8967, loss is 4.922104835510254\n",
      "(64, 33)\n",
      "step 8968, loss is 4.939168930053711\n",
      "(64, 33)\n",
      "step 8969, loss is 4.718681335449219\n",
      "(64, 33)\n",
      "step 8970, loss is 4.7729387283325195\n",
      "(64, 33)\n",
      "step 8971, loss is 4.784338474273682\n",
      "(64, 33)\n",
      "step 8972, loss is 4.974277019500732\n",
      "(64, 33)\n",
      "step 8973, loss is 5.042039394378662\n",
      "(64, 33)\n",
      "step 8974, loss is 4.859689712524414\n",
      "(64, 33)\n",
      "step 8975, loss is 4.8125386238098145\n",
      "(64, 33)\n",
      "step 8976, loss is 4.864596843719482\n",
      "(64, 33)\n",
      "step 8977, loss is 4.852356433868408\n",
      "(64, 33)\n",
      "step 8978, loss is 4.91159725189209\n",
      "(64, 33)\n",
      "step 8979, loss is 4.802624225616455\n",
      "(64, 33)\n",
      "step 8980, loss is 4.964337348937988\n",
      "(64, 33)\n",
      "step 8981, loss is 4.910465717315674\n",
      "(64, 33)\n",
      "step 8982, loss is 4.652530193328857\n",
      "(64, 33)\n",
      "step 8983, loss is 4.9575042724609375\n",
      "(64, 33)\n",
      "step 8984, loss is 4.715141773223877\n",
      "(64, 33)\n",
      "step 8985, loss is 4.929261207580566\n",
      "(64, 33)\n",
      "step 8986, loss is 4.969359397888184\n",
      "(64, 33)\n",
      "step 8987, loss is 4.924410820007324\n",
      "(64, 33)\n",
      "step 8988, loss is 4.841916084289551\n",
      "(64, 33)\n",
      "step 8989, loss is 4.836758136749268\n",
      "(64, 33)\n",
      "step 8990, loss is 4.847622871398926\n",
      "(64, 33)\n",
      "step 8991, loss is 4.951855659484863\n",
      "(64, 33)\n",
      "step 8992, loss is 4.6649861335754395\n",
      "(64, 33)\n",
      "step 8993, loss is 4.820065021514893\n",
      "(64, 33)\n",
      "step 8994, loss is 5.068459510803223\n",
      "(64, 33)\n",
      "step 8995, loss is 4.91520357131958\n",
      "(64, 33)\n",
      "step 8996, loss is 4.920711517333984\n",
      "(64, 33)\n",
      "step 8997, loss is 4.82920503616333\n",
      "(64, 33)\n",
      "step 8998, loss is 4.704456806182861\n",
      "(64, 33)\n",
      "step 8999, loss is 4.696342468261719\n",
      "(64, 33)\n",
      "step 9000, loss is 5.107629776000977\n",
      "(64, 33)\n",
      "step 9001, loss is 4.8564629554748535\n",
      "(64, 33)\n",
      "step 9002, loss is 4.832024097442627\n",
      "(64, 33)\n",
      "step 9003, loss is 4.837979316711426\n",
      "(64, 33)\n",
      "step 9004, loss is 4.933485507965088\n",
      "(64, 33)\n",
      "step 9005, loss is 4.948549747467041\n",
      "(64, 33)\n",
      "step 9006, loss is 4.900332450866699\n",
      "(64, 33)\n",
      "step 9007, loss is 4.961184978485107\n",
      "(64, 33)\n",
      "step 9008, loss is 4.799310207366943\n",
      "(64, 33)\n",
      "step 9009, loss is 4.568938732147217\n",
      "(64, 33)\n",
      "step 9010, loss is 4.898005485534668\n",
      "(64, 33)\n",
      "step 9011, loss is 4.891307353973389\n",
      "(64, 33)\n",
      "step 9012, loss is 4.997170925140381\n",
      "(64, 33)\n",
      "step 9013, loss is 4.924598693847656\n",
      "(64, 33)\n",
      "step 9014, loss is 4.698829650878906\n",
      "(64, 33)\n",
      "step 9015, loss is 4.9661736488342285\n",
      "(64, 33)\n",
      "step 9016, loss is 4.766349792480469\n",
      "(64, 33)\n",
      "step 9017, loss is 4.781230449676514\n",
      "(64, 33)\n",
      "step 9018, loss is 4.884079456329346\n",
      "(64, 33)\n",
      "step 9019, loss is 4.815793991088867\n",
      "(64, 33)\n",
      "step 9020, loss is 4.762849807739258\n",
      "(64, 33)\n",
      "step 9021, loss is 4.758360862731934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 9022, loss is 5.05539083480835\n",
      "(64, 33)\n",
      "step 9023, loss is 4.878798961639404\n",
      "(64, 33)\n",
      "step 9024, loss is 4.882126808166504\n",
      "(64, 33)\n",
      "step 9025, loss is 4.940234184265137\n",
      "(64, 33)\n",
      "step 9026, loss is 4.842532634735107\n",
      "(64, 33)\n",
      "step 9027, loss is 4.85650634765625\n",
      "(64, 33)\n",
      "step 9028, loss is 5.004770755767822\n",
      "(64, 33)\n",
      "step 9029, loss is 4.8729658126831055\n",
      "(64, 33)\n",
      "step 9030, loss is 4.917237281799316\n",
      "(64, 33)\n",
      "step 9031, loss is 4.847021102905273\n",
      "(64, 33)\n",
      "step 9032, loss is 4.837921619415283\n",
      "(64, 33)\n",
      "step 9033, loss is 4.767117500305176\n",
      "(64, 33)\n",
      "step 9034, loss is 4.8236846923828125\n",
      "(64, 33)\n",
      "step 9035, loss is 4.792205810546875\n",
      "(64, 33)\n",
      "step 9036, loss is 4.7959442138671875\n",
      "(64, 33)\n",
      "step 9037, loss is 4.881141185760498\n",
      "(64, 33)\n",
      "step 9038, loss is 4.875269889831543\n",
      "(64, 33)\n",
      "step 9039, loss is 4.970145225524902\n",
      "(64, 33)\n",
      "step 9040, loss is 4.583826065063477\n",
      "(64, 33)\n",
      "step 9041, loss is 4.711932182312012\n",
      "(64, 33)\n",
      "step 9042, loss is 4.853128433227539\n",
      "(64, 33)\n",
      "step 9043, loss is 4.836700916290283\n",
      "(64, 33)\n",
      "step 9044, loss is 4.9879021644592285\n",
      "(64, 33)\n",
      "step 9045, loss is 4.747426986694336\n",
      "(64, 33)\n",
      "step 9046, loss is 5.024666786193848\n",
      "(64, 33)\n",
      "step 9047, loss is 4.746666431427002\n",
      "(64, 33)\n",
      "step 9048, loss is 4.663171768188477\n",
      "(64, 33)\n",
      "step 9049, loss is 4.821682929992676\n",
      "(64, 33)\n",
      "step 9050, loss is 4.833323955535889\n",
      "(64, 33)\n",
      "step 9051, loss is 4.792020797729492\n",
      "(64, 33)\n",
      "step 9052, loss is 4.892645359039307\n",
      "(64, 33)\n",
      "step 9053, loss is 4.894201278686523\n",
      "(64, 33)\n",
      "step 9054, loss is 5.0532026290893555\n",
      "(64, 33)\n",
      "step 9055, loss is 4.854501724243164\n",
      "(64, 33)\n",
      "step 9056, loss is 4.837285995483398\n",
      "(64, 33)\n",
      "step 9057, loss is 4.812992095947266\n",
      "(64, 33)\n",
      "step 9058, loss is 4.778830051422119\n",
      "(64, 33)\n",
      "step 9059, loss is 4.6851725578308105\n",
      "(64, 33)\n",
      "step 9060, loss is 4.844725608825684\n",
      "(64, 33)\n",
      "step 9061, loss is 4.691063404083252\n",
      "(64, 33)\n",
      "step 9062, loss is 4.871095657348633\n",
      "(64, 33)\n",
      "step 9063, loss is 4.870479583740234\n",
      "(64, 33)\n",
      "step 9064, loss is 4.884011745452881\n",
      "(64, 33)\n",
      "step 9065, loss is 4.952376842498779\n",
      "(64, 33)\n",
      "step 9066, loss is 4.933823585510254\n",
      "(64, 33)\n",
      "step 9067, loss is 4.8541669845581055\n",
      "(64, 33)\n",
      "step 9068, loss is 4.927509307861328\n",
      "(64, 33)\n",
      "step 9069, loss is 4.793055534362793\n",
      "(64, 33)\n",
      "step 9070, loss is 4.990553379058838\n",
      "(64, 33)\n",
      "step 9071, loss is 4.890617370605469\n",
      "(64, 33)\n",
      "step 9072, loss is 4.747239589691162\n",
      "(64, 33)\n",
      "step 9073, loss is 4.878387451171875\n",
      "(64, 33)\n",
      "step 9074, loss is 5.011083126068115\n",
      "(64, 33)\n",
      "step 9075, loss is 4.977931022644043\n",
      "(64, 33)\n",
      "step 9076, loss is 4.83986759185791\n",
      "(64, 33)\n",
      "step 9077, loss is 4.747369289398193\n",
      "(64, 33)\n",
      "step 9078, loss is 4.799092769622803\n",
      "(64, 33)\n",
      "step 9079, loss is 4.95274019241333\n",
      "(64, 33)\n",
      "step 9080, loss is 4.821847438812256\n",
      "(64, 33)\n",
      "step 9081, loss is 4.7643938064575195\n",
      "(64, 33)\n",
      "step 9082, loss is 4.777579307556152\n",
      "(64, 33)\n",
      "step 9083, loss is 4.869762420654297\n",
      "(64, 33)\n",
      "step 9084, loss is 5.163755416870117\n",
      "(64, 33)\n",
      "step 9085, loss is 4.821415901184082\n",
      "(64, 33)\n",
      "step 9086, loss is 4.918490409851074\n",
      "(64, 33)\n",
      "step 9087, loss is 4.747093677520752\n",
      "(64, 33)\n",
      "step 9088, loss is 4.8445024490356445\n",
      "(64, 33)\n",
      "step 9089, loss is 4.794972896575928\n",
      "(64, 33)\n",
      "step 9090, loss is 4.727669715881348\n",
      "(64, 33)\n",
      "step 9091, loss is 4.775531768798828\n",
      "(64, 33)\n",
      "step 9092, loss is 4.8479228019714355\n",
      "(64, 33)\n",
      "step 9093, loss is 4.885366916656494\n",
      "(64, 33)\n",
      "step 9094, loss is 4.957678318023682\n",
      "(64, 33)\n",
      "step 9095, loss is 4.803976535797119\n",
      "(64, 33)\n",
      "step 9096, loss is 4.832705497741699\n",
      "(64, 33)\n",
      "step 9097, loss is 4.824073791503906\n",
      "(64, 33)\n",
      "step 9098, loss is 4.782675743103027\n",
      "(64, 33)\n",
      "step 9099, loss is 4.773369789123535\n",
      "(64, 33)\n",
      "step 9100, loss is 4.739070892333984\n",
      "(64, 33)\n",
      "step 9101, loss is 4.7330217361450195\n",
      "(64, 33)\n",
      "step 9102, loss is 4.9566802978515625\n",
      "(64, 33)\n",
      "step 9103, loss is 4.875430107116699\n",
      "(64, 33)\n",
      "step 9104, loss is 4.707986354827881\n",
      "(64, 33)\n",
      "step 9105, loss is 4.941091537475586\n",
      "(64, 33)\n",
      "step 9106, loss is 4.742243766784668\n",
      "(64, 33)\n",
      "step 9107, loss is 4.747175216674805\n",
      "(64, 33)\n",
      "step 9108, loss is 4.874210357666016\n",
      "(64, 33)\n",
      "step 9109, loss is 4.784461975097656\n",
      "(64, 33)\n",
      "step 9110, loss is 4.845469951629639\n",
      "(64, 33)\n",
      "step 9111, loss is 4.906840801239014\n",
      "(64, 33)\n",
      "step 9112, loss is 4.908687114715576\n",
      "(64, 33)\n",
      "step 9113, loss is 4.926312446594238\n",
      "(64, 33)\n",
      "step 9114, loss is 4.820260047912598\n",
      "(64, 33)\n",
      "step 9115, loss is 4.693633556365967\n",
      "(64, 33)\n",
      "step 9116, loss is 4.960021018981934\n",
      "(64, 33)\n",
      "step 9117, loss is 4.810009956359863\n",
      "(64, 33)\n",
      "step 9118, loss is 4.854689121246338\n",
      "(64, 33)\n",
      "step 9119, loss is 4.7375102043151855\n",
      "(64, 33)\n",
      "step 9120, loss is 4.945036888122559\n",
      "(64, 33)\n",
      "step 9121, loss is 4.809886932373047\n",
      "(64, 33)\n",
      "step 9122, loss is 4.905207633972168\n",
      "(64, 33)\n",
      "step 9123, loss is 4.886538505554199\n",
      "(64, 33)\n",
      "step 9124, loss is 4.987633228302002\n",
      "(64, 33)\n",
      "step 9125, loss is 4.704751968383789\n",
      "(64, 33)\n",
      "step 9126, loss is 4.881972312927246\n",
      "(64, 33)\n",
      "step 9127, loss is 4.879937171936035\n",
      "(64, 33)\n",
      "step 9128, loss is 4.731593132019043\n",
      "(64, 33)\n",
      "step 9129, loss is 4.8976216316223145\n",
      "(64, 33)\n",
      "step 9130, loss is 5.085202217102051\n",
      "(64, 33)\n",
      "step 9131, loss is 4.854438781738281\n",
      "(64, 33)\n",
      "step 9132, loss is 4.756725788116455\n",
      "(64, 33)\n",
      "step 9133, loss is 4.839006423950195\n",
      "(64, 33)\n",
      "step 9134, loss is 4.839705944061279\n",
      "(64, 33)\n",
      "step 9135, loss is 4.7699666023254395\n",
      "(64, 33)\n",
      "step 9136, loss is 4.705479621887207\n",
      "(64, 33)\n",
      "step 9137, loss is 4.919384002685547\n",
      "(64, 33)\n",
      "step 9138, loss is 4.690784454345703\n",
      "(64, 33)\n",
      "step 9139, loss is 4.972986221313477\n",
      "(64, 33)\n",
      "step 9140, loss is 4.893341541290283\n",
      "(64, 33)\n",
      "step 9141, loss is 4.813910007476807\n",
      "(64, 33)\n",
      "step 9142, loss is 4.845326900482178\n",
      "(64, 33)\n",
      "step 9143, loss is 4.8676581382751465\n",
      "(64, 33)\n",
      "step 9144, loss is 4.782379627227783\n",
      "(64, 33)\n",
      "step 9145, loss is 4.7834792137146\n",
      "(64, 33)\n",
      "step 9146, loss is 4.855813980102539\n",
      "(64, 33)\n",
      "step 9147, loss is 5.030883312225342\n",
      "(64, 33)\n",
      "step 9148, loss is 4.975625514984131\n",
      "(64, 33)\n",
      "step 9149, loss is 4.626099586486816\n",
      "(64, 33)\n",
      "step 9150, loss is 4.916953086853027\n",
      "(64, 33)\n",
      "step 9151, loss is 5.002989768981934\n",
      "(64, 33)\n",
      "step 9152, loss is 4.754583358764648\n",
      "(64, 33)\n",
      "step 9153, loss is 4.812427997589111\n",
      "(64, 33)\n",
      "step 9154, loss is 4.9333391189575195\n",
      "(64, 33)\n",
      "step 9155, loss is 4.947768211364746\n",
      "(64, 33)\n",
      "step 9156, loss is 4.951754093170166\n",
      "(64, 33)\n",
      "step 9157, loss is 5.056857585906982\n",
      "(64, 33)\n",
      "step 9158, loss is 4.995331764221191\n",
      "(64, 33)\n",
      "step 9159, loss is 4.9212822914123535\n",
      "(64, 33)\n",
      "step 9160, loss is 4.8355326652526855\n",
      "(64, 33)\n",
      "step 9161, loss is 4.868003845214844\n",
      "(64, 33)\n",
      "step 9162, loss is 4.864385604858398\n",
      "(64, 33)\n",
      "step 9163, loss is 4.8015923500061035\n",
      "(64, 33)\n",
      "step 9164, loss is 4.980118274688721\n",
      "(64, 33)\n",
      "step 9165, loss is 4.852960109710693\n",
      "(64, 33)\n",
      "step 9166, loss is 4.854379177093506\n",
      "(64, 33)\n",
      "step 9167, loss is 4.960419654846191\n",
      "(64, 33)\n",
      "step 9168, loss is 4.700369834899902\n",
      "(64, 33)\n",
      "step 9169, loss is 4.910584926605225\n",
      "(64, 33)\n",
      "step 9170, loss is 4.81006383895874\n",
      "(64, 33)\n",
      "step 9171, loss is 4.835789680480957\n",
      "(64, 33)\n",
      "step 9172, loss is 4.818408489227295\n",
      "(64, 33)\n",
      "step 9173, loss is 4.738846302032471\n",
      "(64, 33)\n",
      "step 9174, loss is 4.8573455810546875\n",
      "(64, 33)\n",
      "step 9175, loss is 4.799858093261719\n",
      "(64, 33)\n",
      "step 9176, loss is 4.955604553222656\n",
      "(64, 33)\n",
      "step 9177, loss is 4.994271755218506\n",
      "(64, 33)\n",
      "step 9178, loss is 4.779360771179199\n",
      "(64, 33)\n",
      "step 9179, loss is 4.832614898681641\n",
      "(64, 33)\n",
      "step 9180, loss is 5.046874046325684\n",
      "(64, 33)\n",
      "step 9181, loss is 4.820556640625\n",
      "(64, 33)\n",
      "step 9182, loss is 4.863010883331299\n",
      "(64, 33)\n",
      "step 9183, loss is 4.699862003326416\n",
      "(64, 33)\n",
      "step 9184, loss is 4.715352535247803\n",
      "(64, 33)\n",
      "step 9185, loss is 4.761922836303711\n",
      "(64, 33)\n",
      "step 9186, loss is 4.808891296386719\n",
      "(64, 33)\n",
      "step 9187, loss is 4.970444679260254\n",
      "(64, 33)\n",
      "step 9188, loss is 4.821677207946777\n",
      "(64, 33)\n",
      "step 9189, loss is 4.948116302490234\n",
      "(64, 33)\n",
      "step 9190, loss is 4.696649551391602\n",
      "(64, 33)\n",
      "step 9191, loss is 4.9827094078063965\n",
      "(64, 33)\n",
      "step 9192, loss is 4.971955299377441\n",
      "(64, 33)\n",
      "step 9193, loss is 5.021732330322266\n",
      "(64, 33)\n",
      "step 9194, loss is 4.7704548835754395\n",
      "(64, 33)\n",
      "step 9195, loss is 4.921916484832764\n",
      "(64, 33)\n",
      "step 9196, loss is 4.906136512756348\n",
      "(64, 33)\n",
      "step 9197, loss is 4.9181809425354\n",
      "(64, 33)\n",
      "step 9198, loss is 4.844966411590576\n",
      "(64, 33)\n",
      "step 9199, loss is 4.808146953582764\n",
      "(64, 33)\n",
      "step 9200, loss is 4.837553977966309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 9201, loss is 4.8185038566589355\n",
      "(64, 33)\n",
      "step 9202, loss is 4.8928022384643555\n",
      "(64, 33)\n",
      "step 9203, loss is 4.834625720977783\n",
      "(64, 33)\n",
      "step 9204, loss is 4.8601789474487305\n",
      "(64, 33)\n",
      "step 9205, loss is 4.866652011871338\n",
      "(64, 33)\n",
      "step 9206, loss is 4.8215556144714355\n",
      "(64, 33)\n",
      "step 9207, loss is 4.7879767417907715\n",
      "(64, 33)\n",
      "step 9208, loss is 4.679620742797852\n",
      "(64, 33)\n",
      "step 9209, loss is 4.873448848724365\n",
      "(64, 33)\n",
      "step 9210, loss is 4.865081787109375\n",
      "(64, 33)\n",
      "step 9211, loss is 4.829731464385986\n",
      "(64, 33)\n",
      "step 9212, loss is 4.68548583984375\n",
      "(64, 33)\n",
      "step 9213, loss is 4.706222057342529\n",
      "(64, 33)\n",
      "step 9214, loss is 4.805788040161133\n",
      "(64, 33)\n",
      "step 9215, loss is 4.9177045822143555\n",
      "(64, 33)\n",
      "step 9216, loss is 4.886260032653809\n",
      "(64, 33)\n",
      "step 9217, loss is 4.639496803283691\n",
      "(64, 33)\n",
      "step 9218, loss is 4.848499298095703\n",
      "(64, 33)\n",
      "step 9219, loss is 4.996642589569092\n",
      "(64, 33)\n",
      "step 9220, loss is 4.748874187469482\n",
      "(64, 33)\n",
      "step 9221, loss is 5.155100345611572\n",
      "(64, 33)\n",
      "step 9222, loss is 4.771583080291748\n",
      "(64, 33)\n",
      "step 9223, loss is 4.710623264312744\n",
      "(64, 33)\n",
      "step 9224, loss is 4.91711950302124\n",
      "(64, 33)\n",
      "step 9225, loss is 4.677386283874512\n",
      "(64, 33)\n",
      "step 9226, loss is 5.13819694519043\n",
      "(64, 33)\n",
      "step 9227, loss is 4.838620662689209\n",
      "(64, 33)\n",
      "step 9228, loss is 4.86620569229126\n",
      "(64, 33)\n",
      "step 9229, loss is 5.02732515335083\n",
      "(64, 33)\n",
      "step 9230, loss is 5.0438055992126465\n",
      "(64, 33)\n",
      "step 9231, loss is 4.784058094024658\n",
      "(64, 33)\n",
      "step 9232, loss is 4.851088047027588\n",
      "(64, 33)\n",
      "step 9233, loss is 4.88136625289917\n",
      "(64, 33)\n",
      "step 9234, loss is 4.560844421386719\n",
      "(64, 33)\n",
      "step 9235, loss is 5.025893211364746\n",
      "(64, 33)\n",
      "step 9236, loss is 4.880728244781494\n",
      "(64, 33)\n",
      "step 9237, loss is 5.090437889099121\n",
      "(64, 33)\n",
      "step 9238, loss is 4.909762382507324\n",
      "(64, 33)\n",
      "step 9239, loss is 4.731955528259277\n",
      "(64, 33)\n",
      "step 9240, loss is 4.807257652282715\n",
      "(64, 33)\n",
      "step 9241, loss is 4.810848236083984\n",
      "(64, 33)\n",
      "step 9242, loss is 4.87855863571167\n",
      "(64, 33)\n",
      "step 9243, loss is 4.787748336791992\n",
      "(64, 33)\n",
      "step 9244, loss is 5.012387752532959\n",
      "(64, 33)\n",
      "step 9245, loss is 4.629963397979736\n",
      "(64, 33)\n",
      "step 9246, loss is 4.9304046630859375\n",
      "(64, 33)\n",
      "step 9247, loss is 4.9355010986328125\n",
      "(64, 33)\n",
      "step 9248, loss is 4.796665191650391\n",
      "(64, 33)\n",
      "step 9249, loss is 4.650436878204346\n",
      "(64, 33)\n",
      "step 9250, loss is 4.9919915199279785\n",
      "(64, 33)\n",
      "step 9251, loss is 4.835121154785156\n",
      "(64, 33)\n",
      "step 9252, loss is 4.604994773864746\n",
      "(64, 33)\n",
      "step 9253, loss is 4.670554161071777\n",
      "(64, 33)\n",
      "step 9254, loss is 4.744434833526611\n",
      "(64, 33)\n",
      "step 9255, loss is 4.818650722503662\n",
      "(64, 33)\n",
      "step 9256, loss is 4.8280558586120605\n",
      "(64, 33)\n",
      "step 9257, loss is 4.751590251922607\n",
      "(64, 33)\n",
      "step 9258, loss is 4.930324077606201\n",
      "(64, 33)\n",
      "step 9259, loss is 4.846779823303223\n",
      "(64, 33)\n",
      "step 9260, loss is 4.92635440826416\n",
      "(64, 33)\n",
      "step 9261, loss is 4.859048366546631\n",
      "(64, 33)\n",
      "step 9262, loss is 4.98431921005249\n",
      "(64, 33)\n",
      "step 9263, loss is 4.926732063293457\n",
      "(64, 33)\n",
      "step 9264, loss is 4.758108615875244\n",
      "(64, 33)\n",
      "step 9265, loss is 4.763978004455566\n",
      "(64, 33)\n",
      "step 9266, loss is 4.663437843322754\n",
      "(64, 33)\n",
      "step 9267, loss is 4.867969989776611\n",
      "(64, 33)\n",
      "step 9268, loss is 4.757925033569336\n",
      "(64, 33)\n",
      "step 9269, loss is 4.766478061676025\n",
      "(64, 33)\n",
      "step 9270, loss is 4.774734020233154\n",
      "(64, 33)\n",
      "step 9271, loss is 5.113250732421875\n",
      "(64, 33)\n",
      "step 9272, loss is 4.8471221923828125\n",
      "(64, 33)\n",
      "step 9273, loss is 4.917670249938965\n",
      "(64, 33)\n",
      "step 9274, loss is 4.837625980377197\n",
      "(64, 33)\n",
      "step 9275, loss is 4.6640095710754395\n",
      "(64, 33)\n",
      "step 9276, loss is 4.888904094696045\n",
      "(64, 33)\n",
      "step 9277, loss is 4.906181335449219\n",
      "(64, 33)\n",
      "step 9278, loss is 4.677951335906982\n",
      "(64, 33)\n",
      "step 9279, loss is 4.870920181274414\n",
      "(64, 33)\n",
      "step 9280, loss is 4.8395819664001465\n",
      "(64, 33)\n",
      "step 9281, loss is 4.868483543395996\n",
      "(64, 33)\n",
      "step 9282, loss is 4.821830749511719\n",
      "(64, 33)\n",
      "step 9283, loss is 4.915266036987305\n",
      "(64, 33)\n",
      "step 9284, loss is 4.816314697265625\n",
      "(64, 33)\n",
      "step 9285, loss is 4.826796054840088\n",
      "(64, 33)\n",
      "step 9286, loss is 5.090547561645508\n",
      "(64, 33)\n",
      "step 9287, loss is 4.789896011352539\n",
      "(64, 33)\n",
      "step 9288, loss is 4.869152545928955\n",
      "(64, 33)\n",
      "step 9289, loss is 4.785556793212891\n",
      "(64, 33)\n",
      "step 9290, loss is 4.659518718719482\n",
      "(64, 33)\n",
      "step 9291, loss is 4.8999857902526855\n",
      "(64, 33)\n",
      "step 9292, loss is 4.819843769073486\n",
      "(64, 33)\n",
      "step 9293, loss is 4.715636730194092\n",
      "(64, 33)\n",
      "step 9294, loss is 4.895373344421387\n",
      "(64, 33)\n",
      "step 9295, loss is 4.657223224639893\n",
      "(64, 33)\n",
      "step 9296, loss is 4.881206035614014\n",
      "(64, 33)\n",
      "step 9297, loss is 4.688052177429199\n",
      "(64, 33)\n",
      "step 9298, loss is 4.999454975128174\n",
      "(64, 33)\n",
      "step 9299, loss is 4.833380699157715\n",
      "(64, 33)\n",
      "step 9300, loss is 4.921507358551025\n",
      "(64, 33)\n",
      "step 9301, loss is 4.704239845275879\n",
      "(64, 33)\n",
      "step 9302, loss is 4.912930011749268\n",
      "(64, 33)\n",
      "step 9303, loss is 4.973817825317383\n",
      "(64, 33)\n",
      "step 9304, loss is 4.981719970703125\n",
      "(64, 33)\n",
      "step 9305, loss is 4.860705375671387\n",
      "(64, 33)\n",
      "step 9306, loss is 4.894287109375\n",
      "(64, 33)\n",
      "step 9307, loss is 4.784328937530518\n",
      "(64, 33)\n",
      "step 9308, loss is 4.882810115814209\n",
      "(64, 33)\n",
      "step 9309, loss is 4.962946891784668\n",
      "(64, 33)\n",
      "step 9310, loss is 4.903097629547119\n",
      "(64, 33)\n",
      "step 9311, loss is 4.956565856933594\n",
      "(64, 33)\n",
      "step 9312, loss is 4.732232093811035\n",
      "(64, 33)\n",
      "step 9313, loss is 4.966334819793701\n",
      "(64, 33)\n",
      "step 9314, loss is 4.817752361297607\n",
      "(64, 33)\n",
      "step 9315, loss is 4.7609381675720215\n",
      "(64, 33)\n",
      "step 9316, loss is 4.84165620803833\n",
      "(64, 33)\n",
      "step 9317, loss is 4.65022611618042\n",
      "(64, 33)\n",
      "step 9318, loss is 4.710258483886719\n",
      "(64, 33)\n",
      "step 9319, loss is 4.786473274230957\n",
      "(64, 33)\n",
      "step 9320, loss is 4.801581859588623\n",
      "(64, 33)\n",
      "step 9321, loss is 4.964745044708252\n",
      "(64, 33)\n",
      "step 9322, loss is 4.795163154602051\n",
      "(64, 33)\n",
      "step 9323, loss is 4.7744574546813965\n",
      "(64, 33)\n",
      "step 9324, loss is 4.699656009674072\n",
      "(64, 33)\n",
      "step 9325, loss is 4.796868324279785\n",
      "(64, 33)\n",
      "step 9326, loss is 4.9622273445129395\n",
      "(64, 33)\n",
      "step 9327, loss is 4.753689289093018\n",
      "(64, 33)\n",
      "step 9328, loss is 4.759665012359619\n",
      "(64, 33)\n",
      "step 9329, loss is 4.733878135681152\n",
      "(64, 33)\n",
      "step 9330, loss is 4.6564764976501465\n",
      "(64, 33)\n",
      "step 9331, loss is 4.944664001464844\n",
      "(64, 33)\n",
      "step 9332, loss is 4.849915027618408\n",
      "(64, 33)\n",
      "step 9333, loss is 5.103583812713623\n",
      "(64, 33)\n",
      "step 9334, loss is 4.810056686401367\n",
      "(64, 33)\n",
      "step 9335, loss is 4.963191986083984\n",
      "(64, 33)\n",
      "step 9336, loss is 4.780000686645508\n",
      "(64, 33)\n",
      "step 9337, loss is 4.774702548980713\n",
      "(64, 33)\n",
      "step 9338, loss is 4.727612495422363\n",
      "(64, 33)\n",
      "step 9339, loss is 4.791693687438965\n",
      "(64, 33)\n",
      "step 9340, loss is 4.971797943115234\n",
      "(64, 33)\n",
      "step 9341, loss is 4.896923542022705\n",
      "(64, 33)\n",
      "step 9342, loss is 4.685581684112549\n",
      "(64, 33)\n",
      "step 9343, loss is 4.8195271492004395\n",
      "(64, 33)\n",
      "step 9344, loss is 4.726173400878906\n",
      "(64, 33)\n",
      "step 9345, loss is 4.8523430824279785\n",
      "(64, 33)\n",
      "step 9346, loss is 4.907164573669434\n",
      "(64, 33)\n",
      "step 9347, loss is 4.8413615226745605\n",
      "(64, 33)\n",
      "step 9348, loss is 4.858872413635254\n",
      "(64, 33)\n",
      "step 9349, loss is 4.932957172393799\n",
      "(64, 33)\n",
      "step 9350, loss is 4.767060279846191\n",
      "(64, 33)\n",
      "step 9351, loss is 4.9148945808410645\n",
      "(64, 33)\n",
      "step 9352, loss is 4.76418924331665\n",
      "(64, 33)\n",
      "step 9353, loss is 4.8128485679626465\n",
      "(64, 33)\n",
      "step 9354, loss is 5.052192211151123\n",
      "(64, 33)\n",
      "step 9355, loss is 4.729548454284668\n",
      "(64, 33)\n",
      "step 9356, loss is 4.831277370452881\n",
      "(64, 33)\n",
      "step 9357, loss is 4.6511969566345215\n",
      "(64, 33)\n",
      "step 9358, loss is 4.821456432342529\n",
      "(64, 33)\n",
      "step 9359, loss is 5.0479841232299805\n",
      "(64, 33)\n",
      "step 9360, loss is 4.956747531890869\n",
      "(64, 33)\n",
      "step 9361, loss is 4.667840957641602\n",
      "(64, 33)\n",
      "step 9362, loss is 4.757492542266846\n",
      "(64, 33)\n",
      "step 9363, loss is 4.913776874542236\n",
      "(64, 33)\n",
      "step 9364, loss is 4.885875225067139\n",
      "(64, 33)\n",
      "step 9365, loss is 4.524585247039795\n",
      "(64, 33)\n",
      "step 9366, loss is 4.894001007080078\n",
      "(64, 33)\n",
      "step 9367, loss is 4.815269470214844\n",
      "(64, 33)\n",
      "step 9368, loss is 4.6900153160095215\n",
      "(64, 33)\n",
      "step 9369, loss is 4.790891647338867\n",
      "(64, 33)\n",
      "step 9370, loss is 4.753759860992432\n",
      "(64, 33)\n",
      "step 9371, loss is 4.898730754852295\n",
      "(64, 33)\n",
      "step 9372, loss is 5.024599552154541\n",
      "(64, 33)\n",
      "step 9373, loss is 4.808986663818359\n",
      "(64, 33)\n",
      "step 9374, loss is 4.689465522766113\n",
      "(64, 33)\n",
      "step 9375, loss is 4.831547737121582\n",
      "(64, 33)\n",
      "step 9376, loss is 4.86076021194458\n",
      "(64, 33)\n",
      "step 9377, loss is 4.9732136726379395\n",
      "(64, 33)\n",
      "step 9378, loss is 4.794860363006592\n",
      "(64, 33)\n",
      "step 9379, loss is 4.8130316734313965\n",
      "(64, 33)\n",
      "step 9380, loss is 4.750223636627197\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9381, loss is 4.932510852813721\n",
      "(64, 33)\n",
      "step 9382, loss is 4.753050327301025\n",
      "(64, 33)\n",
      "step 9383, loss is 4.829802989959717\n",
      "(64, 33)\n",
      "step 9384, loss is 4.900914192199707\n",
      "(64, 33)\n",
      "step 9385, loss is 4.80077600479126\n",
      "(64, 33)\n",
      "step 9386, loss is 4.818144798278809\n",
      "(64, 33)\n",
      "step 9387, loss is 4.838537216186523\n",
      "(64, 33)\n",
      "step 9388, loss is 4.888422966003418\n",
      "(64, 33)\n",
      "step 9389, loss is 4.701351165771484\n",
      "(64, 33)\n",
      "step 9390, loss is 4.913945198059082\n",
      "(64, 33)\n",
      "step 9391, loss is 4.8500237464904785\n",
      "(64, 33)\n",
      "step 9392, loss is 5.000083923339844\n",
      "(64, 33)\n",
      "step 9393, loss is 4.714450836181641\n",
      "(64, 33)\n",
      "step 9394, loss is 5.018120765686035\n",
      "(64, 33)\n",
      "step 9395, loss is 4.77708101272583\n",
      "(64, 33)\n",
      "step 9396, loss is 4.956940650939941\n",
      "(64, 33)\n",
      "step 9397, loss is 4.7995076179504395\n",
      "(64, 33)\n",
      "step 9398, loss is 4.797818183898926\n",
      "(64, 33)\n",
      "step 9399, loss is 4.7369465827941895\n",
      "(64, 33)\n",
      "step 9400, loss is 4.800117492675781\n",
      "(64, 33)\n",
      "step 9401, loss is 4.674898624420166\n",
      "(64, 33)\n",
      "step 9402, loss is 4.670646667480469\n",
      "(64, 33)\n",
      "step 9403, loss is 5.024592876434326\n",
      "(64, 33)\n",
      "step 9404, loss is 4.837124824523926\n",
      "(64, 33)\n",
      "step 9405, loss is 4.793500900268555\n",
      "(64, 33)\n",
      "step 9406, loss is 4.995642185211182\n",
      "(64, 33)\n",
      "step 9407, loss is 4.834458351135254\n",
      "(64, 33)\n",
      "step 9408, loss is 4.817424297332764\n",
      "(64, 33)\n",
      "step 9409, loss is 4.711928844451904\n",
      "(64, 33)\n",
      "step 9410, loss is 4.8121819496154785\n",
      "(64, 33)\n",
      "step 9411, loss is 4.754766464233398\n",
      "(64, 33)\n",
      "step 9412, loss is 4.868416786193848\n",
      "(64, 33)\n",
      "step 9413, loss is 4.857420921325684\n",
      "(64, 33)\n",
      "step 9414, loss is 4.971336364746094\n",
      "(64, 33)\n",
      "step 9415, loss is 4.719440460205078\n",
      "(64, 33)\n",
      "step 9416, loss is 4.761739253997803\n",
      "(64, 33)\n",
      "step 9417, loss is 4.820249080657959\n",
      "(64, 33)\n",
      "step 9418, loss is 4.709778308868408\n",
      "(64, 33)\n",
      "step 9419, loss is 4.926521301269531\n",
      "(64, 33)\n",
      "step 9420, loss is 4.884110450744629\n",
      "(64, 33)\n",
      "step 9421, loss is 4.8044939041137695\n",
      "(64, 33)\n",
      "step 9422, loss is 4.920034408569336\n",
      "(64, 33)\n",
      "step 9423, loss is 4.786332607269287\n",
      "(64, 33)\n",
      "step 9424, loss is 4.800542831420898\n",
      "(64, 33)\n",
      "step 9425, loss is 4.749570369720459\n",
      "(64, 33)\n",
      "step 9426, loss is 4.8073320388793945\n",
      "(64, 33)\n",
      "step 9427, loss is 4.849344253540039\n",
      "(64, 33)\n",
      "step 9428, loss is 4.688527584075928\n",
      "(64, 33)\n",
      "step 9429, loss is 4.818443298339844\n",
      "(64, 33)\n",
      "step 9430, loss is 4.78087043762207\n",
      "(64, 33)\n",
      "step 9431, loss is 4.791602611541748\n",
      "(64, 33)\n",
      "step 9432, loss is 4.810182571411133\n",
      "(64, 33)\n",
      "step 9433, loss is 4.820344924926758\n",
      "(64, 33)\n",
      "step 9434, loss is 4.854053497314453\n",
      "(64, 33)\n",
      "step 9435, loss is 4.852287769317627\n",
      "(64, 33)\n",
      "step 9436, loss is 4.786162376403809\n",
      "(64, 33)\n",
      "step 9437, loss is 4.906102657318115\n",
      "(64, 33)\n",
      "step 9438, loss is 4.98807430267334\n",
      "(64, 33)\n",
      "step 9439, loss is 4.714401721954346\n",
      "(64, 33)\n",
      "step 9440, loss is 4.896426677703857\n",
      "(64, 33)\n",
      "step 9441, loss is 4.606998443603516\n",
      "(64, 33)\n",
      "step 9442, loss is 5.031018257141113\n",
      "(64, 33)\n",
      "step 9443, loss is 4.775909423828125\n",
      "(64, 33)\n",
      "step 9444, loss is 4.733590602874756\n",
      "(64, 33)\n",
      "step 9445, loss is 4.854360580444336\n",
      "(64, 33)\n",
      "step 9446, loss is 4.834714889526367\n",
      "(64, 33)\n",
      "step 9447, loss is 4.81793212890625\n",
      "(64, 33)\n",
      "step 9448, loss is 4.9956440925598145\n",
      "(64, 33)\n",
      "step 9449, loss is 4.795489311218262\n",
      "(64, 33)\n",
      "step 9450, loss is 4.8892903327941895\n",
      "(64, 33)\n",
      "step 9451, loss is 4.920967102050781\n",
      "(64, 33)\n",
      "step 9452, loss is 4.758453845977783\n",
      "(64, 33)\n",
      "step 9453, loss is 4.813947677612305\n",
      "(64, 33)\n",
      "step 9454, loss is 4.80557107925415\n",
      "(64, 33)\n",
      "step 9455, loss is 4.905188083648682\n",
      "(64, 33)\n",
      "step 9456, loss is 4.770415306091309\n",
      "(64, 33)\n",
      "step 9457, loss is 4.551323413848877\n",
      "(64, 33)\n",
      "step 9458, loss is 4.859674453735352\n",
      "(64, 33)\n",
      "step 9459, loss is 4.850887298583984\n",
      "(64, 33)\n",
      "step 9460, loss is 5.024072170257568\n",
      "(64, 33)\n",
      "step 9461, loss is 4.92114782333374\n",
      "(64, 33)\n",
      "step 9462, loss is 4.861434459686279\n",
      "(64, 33)\n",
      "step 9463, loss is 4.8165411949157715\n",
      "(64, 33)\n",
      "step 9464, loss is 4.801598072052002\n",
      "(64, 33)\n",
      "step 9465, loss is 4.7599005699157715\n",
      "(64, 33)\n",
      "step 9466, loss is 4.842552185058594\n",
      "(64, 33)\n",
      "step 9467, loss is 4.824615001678467\n",
      "(64, 33)\n",
      "step 9468, loss is 4.9593424797058105\n",
      "(64, 33)\n",
      "step 9469, loss is 4.802801132202148\n",
      "(64, 33)\n",
      "step 9470, loss is 4.825004577636719\n",
      "(64, 33)\n",
      "step 9471, loss is 4.722026348114014\n",
      "(64, 33)\n",
      "step 9472, loss is 4.897329330444336\n",
      "(64, 33)\n",
      "step 9473, loss is 4.674504280090332\n",
      "(64, 33)\n",
      "step 9474, loss is 4.947671890258789\n",
      "(64, 33)\n",
      "step 9475, loss is 4.816910266876221\n",
      "(64, 33)\n",
      "step 9476, loss is 4.767983436584473\n",
      "(64, 33)\n",
      "step 9477, loss is 5.087921142578125\n",
      "(64, 33)\n",
      "step 9478, loss is 4.813818454742432\n",
      "(64, 33)\n",
      "step 9479, loss is 5.022946357727051\n",
      "(64, 33)\n",
      "step 9480, loss is 4.91485595703125\n",
      "(64, 33)\n",
      "step 9481, loss is 5.007818222045898\n",
      "(64, 33)\n",
      "step 9482, loss is 4.681761741638184\n",
      "(64, 33)\n",
      "step 9483, loss is 4.989758491516113\n",
      "(64, 33)\n",
      "step 9484, loss is 4.690847873687744\n",
      "(64, 33)\n",
      "step 9485, loss is 4.753319263458252\n",
      "(64, 33)\n",
      "step 9486, loss is 4.794843673706055\n",
      "(64, 33)\n",
      "step 9487, loss is 4.758176803588867\n",
      "(64, 33)\n",
      "step 9488, loss is 5.109413146972656\n",
      "(64, 33)\n",
      "step 9489, loss is 5.028410911560059\n",
      "(64, 33)\n",
      "step 9490, loss is 4.974057674407959\n",
      "(64, 33)\n",
      "step 9491, loss is 4.830125331878662\n",
      "(64, 33)\n",
      "step 9492, loss is 4.910077095031738\n",
      "(64, 33)\n",
      "step 9493, loss is 4.8844122886657715\n",
      "(64, 33)\n",
      "step 9494, loss is 4.6896467208862305\n",
      "(64, 33)\n",
      "step 9495, loss is 4.674881458282471\n",
      "(64, 33)\n",
      "step 9496, loss is 4.522953987121582\n",
      "(64, 33)\n",
      "step 9497, loss is 4.963841438293457\n",
      "(64, 33)\n",
      "step 9498, loss is 4.867185115814209\n",
      "(64, 33)\n",
      "step 9499, loss is 4.847265720367432\n",
      "(64, 33)\n",
      "step 9500, loss is 4.9484639167785645\n",
      "(64, 33)\n",
      "step 9501, loss is 4.90565824508667\n",
      "(64, 33)\n",
      "step 9502, loss is 4.783661842346191\n",
      "(64, 33)\n",
      "step 9503, loss is 4.689075469970703\n",
      "(64, 33)\n",
      "step 9504, loss is 4.798225402832031\n",
      "(64, 33)\n",
      "step 9505, loss is 4.69299840927124\n",
      "(64, 33)\n",
      "step 9506, loss is 4.921106815338135\n",
      "(64, 33)\n",
      "step 9507, loss is 4.954312801361084\n",
      "(64, 33)\n",
      "step 9508, loss is 4.71505880355835\n",
      "(64, 33)\n",
      "step 9509, loss is 4.821303367614746\n",
      "(64, 33)\n",
      "step 9510, loss is 4.813955783843994\n",
      "(64, 33)\n",
      "step 9511, loss is 4.8069939613342285\n",
      "(64, 33)\n",
      "step 9512, loss is 4.850574016571045\n",
      "(64, 33)\n",
      "step 9513, loss is 5.002548694610596\n",
      "(64, 33)\n",
      "step 9514, loss is 4.816726207733154\n",
      "(64, 33)\n",
      "step 9515, loss is 4.862253189086914\n",
      "(64, 33)\n",
      "step 9516, loss is 4.819660186767578\n",
      "(64, 33)\n",
      "step 9517, loss is 4.750450611114502\n",
      "(64, 33)\n",
      "step 9518, loss is 4.939606666564941\n",
      "(64, 33)\n",
      "step 9519, loss is 4.919948101043701\n",
      "(64, 33)\n",
      "step 9520, loss is 4.761330604553223\n",
      "(64, 33)\n",
      "step 9521, loss is 4.782362937927246\n",
      "(64, 33)\n",
      "step 9522, loss is 4.940523147583008\n",
      "(64, 33)\n",
      "step 9523, loss is 4.752817630767822\n",
      "(64, 33)\n",
      "step 9524, loss is 4.907890796661377\n",
      "(64, 33)\n",
      "step 9525, loss is 4.85115385055542\n",
      "(64, 33)\n",
      "step 9526, loss is 4.8080220222473145\n",
      "(64, 33)\n",
      "step 9527, loss is 4.807908535003662\n",
      "(64, 33)\n",
      "step 9528, loss is 4.740933418273926\n",
      "(64, 33)\n",
      "step 9529, loss is 4.731189250946045\n",
      "(64, 33)\n",
      "step 9530, loss is 5.017569065093994\n",
      "(64, 33)\n",
      "step 9531, loss is 4.9140801429748535\n",
      "(64, 33)\n",
      "step 9532, loss is 4.851224422454834\n",
      "(64, 33)\n",
      "step 9533, loss is 4.986650466918945\n",
      "(64, 33)\n",
      "step 9534, loss is 4.964432716369629\n",
      "(64, 33)\n",
      "step 9535, loss is 4.835404872894287\n",
      "(64, 33)\n",
      "step 9536, loss is 4.643801689147949\n",
      "(64, 33)\n",
      "step 9537, loss is 4.670572757720947\n",
      "(64, 33)\n",
      "step 9538, loss is 4.864746570587158\n",
      "(64, 33)\n",
      "step 9539, loss is 4.907145977020264\n",
      "(64, 33)\n",
      "step 9540, loss is 4.892721652984619\n",
      "(64, 33)\n",
      "step 9541, loss is 4.570727348327637\n",
      "(64, 33)\n",
      "step 9542, loss is 4.998858451843262\n",
      "(64, 33)\n",
      "step 9543, loss is 4.888352394104004\n",
      "(64, 33)\n",
      "step 9544, loss is 4.8329901695251465\n",
      "(64, 33)\n",
      "step 9545, loss is 5.038626670837402\n",
      "(64, 33)\n",
      "step 9546, loss is 4.813624382019043\n",
      "(64, 33)\n",
      "step 9547, loss is 5.044109344482422\n",
      "(64, 33)\n",
      "step 9548, loss is 4.895596504211426\n",
      "(64, 33)\n",
      "step 9549, loss is 4.85054349899292\n",
      "(64, 33)\n",
      "step 9550, loss is 4.848414897918701\n",
      "(64, 33)\n",
      "step 9551, loss is 4.836631774902344\n",
      "(64, 33)\n",
      "step 9552, loss is 4.894218921661377\n",
      "(64, 33)\n",
      "step 9553, loss is 4.86336612701416\n",
      "(64, 33)\n",
      "step 9554, loss is 5.070206165313721\n",
      "(64, 33)\n",
      "step 9555, loss is 4.838743209838867\n",
      "(64, 33)\n",
      "step 9556, loss is 4.767521381378174\n",
      "(64, 33)\n",
      "step 9557, loss is 4.8668999671936035\n",
      "(64, 33)\n",
      "step 9558, loss is 4.906917572021484\n",
      "(64, 33)\n",
      "step 9559, loss is 4.826645851135254\n",
      "(64, 33)\n",
      "step 9560, loss is 4.8501362800598145\n",
      "(64, 33)\n",
      "step 9561, loss is 4.994717121124268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 9562, loss is 4.78176212310791\n",
      "(64, 33)\n",
      "step 9563, loss is 4.7319207191467285\n",
      "(64, 33)\n",
      "step 9564, loss is 4.8603363037109375\n",
      "(64, 33)\n",
      "step 9565, loss is 4.72036600112915\n",
      "(64, 33)\n",
      "step 9566, loss is 4.97819185256958\n",
      "(64, 33)\n",
      "step 9567, loss is 4.823821067810059\n",
      "(64, 33)\n",
      "step 9568, loss is 4.77694034576416\n",
      "(64, 33)\n",
      "step 9569, loss is 4.784695148468018\n",
      "(64, 33)\n",
      "step 9570, loss is 4.724061965942383\n",
      "(64, 33)\n",
      "step 9571, loss is 4.942422866821289\n",
      "(64, 33)\n",
      "step 9572, loss is 4.890112400054932\n",
      "(64, 33)\n",
      "step 9573, loss is 4.984607696533203\n",
      "(64, 33)\n",
      "step 9574, loss is 4.656682014465332\n",
      "(64, 33)\n",
      "step 9575, loss is 4.888947010040283\n",
      "(64, 33)\n",
      "step 9576, loss is 4.864306926727295\n",
      "(64, 33)\n",
      "step 9577, loss is 4.668460369110107\n",
      "(64, 33)\n",
      "step 9578, loss is 4.679223537445068\n",
      "(64, 33)\n",
      "step 9579, loss is 4.803199768066406\n",
      "(64, 33)\n",
      "step 9580, loss is 4.776694297790527\n",
      "(64, 33)\n",
      "step 9581, loss is 4.751275062561035\n",
      "(64, 33)\n",
      "step 9582, loss is 4.826238632202148\n",
      "(64, 33)\n",
      "step 9583, loss is 4.791239261627197\n",
      "(64, 33)\n",
      "step 9584, loss is 4.765220642089844\n",
      "(64, 33)\n",
      "step 9585, loss is 4.890834331512451\n",
      "(64, 33)\n",
      "step 9586, loss is 4.886559963226318\n",
      "(64, 33)\n",
      "step 9587, loss is 5.100831031799316\n",
      "(64, 33)\n",
      "step 9588, loss is 4.920944690704346\n",
      "(64, 33)\n",
      "step 9589, loss is 4.89605712890625\n",
      "(64, 33)\n",
      "step 9590, loss is 4.759039402008057\n",
      "(64, 33)\n",
      "step 9591, loss is 4.803177833557129\n",
      "(64, 33)\n",
      "step 9592, loss is 4.8682780265808105\n",
      "(64, 33)\n",
      "step 9593, loss is 4.979869365692139\n",
      "(64, 33)\n",
      "step 9594, loss is 4.972896575927734\n",
      "(64, 33)\n",
      "step 9595, loss is 4.910697937011719\n",
      "(64, 33)\n",
      "step 9596, loss is 4.892401218414307\n",
      "(64, 33)\n",
      "step 9597, loss is 4.85189151763916\n",
      "(64, 33)\n",
      "step 9598, loss is 4.817357063293457\n",
      "(64, 33)\n",
      "step 9599, loss is 4.907632827758789\n",
      "(64, 33)\n",
      "step 9600, loss is 4.79116153717041\n",
      "(64, 33)\n",
      "step 9601, loss is 4.816894054412842\n",
      "(64, 33)\n",
      "step 9602, loss is 4.735269546508789\n",
      "(64, 33)\n",
      "step 9603, loss is 4.986642360687256\n",
      "(64, 33)\n",
      "step 9604, loss is 4.8261847496032715\n",
      "(64, 33)\n",
      "step 9605, loss is 4.914541244506836\n",
      "(64, 33)\n",
      "step 9606, loss is 4.758842468261719\n",
      "(64, 33)\n",
      "step 9607, loss is 4.719923496246338\n",
      "(64, 33)\n",
      "step 9608, loss is 4.847905158996582\n",
      "(64, 33)\n",
      "step 9609, loss is 4.743410587310791\n",
      "(64, 33)\n",
      "step 9610, loss is 4.933839797973633\n",
      "(64, 33)\n",
      "step 9611, loss is 4.752756118774414\n",
      "(64, 33)\n",
      "step 9612, loss is 4.791752338409424\n",
      "(64, 33)\n",
      "step 9613, loss is 4.868752956390381\n",
      "(64, 33)\n",
      "step 9614, loss is 4.840115070343018\n",
      "(64, 33)\n",
      "step 9615, loss is 4.7315545082092285\n",
      "(64, 33)\n",
      "step 9616, loss is 4.996578693389893\n",
      "(64, 33)\n",
      "step 9617, loss is 4.834736347198486\n",
      "(64, 33)\n",
      "step 9618, loss is 4.966976642608643\n",
      "(64, 33)\n",
      "step 9619, loss is 4.745537757873535\n",
      "(64, 33)\n",
      "step 9620, loss is 4.903097152709961\n",
      "(64, 33)\n",
      "step 9621, loss is 4.837889194488525\n",
      "(64, 33)\n",
      "step 9622, loss is 4.781336784362793\n",
      "(64, 33)\n",
      "step 9623, loss is 5.151119709014893\n",
      "(64, 33)\n",
      "step 9624, loss is 4.520677089691162\n",
      "(64, 33)\n",
      "step 9625, loss is 4.793618679046631\n",
      "(64, 33)\n",
      "step 9626, loss is 4.713054656982422\n",
      "(64, 33)\n",
      "step 9627, loss is 4.929272174835205\n",
      "(64, 33)\n",
      "step 9628, loss is 4.755324363708496\n",
      "(64, 33)\n",
      "step 9629, loss is 4.9004597663879395\n",
      "(64, 33)\n",
      "step 9630, loss is 4.746409893035889\n",
      "(64, 33)\n",
      "step 9631, loss is 4.816385746002197\n",
      "(64, 33)\n",
      "step 9632, loss is 4.790330410003662\n",
      "(64, 33)\n",
      "step 9633, loss is 4.796505451202393\n",
      "(64, 33)\n",
      "step 9634, loss is 4.733163833618164\n",
      "(64, 33)\n",
      "step 9635, loss is 4.74085807800293\n",
      "(64, 33)\n",
      "step 9636, loss is 4.995995998382568\n",
      "(64, 33)\n",
      "step 9637, loss is 4.888610363006592\n",
      "(64, 33)\n",
      "step 9638, loss is 4.958920478820801\n",
      "(64, 33)\n",
      "step 9639, loss is 4.772355556488037\n",
      "(64, 33)\n",
      "step 9640, loss is 4.911628723144531\n",
      "(64, 33)\n",
      "step 9641, loss is 4.967414855957031\n",
      "(64, 33)\n",
      "step 9642, loss is 4.905694484710693\n",
      "(64, 33)\n",
      "step 9643, loss is 4.9727091789245605\n",
      "(64, 33)\n",
      "step 9644, loss is 4.823326110839844\n",
      "(64, 33)\n",
      "step 9645, loss is 5.0397820472717285\n",
      "(64, 33)\n",
      "step 9646, loss is 4.82103157043457\n",
      "(64, 33)\n",
      "step 9647, loss is 4.972823619842529\n",
      "(64, 33)\n",
      "step 9648, loss is 4.885702133178711\n",
      "(64, 33)\n",
      "step 9649, loss is 4.753642559051514\n",
      "(64, 33)\n",
      "step 9650, loss is 5.052262306213379\n",
      "(64, 33)\n",
      "step 9651, loss is 4.856752395629883\n",
      "(64, 33)\n",
      "step 9652, loss is 4.932072639465332\n",
      "(64, 33)\n",
      "step 9653, loss is 4.825769424438477\n",
      "(64, 33)\n",
      "step 9654, loss is 4.881505966186523\n",
      "(64, 33)\n",
      "step 9655, loss is 4.899435997009277\n",
      "(64, 33)\n",
      "step 9656, loss is 4.902200222015381\n",
      "(64, 33)\n",
      "step 9657, loss is 4.866827487945557\n",
      "(64, 33)\n",
      "step 9658, loss is 4.823592662811279\n",
      "(64, 33)\n",
      "step 9659, loss is 4.751644134521484\n",
      "(64, 33)\n",
      "step 9660, loss is 4.664503574371338\n",
      "(64, 33)\n",
      "step 9661, loss is 4.860162734985352\n",
      "(64, 33)\n",
      "step 9662, loss is 5.112702369689941\n",
      "(64, 33)\n",
      "step 9663, loss is 4.769384860992432\n",
      "(64, 33)\n",
      "step 9664, loss is 4.819744110107422\n",
      "(64, 33)\n",
      "step 9665, loss is 4.864271640777588\n",
      "(64, 33)\n",
      "step 9666, loss is 4.729403972625732\n",
      "(64, 33)\n",
      "step 9667, loss is 4.775758266448975\n",
      "(64, 33)\n",
      "step 9668, loss is 4.9562668800354\n",
      "(64, 33)\n",
      "step 9669, loss is 4.91831111907959\n",
      "(64, 33)\n",
      "step 9670, loss is 4.743150234222412\n",
      "(64, 33)\n",
      "step 9671, loss is 4.692844867706299\n",
      "(64, 33)\n",
      "step 9672, loss is 4.801816463470459\n",
      "(64, 33)\n",
      "step 9673, loss is 4.955421447753906\n",
      "(64, 33)\n",
      "step 9674, loss is 4.755095958709717\n",
      "(64, 33)\n",
      "step 9675, loss is 4.873850345611572\n",
      "(64, 33)\n",
      "step 9676, loss is 4.8788676261901855\n",
      "(64, 33)\n",
      "step 9677, loss is 4.77098274230957\n",
      "(64, 33)\n",
      "step 9678, loss is 4.784513473510742\n",
      "(64, 33)\n",
      "step 9679, loss is 4.9424004554748535\n",
      "(64, 33)\n",
      "step 9680, loss is 4.783607482910156\n",
      "(64, 33)\n",
      "step 9681, loss is 4.7637810707092285\n",
      "(64, 33)\n",
      "step 9682, loss is 4.907293319702148\n",
      "(64, 33)\n",
      "step 9683, loss is 4.9348320960998535\n",
      "(64, 33)\n",
      "step 9684, loss is 4.810822486877441\n",
      "(64, 33)\n",
      "step 9685, loss is 4.870800495147705\n",
      "(64, 33)\n",
      "step 9686, loss is 4.804120063781738\n",
      "(64, 33)\n",
      "step 9687, loss is 4.879821300506592\n",
      "(64, 33)\n",
      "step 9688, loss is 4.773683547973633\n",
      "(64, 33)\n",
      "step 9689, loss is 5.087027549743652\n",
      "(64, 33)\n",
      "step 9690, loss is 4.830133438110352\n",
      "(64, 33)\n",
      "step 9691, loss is 4.846047401428223\n",
      "(64, 33)\n",
      "step 9692, loss is 4.69010591506958\n",
      "(64, 33)\n",
      "step 9693, loss is 4.999212741851807\n",
      "(64, 33)\n",
      "step 9694, loss is 4.894676685333252\n",
      "(64, 33)\n",
      "step 9695, loss is 4.878181457519531\n",
      "(64, 33)\n",
      "step 9696, loss is 4.747601509094238\n",
      "(64, 33)\n",
      "step 9697, loss is 4.884475231170654\n",
      "(64, 33)\n",
      "step 9698, loss is 5.011129856109619\n",
      "(64, 33)\n",
      "step 9699, loss is 4.902141094207764\n",
      "(64, 33)\n",
      "step 9700, loss is 4.877540111541748\n",
      "(64, 33)\n",
      "step 9701, loss is 4.841435432434082\n",
      "(64, 33)\n",
      "step 9702, loss is 4.901116847991943\n",
      "(64, 33)\n",
      "step 9703, loss is 4.857704162597656\n",
      "(64, 33)\n",
      "step 9704, loss is 4.860448837280273\n",
      "(64, 33)\n",
      "step 9705, loss is 4.694031715393066\n",
      "(64, 33)\n",
      "step 9706, loss is 4.702212333679199\n",
      "(64, 33)\n",
      "step 9707, loss is 4.785141944885254\n",
      "(64, 33)\n",
      "step 9708, loss is 4.861518859863281\n",
      "(64, 33)\n",
      "step 9709, loss is 5.050729274749756\n",
      "(64, 33)\n",
      "step 9710, loss is 4.907734394073486\n",
      "(64, 33)\n",
      "step 9711, loss is 5.063012599945068\n",
      "(64, 33)\n",
      "step 9712, loss is 4.839435577392578\n",
      "(64, 33)\n",
      "step 9713, loss is 4.948702812194824\n",
      "(64, 33)\n",
      "step 9714, loss is 4.771573066711426\n",
      "(64, 33)\n",
      "step 9715, loss is 4.885685443878174\n",
      "(64, 33)\n",
      "step 9716, loss is 4.752448558807373\n",
      "(64, 33)\n",
      "step 9717, loss is 4.684328556060791\n",
      "(64, 33)\n",
      "step 9718, loss is 4.815878391265869\n",
      "(64, 33)\n",
      "step 9719, loss is 4.848471641540527\n",
      "(64, 33)\n",
      "step 9720, loss is 4.743721961975098\n",
      "(64, 33)\n",
      "step 9721, loss is 4.79913330078125\n",
      "(64, 33)\n",
      "step 9722, loss is 4.833337306976318\n",
      "(64, 33)\n",
      "step 9723, loss is 4.780998706817627\n",
      "(64, 33)\n",
      "step 9724, loss is 4.944437503814697\n",
      "(64, 33)\n",
      "step 9725, loss is 4.876319408416748\n",
      "(64, 33)\n",
      "step 9726, loss is 4.902888774871826\n",
      "(64, 33)\n",
      "step 9727, loss is 4.852169036865234\n",
      "(64, 33)\n",
      "step 9728, loss is 4.806053161621094\n",
      "(64, 33)\n",
      "step 9729, loss is 4.811041355133057\n",
      "(64, 33)\n",
      "step 9730, loss is 5.009115695953369\n",
      "(64, 33)\n",
      "step 9731, loss is 4.988557815551758\n",
      "(64, 33)\n",
      "step 9732, loss is 4.754080295562744\n",
      "(64, 33)\n",
      "step 9733, loss is 4.96628999710083\n",
      "(64, 33)\n",
      "step 9734, loss is 4.811653137207031\n",
      "(64, 33)\n",
      "step 9735, loss is 4.765828609466553\n",
      "(64, 33)\n",
      "step 9736, loss is 4.749223709106445\n",
      "(64, 33)\n",
      "step 9737, loss is 4.710612773895264\n",
      "(64, 33)\n",
      "step 9738, loss is 4.89000940322876\n",
      "(64, 33)\n",
      "step 9739, loss is 4.9515380859375\n",
      "(64, 33)\n",
      "step 9740, loss is 4.616563320159912\n",
      "(64, 33)\n",
      "step 9741, loss is 4.739424228668213\n",
      "(64, 33)\n",
      "step 9742, loss is 4.946147441864014\n",
      "(64, 33)\n",
      "step 9743, loss is 4.855543613433838\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9744, loss is 4.976893901824951\n",
      "(64, 33)\n",
      "step 9745, loss is 4.914581775665283\n",
      "(64, 33)\n",
      "step 9746, loss is 4.757992744445801\n",
      "(64, 33)\n",
      "step 9747, loss is 5.077762603759766\n",
      "(64, 33)\n",
      "step 9748, loss is 4.9297261238098145\n",
      "(64, 33)\n",
      "step 9749, loss is 4.8115153312683105\n",
      "(64, 33)\n",
      "step 9750, loss is 4.72704553604126\n",
      "(64, 33)\n",
      "step 9751, loss is 4.826251983642578\n",
      "(64, 33)\n",
      "step 9752, loss is 4.699348449707031\n",
      "(64, 33)\n",
      "step 9753, loss is 4.930286407470703\n",
      "(64, 33)\n",
      "step 9754, loss is 4.688812255859375\n",
      "(64, 33)\n",
      "step 9755, loss is 4.850049018859863\n",
      "(64, 33)\n",
      "step 9756, loss is 4.886922359466553\n",
      "(64, 33)\n",
      "step 9757, loss is 4.887974739074707\n",
      "(64, 33)\n",
      "step 9758, loss is 4.81693696975708\n",
      "(64, 33)\n",
      "step 9759, loss is 4.76383113861084\n",
      "(64, 33)\n",
      "step 9760, loss is 4.818185329437256\n",
      "(64, 33)\n",
      "step 9761, loss is 4.866379261016846\n",
      "(64, 33)\n",
      "step 9762, loss is 4.863187789916992\n",
      "(64, 33)\n",
      "step 9763, loss is 4.793557643890381\n",
      "(64, 33)\n",
      "step 9764, loss is 4.9682297706604\n",
      "(64, 33)\n",
      "step 9765, loss is 4.746800899505615\n",
      "(64, 33)\n",
      "step 9766, loss is 4.8784613609313965\n",
      "(64, 33)\n",
      "step 9767, loss is 4.826861381530762\n",
      "(64, 33)\n",
      "step 9768, loss is 4.941081523895264\n",
      "(64, 33)\n",
      "step 9769, loss is 4.8530778884887695\n",
      "(64, 33)\n",
      "step 9770, loss is 4.868368625640869\n",
      "(64, 33)\n",
      "step 9771, loss is 4.991644382476807\n",
      "(64, 33)\n",
      "step 9772, loss is 4.808789253234863\n",
      "(64, 33)\n",
      "step 9773, loss is 4.8341593742370605\n",
      "(64, 33)\n",
      "step 9774, loss is 4.721037864685059\n",
      "(64, 33)\n",
      "step 9775, loss is 4.892996788024902\n",
      "(64, 33)\n",
      "step 9776, loss is 4.698075771331787\n",
      "(64, 33)\n",
      "step 9777, loss is 4.663508415222168\n",
      "(64, 33)\n",
      "step 9778, loss is 4.754815101623535\n",
      "(64, 33)\n",
      "step 9779, loss is 4.877867698669434\n",
      "(64, 33)\n",
      "step 9780, loss is 4.782370090484619\n",
      "(64, 33)\n",
      "step 9781, loss is 4.903286933898926\n",
      "(64, 33)\n",
      "step 9782, loss is 4.942700386047363\n",
      "(64, 33)\n",
      "step 9783, loss is 4.824527263641357\n",
      "(64, 33)\n",
      "step 9784, loss is 4.7563700675964355\n",
      "(64, 33)\n",
      "step 9785, loss is 4.897122383117676\n",
      "(64, 33)\n",
      "step 9786, loss is 4.808807849884033\n",
      "(64, 33)\n",
      "step 9787, loss is 4.831387996673584\n",
      "(64, 33)\n",
      "step 9788, loss is 4.7344970703125\n",
      "(64, 33)\n",
      "step 9789, loss is 4.821070671081543\n",
      "(64, 33)\n",
      "step 9790, loss is 4.798624515533447\n",
      "(64, 33)\n",
      "step 9791, loss is 4.919064998626709\n",
      "(64, 33)\n",
      "step 9792, loss is 4.879683494567871\n",
      "(64, 33)\n",
      "step 9793, loss is 4.938908576965332\n",
      "(64, 33)\n",
      "step 9794, loss is 4.9017109870910645\n",
      "(64, 33)\n",
      "step 9795, loss is 4.925395965576172\n",
      "(64, 33)\n",
      "step 9796, loss is 4.9138689041137695\n",
      "(64, 33)\n",
      "step 9797, loss is 4.900576591491699\n",
      "(64, 33)\n",
      "step 9798, loss is 4.7025227546691895\n",
      "(64, 33)\n",
      "step 9799, loss is 4.894705772399902\n",
      "(64, 33)\n",
      "step 9800, loss is 4.784017562866211\n",
      "(64, 33)\n",
      "step 9801, loss is 4.995896816253662\n",
      "(64, 33)\n",
      "step 9802, loss is 4.8019585609436035\n",
      "(64, 33)\n",
      "step 9803, loss is 4.846212863922119\n",
      "(64, 33)\n",
      "step 9804, loss is 4.7616047859191895\n",
      "(64, 33)\n",
      "step 9805, loss is 4.750820159912109\n",
      "(64, 33)\n",
      "step 9806, loss is 4.96122932434082\n",
      "(64, 33)\n",
      "step 9807, loss is 4.799514293670654\n",
      "(64, 33)\n",
      "step 9808, loss is 4.779694080352783\n",
      "(64, 33)\n",
      "step 9809, loss is 4.7460198402404785\n",
      "(64, 33)\n",
      "step 9810, loss is 4.9351372718811035\n",
      "(64, 33)\n",
      "step 9811, loss is 4.762760639190674\n",
      "(64, 33)\n",
      "step 9812, loss is 4.894220352172852\n",
      "(64, 33)\n",
      "step 9813, loss is 4.875638008117676\n",
      "(64, 33)\n",
      "step 9814, loss is 4.829728603363037\n",
      "(64, 33)\n",
      "step 9815, loss is 4.810956954956055\n",
      "(64, 33)\n",
      "step 9816, loss is 4.803465366363525\n",
      "(64, 33)\n",
      "step 9817, loss is 4.98552942276001\n",
      "(64, 33)\n",
      "step 9818, loss is 4.973690509796143\n",
      "(64, 33)\n",
      "step 9819, loss is 4.935772895812988\n",
      "(64, 33)\n",
      "step 9820, loss is 4.983806133270264\n",
      "(64, 33)\n",
      "step 9821, loss is 4.805235862731934\n",
      "(64, 33)\n",
      "step 9822, loss is 4.857324600219727\n",
      "(64, 33)\n",
      "step 9823, loss is 4.80075740814209\n",
      "(64, 33)\n",
      "step 9824, loss is 4.972146034240723\n",
      "(64, 33)\n",
      "step 9825, loss is 4.882134437561035\n",
      "(64, 33)\n",
      "step 9826, loss is 4.732761859893799\n",
      "(64, 33)\n",
      "step 9827, loss is 4.740877151489258\n",
      "(64, 33)\n",
      "step 9828, loss is 4.746922969818115\n",
      "(64, 33)\n",
      "step 9829, loss is 4.810070991516113\n",
      "(64, 33)\n",
      "step 9830, loss is 4.839570045471191\n",
      "(64, 33)\n",
      "step 9831, loss is 4.809805393218994\n",
      "(64, 33)\n",
      "step 9832, loss is 4.969267845153809\n",
      "(64, 33)\n",
      "step 9833, loss is 4.694406986236572\n",
      "(64, 33)\n",
      "step 9834, loss is 4.801716327667236\n",
      "(64, 33)\n",
      "step 9835, loss is 4.914679050445557\n",
      "(64, 33)\n",
      "step 9836, loss is 4.961158275604248\n",
      "(64, 33)\n",
      "step 9837, loss is 4.763596057891846\n",
      "(64, 33)\n",
      "step 9838, loss is 4.717249870300293\n",
      "(64, 33)\n",
      "step 9839, loss is 4.835268974304199\n",
      "(64, 33)\n",
      "step 9840, loss is 4.9350104331970215\n",
      "(64, 33)\n",
      "step 9841, loss is 4.952194690704346\n",
      "(64, 33)\n",
      "step 9842, loss is 4.78434944152832\n",
      "(64, 33)\n",
      "step 9843, loss is 4.8154988288879395\n",
      "(64, 33)\n",
      "step 9844, loss is 4.801321983337402\n",
      "(64, 33)\n",
      "step 9845, loss is 4.834191799163818\n",
      "(64, 33)\n",
      "step 9846, loss is 4.813319206237793\n",
      "(64, 33)\n",
      "step 9847, loss is 4.860043525695801\n",
      "(64, 33)\n",
      "step 9848, loss is 4.643308162689209\n",
      "(64, 33)\n",
      "step 9849, loss is 4.6625471115112305\n",
      "(64, 33)\n",
      "step 9850, loss is 4.663180828094482\n",
      "(64, 33)\n",
      "step 9851, loss is 4.893915176391602\n",
      "(64, 33)\n",
      "step 9852, loss is 4.88429594039917\n",
      "(64, 33)\n",
      "step 9853, loss is 4.987231731414795\n",
      "(64, 33)\n",
      "step 9854, loss is 4.755998611450195\n",
      "(64, 33)\n",
      "step 9855, loss is 4.909900188446045\n",
      "(64, 33)\n",
      "step 9856, loss is 4.9949822425842285\n",
      "(64, 33)\n",
      "step 9857, loss is 4.836227893829346\n",
      "(64, 33)\n",
      "step 9858, loss is 4.663623809814453\n",
      "(64, 33)\n",
      "step 9859, loss is 4.915561199188232\n",
      "(64, 33)\n",
      "step 9860, loss is 4.7869768142700195\n",
      "(64, 33)\n",
      "step 9861, loss is 4.858423709869385\n",
      "(64, 33)\n",
      "step 9862, loss is 4.899266242980957\n",
      "(64, 33)\n",
      "step 9863, loss is 4.682703971862793\n",
      "(64, 33)\n",
      "step 9864, loss is 4.928202152252197\n",
      "(64, 33)\n",
      "step 9865, loss is 4.864640235900879\n",
      "(64, 33)\n",
      "step 9866, loss is 4.927072525024414\n",
      "(64, 33)\n",
      "step 9867, loss is 4.594257831573486\n",
      "(64, 33)\n",
      "step 9868, loss is 4.842166900634766\n",
      "(64, 33)\n",
      "step 9869, loss is 5.0365681648254395\n",
      "(64, 33)\n",
      "step 9870, loss is 5.089502334594727\n",
      "(64, 33)\n",
      "step 9871, loss is 4.735260963439941\n",
      "(64, 33)\n",
      "step 9872, loss is 4.677124977111816\n",
      "(64, 33)\n",
      "step 9873, loss is 4.596123218536377\n",
      "(64, 33)\n",
      "step 9874, loss is 4.814333438873291\n",
      "(64, 33)\n",
      "step 9875, loss is 4.778116226196289\n",
      "(64, 33)\n",
      "step 9876, loss is 4.912834167480469\n",
      "(64, 33)\n",
      "step 9877, loss is 4.908408164978027\n",
      "(64, 33)\n",
      "step 9878, loss is 4.762033939361572\n",
      "(64, 33)\n",
      "step 9879, loss is 4.800240516662598\n",
      "(64, 33)\n",
      "step 9880, loss is 4.638930797576904\n",
      "(64, 33)\n",
      "step 9881, loss is 4.88279390335083\n",
      "(64, 33)\n",
      "step 9882, loss is 4.955562591552734\n",
      "(64, 33)\n",
      "step 9883, loss is 4.911746025085449\n",
      "(64, 33)\n",
      "step 9884, loss is 4.987575054168701\n",
      "(64, 33)\n",
      "step 9885, loss is 4.661726474761963\n",
      "(64, 33)\n",
      "step 9886, loss is 4.735873699188232\n",
      "(64, 33)\n",
      "step 9887, loss is 4.877523422241211\n",
      "(64, 33)\n",
      "step 9888, loss is 4.766402721405029\n",
      "(64, 33)\n",
      "step 9889, loss is 4.843369960784912\n",
      "(64, 33)\n",
      "step 9890, loss is 4.876309394836426\n",
      "(64, 33)\n",
      "step 9891, loss is 4.820884704589844\n",
      "(64, 33)\n",
      "step 9892, loss is 4.980616092681885\n",
      "(64, 33)\n",
      "step 9893, loss is 4.792814254760742\n",
      "(64, 33)\n",
      "step 9894, loss is 4.968245506286621\n",
      "(64, 33)\n",
      "step 9895, loss is 4.806034564971924\n",
      "(64, 33)\n",
      "step 9896, loss is 4.934260368347168\n",
      "(64, 33)\n",
      "step 9897, loss is 4.8128981590271\n",
      "(64, 33)\n",
      "step 9898, loss is 5.077208518981934\n",
      "(64, 33)\n",
      "step 9899, loss is 4.774634838104248\n",
      "(64, 33)\n",
      "step 9900, loss is 5.06024694442749\n",
      "(64, 33)\n",
      "step 9901, loss is 4.7792582511901855\n",
      "(64, 33)\n",
      "step 9902, loss is 4.7601542472839355\n",
      "(64, 33)\n",
      "step 9903, loss is 4.866969585418701\n",
      "(64, 33)\n",
      "step 9904, loss is 4.661139488220215\n",
      "(64, 33)\n",
      "step 9905, loss is 5.0849738121032715\n",
      "(64, 33)\n",
      "step 9906, loss is 4.916023254394531\n",
      "(64, 33)\n",
      "step 9907, loss is 4.794637203216553\n",
      "(64, 33)\n",
      "step 9908, loss is 4.865509033203125\n",
      "(64, 33)\n",
      "step 9909, loss is 4.852060317993164\n",
      "(64, 33)\n",
      "step 9910, loss is 4.80443811416626\n",
      "(64, 33)\n",
      "step 9911, loss is 4.831070423126221\n",
      "(64, 33)\n",
      "step 9912, loss is 4.802886962890625\n",
      "(64, 33)\n",
      "step 9913, loss is 4.724725723266602\n",
      "(64, 33)\n",
      "step 9914, loss is 4.882004261016846\n",
      "(64, 33)\n",
      "step 9915, loss is 4.926342487335205\n",
      "(64, 33)\n",
      "step 9916, loss is 4.79995584487915\n",
      "(64, 33)\n",
      "step 9917, loss is 5.037322521209717\n",
      "(64, 33)\n",
      "step 9918, loss is 4.745508193969727\n",
      "(64, 33)\n",
      "step 9919, loss is 4.937073230743408\n",
      "(64, 33)\n",
      "step 9920, loss is 4.7831315994262695\n",
      "(64, 33)\n",
      "step 9921, loss is 4.547506332397461\n",
      "(64, 33)\n",
      "step 9922, loss is 4.822103977203369\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9923, loss is 4.83342981338501\n",
      "(64, 33)\n",
      "step 9924, loss is 4.790098667144775\n",
      "(64, 33)\n",
      "step 9925, loss is 4.813104629516602\n",
      "(64, 33)\n",
      "step 9926, loss is 4.940476894378662\n",
      "(64, 33)\n",
      "step 9927, loss is 4.9177985191345215\n",
      "(64, 33)\n",
      "step 9928, loss is 4.7392706871032715\n",
      "(64, 33)\n",
      "step 9929, loss is 4.87432861328125\n",
      "(64, 33)\n",
      "step 9930, loss is 4.807796955108643\n",
      "(64, 33)\n",
      "step 9931, loss is 4.985963344573975\n",
      "(64, 33)\n",
      "step 9932, loss is 4.846296787261963\n",
      "(64, 33)\n",
      "step 9933, loss is 4.929076194763184\n",
      "(64, 33)\n",
      "step 9934, loss is 4.735786437988281\n",
      "(64, 33)\n",
      "step 9935, loss is 4.758782863616943\n",
      "(64, 33)\n",
      "step 9936, loss is 4.749307155609131\n",
      "(64, 33)\n",
      "step 9937, loss is 4.913998126983643\n",
      "(64, 33)\n",
      "step 9938, loss is 4.801957607269287\n",
      "(64, 33)\n",
      "step 9939, loss is 4.893556594848633\n",
      "(64, 33)\n",
      "step 9940, loss is 4.7140212059021\n",
      "(64, 33)\n",
      "step 9941, loss is 4.814608573913574\n",
      "(64, 33)\n",
      "step 9942, loss is 4.926655292510986\n",
      "(64, 33)\n",
      "step 9943, loss is 4.780239105224609\n",
      "(64, 33)\n",
      "step 9944, loss is 4.831879138946533\n",
      "(64, 33)\n",
      "step 9945, loss is 4.785586833953857\n",
      "(64, 33)\n",
      "step 9946, loss is 4.946512222290039\n",
      "(64, 33)\n",
      "step 9947, loss is 4.75687837600708\n",
      "(64, 33)\n",
      "step 9948, loss is 4.804695129394531\n",
      "(64, 33)\n",
      "step 9949, loss is 4.979607582092285\n",
      "(64, 33)\n",
      "step 9950, loss is 4.924395561218262\n",
      "(64, 33)\n",
      "step 9951, loss is 4.755450248718262\n",
      "(64, 33)\n",
      "step 9952, loss is 4.871140956878662\n",
      "(64, 33)\n",
      "step 9953, loss is 4.916433334350586\n",
      "(64, 33)\n",
      "step 9954, loss is 4.748601913452148\n",
      "(64, 33)\n",
      "step 9955, loss is 4.784021377563477\n",
      "(64, 33)\n",
      "step 9956, loss is 5.024502754211426\n",
      "(64, 33)\n",
      "step 9957, loss is 4.946695327758789\n",
      "(64, 33)\n",
      "step 9958, loss is 4.9060959815979\n",
      "(64, 33)\n",
      "step 9959, loss is 5.004879474639893\n",
      "(64, 33)\n",
      "step 9960, loss is 5.009467124938965\n",
      "(64, 33)\n",
      "step 9961, loss is 4.8394455909729\n",
      "(64, 33)\n",
      "step 9962, loss is 4.635273456573486\n",
      "(64, 33)\n",
      "step 9963, loss is 4.905294895172119\n",
      "(64, 33)\n",
      "step 9964, loss is 4.730216979980469\n",
      "(64, 33)\n",
      "step 9965, loss is 4.981546878814697\n",
      "(64, 33)\n",
      "step 9966, loss is 4.975219249725342\n",
      "(64, 33)\n",
      "step 9967, loss is 5.00133752822876\n",
      "(64, 33)\n",
      "step 9968, loss is 4.912914752960205\n",
      "(64, 33)\n",
      "step 9969, loss is 4.781771659851074\n",
      "(64, 33)\n",
      "step 9970, loss is 4.7904863357543945\n",
      "(64, 33)\n",
      "step 9971, loss is 4.927237510681152\n",
      "(64, 33)\n",
      "step 9972, loss is 4.854591369628906\n",
      "(64, 33)\n",
      "step 9973, loss is 4.829300880432129\n",
      "(64, 33)\n",
      "step 9974, loss is 4.8890814781188965\n",
      "(64, 33)\n",
      "step 9975, loss is 4.686249732971191\n",
      "(64, 33)\n",
      "step 9976, loss is 4.849167346954346\n",
      "(64, 33)\n",
      "step 9977, loss is 4.866257190704346\n",
      "(64, 33)\n",
      "step 9978, loss is 4.680021286010742\n",
      "(64, 33)\n",
      "step 9979, loss is 4.872819423675537\n",
      "(64, 33)\n",
      "step 9980, loss is 4.7052836418151855\n",
      "(64, 33)\n",
      "step 9981, loss is 4.773698329925537\n",
      "(64, 33)\n",
      "step 9982, loss is 4.695820331573486\n",
      "(64, 33)\n",
      "step 9983, loss is 4.8097968101501465\n",
      "(64, 33)\n",
      "step 9984, loss is 4.98697566986084\n",
      "(64, 33)\n",
      "step 9985, loss is 4.836816310882568\n",
      "(64, 33)\n",
      "step 9986, loss is 4.861630916595459\n",
      "(64, 33)\n",
      "step 9987, loss is 4.775363922119141\n",
      "(64, 33)\n",
      "step 9988, loss is 4.674711227416992\n",
      "(64, 33)\n",
      "step 9989, loss is 4.789257526397705\n",
      "(64, 33)\n",
      "step 9990, loss is 4.82763147354126\n",
      "(64, 33)\n",
      "step 9991, loss is 4.937982559204102\n",
      "(64, 33)\n",
      "step 9992, loss is 4.784529209136963\n",
      "(64, 33)\n",
      "step 9993, loss is 4.776028633117676\n",
      "(64, 33)\n",
      "step 9994, loss is 4.932683944702148\n",
      "(64, 33)\n",
      "step 9995, loss is 4.759997844696045\n",
      "(64, 33)\n",
      "step 9996, loss is 4.860431671142578\n",
      "(64, 33)\n",
      "step 9997, loss is 4.796959400177002\n",
      "(64, 33)\n",
      "step 9998, loss is 4.794406890869141\n",
      "(64, 33)\n",
      "step 9999, loss is 4.984248638153076\n",
      "(64, 33)\n",
      "step 10000, loss is 4.685790538787842\n",
      "(64, 33)\n",
      "step 10001, loss is 4.734828472137451\n",
      "(64, 33)\n",
      "step 10002, loss is 4.817055702209473\n",
      "(64, 33)\n",
      "step 10003, loss is 4.8843607902526855\n",
      "(64, 33)\n",
      "step 10004, loss is 4.894731044769287\n",
      "(64, 33)\n",
      "step 10005, loss is 4.885916233062744\n",
      "(64, 33)\n",
      "step 10006, loss is 4.897773265838623\n",
      "(64, 33)\n",
      "step 10007, loss is 4.65906286239624\n",
      "(64, 33)\n",
      "step 10008, loss is 4.874333381652832\n",
      "(64, 33)\n",
      "step 10009, loss is 4.945791244506836\n",
      "(64, 33)\n",
      "step 10010, loss is 4.9924774169921875\n",
      "(64, 33)\n",
      "step 10011, loss is 4.85633659362793\n",
      "(64, 33)\n",
      "step 10012, loss is 4.812588691711426\n",
      "(64, 33)\n",
      "step 10013, loss is 4.596259117126465\n",
      "(64, 33)\n",
      "step 10014, loss is 4.731533527374268\n",
      "(64, 33)\n",
      "step 10015, loss is 4.763403415679932\n",
      "(64, 33)\n",
      "step 10016, loss is 4.888912677764893\n",
      "(64, 33)\n",
      "step 10017, loss is 4.977914810180664\n",
      "(64, 33)\n",
      "step 10018, loss is 4.7534379959106445\n",
      "(64, 33)\n",
      "step 10019, loss is 4.868959426879883\n",
      "(64, 33)\n",
      "step 10020, loss is 4.744019508361816\n",
      "(64, 33)\n",
      "step 10021, loss is 4.780433654785156\n",
      "(64, 33)\n",
      "step 10022, loss is 4.895486354827881\n",
      "(64, 33)\n",
      "step 10023, loss is 5.05728006362915\n",
      "(64, 33)\n",
      "step 10024, loss is 4.721508026123047\n",
      "(64, 33)\n",
      "step 10025, loss is 4.817637920379639\n",
      "(64, 33)\n",
      "step 10026, loss is 4.857898712158203\n",
      "(64, 33)\n",
      "step 10027, loss is 5.011178493499756\n",
      "(64, 33)\n",
      "step 10028, loss is 4.759313106536865\n",
      "(64, 33)\n",
      "step 10029, loss is 4.773261070251465\n",
      "(64, 33)\n",
      "step 10030, loss is 4.859994888305664\n",
      "(64, 33)\n",
      "step 10031, loss is 5.054067611694336\n",
      "(64, 33)\n",
      "step 10032, loss is 4.8204755783081055\n",
      "(64, 33)\n",
      "step 10033, loss is 4.927194118499756\n",
      "(64, 33)\n",
      "step 10034, loss is 4.9744954109191895\n",
      "(64, 33)\n",
      "step 10035, loss is 4.949340343475342\n",
      "(64, 33)\n",
      "step 10036, loss is 4.931066989898682\n",
      "(64, 33)\n",
      "step 10037, loss is 4.806258201599121\n",
      "(64, 33)\n",
      "step 10038, loss is 4.649621963500977\n",
      "(64, 33)\n",
      "step 10039, loss is 4.752495765686035\n",
      "(64, 33)\n",
      "step 10040, loss is 4.753735542297363\n",
      "(64, 33)\n",
      "step 10041, loss is 4.9251861572265625\n",
      "(64, 33)\n",
      "step 10042, loss is 4.712867259979248\n",
      "(64, 33)\n",
      "step 10043, loss is 4.776686191558838\n",
      "(64, 33)\n",
      "step 10044, loss is 4.847593307495117\n",
      "(64, 33)\n",
      "step 10045, loss is 5.08504581451416\n",
      "(64, 33)\n",
      "step 10046, loss is 4.952681541442871\n",
      "(64, 33)\n",
      "step 10047, loss is 4.9415974617004395\n",
      "(64, 33)\n",
      "step 10048, loss is 4.834967613220215\n",
      "(64, 33)\n",
      "step 10049, loss is 4.850388050079346\n",
      "(64, 33)\n",
      "step 10050, loss is 4.867135524749756\n",
      "(64, 33)\n",
      "step 10051, loss is 5.046225070953369\n",
      "(64, 33)\n",
      "step 10052, loss is 4.87671422958374\n",
      "(64, 33)\n",
      "step 10053, loss is 4.884965896606445\n",
      "(64, 33)\n",
      "step 10054, loss is 5.022801399230957\n",
      "(64, 33)\n",
      "step 10055, loss is 4.8401970863342285\n",
      "(64, 33)\n",
      "step 10056, loss is 4.862740516662598\n",
      "(64, 33)\n",
      "step 10057, loss is 4.900753021240234\n",
      "(64, 33)\n",
      "step 10058, loss is 4.97723388671875\n",
      "(64, 33)\n",
      "step 10059, loss is 4.988752841949463\n",
      "(64, 33)\n",
      "step 10060, loss is 4.806713104248047\n",
      "(64, 33)\n",
      "step 10061, loss is 4.863142490386963\n",
      "(64, 33)\n",
      "step 10062, loss is 4.798957824707031\n",
      "(64, 33)\n",
      "step 10063, loss is 4.881268501281738\n",
      "(64, 33)\n",
      "step 10064, loss is 4.96339225769043\n",
      "(64, 33)\n",
      "step 10065, loss is 4.840484619140625\n",
      "(64, 33)\n",
      "step 10066, loss is 4.961976528167725\n",
      "(64, 33)\n",
      "step 10067, loss is 4.853395938873291\n",
      "(64, 33)\n",
      "step 10068, loss is 4.8693084716796875\n",
      "(64, 33)\n",
      "step 10069, loss is 4.9878387451171875\n",
      "(64, 33)\n",
      "step 10070, loss is 4.911078929901123\n",
      "(64, 33)\n",
      "step 10071, loss is 4.971935749053955\n",
      "(64, 33)\n",
      "step 10072, loss is 4.894927024841309\n",
      "(64, 33)\n",
      "step 10073, loss is 4.937144756317139\n",
      "(64, 33)\n",
      "step 10074, loss is 4.878650188446045\n",
      "(64, 33)\n",
      "step 10075, loss is 4.8507399559021\n",
      "(64, 33)\n",
      "step 10076, loss is 4.680360317230225\n",
      "(64, 33)\n",
      "step 10077, loss is 4.993661403656006\n",
      "(64, 33)\n",
      "step 10078, loss is 4.985069274902344\n",
      "(64, 33)\n",
      "step 10079, loss is 4.8211774826049805\n",
      "(64, 33)\n",
      "step 10080, loss is 4.892335891723633\n",
      "(64, 33)\n",
      "step 10081, loss is 4.828987121582031\n",
      "(64, 33)\n",
      "step 10082, loss is 4.9056854248046875\n",
      "(64, 33)\n",
      "step 10083, loss is 4.77102518081665\n",
      "(64, 33)\n",
      "step 10084, loss is 5.1031341552734375\n",
      "(64, 33)\n",
      "step 10085, loss is 4.893703460693359\n",
      "(64, 33)\n",
      "step 10086, loss is 4.815118789672852\n",
      "(64, 33)\n",
      "step 10087, loss is 4.898604869842529\n",
      "(64, 33)\n",
      "step 10088, loss is 5.013246059417725\n",
      "(64, 33)\n",
      "step 10089, loss is 4.709217548370361\n",
      "(64, 33)\n",
      "step 10090, loss is 4.898366451263428\n",
      "(64, 33)\n",
      "step 10091, loss is 4.788055896759033\n",
      "(64, 33)\n",
      "step 10092, loss is 4.853000640869141\n",
      "(64, 33)\n",
      "step 10093, loss is 5.0812859535217285\n",
      "(64, 33)\n",
      "step 10094, loss is 4.882726669311523\n",
      "(64, 33)\n",
      "step 10095, loss is 4.6701507568359375\n",
      "(64, 33)\n",
      "step 10096, loss is 4.818897724151611\n",
      "(64, 33)\n",
      "step 10097, loss is 5.051323413848877\n",
      "(64, 33)\n",
      "step 10098, loss is 4.817110061645508\n",
      "(64, 33)\n",
      "step 10099, loss is 4.80100154876709\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10100, loss is 4.76757287979126\n",
      "(64, 33)\n",
      "step 10101, loss is 4.845499038696289\n",
      "(64, 33)\n",
      "step 10102, loss is 4.926667213439941\n",
      "(64, 33)\n",
      "step 10103, loss is 4.871473789215088\n",
      "(64, 33)\n",
      "step 10104, loss is 4.715176582336426\n",
      "(64, 33)\n",
      "step 10105, loss is 4.839486122131348\n",
      "(64, 33)\n",
      "step 10106, loss is 5.02614688873291\n",
      "(64, 33)\n",
      "step 10107, loss is 4.797374248504639\n",
      "(64, 33)\n",
      "step 10108, loss is 4.913816452026367\n",
      "(64, 33)\n",
      "step 10109, loss is 4.76945686340332\n",
      "(64, 33)\n",
      "step 10110, loss is 4.838675498962402\n",
      "(64, 33)\n",
      "step 10111, loss is 4.806222915649414\n",
      "(64, 33)\n",
      "step 10112, loss is 4.775677680969238\n",
      "(64, 33)\n",
      "step 10113, loss is 4.8326945304870605\n",
      "(64, 33)\n",
      "step 10114, loss is 4.515127658843994\n",
      "(64, 33)\n",
      "step 10115, loss is 4.887996196746826\n",
      "(64, 33)\n",
      "step 10116, loss is 4.658376693725586\n",
      "(64, 33)\n",
      "step 10117, loss is 4.972629070281982\n",
      "(64, 33)\n",
      "step 10118, loss is 4.84338903427124\n",
      "(64, 33)\n",
      "step 10119, loss is 5.182103633880615\n",
      "(64, 33)\n",
      "step 10120, loss is 4.818470001220703\n",
      "(64, 33)\n",
      "step 10121, loss is 4.953117370605469\n",
      "(64, 33)\n",
      "step 10122, loss is 4.87323522567749\n",
      "(64, 33)\n",
      "step 10123, loss is 4.814719200134277\n",
      "(64, 33)\n",
      "step 10124, loss is 4.923855304718018\n",
      "(64, 33)\n",
      "step 10125, loss is 4.902716636657715\n",
      "(64, 33)\n",
      "step 10126, loss is 4.960431098937988\n",
      "(64, 33)\n",
      "step 10127, loss is 4.574560165405273\n",
      "(64, 33)\n",
      "step 10128, loss is 4.851434230804443\n",
      "(64, 33)\n",
      "step 10129, loss is 4.9353861808776855\n",
      "(64, 33)\n",
      "step 10130, loss is 4.730693340301514\n",
      "(64, 33)\n",
      "step 10131, loss is 4.863720893859863\n",
      "(64, 33)\n",
      "step 10132, loss is 4.9257588386535645\n",
      "(64, 33)\n",
      "step 10133, loss is 4.808464527130127\n",
      "(64, 33)\n",
      "step 10134, loss is 4.926374435424805\n",
      "(64, 33)\n",
      "step 10135, loss is 4.8732476234436035\n",
      "(64, 33)\n",
      "step 10136, loss is 4.852624893188477\n",
      "(64, 33)\n",
      "step 10137, loss is 4.987776756286621\n",
      "(64, 33)\n",
      "step 10138, loss is 4.951906204223633\n",
      "(64, 33)\n",
      "step 10139, loss is 4.92308235168457\n",
      "(64, 33)\n",
      "step 10140, loss is 4.921296119689941\n",
      "(64, 33)\n",
      "step 10141, loss is 4.834232330322266\n",
      "(64, 33)\n",
      "step 10142, loss is 4.888406753540039\n",
      "(64, 33)\n",
      "step 10143, loss is 4.961357593536377\n",
      "(64, 33)\n",
      "step 10144, loss is 4.673564910888672\n",
      "(64, 33)\n",
      "step 10145, loss is 4.923491954803467\n",
      "(64, 33)\n",
      "step 10146, loss is 4.943331241607666\n",
      "(64, 33)\n",
      "step 10147, loss is 4.937312602996826\n",
      "(64, 33)\n",
      "step 10148, loss is 5.056318283081055\n",
      "(64, 33)\n",
      "step 10149, loss is 4.671270847320557\n",
      "(64, 33)\n",
      "step 10150, loss is 4.796267032623291\n",
      "(64, 33)\n",
      "step 10151, loss is 4.787257194519043\n",
      "(64, 33)\n",
      "step 10152, loss is 4.785863876342773\n",
      "(64, 33)\n",
      "step 10153, loss is 4.8596343994140625\n",
      "(64, 33)\n",
      "step 10154, loss is 5.040663719177246\n",
      "(64, 33)\n",
      "step 10155, loss is 4.7000861167907715\n",
      "(64, 33)\n",
      "step 10156, loss is 4.859454154968262\n",
      "(64, 33)\n",
      "step 10157, loss is 4.80515193939209\n",
      "(64, 33)\n",
      "step 10158, loss is 4.745763778686523\n",
      "(64, 33)\n",
      "step 10159, loss is 4.7677178382873535\n",
      "(64, 33)\n",
      "step 10160, loss is 4.866579055786133\n",
      "(64, 33)\n",
      "step 10161, loss is 5.0266032218933105\n",
      "(64, 33)\n",
      "step 10162, loss is 4.8536057472229\n",
      "(64, 33)\n",
      "step 10163, loss is 4.677591800689697\n",
      "(64, 33)\n",
      "step 10164, loss is 4.925288200378418\n",
      "(64, 33)\n",
      "step 10165, loss is 4.602508544921875\n",
      "(64, 33)\n",
      "step 10166, loss is 4.8835577964782715\n",
      "(64, 33)\n",
      "step 10167, loss is 4.788595199584961\n",
      "(64, 33)\n",
      "step 10168, loss is 4.791563510894775\n",
      "(64, 33)\n",
      "step 10169, loss is 4.596982002258301\n",
      "(64, 33)\n",
      "step 10170, loss is 4.772653579711914\n",
      "(64, 33)\n",
      "step 10171, loss is 4.842543601989746\n",
      "(64, 33)\n",
      "step 10172, loss is 4.8797783851623535\n",
      "(64, 33)\n",
      "step 10173, loss is 4.669346809387207\n",
      "(64, 33)\n",
      "step 10174, loss is 4.89775276184082\n",
      "(64, 33)\n",
      "step 10175, loss is 4.821846961975098\n",
      "(64, 33)\n",
      "step 10176, loss is 4.772785186767578\n",
      "(64, 33)\n",
      "step 10177, loss is 4.877607822418213\n",
      "(64, 33)\n",
      "step 10178, loss is 4.864500999450684\n",
      "(64, 33)\n",
      "step 10179, loss is 4.697515964508057\n",
      "(64, 33)\n",
      "step 10180, loss is 4.820554733276367\n",
      "(64, 33)\n",
      "step 10181, loss is 4.745259761810303\n",
      "(64, 33)\n",
      "step 10182, loss is 4.942979335784912\n",
      "(64, 33)\n",
      "step 10183, loss is 4.863107204437256\n",
      "(64, 33)\n",
      "step 10184, loss is 4.916611194610596\n",
      "(64, 33)\n",
      "step 10185, loss is 4.779289245605469\n",
      "(64, 33)\n",
      "step 10186, loss is 4.686241626739502\n",
      "(64, 33)\n",
      "step 10187, loss is 4.987199783325195\n",
      "(64, 33)\n",
      "step 10188, loss is 4.921921253204346\n",
      "(64, 33)\n",
      "step 10189, loss is 4.6922831535339355\n",
      "(64, 33)\n",
      "step 10190, loss is 4.766547203063965\n",
      "(64, 33)\n",
      "step 10191, loss is 4.924277305603027\n",
      "(64, 33)\n",
      "step 10192, loss is 4.952586650848389\n",
      "(64, 33)\n",
      "step 10193, loss is 4.907634735107422\n",
      "(64, 33)\n",
      "step 10194, loss is 4.843967437744141\n",
      "(64, 33)\n",
      "step 10195, loss is 4.798453330993652\n",
      "(64, 33)\n",
      "step 10196, loss is 4.680022239685059\n",
      "(64, 33)\n",
      "step 10197, loss is 4.946958065032959\n",
      "(64, 33)\n",
      "step 10198, loss is 4.89799165725708\n",
      "(64, 33)\n",
      "step 10199, loss is 4.904082298278809\n",
      "(64, 33)\n",
      "step 10200, loss is 5.027222633361816\n",
      "(64, 33)\n",
      "step 10201, loss is 4.7667083740234375\n",
      "(64, 33)\n",
      "step 10202, loss is 4.842088222503662\n",
      "(64, 33)\n",
      "step 10203, loss is 4.831728935241699\n",
      "(64, 33)\n",
      "step 10204, loss is 4.765347003936768\n",
      "(64, 33)\n",
      "step 10205, loss is 4.8346266746521\n",
      "(64, 33)\n",
      "step 10206, loss is 4.798635959625244\n",
      "(64, 33)\n",
      "step 10207, loss is 4.734010219573975\n",
      "(64, 33)\n",
      "step 10208, loss is 4.783830165863037\n",
      "(64, 33)\n",
      "step 10209, loss is 5.046287536621094\n",
      "(64, 33)\n",
      "step 10210, loss is 4.836721897125244\n",
      "(64, 33)\n",
      "step 10211, loss is 4.767461776733398\n",
      "(64, 33)\n",
      "step 10212, loss is 4.836758136749268\n",
      "(64, 33)\n",
      "step 10213, loss is 4.911118030548096\n",
      "(64, 33)\n",
      "step 10214, loss is 4.883393287658691\n",
      "(64, 33)\n",
      "step 10215, loss is 4.950597286224365\n",
      "(64, 33)\n",
      "step 10216, loss is 4.6739702224731445\n",
      "(64, 33)\n",
      "step 10217, loss is 4.746830940246582\n",
      "(64, 33)\n",
      "step 10218, loss is 5.0303955078125\n",
      "(64, 33)\n",
      "step 10219, loss is 4.896939277648926\n",
      "(64, 33)\n",
      "step 10220, loss is 4.9098076820373535\n",
      "(64, 33)\n",
      "step 10221, loss is 5.014939785003662\n",
      "(64, 33)\n",
      "step 10222, loss is 4.748036861419678\n",
      "(64, 33)\n",
      "step 10223, loss is 4.771578788757324\n",
      "(64, 33)\n",
      "step 10224, loss is 5.022414207458496\n",
      "(64, 33)\n",
      "step 10225, loss is 5.051700592041016\n",
      "(64, 33)\n",
      "step 10226, loss is 4.803180694580078\n",
      "(64, 33)\n",
      "step 10227, loss is 4.777736663818359\n",
      "(64, 33)\n",
      "step 10228, loss is 4.878261089324951\n",
      "(64, 33)\n",
      "step 10229, loss is 4.722991943359375\n",
      "(64, 33)\n",
      "step 10230, loss is 5.121654033660889\n",
      "(64, 33)\n",
      "step 10231, loss is 4.892066955566406\n",
      "(64, 33)\n",
      "step 10232, loss is 4.987377643585205\n",
      "(64, 33)\n",
      "step 10233, loss is 4.585898399353027\n",
      "(64, 33)\n",
      "step 10234, loss is 4.83328914642334\n",
      "(64, 33)\n",
      "step 10235, loss is 4.772489547729492\n",
      "(64, 33)\n",
      "step 10236, loss is 4.922955513000488\n",
      "(64, 33)\n",
      "step 10237, loss is 4.773758888244629\n",
      "(64, 33)\n",
      "step 10238, loss is 4.731952667236328\n",
      "(64, 33)\n",
      "step 10239, loss is 4.691835880279541\n",
      "(64, 33)\n",
      "step 10240, loss is 4.980318069458008\n",
      "(64, 33)\n",
      "step 10241, loss is 5.076802730560303\n",
      "(64, 33)\n",
      "step 10242, loss is 4.765492916107178\n",
      "(64, 33)\n",
      "step 10243, loss is 4.751605033874512\n",
      "(64, 33)\n",
      "step 10244, loss is 4.826296329498291\n",
      "(64, 33)\n",
      "step 10245, loss is 4.820436477661133\n",
      "(64, 33)\n",
      "step 10246, loss is 4.965281963348389\n",
      "(64, 33)\n",
      "step 10247, loss is 4.787022590637207\n",
      "(64, 33)\n",
      "step 10248, loss is 4.843019008636475\n",
      "(64, 33)\n",
      "step 10249, loss is 4.880860328674316\n",
      "(64, 33)\n",
      "step 10250, loss is 4.8162665367126465\n",
      "(64, 33)\n",
      "step 10251, loss is 5.0588884353637695\n",
      "(64, 33)\n",
      "step 10252, loss is 4.717644691467285\n",
      "(64, 33)\n",
      "step 10253, loss is 4.919157981872559\n",
      "(64, 33)\n",
      "step 10254, loss is 4.702515602111816\n",
      "(64, 33)\n",
      "step 10255, loss is 4.813569068908691\n",
      "(64, 33)\n",
      "step 10256, loss is 4.765361785888672\n",
      "(64, 33)\n",
      "step 10257, loss is 4.810076713562012\n",
      "(64, 33)\n",
      "step 10258, loss is 4.804660797119141\n",
      "(64, 33)\n",
      "step 10259, loss is 4.796362400054932\n",
      "(64, 33)\n",
      "step 10260, loss is 4.936822891235352\n",
      "(64, 33)\n",
      "step 10261, loss is 4.636932849884033\n",
      "(64, 33)\n",
      "step 10262, loss is 4.949830532073975\n",
      "(64, 33)\n",
      "step 10263, loss is 4.657233715057373\n",
      "(64, 33)\n",
      "step 10264, loss is 4.635485649108887\n",
      "(64, 33)\n",
      "step 10265, loss is 4.8865251541137695\n",
      "(64, 33)\n",
      "step 10266, loss is 4.804285526275635\n",
      "(64, 33)\n",
      "step 10267, loss is 4.840756416320801\n",
      "(64, 33)\n",
      "step 10268, loss is 4.752200126647949\n",
      "(64, 33)\n",
      "step 10269, loss is 4.9362711906433105\n",
      "(64, 33)\n",
      "step 10270, loss is 4.8989057540893555\n",
      "(64, 33)\n",
      "step 10271, loss is 4.939903736114502\n",
      "(64, 33)\n",
      "step 10272, loss is 4.803272247314453\n",
      "(64, 33)\n",
      "step 10273, loss is 4.619722843170166\n",
      "(64, 33)\n",
      "step 10274, loss is 4.6249775886535645\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10275, loss is 5.01369571685791\n",
      "(64, 33)\n",
      "step 10276, loss is 4.9210286140441895\n",
      "(64, 33)\n",
      "step 10277, loss is 4.951149940490723\n",
      "(64, 33)\n",
      "step 10278, loss is 4.813482284545898\n",
      "(64, 33)\n",
      "step 10279, loss is 4.7292799949646\n",
      "(64, 33)\n",
      "step 10280, loss is 4.727447032928467\n",
      "(64, 33)\n",
      "step 10281, loss is 4.7975969314575195\n",
      "(64, 33)\n",
      "step 10282, loss is 4.839863300323486\n",
      "(64, 33)\n",
      "step 10283, loss is 4.811206340789795\n",
      "(64, 33)\n",
      "step 10284, loss is 4.929072856903076\n",
      "(64, 33)\n",
      "step 10285, loss is 4.95242166519165\n",
      "(64, 33)\n",
      "step 10286, loss is 4.5997467041015625\n",
      "(64, 33)\n",
      "step 10287, loss is 4.909274101257324\n",
      "(64, 33)\n",
      "step 10288, loss is 4.928203105926514\n",
      "(64, 33)\n",
      "step 10289, loss is 4.6566338539123535\n",
      "(64, 33)\n",
      "step 10290, loss is 5.022900104522705\n",
      "(64, 33)\n",
      "step 10291, loss is 4.794033527374268\n",
      "(64, 33)\n",
      "step 10292, loss is 4.745001792907715\n",
      "(64, 33)\n",
      "step 10293, loss is 4.91600227355957\n",
      "(64, 33)\n",
      "step 10294, loss is 5.087573528289795\n",
      "(64, 33)\n",
      "step 10295, loss is 4.795280456542969\n",
      "(64, 33)\n",
      "step 10296, loss is 4.934821605682373\n",
      "(64, 33)\n",
      "step 10297, loss is 4.873175621032715\n",
      "(64, 33)\n",
      "step 10298, loss is 4.892494201660156\n",
      "(64, 33)\n",
      "step 10299, loss is 4.809164524078369\n",
      "(64, 33)\n",
      "step 10300, loss is 4.876247406005859\n",
      "(64, 33)\n",
      "step 10301, loss is 4.752140045166016\n",
      "(64, 33)\n",
      "step 10302, loss is 4.740358829498291\n",
      "(64, 33)\n",
      "step 10303, loss is 4.901661396026611\n",
      "(64, 33)\n",
      "step 10304, loss is 5.030263423919678\n",
      "(64, 33)\n",
      "step 10305, loss is 4.744680404663086\n",
      "(64, 33)\n",
      "step 10306, loss is 4.631032466888428\n",
      "(64, 33)\n",
      "step 10307, loss is 4.809367656707764\n",
      "(64, 33)\n",
      "step 10308, loss is 4.723191738128662\n",
      "(64, 33)\n",
      "step 10309, loss is 4.802395820617676\n",
      "(64, 33)\n",
      "step 10310, loss is 4.922623634338379\n",
      "(64, 33)\n",
      "step 10311, loss is 4.920929431915283\n",
      "(64, 33)\n",
      "step 10312, loss is 4.9769673347473145\n",
      "(64, 33)\n",
      "step 10313, loss is 4.7113847732543945\n",
      "(64, 33)\n",
      "step 10314, loss is 4.920140743255615\n",
      "(64, 33)\n",
      "step 10315, loss is 4.724014759063721\n",
      "(64, 33)\n",
      "step 10316, loss is 4.787608623504639\n",
      "(64, 33)\n",
      "step 10317, loss is 4.974887847900391\n",
      "(64, 33)\n",
      "step 10318, loss is 4.79559326171875\n",
      "(64, 33)\n",
      "step 10319, loss is 4.926922798156738\n",
      "(64, 33)\n",
      "step 10320, loss is 4.93942928314209\n",
      "(64, 33)\n",
      "step 10321, loss is 4.804740905761719\n",
      "(64, 33)\n",
      "step 10322, loss is 4.820701599121094\n",
      "(64, 33)\n",
      "step 10323, loss is 4.846802234649658\n",
      "(64, 33)\n",
      "step 10324, loss is 4.743741035461426\n",
      "(64, 33)\n",
      "step 10325, loss is 4.740149021148682\n",
      "(64, 33)\n",
      "step 10326, loss is 4.956931114196777\n",
      "(64, 33)\n",
      "step 10327, loss is 4.767104625701904\n",
      "(64, 33)\n",
      "step 10328, loss is 4.930060386657715\n",
      "(64, 33)\n",
      "step 10329, loss is 4.830944538116455\n",
      "(64, 33)\n",
      "step 10330, loss is 4.6961236000061035\n",
      "(64, 33)\n",
      "step 10331, loss is 4.801234722137451\n",
      "(64, 33)\n",
      "step 10332, loss is 4.87556266784668\n",
      "(64, 33)\n",
      "step 10333, loss is 4.782948017120361\n",
      "(64, 33)\n",
      "step 10334, loss is 4.60854959487915\n",
      "(64, 33)\n",
      "step 10335, loss is 4.771092891693115\n",
      "(64, 33)\n",
      "step 10336, loss is 4.933446884155273\n",
      "(64, 33)\n",
      "step 10337, loss is 4.701202869415283\n",
      "(64, 33)\n",
      "step 10338, loss is 4.833577632904053\n",
      "(64, 33)\n",
      "step 10339, loss is 4.660428047180176\n",
      "(64, 33)\n",
      "step 10340, loss is 4.789175510406494\n",
      "(64, 33)\n",
      "step 10341, loss is 4.828248977661133\n",
      "(64, 33)\n",
      "step 10342, loss is 4.879306316375732\n",
      "(64, 33)\n",
      "step 10343, loss is 5.040677070617676\n",
      "(64, 33)\n",
      "step 10344, loss is 4.914440631866455\n",
      "(64, 33)\n",
      "step 10345, loss is 4.849231719970703\n",
      "(64, 33)\n",
      "step 10346, loss is 4.8553032875061035\n",
      "(64, 33)\n",
      "step 10347, loss is 4.724808216094971\n",
      "(64, 33)\n",
      "step 10348, loss is 4.631083965301514\n",
      "(64, 33)\n",
      "step 10349, loss is 4.895890235900879\n",
      "(64, 33)\n",
      "step 10350, loss is 4.678933143615723\n",
      "(64, 33)\n",
      "step 10351, loss is 4.809164047241211\n",
      "(64, 33)\n",
      "step 10352, loss is 4.814571380615234\n",
      "(64, 33)\n",
      "step 10353, loss is 4.702556133270264\n",
      "(64, 33)\n",
      "step 10354, loss is 4.728018760681152\n",
      "(64, 33)\n",
      "step 10355, loss is 4.830472469329834\n",
      "(64, 33)\n",
      "step 10356, loss is 4.893941402435303\n",
      "(64, 33)\n",
      "step 10357, loss is 4.9497504234313965\n",
      "(64, 33)\n",
      "step 10358, loss is 4.890530586242676\n",
      "(64, 33)\n",
      "step 10359, loss is 4.678191184997559\n",
      "(64, 33)\n",
      "step 10360, loss is 4.757678985595703\n",
      "(64, 33)\n",
      "step 10361, loss is 4.7003912925720215\n",
      "(64, 33)\n",
      "step 10362, loss is 4.955984592437744\n",
      "(64, 33)\n",
      "step 10363, loss is 4.860250473022461\n",
      "(64, 33)\n",
      "step 10364, loss is 4.7626800537109375\n",
      "(64, 33)\n",
      "step 10365, loss is 4.859175682067871\n",
      "(64, 33)\n",
      "step 10366, loss is 4.813505172729492\n",
      "(64, 33)\n",
      "step 10367, loss is 4.721675872802734\n",
      "(64, 33)\n",
      "step 10368, loss is 4.8561248779296875\n",
      "(64, 33)\n",
      "step 10369, loss is 4.750097274780273\n",
      "(64, 33)\n",
      "step 10370, loss is 4.690208435058594\n",
      "(64, 33)\n",
      "step 10371, loss is 4.870823860168457\n",
      "(64, 33)\n",
      "step 10372, loss is 4.816697597503662\n",
      "(64, 33)\n",
      "step 10373, loss is 4.9219536781311035\n",
      "(64, 33)\n",
      "step 10374, loss is 4.95611572265625\n",
      "(64, 33)\n",
      "step 10375, loss is 4.757469654083252\n",
      "(64, 33)\n",
      "step 10376, loss is 4.869378089904785\n",
      "(64, 33)\n",
      "step 10377, loss is 4.858641624450684\n",
      "(64, 33)\n",
      "step 10378, loss is 4.962447643280029\n",
      "(64, 33)\n",
      "step 10379, loss is 4.916193962097168\n",
      "(64, 33)\n",
      "step 10380, loss is 4.882743835449219\n",
      "(64, 33)\n",
      "step 10381, loss is 4.766902446746826\n",
      "(64, 33)\n",
      "step 10382, loss is 5.054778575897217\n",
      "(64, 33)\n",
      "step 10383, loss is 4.747673988342285\n",
      "(64, 33)\n",
      "step 10384, loss is 4.895273685455322\n",
      "(64, 33)\n",
      "step 10385, loss is 4.824582099914551\n",
      "(64, 33)\n",
      "step 10386, loss is 4.9140625\n",
      "(64, 33)\n",
      "step 10387, loss is 4.787376403808594\n",
      "(64, 33)\n",
      "step 10388, loss is 4.853074073791504\n",
      "(64, 33)\n",
      "step 10389, loss is 4.806779861450195\n",
      "(64, 33)\n",
      "step 10390, loss is 4.913982391357422\n",
      "(64, 33)\n",
      "step 10391, loss is 4.817204475402832\n",
      "(64, 33)\n",
      "step 10392, loss is 4.741390228271484\n",
      "(64, 33)\n",
      "step 10393, loss is 4.637616157531738\n",
      "(64, 33)\n",
      "step 10394, loss is 4.79632568359375\n",
      "(64, 33)\n",
      "step 10395, loss is 4.693694114685059\n",
      "(64, 33)\n",
      "step 10396, loss is 4.803269863128662\n",
      "(64, 33)\n",
      "step 10397, loss is 4.770171642303467\n",
      "(64, 33)\n",
      "step 10398, loss is 4.8570051193237305\n",
      "(64, 33)\n",
      "step 10399, loss is 4.96838903427124\n",
      "(64, 33)\n",
      "step 10400, loss is 5.025880336761475\n",
      "(64, 33)\n",
      "step 10401, loss is 4.6374831199646\n",
      "(64, 33)\n",
      "step 10402, loss is 4.8323187828063965\n",
      "(64, 33)\n",
      "step 10403, loss is 4.70845890045166\n",
      "(64, 33)\n",
      "step 10404, loss is 4.724440574645996\n",
      "(64, 33)\n",
      "step 10405, loss is 4.97715425491333\n",
      "(64, 33)\n",
      "step 10406, loss is 4.5600175857543945\n",
      "(64, 33)\n",
      "step 10407, loss is 4.670899391174316\n",
      "(64, 33)\n",
      "step 10408, loss is 4.951827526092529\n",
      "(64, 33)\n",
      "step 10409, loss is 4.650725364685059\n",
      "(64, 33)\n",
      "step 10410, loss is 4.788580894470215\n",
      "(64, 33)\n",
      "step 10411, loss is 4.746611595153809\n",
      "(64, 33)\n",
      "step 10412, loss is 4.710563659667969\n",
      "(64, 33)\n",
      "step 10413, loss is 4.84321403503418\n",
      "(64, 33)\n",
      "step 10414, loss is 4.741661548614502\n",
      "(64, 33)\n",
      "step 10415, loss is 4.669125556945801\n",
      "(64, 33)\n",
      "step 10416, loss is 4.928368091583252\n",
      "(64, 33)\n",
      "step 10417, loss is 4.830359935760498\n",
      "(64, 33)\n",
      "step 10418, loss is 4.930406093597412\n",
      "(64, 33)\n",
      "step 10419, loss is 4.944086074829102\n",
      "(64, 33)\n",
      "step 10420, loss is 4.866192817687988\n",
      "(64, 33)\n",
      "step 10421, loss is 4.779642105102539\n",
      "(64, 33)\n",
      "step 10422, loss is 4.931744575500488\n",
      "(64, 33)\n",
      "step 10423, loss is 4.98494291305542\n",
      "(64, 33)\n",
      "step 10424, loss is 4.72833251953125\n",
      "(64, 33)\n",
      "step 10425, loss is 4.699343204498291\n",
      "(64, 33)\n",
      "step 10426, loss is 4.8293280601501465\n",
      "(64, 33)\n",
      "step 10427, loss is 4.970692157745361\n",
      "(64, 33)\n",
      "step 10428, loss is 4.748468399047852\n",
      "(64, 33)\n",
      "step 10429, loss is 4.782371520996094\n",
      "(64, 33)\n",
      "step 10430, loss is 4.862049579620361\n",
      "(64, 33)\n",
      "step 10431, loss is 4.874705791473389\n",
      "(64, 33)\n",
      "step 10432, loss is 4.915670871734619\n",
      "(64, 33)\n",
      "step 10433, loss is 4.7349042892456055\n",
      "(64, 33)\n",
      "step 10434, loss is 4.798667907714844\n",
      "(64, 33)\n",
      "step 10435, loss is 4.890530109405518\n",
      "(64, 33)\n",
      "step 10436, loss is 4.9264302253723145\n",
      "(64, 33)\n",
      "step 10437, loss is 4.638853073120117\n",
      "(64, 33)\n",
      "step 10438, loss is 4.877686500549316\n",
      "(64, 33)\n",
      "step 10439, loss is 4.972509384155273\n",
      "(64, 33)\n",
      "step 10440, loss is 4.908501148223877\n",
      "(64, 33)\n",
      "step 10441, loss is 4.828284740447998\n",
      "(64, 33)\n",
      "step 10442, loss is 4.827484130859375\n",
      "(64, 33)\n",
      "step 10443, loss is 4.8286590576171875\n",
      "(64, 33)\n",
      "step 10444, loss is 4.766450881958008\n",
      "(64, 33)\n",
      "step 10445, loss is 4.784435749053955\n",
      "(64, 33)\n",
      "step 10446, loss is 4.884315013885498\n",
      "(64, 33)\n",
      "step 10447, loss is 4.9155402183532715\n",
      "(64, 33)\n",
      "step 10448, loss is 4.607781887054443\n",
      "(64, 33)\n",
      "step 10449, loss is 4.7283453941345215\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10450, loss is 4.967435359954834\n",
      "(64, 33)\n",
      "step 10451, loss is 4.890981674194336\n",
      "(64, 33)\n",
      "step 10452, loss is 4.654191493988037\n",
      "(64, 33)\n",
      "step 10453, loss is 4.913595676422119\n",
      "(64, 33)\n",
      "step 10454, loss is 4.888880252838135\n",
      "(64, 33)\n",
      "step 10455, loss is 4.75251579284668\n",
      "(64, 33)\n",
      "step 10456, loss is 4.9273457527160645\n",
      "(64, 33)\n",
      "step 10457, loss is 4.742957592010498\n",
      "(64, 33)\n",
      "step 10458, loss is 4.727937698364258\n",
      "(64, 33)\n",
      "step 10459, loss is 4.70568323135376\n",
      "(64, 33)\n",
      "step 10460, loss is 4.8700971603393555\n",
      "(64, 33)\n",
      "step 10461, loss is 4.749581813812256\n",
      "(64, 33)\n",
      "step 10462, loss is 4.877788543701172\n",
      "(64, 33)\n",
      "step 10463, loss is 4.797142028808594\n",
      "(64, 33)\n",
      "step 10464, loss is 4.922135353088379\n",
      "(64, 33)\n",
      "step 10465, loss is 4.862867832183838\n",
      "(64, 33)\n",
      "step 10466, loss is 4.84710693359375\n",
      "(64, 33)\n",
      "step 10467, loss is 4.865352153778076\n",
      "(64, 33)\n",
      "step 10468, loss is 4.671308994293213\n",
      "(64, 33)\n",
      "step 10469, loss is 4.773465633392334\n",
      "(64, 33)\n",
      "step 10470, loss is 4.701852321624756\n",
      "(64, 33)\n",
      "step 10471, loss is 4.85581636428833\n",
      "(64, 33)\n",
      "step 10472, loss is 4.857151031494141\n",
      "(64, 33)\n",
      "step 10473, loss is 4.947874069213867\n",
      "(64, 33)\n",
      "step 10474, loss is 4.8388800621032715\n",
      "(64, 33)\n",
      "step 10475, loss is 4.83453369140625\n",
      "(64, 33)\n",
      "step 10476, loss is 4.778927326202393\n",
      "(64, 33)\n",
      "step 10477, loss is 4.918997764587402\n",
      "(64, 33)\n",
      "step 10478, loss is 4.846797943115234\n",
      "(64, 33)\n",
      "step 10479, loss is 4.737534046173096\n",
      "(64, 33)\n",
      "step 10480, loss is 4.673764228820801\n",
      "(64, 33)\n",
      "step 10481, loss is 4.904950141906738\n",
      "(64, 33)\n",
      "step 10482, loss is 4.925203800201416\n",
      "(64, 33)\n",
      "step 10483, loss is 4.82346248626709\n",
      "(64, 33)\n",
      "step 10484, loss is 4.74566125869751\n",
      "(64, 33)\n",
      "step 10485, loss is 4.844211101531982\n",
      "(64, 33)\n",
      "step 10486, loss is 4.815932750701904\n",
      "(64, 33)\n",
      "step 10487, loss is 4.638216018676758\n",
      "(64, 33)\n",
      "step 10488, loss is 4.933664798736572\n",
      "(64, 33)\n",
      "step 10489, loss is 4.823901653289795\n",
      "(64, 33)\n",
      "step 10490, loss is 4.856070041656494\n",
      "(64, 33)\n",
      "step 10491, loss is 4.793288707733154\n",
      "(64, 33)\n",
      "step 10492, loss is 4.725411891937256\n",
      "(64, 33)\n",
      "step 10493, loss is 4.984983444213867\n",
      "(64, 33)\n",
      "step 10494, loss is 4.936742782592773\n",
      "(64, 33)\n",
      "step 10495, loss is 4.776838779449463\n",
      "(64, 33)\n",
      "step 10496, loss is 4.772795677185059\n",
      "(64, 33)\n",
      "step 10497, loss is 4.900404453277588\n",
      "(64, 33)\n",
      "step 10498, loss is 4.747829914093018\n",
      "(64, 33)\n",
      "step 10499, loss is 4.842722415924072\n",
      "(64, 33)\n",
      "step 10500, loss is 4.990405559539795\n",
      "(64, 33)\n",
      "step 10501, loss is 4.835306644439697\n",
      "(64, 33)\n",
      "step 10502, loss is 4.772892951965332\n",
      "(64, 33)\n",
      "step 10503, loss is 4.827987194061279\n",
      "(64, 33)\n",
      "step 10504, loss is 4.833836555480957\n",
      "(64, 33)\n",
      "step 10505, loss is 4.815552234649658\n",
      "(64, 33)\n",
      "step 10506, loss is 4.967055797576904\n",
      "(64, 33)\n",
      "step 10507, loss is 4.89387845993042\n",
      "(64, 33)\n",
      "step 10508, loss is 4.716745376586914\n",
      "(64, 33)\n",
      "step 10509, loss is 4.6452813148498535\n",
      "(64, 33)\n",
      "step 10510, loss is 4.910492420196533\n",
      "(64, 33)\n",
      "step 10511, loss is 4.964198589324951\n",
      "(64, 33)\n",
      "step 10512, loss is 4.848330497741699\n",
      "(64, 33)\n",
      "step 10513, loss is 4.968795299530029\n",
      "(64, 33)\n",
      "step 10514, loss is 4.839081764221191\n",
      "(64, 33)\n",
      "step 10515, loss is 4.685779094696045\n",
      "(64, 33)\n",
      "step 10516, loss is 4.872463703155518\n",
      "(64, 33)\n",
      "step 10517, loss is 4.928966999053955\n",
      "(64, 33)\n",
      "step 10518, loss is 4.872972011566162\n",
      "(64, 33)\n",
      "step 10519, loss is 4.785972595214844\n",
      "(64, 33)\n",
      "step 10520, loss is 4.833685398101807\n",
      "(64, 33)\n",
      "step 10521, loss is 4.808797836303711\n",
      "(64, 33)\n",
      "step 10522, loss is 4.821651458740234\n",
      "(64, 33)\n",
      "step 10523, loss is 4.902798652648926\n",
      "(64, 33)\n",
      "step 10524, loss is 4.947486877441406\n",
      "(64, 33)\n",
      "step 10525, loss is 4.773867607116699\n",
      "(64, 33)\n",
      "step 10526, loss is 4.847673416137695\n",
      "(64, 33)\n",
      "step 10527, loss is 4.753279685974121\n",
      "(64, 33)\n",
      "step 10528, loss is 4.708140850067139\n",
      "(64, 33)\n",
      "step 10529, loss is 4.830524921417236\n",
      "(64, 33)\n",
      "step 10530, loss is 4.746462821960449\n",
      "(64, 33)\n",
      "step 10531, loss is 4.77390718460083\n",
      "(64, 33)\n",
      "step 10532, loss is 4.7532243728637695\n",
      "(64, 33)\n",
      "step 10533, loss is 4.886660575866699\n",
      "(64, 33)\n",
      "step 10534, loss is 5.044147491455078\n",
      "(64, 33)\n",
      "step 10535, loss is 4.906940937042236\n",
      "(64, 33)\n",
      "step 10536, loss is 4.943039894104004\n",
      "(64, 33)\n",
      "step 10537, loss is 4.8947343826293945\n",
      "(64, 33)\n",
      "step 10538, loss is 4.677601337432861\n",
      "(64, 33)\n",
      "step 10539, loss is 4.933041095733643\n",
      "(64, 33)\n",
      "step 10540, loss is 4.758498668670654\n",
      "(64, 33)\n",
      "step 10541, loss is 4.756706237792969\n",
      "(64, 33)\n",
      "step 10542, loss is 4.9578142166137695\n",
      "(64, 33)\n",
      "step 10543, loss is 4.838764190673828\n",
      "(64, 33)\n",
      "step 10544, loss is 4.839225769042969\n",
      "(64, 33)\n",
      "step 10545, loss is 4.905250072479248\n",
      "(64, 33)\n",
      "step 10546, loss is 4.790514945983887\n",
      "(64, 33)\n",
      "step 10547, loss is 4.754893779754639\n",
      "(64, 33)\n",
      "step 10548, loss is 4.8352837562561035\n",
      "(64, 33)\n",
      "step 10549, loss is 4.810857772827148\n",
      "(64, 33)\n",
      "step 10550, loss is 4.798698902130127\n",
      "(64, 33)\n",
      "step 10551, loss is 4.9231133460998535\n",
      "(64, 33)\n",
      "step 10552, loss is 4.96589994430542\n",
      "(64, 33)\n",
      "step 10553, loss is 4.828112602233887\n",
      "(64, 33)\n",
      "step 10554, loss is 4.905837059020996\n",
      "(64, 33)\n",
      "step 10555, loss is 4.524654388427734\n",
      "(64, 33)\n",
      "step 10556, loss is 4.759640693664551\n",
      "(64, 33)\n",
      "step 10557, loss is 4.726670742034912\n",
      "(64, 33)\n",
      "step 10558, loss is 4.78369665145874\n",
      "(64, 33)\n",
      "step 10559, loss is 4.966435432434082\n",
      "(64, 33)\n",
      "step 10560, loss is 4.935523509979248\n",
      "(64, 33)\n",
      "step 10561, loss is 4.993317604064941\n",
      "(64, 33)\n",
      "step 10562, loss is 4.862504482269287\n",
      "(64, 33)\n",
      "step 10563, loss is 4.83902645111084\n",
      "(64, 33)\n",
      "step 10564, loss is 4.850826263427734\n",
      "(64, 33)\n",
      "step 10565, loss is 4.9633917808532715\n",
      "(64, 33)\n",
      "step 10566, loss is 4.720060348510742\n",
      "(64, 33)\n",
      "step 10567, loss is 4.848237991333008\n",
      "(64, 33)\n",
      "step 10568, loss is 4.962230205535889\n",
      "(64, 33)\n",
      "step 10569, loss is 4.905968189239502\n",
      "(64, 33)\n",
      "step 10570, loss is 4.690247058868408\n",
      "(64, 33)\n",
      "step 10571, loss is 4.758602142333984\n",
      "(64, 33)\n",
      "step 10572, loss is 4.883192539215088\n",
      "(64, 33)\n",
      "step 10573, loss is 4.722383975982666\n",
      "(64, 33)\n",
      "step 10574, loss is 4.796975612640381\n",
      "(64, 33)\n",
      "step 10575, loss is 4.699416637420654\n",
      "(64, 33)\n",
      "step 10576, loss is 4.841567516326904\n",
      "(64, 33)\n",
      "step 10577, loss is 4.690423488616943\n",
      "(64, 33)\n",
      "step 10578, loss is 4.8413238525390625\n",
      "(64, 33)\n",
      "step 10579, loss is 4.71004581451416\n",
      "(64, 33)\n",
      "step 10580, loss is 4.780916690826416\n",
      "(64, 33)\n",
      "step 10581, loss is 4.714917182922363\n",
      "(64, 33)\n",
      "step 10582, loss is 4.8680901527404785\n",
      "(64, 33)\n",
      "step 10583, loss is 4.941921710968018\n",
      "(64, 33)\n",
      "step 10584, loss is 4.846765995025635\n",
      "(64, 33)\n",
      "step 10585, loss is 4.7720723152160645\n",
      "(64, 33)\n",
      "step 10586, loss is 4.706773281097412\n",
      "(64, 33)\n",
      "step 10587, loss is 4.917903423309326\n",
      "(64, 33)\n",
      "step 10588, loss is 4.745126247406006\n",
      "(64, 33)\n",
      "step 10589, loss is 4.803863048553467\n",
      "(64, 33)\n",
      "step 10590, loss is 4.676024913787842\n",
      "(64, 33)\n",
      "step 10591, loss is 4.964265823364258\n",
      "(64, 33)\n",
      "step 10592, loss is 4.970395088195801\n",
      "(64, 33)\n",
      "step 10593, loss is 5.005958557128906\n",
      "(64, 33)\n",
      "step 10594, loss is 4.787477970123291\n",
      "(64, 33)\n",
      "step 10595, loss is 4.9556427001953125\n",
      "(64, 33)\n",
      "step 10596, loss is 4.7495317459106445\n",
      "(64, 33)\n",
      "step 10597, loss is 4.8730645179748535\n",
      "(64, 33)\n",
      "step 10598, loss is 5.019715309143066\n",
      "(64, 33)\n",
      "step 10599, loss is 4.764032363891602\n",
      "(64, 33)\n",
      "step 10600, loss is 4.782653331756592\n",
      "(64, 33)\n",
      "step 10601, loss is 4.9192047119140625\n",
      "(64, 33)\n",
      "step 10602, loss is 4.756699562072754\n",
      "(64, 33)\n",
      "step 10603, loss is 4.857861042022705\n",
      "(64, 33)\n",
      "step 10604, loss is 4.720512390136719\n",
      "(64, 33)\n",
      "step 10605, loss is 4.793972969055176\n",
      "(64, 33)\n",
      "step 10606, loss is 4.917566299438477\n",
      "(64, 33)\n",
      "step 10607, loss is 5.005821228027344\n",
      "(64, 33)\n",
      "step 10608, loss is 4.908290863037109\n",
      "(64, 33)\n",
      "step 10609, loss is 4.971360683441162\n",
      "(64, 33)\n",
      "step 10610, loss is 4.893660068511963\n",
      "(64, 33)\n",
      "step 10611, loss is 4.88589334487915\n",
      "(64, 33)\n",
      "step 10612, loss is 4.978855133056641\n",
      "(64, 33)\n",
      "step 10613, loss is 4.852419853210449\n",
      "(64, 33)\n",
      "step 10614, loss is 4.787611961364746\n",
      "(64, 33)\n",
      "step 10615, loss is 4.795582294464111\n",
      "(64, 33)\n",
      "step 10616, loss is 4.808403015136719\n",
      "(64, 33)\n",
      "step 10617, loss is 4.92415714263916\n",
      "(64, 33)\n",
      "step 10618, loss is 4.846498489379883\n",
      "(64, 33)\n",
      "step 10619, loss is 4.900046348571777\n",
      "(64, 33)\n",
      "step 10620, loss is 4.849544048309326\n",
      "(64, 33)\n",
      "step 10621, loss is 4.8406853675842285\n",
      "(64, 33)\n",
      "step 10622, loss is 4.904407978057861\n",
      "(64, 33)\n",
      "step 10623, loss is 4.940404891967773\n",
      "(64, 33)\n",
      "step 10624, loss is 4.702742099761963\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10625, loss is 4.885833740234375\n",
      "(64, 33)\n",
      "step 10626, loss is 4.754721164703369\n",
      "(64, 33)\n",
      "step 10627, loss is 4.934033393859863\n",
      "(64, 33)\n",
      "step 10628, loss is 4.748796463012695\n",
      "(64, 33)\n",
      "step 10629, loss is 4.832237720489502\n",
      "(64, 33)\n",
      "step 10630, loss is 4.719968795776367\n",
      "(64, 33)\n",
      "step 10631, loss is 4.839494228363037\n",
      "(64, 33)\n",
      "step 10632, loss is 4.685491561889648\n",
      "(64, 33)\n",
      "step 10633, loss is 4.9557881355285645\n",
      "(64, 33)\n",
      "step 10634, loss is 4.737302303314209\n",
      "(64, 33)\n",
      "step 10635, loss is 4.668695449829102\n",
      "(64, 33)\n",
      "step 10636, loss is 4.931890964508057\n",
      "(64, 33)\n",
      "step 10637, loss is 4.5927534103393555\n",
      "(64, 33)\n",
      "step 10638, loss is 4.685764312744141\n",
      "(64, 33)\n",
      "step 10639, loss is 4.755163192749023\n",
      "(64, 33)\n",
      "step 10640, loss is 4.871412754058838\n",
      "(64, 33)\n",
      "step 10641, loss is 4.794646263122559\n",
      "(64, 33)\n",
      "step 10642, loss is 4.7660393714904785\n",
      "(64, 33)\n",
      "step 10643, loss is 4.615137577056885\n",
      "(64, 33)\n",
      "step 10644, loss is 4.73234224319458\n",
      "(64, 33)\n",
      "step 10645, loss is 4.912441253662109\n",
      "(64, 33)\n",
      "step 10646, loss is 4.7462992668151855\n",
      "(64, 33)\n",
      "step 10647, loss is 4.846344947814941\n",
      "(64, 33)\n",
      "step 10648, loss is 4.716733932495117\n",
      "(64, 33)\n",
      "step 10649, loss is 4.703302383422852\n",
      "(64, 33)\n",
      "step 10650, loss is 5.021165370941162\n",
      "(64, 33)\n",
      "step 10651, loss is 4.851769924163818\n",
      "(64, 33)\n",
      "step 10652, loss is 4.878898620605469\n",
      "(64, 33)\n",
      "step 10653, loss is 4.786611080169678\n",
      "(64, 33)\n",
      "step 10654, loss is 4.782149314880371\n",
      "(64, 33)\n",
      "step 10655, loss is 4.8826093673706055\n",
      "(64, 33)\n",
      "step 10656, loss is 4.902780055999756\n",
      "(64, 33)\n",
      "step 10657, loss is 4.881885051727295\n",
      "(64, 33)\n",
      "step 10658, loss is 4.790151596069336\n",
      "(64, 33)\n",
      "step 10659, loss is 4.800512790679932\n",
      "(64, 33)\n",
      "step 10660, loss is 4.818900108337402\n",
      "(64, 33)\n",
      "step 10661, loss is 4.88071870803833\n",
      "(64, 33)\n",
      "step 10662, loss is 4.9488091468811035\n",
      "(64, 33)\n",
      "step 10663, loss is 4.860998153686523\n",
      "(64, 33)\n",
      "step 10664, loss is 4.791676998138428\n",
      "(64, 33)\n",
      "step 10665, loss is 4.89291524887085\n",
      "(64, 33)\n",
      "step 10666, loss is 4.992532730102539\n",
      "(64, 33)\n",
      "step 10667, loss is 5.036988258361816\n",
      "(64, 33)\n",
      "step 10668, loss is 4.8626885414123535\n",
      "(64, 33)\n",
      "step 10669, loss is 4.906769752502441\n",
      "(64, 33)\n",
      "step 10670, loss is 4.78416633605957\n",
      "(64, 33)\n",
      "step 10671, loss is 4.781578063964844\n",
      "(64, 33)\n",
      "step 10672, loss is 4.84411096572876\n",
      "(64, 33)\n",
      "step 10673, loss is 4.843286991119385\n",
      "(64, 33)\n",
      "step 10674, loss is 4.995497226715088\n",
      "(64, 33)\n",
      "step 10675, loss is 4.718574523925781\n",
      "(64, 33)\n",
      "step 10676, loss is 5.014076232910156\n",
      "(64, 33)\n",
      "step 10677, loss is 4.726373195648193\n",
      "(64, 33)\n",
      "step 10678, loss is 4.907412528991699\n",
      "(64, 33)\n",
      "step 10679, loss is 4.683418273925781\n",
      "(64, 33)\n",
      "step 10680, loss is 4.717930793762207\n",
      "(64, 33)\n",
      "step 10681, loss is 4.82017183303833\n",
      "(64, 33)\n",
      "step 10682, loss is 4.82432222366333\n",
      "(64, 33)\n",
      "step 10683, loss is 4.966634750366211\n",
      "(64, 33)\n",
      "step 10684, loss is 4.915467739105225\n",
      "(64, 33)\n",
      "step 10685, loss is 4.9841461181640625\n",
      "(64, 33)\n",
      "step 10686, loss is 4.763421535491943\n",
      "(64, 33)\n",
      "step 10687, loss is 4.917473316192627\n",
      "(64, 33)\n",
      "step 10688, loss is 4.747344493865967\n",
      "(64, 33)\n",
      "step 10689, loss is 4.772339820861816\n",
      "(64, 33)\n",
      "step 10690, loss is 4.842456340789795\n",
      "(64, 33)\n",
      "step 10691, loss is 4.84501838684082\n",
      "(64, 33)\n",
      "step 10692, loss is 4.967838764190674\n",
      "(64, 33)\n",
      "step 10693, loss is 4.934074401855469\n",
      "(64, 33)\n",
      "step 10694, loss is 4.682097434997559\n",
      "(64, 33)\n",
      "step 10695, loss is 4.946476459503174\n",
      "(64, 33)\n",
      "step 10696, loss is 4.877592086791992\n",
      "(64, 33)\n",
      "step 10697, loss is 4.837474822998047\n",
      "(64, 33)\n",
      "step 10698, loss is 4.7545576095581055\n",
      "(64, 33)\n",
      "step 10699, loss is 4.97334623336792\n",
      "(64, 33)\n",
      "step 10700, loss is 4.9745635986328125\n",
      "(64, 33)\n",
      "step 10701, loss is 4.984855651855469\n",
      "(64, 33)\n",
      "step 10702, loss is 4.8107709884643555\n",
      "(64, 33)\n",
      "step 10703, loss is 4.719937801361084\n",
      "(64, 33)\n",
      "step 10704, loss is 4.765947341918945\n",
      "(64, 33)\n",
      "step 10705, loss is 4.856795787811279\n",
      "(64, 33)\n",
      "step 10706, loss is 5.035877704620361\n",
      "(64, 33)\n",
      "step 10707, loss is 4.663088321685791\n",
      "(64, 33)\n",
      "step 10708, loss is 4.8788628578186035\n",
      "(64, 33)\n",
      "step 10709, loss is 4.908300876617432\n",
      "(64, 33)\n",
      "step 10710, loss is 4.9163994789123535\n",
      "(64, 33)\n",
      "step 10711, loss is 4.702871799468994\n",
      "(64, 33)\n",
      "step 10712, loss is 4.985075950622559\n",
      "(64, 33)\n",
      "step 10713, loss is 5.069266319274902\n",
      "(64, 33)\n",
      "step 10714, loss is 4.985591411590576\n",
      "(64, 33)\n",
      "step 10715, loss is 4.974978923797607\n",
      "(64, 33)\n",
      "step 10716, loss is 4.841653347015381\n",
      "(64, 33)\n",
      "step 10717, loss is 4.94084358215332\n",
      "(64, 33)\n",
      "step 10718, loss is 4.926051139831543\n",
      "(64, 33)\n",
      "step 10719, loss is 4.785068511962891\n",
      "(64, 33)\n",
      "step 10720, loss is 4.477476119995117\n",
      "(64, 33)\n",
      "step 10721, loss is 4.836068153381348\n",
      "(64, 33)\n",
      "step 10722, loss is 4.833156108856201\n",
      "(64, 33)\n",
      "step 10723, loss is 4.871653079986572\n",
      "(64, 33)\n",
      "step 10724, loss is 4.6406683921813965\n",
      "(64, 33)\n",
      "step 10725, loss is 4.767894268035889\n",
      "(64, 33)\n",
      "step 10726, loss is 4.82456111907959\n",
      "(64, 33)\n",
      "step 10727, loss is 5.015873908996582\n",
      "(64, 33)\n",
      "step 10728, loss is 4.803199768066406\n",
      "(64, 33)\n",
      "step 10729, loss is 4.835748672485352\n",
      "(64, 33)\n",
      "step 10730, loss is 4.776131629943848\n",
      "(64, 33)\n",
      "step 10731, loss is 4.765196323394775\n",
      "(64, 33)\n",
      "step 10732, loss is 4.838252544403076\n",
      "(64, 33)\n",
      "step 10733, loss is 4.7771453857421875\n",
      "(64, 33)\n",
      "step 10734, loss is 4.718350410461426\n",
      "(64, 33)\n",
      "step 10735, loss is 4.874368190765381\n",
      "(64, 33)\n",
      "step 10736, loss is 4.753415107727051\n",
      "(64, 33)\n",
      "step 10737, loss is 4.840271949768066\n",
      "(64, 33)\n",
      "step 10738, loss is 4.704521656036377\n",
      "(64, 33)\n",
      "step 10739, loss is 4.843660354614258\n",
      "(64, 33)\n",
      "step 10740, loss is 4.633588790893555\n",
      "(64, 33)\n",
      "step 10741, loss is 4.857210159301758\n",
      "(64, 33)\n",
      "step 10742, loss is 4.766313076019287\n",
      "(64, 33)\n",
      "step 10743, loss is 5.001862525939941\n",
      "(64, 33)\n",
      "step 10744, loss is 4.691572666168213\n",
      "(64, 33)\n",
      "step 10745, loss is 4.736802577972412\n",
      "(64, 33)\n",
      "step 10746, loss is 4.90753173828125\n",
      "(64, 33)\n",
      "step 10747, loss is 4.928701877593994\n",
      "(64, 33)\n",
      "step 10748, loss is 4.696377277374268\n",
      "(64, 33)\n",
      "step 10749, loss is 4.732112884521484\n",
      "(64, 33)\n",
      "step 10750, loss is 4.873505592346191\n",
      "(64, 33)\n",
      "step 10751, loss is 4.8902692794799805\n",
      "(64, 33)\n",
      "step 10752, loss is 4.787567615509033\n",
      "(64, 33)\n",
      "step 10753, loss is 4.899929046630859\n",
      "(64, 33)\n",
      "step 10754, loss is 4.909873008728027\n",
      "(64, 33)\n",
      "step 10755, loss is 4.883484363555908\n",
      "(64, 33)\n",
      "step 10756, loss is 4.747178554534912\n",
      "(64, 33)\n",
      "step 10757, loss is 4.877850532531738\n",
      "(64, 33)\n",
      "step 10758, loss is 4.888906478881836\n",
      "(64, 33)\n",
      "step 10759, loss is 4.805319309234619\n",
      "(64, 33)\n",
      "step 10760, loss is 4.814934253692627\n",
      "(64, 33)\n",
      "step 10761, loss is 4.7794060707092285\n",
      "(64, 33)\n",
      "step 10762, loss is 4.920786380767822\n",
      "(64, 33)\n",
      "step 10763, loss is 4.873016834259033\n",
      "(64, 33)\n",
      "step 10764, loss is 4.6754913330078125\n",
      "(64, 33)\n",
      "step 10765, loss is 4.808538913726807\n",
      "(64, 33)\n",
      "step 10766, loss is 5.111914157867432\n",
      "(64, 33)\n",
      "step 10767, loss is 4.850911617279053\n",
      "(64, 33)\n",
      "step 10768, loss is 4.834292888641357\n",
      "(64, 33)\n",
      "step 10769, loss is 4.819313049316406\n",
      "(64, 33)\n",
      "step 10770, loss is 4.745359420776367\n",
      "(64, 33)\n",
      "step 10771, loss is 4.890720844268799\n",
      "(64, 33)\n",
      "step 10772, loss is 4.848093032836914\n",
      "(64, 33)\n",
      "step 10773, loss is 4.744974613189697\n",
      "(64, 33)\n",
      "step 10774, loss is 4.712332725524902\n",
      "(64, 33)\n",
      "step 10775, loss is 4.870273113250732\n",
      "(64, 33)\n",
      "step 10776, loss is 4.758477687835693\n",
      "(64, 33)\n",
      "step 10777, loss is 4.819088935852051\n",
      "(64, 33)\n",
      "step 10778, loss is 4.7943902015686035\n",
      "(64, 33)\n",
      "step 10779, loss is 4.705940246582031\n",
      "(64, 33)\n",
      "step 10780, loss is 4.807398796081543\n",
      "(64, 33)\n",
      "step 10781, loss is 4.8472981452941895\n",
      "(64, 33)\n",
      "step 10782, loss is 4.903479099273682\n",
      "(64, 33)\n",
      "step 10783, loss is 4.858408451080322\n",
      "(64, 33)\n",
      "step 10784, loss is 4.729355335235596\n",
      "(64, 33)\n",
      "step 10785, loss is 4.8273468017578125\n",
      "(64, 33)\n",
      "step 10786, loss is 4.860806465148926\n",
      "(64, 33)\n",
      "step 10787, loss is 4.877116680145264\n",
      "(64, 33)\n",
      "step 10788, loss is 4.695031642913818\n",
      "(64, 33)\n",
      "step 10789, loss is 4.899946689605713\n",
      "(64, 33)\n",
      "step 10790, loss is 4.7862067222595215\n",
      "(64, 33)\n",
      "step 10791, loss is 4.8748064041137695\n",
      "(64, 33)\n",
      "step 10792, loss is 4.998547077178955\n",
      "(64, 33)\n",
      "step 10793, loss is 4.728660583496094\n",
      "(64, 33)\n",
      "step 10794, loss is 4.907374382019043\n",
      "(64, 33)\n",
      "step 10795, loss is 4.829111576080322\n",
      "(64, 33)\n",
      "step 10796, loss is 4.608475208282471\n",
      "(64, 33)\n",
      "step 10797, loss is 4.802190780639648\n",
      "(64, 33)\n",
      "step 10798, loss is 4.959304332733154\n",
      "(64, 33)\n",
      "step 10799, loss is 4.752870082855225\n",
      "(64, 33)\n",
      "step 10800, loss is 4.73533821105957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 10801, loss is 4.8731913566589355\n",
      "(64, 33)\n",
      "step 10802, loss is 4.853641986846924\n",
      "(64, 33)\n",
      "step 10803, loss is 4.950630187988281\n",
      "(64, 33)\n",
      "step 10804, loss is 4.668058395385742\n",
      "(64, 33)\n",
      "step 10805, loss is 4.89869499206543\n",
      "(64, 33)\n",
      "step 10806, loss is 4.751173496246338\n",
      "(64, 33)\n",
      "step 10807, loss is 4.820361614227295\n",
      "(64, 33)\n",
      "step 10808, loss is 4.905643463134766\n",
      "(64, 33)\n",
      "step 10809, loss is 4.826934337615967\n",
      "(64, 33)\n",
      "step 10810, loss is 4.8348236083984375\n",
      "(64, 33)\n",
      "step 10811, loss is 4.8216753005981445\n",
      "(64, 33)\n",
      "step 10812, loss is 4.594298839569092\n",
      "(64, 33)\n",
      "step 10813, loss is 4.734654903411865\n",
      "(64, 33)\n",
      "step 10814, loss is 4.898114204406738\n",
      "(64, 33)\n",
      "step 10815, loss is 4.898004531860352\n",
      "(64, 33)\n",
      "step 10816, loss is 4.783111572265625\n",
      "(64, 33)\n",
      "step 10817, loss is 4.7783331871032715\n",
      "(64, 33)\n",
      "step 10818, loss is 4.846129417419434\n",
      "(64, 33)\n",
      "step 10819, loss is 4.899636745452881\n",
      "(64, 33)\n",
      "step 10820, loss is 4.819520473480225\n",
      "(64, 33)\n",
      "step 10821, loss is 4.718172550201416\n",
      "(64, 33)\n",
      "step 10822, loss is 4.819488048553467\n",
      "(64, 33)\n",
      "step 10823, loss is 4.893721103668213\n",
      "(64, 33)\n",
      "step 10824, loss is 4.761086463928223\n",
      "(64, 33)\n",
      "step 10825, loss is 4.88729190826416\n",
      "(64, 33)\n",
      "step 10826, loss is 4.651460647583008\n",
      "(64, 33)\n",
      "step 10827, loss is 4.819350719451904\n",
      "(64, 33)\n",
      "step 10828, loss is 4.841047763824463\n",
      "(64, 33)\n",
      "step 10829, loss is 4.9264092445373535\n",
      "(64, 33)\n",
      "step 10830, loss is 4.762680530548096\n",
      "(64, 33)\n",
      "step 10831, loss is 4.872711658477783\n",
      "(64, 33)\n",
      "step 10832, loss is 4.987837791442871\n",
      "(64, 33)\n",
      "step 10833, loss is 5.025457382202148\n",
      "(64, 33)\n",
      "step 10834, loss is 4.913959980010986\n",
      "(64, 33)\n",
      "step 10835, loss is 4.823912143707275\n",
      "(64, 33)\n",
      "step 10836, loss is 4.7987895011901855\n",
      "(64, 33)\n",
      "step 10837, loss is 4.941413402557373\n",
      "(64, 33)\n",
      "step 10838, loss is 4.82069730758667\n",
      "(64, 33)\n",
      "step 10839, loss is 4.641398906707764\n",
      "(64, 33)\n",
      "step 10840, loss is 4.92182731628418\n",
      "(64, 33)\n",
      "step 10841, loss is 4.940082550048828\n",
      "(64, 33)\n",
      "step 10842, loss is 4.607847213745117\n",
      "(64, 33)\n",
      "step 10843, loss is 5.017360210418701\n",
      "(64, 33)\n",
      "step 10844, loss is 4.826632022857666\n",
      "(64, 33)\n",
      "step 10845, loss is 5.096370220184326\n",
      "(64, 33)\n",
      "step 10846, loss is 4.660137176513672\n",
      "(64, 33)\n",
      "step 10847, loss is 4.801900863647461\n",
      "(64, 33)\n",
      "step 10848, loss is 4.872522354125977\n",
      "(64, 33)\n",
      "step 10849, loss is 4.841187953948975\n",
      "(64, 33)\n",
      "step 10850, loss is 4.834690570831299\n",
      "(64, 33)\n",
      "step 10851, loss is 4.938186168670654\n",
      "(64, 33)\n",
      "step 10852, loss is 4.812963485717773\n",
      "(64, 33)\n",
      "step 10853, loss is 5.020957946777344\n",
      "(64, 33)\n",
      "step 10854, loss is 4.8082075119018555\n",
      "(64, 33)\n",
      "step 10855, loss is 4.817803859710693\n",
      "(64, 33)\n",
      "step 10856, loss is 4.741819858551025\n",
      "(64, 33)\n",
      "step 10857, loss is 4.772418975830078\n",
      "(64, 33)\n",
      "step 10858, loss is 4.983203411102295\n",
      "(64, 33)\n",
      "step 10859, loss is 4.843240261077881\n",
      "(64, 33)\n",
      "step 10860, loss is 4.965097427368164\n",
      "(64, 33)\n",
      "step 10861, loss is 4.676475524902344\n",
      "(64, 33)\n",
      "step 10862, loss is 4.875729084014893\n",
      "(64, 33)\n",
      "step 10863, loss is 4.8272175788879395\n",
      "(64, 33)\n",
      "step 10864, loss is 4.765493869781494\n",
      "(64, 33)\n",
      "step 10865, loss is 4.9314188957214355\n",
      "(64, 33)\n",
      "step 10866, loss is 4.876293659210205\n",
      "(64, 33)\n",
      "step 10867, loss is 5.103028297424316\n",
      "(64, 33)\n",
      "step 10868, loss is 4.915157794952393\n",
      "(64, 33)\n",
      "step 10869, loss is 4.980058193206787\n",
      "(64, 33)\n",
      "step 10870, loss is 4.755038261413574\n",
      "(64, 33)\n",
      "step 10871, loss is 4.7498459815979\n",
      "(64, 33)\n",
      "step 10872, loss is 4.80649471282959\n",
      "(64, 33)\n",
      "step 10873, loss is 4.871715068817139\n",
      "(64, 33)\n",
      "step 10874, loss is 4.804321765899658\n",
      "(64, 33)\n",
      "step 10875, loss is 4.824476718902588\n",
      "(64, 33)\n",
      "step 10876, loss is 4.8821868896484375\n",
      "(64, 33)\n",
      "step 10877, loss is 4.622000694274902\n",
      "(64, 33)\n",
      "step 10878, loss is 4.790431976318359\n",
      "(64, 33)\n",
      "step 10879, loss is 4.951313018798828\n",
      "(64, 33)\n",
      "step 10880, loss is 4.75705099105835\n",
      "(64, 33)\n",
      "step 10881, loss is 4.837242126464844\n",
      "(64, 33)\n",
      "step 10882, loss is 4.786534309387207\n",
      "(64, 33)\n",
      "step 10883, loss is 5.012423038482666\n",
      "(64, 33)\n",
      "step 10884, loss is 4.831860542297363\n",
      "(64, 33)\n",
      "step 10885, loss is 4.7668538093566895\n",
      "(64, 33)\n",
      "step 10886, loss is 4.954385757446289\n",
      "(64, 33)\n",
      "step 10887, loss is 4.853858947753906\n",
      "(64, 33)\n",
      "step 10888, loss is 4.790310382843018\n",
      "(64, 33)\n",
      "step 10889, loss is 4.868813991546631\n",
      "(64, 33)\n",
      "step 10890, loss is 4.910576820373535\n",
      "(64, 33)\n",
      "step 10891, loss is 4.9327898025512695\n",
      "(64, 33)\n",
      "step 10892, loss is 4.953742504119873\n",
      "(64, 33)\n",
      "step 10893, loss is 4.913563251495361\n",
      "(64, 33)\n",
      "step 10894, loss is 4.912149906158447\n",
      "(64, 33)\n",
      "step 10895, loss is 4.801846027374268\n",
      "(64, 33)\n",
      "step 10896, loss is 4.78079891204834\n",
      "(64, 33)\n",
      "step 10897, loss is 4.802483558654785\n",
      "(64, 33)\n",
      "step 10898, loss is 4.729124069213867\n",
      "(64, 33)\n",
      "step 10899, loss is 4.72981595993042\n",
      "(64, 33)\n",
      "step 10900, loss is 4.753669738769531\n",
      "(64, 33)\n",
      "step 10901, loss is 4.9657673835754395\n",
      "(64, 33)\n",
      "step 10902, loss is 4.720351696014404\n",
      "(64, 33)\n",
      "step 10903, loss is 4.7995285987854\n",
      "(64, 33)\n",
      "step 10904, loss is 4.988348007202148\n",
      "(64, 33)\n",
      "step 10905, loss is 4.7672929763793945\n",
      "(64, 33)\n",
      "step 10906, loss is 4.912607669830322\n",
      "(64, 33)\n",
      "step 10907, loss is 4.845189094543457\n",
      "(64, 33)\n",
      "step 10908, loss is 5.079142093658447\n",
      "(64, 33)\n",
      "step 10909, loss is 4.757989406585693\n",
      "(64, 33)\n",
      "step 10910, loss is 4.773614883422852\n",
      "(64, 33)\n",
      "step 10911, loss is 4.692681312561035\n",
      "(64, 33)\n",
      "step 10912, loss is 4.707949638366699\n",
      "(64, 33)\n",
      "step 10913, loss is 4.8414435386657715\n",
      "(64, 33)\n",
      "step 10914, loss is 4.779311180114746\n",
      "(64, 33)\n",
      "step 10915, loss is 4.749260902404785\n",
      "(64, 33)\n",
      "step 10916, loss is 4.803659439086914\n",
      "(64, 33)\n",
      "step 10917, loss is 4.830377578735352\n",
      "(64, 33)\n",
      "step 10918, loss is 4.846055507659912\n",
      "(64, 33)\n",
      "step 10919, loss is 4.9285359382629395\n",
      "(64, 33)\n",
      "step 10920, loss is 4.8419880867004395\n",
      "(64, 33)\n",
      "step 10921, loss is 4.708979606628418\n",
      "(64, 33)\n",
      "step 10922, loss is 4.806423187255859\n",
      "(64, 33)\n",
      "step 10923, loss is 4.829843521118164\n",
      "(64, 33)\n",
      "step 10924, loss is 4.721673011779785\n",
      "(64, 33)\n",
      "step 10925, loss is 4.9767327308654785\n",
      "(64, 33)\n",
      "step 10926, loss is 4.961010932922363\n",
      "(64, 33)\n",
      "step 10927, loss is 4.7573981285095215\n",
      "(64, 33)\n",
      "step 10928, loss is 4.643144130706787\n",
      "(64, 33)\n",
      "step 10929, loss is 4.989297866821289\n",
      "(64, 33)\n",
      "step 10930, loss is 4.8540873527526855\n",
      "(64, 33)\n",
      "step 10931, loss is 4.941884517669678\n",
      "(64, 33)\n",
      "step 10932, loss is 4.74147367477417\n",
      "(64, 33)\n",
      "step 10933, loss is 4.905124664306641\n",
      "(64, 33)\n",
      "step 10934, loss is 4.83840274810791\n",
      "(64, 33)\n",
      "step 10935, loss is 4.740114212036133\n",
      "(64, 33)\n",
      "step 10936, loss is 4.620265483856201\n",
      "(64, 33)\n",
      "step 10937, loss is 4.940155982971191\n",
      "(64, 33)\n",
      "step 10938, loss is 4.739652156829834\n",
      "(64, 33)\n",
      "step 10939, loss is 5.051092147827148\n",
      "(64, 33)\n",
      "step 10940, loss is 4.862364768981934\n",
      "(64, 33)\n",
      "step 10941, loss is 4.8771233558654785\n",
      "(64, 33)\n",
      "step 10942, loss is 4.77157735824585\n",
      "(64, 33)\n",
      "step 10943, loss is 4.916379928588867\n",
      "(64, 33)\n",
      "step 10944, loss is 4.823986053466797\n",
      "(64, 33)\n",
      "step 10945, loss is 4.79129695892334\n",
      "(64, 33)\n",
      "step 10946, loss is 5.061894416809082\n",
      "(64, 33)\n",
      "step 10947, loss is 4.692515850067139\n",
      "(64, 33)\n",
      "step 10948, loss is 4.844481945037842\n",
      "(64, 33)\n",
      "step 10949, loss is 4.780450820922852\n",
      "(64, 33)\n",
      "step 10950, loss is 4.771987438201904\n",
      "(64, 33)\n",
      "step 10951, loss is 4.644052982330322\n",
      "(64, 33)\n",
      "step 10952, loss is 4.745206832885742\n",
      "(64, 33)\n",
      "step 10953, loss is 4.985558986663818\n",
      "(64, 33)\n",
      "step 10954, loss is 4.7344255447387695\n",
      "(64, 33)\n",
      "step 10955, loss is 4.755616664886475\n",
      "(64, 33)\n",
      "step 10956, loss is 4.818426609039307\n",
      "(64, 33)\n",
      "step 10957, loss is 4.365284442901611\n",
      "(64, 33)\n",
      "step 10958, loss is 4.655501842498779\n",
      "(64, 33)\n",
      "step 10959, loss is 4.684317111968994\n",
      "(64, 33)\n",
      "step 10960, loss is 4.94989538192749\n",
      "(64, 33)\n",
      "step 10961, loss is 4.647795677185059\n",
      "(64, 33)\n",
      "step 10962, loss is 4.861429691314697\n",
      "(64, 33)\n",
      "step 10963, loss is 4.749922275543213\n",
      "(64, 33)\n",
      "step 10964, loss is 4.719059944152832\n",
      "(64, 33)\n",
      "step 10965, loss is 4.8702263832092285\n",
      "(64, 33)\n",
      "step 10966, loss is 4.830040454864502\n",
      "(64, 33)\n",
      "step 10967, loss is 4.8472819328308105\n",
      "(64, 33)\n",
      "step 10968, loss is 4.885874271392822\n",
      "(64, 33)\n",
      "step 10969, loss is 4.970575332641602\n",
      "(64, 33)\n",
      "step 10970, loss is 4.859009742736816\n",
      "(64, 33)\n",
      "step 10971, loss is 4.920263290405273\n",
      "(64, 33)\n",
      "step 10972, loss is 4.680984020233154\n",
      "(64, 33)\n",
      "step 10973, loss is 4.784170627593994\n",
      "(64, 33)\n",
      "step 10974, loss is 4.8885498046875\n",
      "(64, 33)\n",
      "step 10975, loss is 5.027566909790039\n",
      "(64, 33)\n",
      "step 10976, loss is 4.939758777618408\n",
      "(64, 33)\n",
      "step 10977, loss is 4.703678131103516\n",
      "(64, 33)\n",
      "step 10978, loss is 4.787194728851318\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10979, loss is 4.854305267333984\n",
      "(64, 33)\n",
      "step 10980, loss is 4.901589393615723\n",
      "(64, 33)\n",
      "step 10981, loss is 4.90907096862793\n",
      "(64, 33)\n",
      "step 10982, loss is 4.554532051086426\n",
      "(64, 33)\n",
      "step 10983, loss is 4.84446907043457\n",
      "(64, 33)\n",
      "step 10984, loss is 4.700292587280273\n",
      "(64, 33)\n",
      "step 10985, loss is 4.706057548522949\n",
      "(64, 33)\n",
      "step 10986, loss is 4.966721057891846\n",
      "(64, 33)\n",
      "step 10987, loss is 4.945655822753906\n",
      "(64, 33)\n",
      "step 10988, loss is 4.916675567626953\n",
      "(64, 33)\n",
      "step 10989, loss is 4.864466667175293\n",
      "(64, 33)\n",
      "step 10990, loss is 4.72349739074707\n",
      "(64, 33)\n",
      "step 10991, loss is 4.683447360992432\n",
      "(64, 33)\n",
      "step 10992, loss is 4.841436862945557\n",
      "(64, 33)\n",
      "step 10993, loss is 4.937033176422119\n",
      "(64, 33)\n",
      "step 10994, loss is 4.901881217956543\n",
      "(64, 33)\n",
      "step 10995, loss is 4.917644023895264\n",
      "(64, 33)\n",
      "step 10996, loss is 4.976362705230713\n",
      "(64, 33)\n",
      "step 10997, loss is 4.950158596038818\n",
      "(64, 33)\n",
      "step 10998, loss is 4.828495979309082\n",
      "(64, 33)\n",
      "step 10999, loss is 4.789990425109863\n",
      "(64, 33)\n",
      "step 11000, loss is 4.706330299377441\n",
      "(64, 33)\n",
      "step 11001, loss is 4.840856075286865\n",
      "(64, 33)\n",
      "step 11002, loss is 4.61777925491333\n",
      "(64, 33)\n",
      "step 11003, loss is 4.8902435302734375\n",
      "(64, 33)\n",
      "step 11004, loss is 4.8973259925842285\n",
      "(64, 33)\n",
      "step 11005, loss is 4.792587757110596\n",
      "(64, 33)\n",
      "step 11006, loss is 4.930296897888184\n",
      "(64, 33)\n",
      "step 11007, loss is 4.7508978843688965\n",
      "(64, 33)\n",
      "step 11008, loss is 4.733212471008301\n",
      "(64, 33)\n",
      "step 11009, loss is 4.825783729553223\n",
      "(64, 33)\n",
      "step 11010, loss is 4.72883415222168\n",
      "(64, 33)\n",
      "step 11011, loss is 4.708917140960693\n",
      "(64, 33)\n",
      "step 11012, loss is 4.818233489990234\n",
      "(64, 33)\n",
      "step 11013, loss is 4.683340072631836\n",
      "(64, 33)\n",
      "step 11014, loss is 4.731637954711914\n",
      "(64, 33)\n",
      "step 11015, loss is 4.69089412689209\n",
      "(64, 33)\n",
      "step 11016, loss is 4.8257060050964355\n",
      "(64, 33)\n",
      "step 11017, loss is 5.1368021965026855\n",
      "(64, 33)\n",
      "step 11018, loss is 5.104576110839844\n",
      "(64, 33)\n",
      "step 11019, loss is 4.690803527832031\n",
      "(64, 33)\n",
      "step 11020, loss is 4.626424312591553\n",
      "(64, 33)\n",
      "step 11021, loss is 4.912631034851074\n",
      "(64, 33)\n",
      "step 11022, loss is 4.770602226257324\n",
      "(64, 33)\n",
      "step 11023, loss is 5.067281723022461\n",
      "(64, 33)\n",
      "step 11024, loss is 4.647396087646484\n",
      "(64, 33)\n",
      "step 11025, loss is 4.847586631774902\n",
      "(64, 33)\n",
      "step 11026, loss is 4.802222728729248\n",
      "(64, 33)\n",
      "step 11027, loss is 4.843255043029785\n",
      "(64, 33)\n",
      "step 11028, loss is 4.836720943450928\n",
      "(64, 33)\n",
      "step 11029, loss is 4.979940414428711\n",
      "(64, 33)\n",
      "step 11030, loss is 4.729857921600342\n",
      "(64, 33)\n",
      "step 11031, loss is 4.613227367401123\n",
      "(64, 33)\n",
      "step 11032, loss is 4.766541481018066\n",
      "(64, 33)\n",
      "step 11033, loss is 4.820544242858887\n",
      "(64, 33)\n",
      "step 11034, loss is 4.934787273406982\n",
      "(64, 33)\n",
      "step 11035, loss is 4.493647575378418\n",
      "(64, 33)\n",
      "step 11036, loss is 4.712103366851807\n",
      "(64, 33)\n",
      "step 11037, loss is 4.936038970947266\n",
      "(64, 33)\n",
      "step 11038, loss is 4.792206287384033\n",
      "(64, 33)\n",
      "step 11039, loss is 4.744218349456787\n",
      "(64, 33)\n",
      "step 11040, loss is 4.808971881866455\n",
      "(64, 33)\n",
      "step 11041, loss is 4.837066650390625\n",
      "(64, 33)\n",
      "step 11042, loss is 4.560909748077393\n",
      "(64, 33)\n",
      "step 11043, loss is 4.817448616027832\n",
      "(64, 33)\n",
      "step 11044, loss is 4.943253517150879\n",
      "(64, 33)\n",
      "step 11045, loss is 4.86163330078125\n",
      "(64, 33)\n",
      "step 11046, loss is 4.883901596069336\n",
      "(64, 33)\n",
      "step 11047, loss is 4.7453460693359375\n",
      "(64, 33)\n",
      "step 11048, loss is 4.7437567710876465\n",
      "(64, 33)\n",
      "step 11049, loss is 4.753056526184082\n",
      "(64, 33)\n",
      "step 11050, loss is 4.804762363433838\n",
      "(64, 33)\n",
      "step 11051, loss is 4.772876739501953\n",
      "(64, 33)\n",
      "step 11052, loss is 4.924759387969971\n",
      "(64, 33)\n",
      "step 11053, loss is 4.811978340148926\n",
      "(64, 33)\n",
      "step 11054, loss is 4.76058292388916\n",
      "(64, 33)\n",
      "step 11055, loss is 4.868499279022217\n",
      "(64, 33)\n",
      "step 11056, loss is 4.84368896484375\n",
      "(64, 33)\n",
      "step 11057, loss is 4.678540229797363\n",
      "(64, 33)\n",
      "step 11058, loss is 4.714967250823975\n",
      "(64, 33)\n",
      "step 11059, loss is 4.902451992034912\n",
      "(64, 33)\n",
      "step 11060, loss is 4.745541572570801\n",
      "(64, 33)\n",
      "step 11061, loss is 4.780686855316162\n",
      "(64, 33)\n",
      "step 11062, loss is 4.67098331451416\n",
      "(64, 33)\n",
      "step 11063, loss is 4.796813011169434\n",
      "(64, 33)\n",
      "step 11064, loss is 4.80195951461792\n",
      "(64, 33)\n",
      "step 11065, loss is 4.689055919647217\n",
      "(64, 33)\n",
      "step 11066, loss is 4.8764729499816895\n",
      "(64, 33)\n",
      "step 11067, loss is 4.789076328277588\n",
      "(64, 33)\n",
      "step 11068, loss is 4.793063640594482\n",
      "(64, 33)\n",
      "step 11069, loss is 4.858652591705322\n",
      "(64, 33)\n",
      "step 11070, loss is 4.796900272369385\n",
      "(64, 33)\n",
      "step 11071, loss is 4.693953990936279\n",
      "(64, 33)\n",
      "step 11072, loss is 4.8567914962768555\n",
      "(64, 33)\n",
      "step 11073, loss is 4.908448219299316\n",
      "(64, 33)\n",
      "step 11074, loss is 4.845517158508301\n",
      "(64, 33)\n",
      "step 11075, loss is 4.6914801597595215\n",
      "(64, 33)\n",
      "step 11076, loss is 4.756607532501221\n",
      "(64, 33)\n",
      "step 11077, loss is 4.720692157745361\n",
      "(64, 33)\n",
      "step 11078, loss is 4.678313255310059\n",
      "(64, 33)\n",
      "step 11079, loss is 4.834333419799805\n",
      "(64, 33)\n",
      "step 11080, loss is 4.769649028778076\n",
      "(64, 33)\n",
      "step 11081, loss is 4.861459255218506\n",
      "(64, 33)\n",
      "step 11082, loss is 4.705390930175781\n",
      "(64, 33)\n",
      "step 11083, loss is 4.776302337646484\n",
      "(64, 33)\n",
      "step 11084, loss is 5.003176689147949\n",
      "(64, 33)\n",
      "step 11085, loss is 4.800967216491699\n",
      "(64, 33)\n",
      "step 11086, loss is 4.657777309417725\n",
      "(64, 33)\n",
      "step 11087, loss is 4.789280891418457\n",
      "(64, 33)\n",
      "step 11088, loss is 4.958778381347656\n",
      "(64, 33)\n",
      "step 11089, loss is 4.656254768371582\n",
      "(64, 33)\n",
      "step 11090, loss is 4.850656032562256\n",
      "(64, 33)\n",
      "step 11091, loss is 4.717682838439941\n",
      "(64, 33)\n",
      "step 11092, loss is 5.02654504776001\n",
      "(64, 33)\n",
      "step 11093, loss is 4.892010688781738\n",
      "(64, 33)\n",
      "step 11094, loss is 4.622660160064697\n",
      "(64, 33)\n",
      "step 11095, loss is 4.890257835388184\n",
      "(64, 33)\n",
      "step 11096, loss is 4.693441867828369\n",
      "(64, 33)\n",
      "step 11097, loss is 4.677074432373047\n",
      "(64, 33)\n",
      "step 11098, loss is 5.014594078063965\n",
      "(64, 33)\n",
      "step 11099, loss is 4.822685241699219\n",
      "(64, 33)\n",
      "step 11100, loss is 4.753576755523682\n",
      "(64, 33)\n",
      "step 11101, loss is 4.819468975067139\n",
      "(64, 33)\n",
      "step 11102, loss is 5.085180759429932\n",
      "(64, 33)\n",
      "step 11103, loss is 4.714330673217773\n",
      "(64, 33)\n",
      "step 11104, loss is 4.568587779998779\n",
      "(64, 33)\n",
      "step 11105, loss is 4.965226650238037\n",
      "(64, 33)\n",
      "step 11106, loss is 4.839280128479004\n",
      "(64, 33)\n",
      "step 11107, loss is 4.698765277862549\n",
      "(64, 33)\n",
      "step 11108, loss is 4.8659772872924805\n",
      "(64, 33)\n",
      "step 11109, loss is 4.77482271194458\n",
      "(64, 33)\n",
      "step 11110, loss is 4.631977081298828\n",
      "(64, 33)\n",
      "step 11111, loss is 4.7363128662109375\n",
      "(64, 33)\n",
      "step 11112, loss is 4.67770528793335\n",
      "(64, 33)\n",
      "step 11113, loss is 4.928668022155762\n",
      "(64, 33)\n",
      "step 11114, loss is 4.925594806671143\n",
      "(64, 33)\n",
      "step 11115, loss is 4.691086292266846\n",
      "(64, 33)\n",
      "step 11116, loss is 4.937831878662109\n",
      "(64, 33)\n",
      "step 11117, loss is 4.847578525543213\n",
      "(64, 33)\n",
      "step 11118, loss is 4.809335231781006\n",
      "(64, 33)\n",
      "step 11119, loss is 4.869789123535156\n",
      "(64, 33)\n",
      "step 11120, loss is 4.912503242492676\n",
      "(64, 33)\n",
      "step 11121, loss is 4.926318645477295\n",
      "(64, 33)\n",
      "step 11122, loss is 4.873622894287109\n",
      "(64, 33)\n",
      "step 11123, loss is 5.015412330627441\n",
      "(64, 33)\n",
      "step 11124, loss is 4.833837985992432\n",
      "(64, 33)\n",
      "step 11125, loss is 4.876025676727295\n",
      "(64, 33)\n",
      "step 11126, loss is 4.773926258087158\n",
      "(64, 33)\n",
      "step 11127, loss is 4.861087799072266\n",
      "(64, 33)\n",
      "step 11128, loss is 4.665382385253906\n",
      "(64, 33)\n",
      "step 11129, loss is 4.685845375061035\n",
      "(64, 33)\n",
      "step 11130, loss is 4.882241725921631\n",
      "(64, 33)\n",
      "step 11131, loss is 4.738424777984619\n",
      "(64, 33)\n",
      "step 11132, loss is 5.141849517822266\n",
      "(64, 33)\n",
      "step 11133, loss is 4.733950614929199\n",
      "(64, 33)\n",
      "step 11134, loss is 4.718505859375\n",
      "(64, 33)\n",
      "step 11135, loss is 4.83671760559082\n",
      "(64, 33)\n",
      "step 11136, loss is 4.625051021575928\n",
      "(64, 33)\n",
      "step 11137, loss is 4.6310200691223145\n",
      "(64, 33)\n",
      "step 11138, loss is 4.844756603240967\n",
      "(64, 33)\n",
      "step 11139, loss is 4.895352363586426\n",
      "(64, 33)\n",
      "step 11140, loss is 4.746356964111328\n",
      "(64, 33)\n",
      "step 11141, loss is 4.995481491088867\n",
      "(64, 33)\n",
      "step 11142, loss is 4.7557501792907715\n",
      "(64, 33)\n",
      "step 11143, loss is 4.884683132171631\n",
      "(64, 33)\n",
      "step 11144, loss is 4.9376444816589355\n",
      "(64, 33)\n",
      "step 11145, loss is 4.819131851196289\n",
      "(64, 33)\n",
      "step 11146, loss is 4.8746256828308105\n",
      "(64, 33)\n",
      "step 11147, loss is 4.731997489929199\n",
      "(64, 33)\n",
      "step 11148, loss is 4.870000839233398\n",
      "(64, 33)\n",
      "step 11149, loss is 4.851722717285156\n",
      "(64, 33)\n",
      "step 11150, loss is 4.842744827270508\n",
      "(64, 33)\n",
      "step 11151, loss is 4.819631099700928\n",
      "(64, 33)\n",
      "step 11152, loss is 4.946647644042969\n",
      "(64, 33)\n",
      "step 11153, loss is 4.598809719085693\n",
      "(64, 33)\n",
      "step 11154, loss is 4.76981782913208\n",
      "(64, 33)\n",
      "step 11155, loss is 4.823702812194824\n",
      "(64, 33)\n",
      "step 11156, loss is 4.927105903625488\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11157, loss is 4.783566474914551\n",
      "(64, 33)\n",
      "step 11158, loss is 4.600338459014893\n",
      "(64, 33)\n",
      "step 11159, loss is 4.779164791107178\n",
      "(64, 33)\n",
      "step 11160, loss is 4.796423435211182\n",
      "(64, 33)\n",
      "step 11161, loss is 4.950912952423096\n",
      "(64, 33)\n",
      "step 11162, loss is 4.57796573638916\n",
      "(64, 33)\n",
      "step 11163, loss is 4.85811185836792\n",
      "(64, 33)\n",
      "step 11164, loss is 4.814038276672363\n",
      "(64, 33)\n",
      "step 11165, loss is 4.947267532348633\n",
      "(64, 33)\n",
      "step 11166, loss is 4.852617263793945\n",
      "(64, 33)\n",
      "step 11167, loss is 4.731320858001709\n",
      "(64, 33)\n",
      "step 11168, loss is 4.737119674682617\n",
      "(64, 33)\n",
      "step 11169, loss is 4.727059364318848\n",
      "(64, 33)\n",
      "step 11170, loss is 4.601210117340088\n",
      "(64, 33)\n",
      "step 11171, loss is 4.92962646484375\n",
      "(64, 33)\n",
      "step 11172, loss is 4.93471622467041\n",
      "(64, 33)\n",
      "step 11173, loss is 4.756321907043457\n",
      "(64, 33)\n",
      "step 11174, loss is 4.776369094848633\n",
      "(64, 33)\n",
      "step 11175, loss is 4.674159526824951\n",
      "(64, 33)\n",
      "step 11176, loss is 4.863900184631348\n",
      "(64, 33)\n",
      "step 11177, loss is 4.979248046875\n",
      "(64, 33)\n",
      "step 11178, loss is 4.917125701904297\n",
      "(64, 33)\n",
      "step 11179, loss is 4.783851623535156\n",
      "(64, 33)\n",
      "step 11180, loss is 4.935809135437012\n",
      "(64, 33)\n",
      "step 11181, loss is 4.705874919891357\n",
      "(64, 33)\n",
      "step 11182, loss is 4.961920738220215\n",
      "(64, 33)\n",
      "step 11183, loss is 5.010178565979004\n",
      "(64, 33)\n",
      "step 11184, loss is 4.827457904815674\n",
      "(64, 33)\n",
      "step 11185, loss is 5.010611057281494\n",
      "(64, 33)\n",
      "step 11186, loss is 4.868840217590332\n",
      "(64, 33)\n",
      "step 11187, loss is 4.990734100341797\n",
      "(64, 33)\n",
      "step 11188, loss is 4.808578014373779\n",
      "(64, 33)\n",
      "step 11189, loss is 4.8755717277526855\n",
      "(64, 33)\n",
      "step 11190, loss is 4.9041008949279785\n",
      "(64, 33)\n",
      "step 11191, loss is 4.660102844238281\n",
      "(64, 33)\n",
      "step 11192, loss is 4.92717981338501\n",
      "(64, 33)\n",
      "step 11193, loss is 4.94035005569458\n",
      "(64, 33)\n",
      "step 11194, loss is 4.90164041519165\n",
      "(64, 33)\n",
      "step 11195, loss is 4.816225528717041\n",
      "(64, 33)\n",
      "step 11196, loss is 4.702029705047607\n",
      "(64, 33)\n",
      "step 11197, loss is 4.741358280181885\n",
      "(64, 33)\n",
      "step 11198, loss is 4.716179370880127\n",
      "(64, 33)\n",
      "step 11199, loss is 4.807748794555664\n",
      "(64, 33)\n",
      "step 11200, loss is 4.663640022277832\n",
      "(64, 33)\n",
      "step 11201, loss is 4.824180603027344\n",
      "(64, 33)\n",
      "step 11202, loss is 4.934739589691162\n",
      "(64, 33)\n",
      "step 11203, loss is 4.9965033531188965\n",
      "(64, 33)\n",
      "step 11204, loss is 5.0180816650390625\n",
      "(64, 33)\n",
      "step 11205, loss is 4.854860782623291\n",
      "(64, 33)\n",
      "step 11206, loss is 4.751136779785156\n",
      "(64, 33)\n",
      "step 11207, loss is 4.732114791870117\n",
      "(64, 33)\n",
      "step 11208, loss is 4.812579154968262\n",
      "(64, 33)\n",
      "step 11209, loss is 4.767329216003418\n",
      "(64, 33)\n",
      "step 11210, loss is 5.06143045425415\n",
      "(64, 33)\n",
      "step 11211, loss is 4.754312038421631\n",
      "(64, 33)\n",
      "step 11212, loss is 5.016134738922119\n",
      "(64, 33)\n",
      "step 11213, loss is 4.803939342498779\n",
      "(64, 33)\n",
      "step 11214, loss is 4.750446319580078\n",
      "(64, 33)\n",
      "step 11215, loss is 4.822532653808594\n",
      "(64, 33)\n",
      "step 11216, loss is 4.790251731872559\n",
      "(64, 33)\n",
      "step 11217, loss is 4.892008304595947\n",
      "(64, 33)\n",
      "step 11218, loss is 4.754918575286865\n",
      "(64, 33)\n",
      "step 11219, loss is 4.754014492034912\n",
      "(64, 33)\n",
      "step 11220, loss is 4.709159851074219\n",
      "(64, 33)\n",
      "step 11221, loss is 4.943475723266602\n",
      "(64, 33)\n",
      "step 11222, loss is 4.9732184410095215\n",
      "(64, 33)\n",
      "step 11223, loss is 4.750511646270752\n",
      "(64, 33)\n",
      "step 11224, loss is 4.8864006996154785\n",
      "(64, 33)\n",
      "step 11225, loss is 4.817534923553467\n",
      "(64, 33)\n",
      "step 11226, loss is 4.737067699432373\n",
      "(64, 33)\n",
      "step 11227, loss is 4.873222351074219\n",
      "(64, 33)\n",
      "step 11228, loss is 4.930628299713135\n",
      "(64, 33)\n",
      "step 11229, loss is 4.985595703125\n",
      "(64, 33)\n",
      "step 11230, loss is 4.670767784118652\n",
      "(64, 33)\n",
      "step 11231, loss is 4.778327465057373\n",
      "(64, 33)\n",
      "step 11232, loss is 4.739444255828857\n",
      "(64, 33)\n",
      "step 11233, loss is 4.553157806396484\n",
      "(64, 33)\n",
      "step 11234, loss is 4.815906047821045\n",
      "(64, 33)\n",
      "step 11235, loss is 4.823764324188232\n",
      "(64, 33)\n",
      "step 11236, loss is 4.9150519371032715\n",
      "(64, 33)\n",
      "step 11237, loss is 4.794760704040527\n",
      "(64, 33)\n",
      "step 11238, loss is 4.851748466491699\n",
      "(64, 33)\n",
      "step 11239, loss is 5.11337947845459\n",
      "(64, 33)\n",
      "step 11240, loss is 5.0421037673950195\n",
      "(64, 33)\n",
      "step 11241, loss is 4.717341899871826\n",
      "(64, 33)\n",
      "step 11242, loss is 4.908656120300293\n",
      "(64, 33)\n",
      "step 11243, loss is 4.707074165344238\n",
      "(64, 33)\n",
      "step 11244, loss is 4.842784404754639\n",
      "(64, 33)\n",
      "step 11245, loss is 4.854073524475098\n",
      "(64, 33)\n",
      "step 11246, loss is 4.881204605102539\n",
      "(64, 33)\n",
      "step 11247, loss is 4.771620273590088\n",
      "(64, 33)\n",
      "step 11248, loss is 4.917119026184082\n",
      "(64, 33)\n",
      "step 11249, loss is 4.79496431350708\n",
      "(64, 33)\n",
      "step 11250, loss is 4.7683258056640625\n",
      "(64, 33)\n",
      "step 11251, loss is 4.895181655883789\n",
      "(64, 33)\n",
      "step 11252, loss is 4.788143634796143\n",
      "(64, 33)\n",
      "step 11253, loss is 4.931375026702881\n",
      "(64, 33)\n",
      "step 11254, loss is 4.923152923583984\n",
      "(64, 33)\n",
      "step 11255, loss is 4.808837890625\n",
      "(64, 33)\n",
      "step 11256, loss is 4.815308570861816\n",
      "(64, 33)\n",
      "step 11257, loss is 4.780322551727295\n",
      "(64, 33)\n",
      "step 11258, loss is 4.801586151123047\n",
      "(64, 33)\n",
      "step 11259, loss is 4.905342102050781\n",
      "(64, 33)\n",
      "step 11260, loss is 4.779844760894775\n",
      "(64, 33)\n",
      "step 11261, loss is 4.891186237335205\n",
      "(64, 33)\n",
      "step 11262, loss is 4.8242316246032715\n",
      "(64, 33)\n",
      "step 11263, loss is 4.706757068634033\n",
      "(64, 33)\n",
      "step 11264, loss is 4.586772918701172\n",
      "(64, 33)\n",
      "step 11265, loss is 4.812283992767334\n",
      "(64, 33)\n",
      "step 11266, loss is 4.840950965881348\n",
      "(64, 33)\n",
      "step 11267, loss is 4.916592597961426\n",
      "(64, 33)\n",
      "step 11268, loss is 4.808069705963135\n",
      "(64, 33)\n",
      "step 11269, loss is 4.64876127243042\n",
      "(64, 33)\n",
      "step 11270, loss is 4.810702323913574\n",
      "(64, 33)\n",
      "step 11271, loss is 4.847182273864746\n",
      "(64, 33)\n",
      "step 11272, loss is 4.7875566482543945\n",
      "(64, 33)\n",
      "step 11273, loss is 4.941295146942139\n",
      "(64, 33)\n",
      "step 11274, loss is 4.848423004150391\n",
      "(64, 33)\n",
      "step 11275, loss is 4.831214427947998\n",
      "(64, 33)\n",
      "step 11276, loss is 4.904975891113281\n",
      "(64, 33)\n",
      "step 11277, loss is 5.034262180328369\n",
      "(64, 33)\n",
      "step 11278, loss is 4.9098687171936035\n",
      "(64, 33)\n",
      "step 11279, loss is 5.0602922439575195\n",
      "(64, 33)\n",
      "step 11280, loss is 4.792116641998291\n",
      "(64, 33)\n",
      "step 11281, loss is 4.8731489181518555\n",
      "(64, 33)\n",
      "step 11282, loss is 4.807486057281494\n",
      "(64, 33)\n",
      "step 11283, loss is 4.719320774078369\n",
      "(64, 33)\n",
      "step 11284, loss is 4.846966743469238\n",
      "(64, 33)\n",
      "step 11285, loss is 4.797411918640137\n",
      "(64, 33)\n",
      "step 11286, loss is 4.9886698722839355\n",
      "(64, 33)\n",
      "step 11287, loss is 4.6873579025268555\n",
      "(64, 33)\n",
      "step 11288, loss is 4.702554702758789\n",
      "(64, 33)\n",
      "step 11289, loss is 4.880310535430908\n",
      "(64, 33)\n",
      "step 11290, loss is 4.881643772125244\n",
      "(64, 33)\n",
      "step 11291, loss is 4.747973918914795\n",
      "(64, 33)\n",
      "step 11292, loss is 4.959105968475342\n",
      "(64, 33)\n",
      "step 11293, loss is 4.823141574859619\n",
      "(64, 33)\n",
      "step 11294, loss is 4.925210952758789\n",
      "(64, 33)\n",
      "step 11295, loss is 4.984223365783691\n",
      "(64, 33)\n",
      "step 11296, loss is 4.759821891784668\n",
      "(64, 33)\n",
      "step 11297, loss is 5.0131144523620605\n",
      "(64, 33)\n",
      "step 11298, loss is 4.749384880065918\n",
      "(64, 33)\n",
      "step 11299, loss is 4.892436981201172\n",
      "(64, 33)\n",
      "step 11300, loss is 4.7924699783325195\n",
      "(64, 33)\n",
      "step 11301, loss is 4.756978988647461\n",
      "(64, 33)\n",
      "step 11302, loss is 4.886597633361816\n",
      "(64, 33)\n",
      "step 11303, loss is 4.90546178817749\n",
      "(64, 33)\n",
      "step 11304, loss is 4.871049880981445\n",
      "(64, 33)\n",
      "step 11305, loss is 4.843095302581787\n",
      "(64, 33)\n",
      "step 11306, loss is 4.756069660186768\n",
      "(64, 33)\n",
      "step 11307, loss is 4.755992412567139\n",
      "(64, 33)\n",
      "step 11308, loss is 5.04115104675293\n",
      "(64, 33)\n",
      "step 11309, loss is 4.834466934204102\n",
      "(64, 33)\n",
      "step 11310, loss is 4.922361373901367\n",
      "(64, 33)\n",
      "step 11311, loss is 4.793423175811768\n",
      "(64, 33)\n",
      "step 11312, loss is 4.92823600769043\n",
      "(64, 33)\n",
      "step 11313, loss is 4.837582111358643\n",
      "(64, 33)\n",
      "step 11314, loss is 4.9618096351623535\n",
      "(64, 33)\n",
      "step 11315, loss is 5.035107135772705\n",
      "(64, 33)\n",
      "step 11316, loss is 4.948432922363281\n",
      "(64, 33)\n",
      "step 11317, loss is 4.784428119659424\n",
      "(64, 33)\n",
      "step 11318, loss is 5.062961101531982\n",
      "(64, 33)\n",
      "step 11319, loss is 4.690337181091309\n",
      "(64, 33)\n",
      "step 11320, loss is 4.74485445022583\n",
      "(64, 33)\n",
      "step 11321, loss is 4.6405439376831055\n",
      "(64, 33)\n",
      "step 11322, loss is 5.03018856048584\n",
      "(64, 33)\n",
      "step 11323, loss is 4.891709327697754\n",
      "(64, 33)\n",
      "step 11324, loss is 4.782741546630859\n",
      "(64, 33)\n",
      "step 11325, loss is 4.9265241622924805\n",
      "(64, 33)\n",
      "step 11326, loss is 4.889106750488281\n",
      "(64, 33)\n",
      "step 11327, loss is 5.009589195251465\n",
      "(64, 33)\n",
      "step 11328, loss is 4.6923675537109375\n",
      "(64, 33)\n",
      "step 11329, loss is 4.979517459869385\n",
      "(64, 33)\n",
      "step 11330, loss is 4.909614562988281\n",
      "(64, 33)\n",
      "step 11331, loss is 5.025190353393555\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11332, loss is 4.861598014831543\n",
      "(64, 33)\n",
      "step 11333, loss is 4.682227611541748\n",
      "(64, 33)\n",
      "step 11334, loss is 4.979209899902344\n",
      "(64, 33)\n",
      "step 11335, loss is 4.909950256347656\n",
      "(64, 33)\n",
      "step 11336, loss is 4.78609561920166\n",
      "(64, 33)\n",
      "step 11337, loss is 4.839745044708252\n",
      "(64, 33)\n",
      "step 11338, loss is 4.839077472686768\n",
      "(64, 33)\n",
      "step 11339, loss is 4.653817176818848\n",
      "(64, 33)\n",
      "step 11340, loss is 5.001885414123535\n",
      "(64, 33)\n",
      "step 11341, loss is 4.784822940826416\n",
      "(64, 33)\n",
      "step 11342, loss is 4.717580318450928\n",
      "(64, 33)\n",
      "step 11343, loss is 4.863681793212891\n",
      "(64, 33)\n",
      "step 11344, loss is 4.697734832763672\n",
      "(64, 33)\n",
      "step 11345, loss is 4.7461957931518555\n",
      "(64, 33)\n",
      "step 11346, loss is 4.919103145599365\n",
      "(64, 33)\n",
      "step 11347, loss is 4.751014709472656\n",
      "(64, 33)\n",
      "step 11348, loss is 4.826229095458984\n",
      "(64, 33)\n",
      "step 11349, loss is 5.058292388916016\n",
      "(64, 33)\n",
      "step 11350, loss is 4.761574745178223\n",
      "(64, 33)\n",
      "step 11351, loss is 4.8304243087768555\n",
      "(64, 33)\n",
      "step 11352, loss is 4.787095069885254\n",
      "(64, 33)\n",
      "step 11353, loss is 4.662891864776611\n",
      "(64, 33)\n",
      "step 11354, loss is 4.855519771575928\n",
      "(64, 33)\n",
      "step 11355, loss is 4.822484016418457\n",
      "(64, 33)\n",
      "step 11356, loss is 4.96263313293457\n",
      "(64, 33)\n",
      "step 11357, loss is 4.837376117706299\n",
      "(64, 33)\n",
      "step 11358, loss is 4.67687463760376\n",
      "(64, 33)\n",
      "step 11359, loss is 4.831474304199219\n",
      "(64, 33)\n",
      "step 11360, loss is 4.673704624176025\n",
      "(64, 33)\n",
      "step 11361, loss is 4.835307598114014\n",
      "(64, 33)\n",
      "step 11362, loss is 4.858567237854004\n",
      "(64, 33)\n",
      "step 11363, loss is 4.75710916519165\n",
      "(64, 33)\n",
      "step 11364, loss is 5.042694568634033\n",
      "(64, 33)\n",
      "step 11365, loss is 4.814963340759277\n",
      "(64, 33)\n",
      "step 11366, loss is 4.789975643157959\n",
      "(64, 33)\n",
      "step 11367, loss is 4.752167224884033\n",
      "(64, 33)\n",
      "step 11368, loss is 4.652091979980469\n",
      "(64, 33)\n",
      "step 11369, loss is 4.6796040534973145\n",
      "(64, 33)\n",
      "step 11370, loss is 4.847423553466797\n",
      "(64, 33)\n",
      "step 11371, loss is 4.966479778289795\n",
      "(64, 33)\n",
      "step 11372, loss is 4.941958904266357\n",
      "(64, 33)\n",
      "step 11373, loss is 4.71738338470459\n",
      "(64, 33)\n",
      "step 11374, loss is 5.007364749908447\n",
      "(64, 33)\n",
      "step 11375, loss is 5.070130348205566\n",
      "(64, 33)\n",
      "step 11376, loss is 4.920077800750732\n",
      "(64, 33)\n",
      "step 11377, loss is 4.856579780578613\n",
      "(64, 33)\n",
      "step 11378, loss is 4.879310607910156\n",
      "(64, 33)\n",
      "step 11379, loss is 4.788446426391602\n",
      "(64, 33)\n",
      "step 11380, loss is 4.596493244171143\n",
      "(64, 33)\n",
      "step 11381, loss is 4.8202667236328125\n",
      "(64, 33)\n",
      "step 11382, loss is 4.808333873748779\n",
      "(64, 33)\n",
      "step 11383, loss is 4.847035884857178\n",
      "(64, 33)\n",
      "step 11384, loss is 4.958118438720703\n",
      "(64, 33)\n",
      "step 11385, loss is 4.898372173309326\n",
      "(64, 33)\n",
      "step 11386, loss is 4.8776535987854\n",
      "(64, 33)\n",
      "step 11387, loss is 4.724538326263428\n",
      "(64, 33)\n",
      "step 11388, loss is 4.765049457550049\n",
      "(64, 33)\n",
      "step 11389, loss is 4.702829360961914\n",
      "(64, 33)\n",
      "step 11390, loss is 4.856796741485596\n",
      "(64, 33)\n",
      "step 11391, loss is 4.824790000915527\n",
      "(64, 33)\n",
      "step 11392, loss is 4.612132549285889\n",
      "(64, 33)\n",
      "step 11393, loss is 4.896516799926758\n",
      "(64, 33)\n",
      "step 11394, loss is 4.895341873168945\n",
      "(64, 33)\n",
      "step 11395, loss is 4.787353515625\n",
      "(64, 33)\n",
      "step 11396, loss is 4.917796611785889\n",
      "(64, 33)\n",
      "step 11397, loss is 4.791207790374756\n",
      "(64, 33)\n",
      "step 11398, loss is 4.90003776550293\n",
      "(64, 33)\n",
      "step 11399, loss is 4.966525077819824\n",
      "(64, 33)\n",
      "step 11400, loss is 4.752416610717773\n",
      "(64, 33)\n",
      "step 11401, loss is 4.971925735473633\n",
      "(64, 33)\n",
      "step 11402, loss is 4.655529975891113\n",
      "(64, 33)\n",
      "step 11403, loss is 4.906295299530029\n",
      "(64, 33)\n",
      "step 11404, loss is 4.69022798538208\n",
      "(64, 33)\n",
      "step 11405, loss is 4.836448669433594\n",
      "(64, 33)\n",
      "step 11406, loss is 4.81897497177124\n",
      "(64, 33)\n",
      "step 11407, loss is 4.717128753662109\n",
      "(64, 33)\n",
      "step 11408, loss is 4.830756187438965\n",
      "(64, 33)\n",
      "step 11409, loss is 4.941493988037109\n",
      "(64, 33)\n",
      "step 11410, loss is 4.7394890785217285\n",
      "(64, 33)\n",
      "step 11411, loss is 4.701398849487305\n",
      "(64, 33)\n",
      "step 11412, loss is 4.619136333465576\n",
      "(64, 33)\n",
      "step 11413, loss is 4.8474812507629395\n",
      "(64, 33)\n",
      "step 11414, loss is 4.8623480796813965\n",
      "(64, 33)\n",
      "step 11415, loss is 4.922934055328369\n",
      "(64, 33)\n",
      "step 11416, loss is 4.7883524894714355\n",
      "(64, 33)\n",
      "step 11417, loss is 4.84517240524292\n",
      "(64, 33)\n",
      "step 11418, loss is 4.777841091156006\n",
      "(64, 33)\n",
      "step 11419, loss is 4.604041576385498\n",
      "(64, 33)\n",
      "step 11420, loss is 4.737762928009033\n",
      "(64, 33)\n",
      "step 11421, loss is 4.857570648193359\n",
      "(64, 33)\n",
      "step 11422, loss is 4.966343879699707\n",
      "(64, 33)\n",
      "step 11423, loss is 4.77736759185791\n",
      "(64, 33)\n",
      "step 11424, loss is 4.744868278503418\n",
      "(64, 33)\n",
      "step 11425, loss is 4.922616958618164\n",
      "(64, 33)\n",
      "step 11426, loss is 4.727899551391602\n",
      "(64, 33)\n",
      "step 11427, loss is 4.719668865203857\n",
      "(64, 33)\n",
      "step 11428, loss is 4.8013997077941895\n",
      "(64, 33)\n",
      "step 11429, loss is 4.771571636199951\n",
      "(64, 33)\n",
      "step 11430, loss is 4.759221076965332\n",
      "(64, 33)\n",
      "step 11431, loss is 4.949606418609619\n",
      "(64, 33)\n",
      "step 11432, loss is 4.9355268478393555\n",
      "(64, 33)\n",
      "step 11433, loss is 4.841404438018799\n",
      "(64, 33)\n",
      "step 11434, loss is 4.914639949798584\n",
      "(64, 33)\n",
      "step 11435, loss is 4.965014457702637\n",
      "(64, 33)\n",
      "step 11436, loss is 4.885667324066162\n",
      "(64, 33)\n",
      "step 11437, loss is 4.887306213378906\n",
      "(64, 33)\n",
      "step 11438, loss is 5.054288387298584\n",
      "(64, 33)\n",
      "step 11439, loss is 4.859405994415283\n",
      "(64, 33)\n",
      "step 11440, loss is 4.833720684051514\n",
      "(64, 33)\n",
      "step 11441, loss is 4.858652591705322\n",
      "(64, 33)\n",
      "step 11442, loss is 4.819651126861572\n",
      "(64, 33)\n",
      "step 11443, loss is 5.007429599761963\n",
      "(64, 33)\n",
      "step 11444, loss is 5.007438659667969\n",
      "(64, 33)\n",
      "step 11445, loss is 5.020937442779541\n",
      "(64, 33)\n",
      "step 11446, loss is 4.9571309089660645\n",
      "(64, 33)\n",
      "step 11447, loss is 4.939721584320068\n",
      "(64, 33)\n",
      "step 11448, loss is 4.882380485534668\n",
      "(64, 33)\n",
      "step 11449, loss is 4.933280944824219\n",
      "(64, 33)\n",
      "step 11450, loss is 4.8178205490112305\n",
      "(64, 33)\n",
      "step 11451, loss is 4.92661714553833\n",
      "(64, 33)\n",
      "step 11452, loss is 4.864558696746826\n",
      "(64, 33)\n",
      "step 11453, loss is 4.886320114135742\n",
      "(64, 33)\n",
      "step 11454, loss is 4.8993120193481445\n",
      "(64, 33)\n",
      "step 11455, loss is 4.83337926864624\n",
      "(64, 33)\n",
      "step 11456, loss is 4.6320390701293945\n",
      "(64, 33)\n",
      "step 11457, loss is 4.867166042327881\n",
      "(64, 33)\n",
      "step 11458, loss is 4.793026447296143\n",
      "(64, 33)\n",
      "step 11459, loss is 4.749871253967285\n",
      "(64, 33)\n",
      "step 11460, loss is 4.88538932800293\n",
      "(64, 33)\n",
      "step 11461, loss is 4.759457588195801\n",
      "(64, 33)\n",
      "step 11462, loss is 4.925537109375\n",
      "(64, 33)\n",
      "step 11463, loss is 4.68326473236084\n",
      "(64, 33)\n",
      "step 11464, loss is 4.737753868103027\n",
      "(64, 33)\n",
      "step 11465, loss is 4.665345668792725\n",
      "(64, 33)\n",
      "step 11466, loss is 4.904367446899414\n",
      "(64, 33)\n",
      "step 11467, loss is 4.669956684112549\n",
      "(64, 33)\n",
      "step 11468, loss is 4.782584190368652\n",
      "(64, 33)\n",
      "step 11469, loss is 4.987323760986328\n",
      "(64, 33)\n",
      "step 11470, loss is 4.845841407775879\n",
      "(64, 33)\n",
      "step 11471, loss is 4.886775970458984\n",
      "(64, 33)\n",
      "step 11472, loss is 4.712808132171631\n",
      "(64, 33)\n",
      "step 11473, loss is 4.973886013031006\n",
      "(64, 33)\n",
      "step 11474, loss is 4.846277713775635\n",
      "(64, 33)\n",
      "step 11475, loss is 4.876615047454834\n",
      "(64, 33)\n",
      "step 11476, loss is 4.861265659332275\n",
      "(64, 33)\n",
      "step 11477, loss is 4.9377875328063965\n",
      "(64, 33)\n",
      "step 11478, loss is 4.8880085945129395\n",
      "(64, 33)\n",
      "step 11479, loss is 4.824667453765869\n",
      "(64, 33)\n",
      "step 11480, loss is 4.977407932281494\n",
      "(64, 33)\n",
      "step 11481, loss is 4.682275295257568\n",
      "(64, 33)\n",
      "step 11482, loss is 4.841579437255859\n",
      "(64, 33)\n",
      "step 11483, loss is 4.7893171310424805\n",
      "(64, 33)\n",
      "step 11484, loss is 4.772520542144775\n",
      "(64, 33)\n",
      "step 11485, loss is 4.787531852722168\n",
      "(64, 33)\n",
      "step 11486, loss is 4.85821533203125\n",
      "(64, 33)\n",
      "step 11487, loss is 4.946431636810303\n",
      "(64, 33)\n",
      "step 11488, loss is 4.917250633239746\n",
      "(64, 33)\n",
      "step 11489, loss is 4.974281311035156\n",
      "(64, 33)\n",
      "step 11490, loss is 4.834742069244385\n",
      "(64, 33)\n",
      "step 11491, loss is 4.844921588897705\n",
      "(64, 33)\n",
      "step 11492, loss is 4.982938289642334\n",
      "(64, 33)\n",
      "step 11493, loss is 4.950346946716309\n",
      "(64, 33)\n",
      "step 11494, loss is 4.64582633972168\n",
      "(64, 33)\n",
      "step 11495, loss is 4.879059314727783\n",
      "(64, 33)\n",
      "step 11496, loss is 4.776100158691406\n",
      "(64, 33)\n",
      "step 11497, loss is 4.956830978393555\n",
      "(64, 33)\n",
      "step 11498, loss is 4.6241455078125\n",
      "(64, 33)\n",
      "step 11499, loss is 4.961618900299072\n",
      "(64, 33)\n",
      "step 11500, loss is 4.732071399688721\n",
      "(64, 33)\n",
      "step 11501, loss is 4.965400218963623\n",
      "(64, 33)\n",
      "step 11502, loss is 4.8939595222473145\n",
      "(64, 33)\n",
      "step 11503, loss is 4.684104919433594\n",
      "(64, 33)\n",
      "step 11504, loss is 4.702001094818115\n",
      "(64, 33)\n",
      "step 11505, loss is 4.851637363433838\n",
      "(64, 33)\n",
      "step 11506, loss is 4.906044960021973\n",
      "(64, 33)\n",
      "step 11507, loss is 4.7905707359313965\n",
      "(64, 33)\n",
      "step 11508, loss is 4.861635208129883\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11509, loss is 4.778099536895752\n",
      "(64, 33)\n",
      "step 11510, loss is 4.571143627166748\n",
      "(64, 33)\n",
      "step 11511, loss is 4.909382343292236\n",
      "(64, 33)\n",
      "step 11512, loss is 4.947220325469971\n",
      "(64, 33)\n",
      "step 11513, loss is 4.688967227935791\n",
      "(64, 33)\n",
      "step 11514, loss is 4.8200531005859375\n",
      "(64, 33)\n",
      "step 11515, loss is 4.845558166503906\n",
      "(64, 33)\n",
      "step 11516, loss is 4.8723320960998535\n",
      "(64, 33)\n",
      "step 11517, loss is 4.7622175216674805\n",
      "(64, 33)\n",
      "step 11518, loss is 4.993915557861328\n",
      "(64, 33)\n",
      "step 11519, loss is 4.663262367248535\n",
      "(64, 33)\n",
      "step 11520, loss is 4.907032012939453\n",
      "(64, 33)\n",
      "step 11521, loss is 4.783371925354004\n",
      "(64, 33)\n",
      "step 11522, loss is 4.8927998542785645\n",
      "(64, 33)\n",
      "step 11523, loss is 4.823549747467041\n",
      "(64, 33)\n",
      "step 11524, loss is 4.869783401489258\n",
      "(64, 33)\n",
      "step 11525, loss is 4.755924224853516\n",
      "(64, 33)\n",
      "step 11526, loss is 4.784519195556641\n",
      "(64, 33)\n",
      "step 11527, loss is 4.988369464874268\n",
      "(64, 33)\n",
      "step 11528, loss is 4.978973865509033\n",
      "(64, 33)\n",
      "step 11529, loss is 4.773997783660889\n",
      "(64, 33)\n",
      "step 11530, loss is 4.9936113357543945\n",
      "(64, 33)\n",
      "step 11531, loss is 4.751733779907227\n",
      "(64, 33)\n",
      "step 11532, loss is 4.805551052093506\n",
      "(64, 33)\n",
      "step 11533, loss is 4.908863067626953\n",
      "(64, 33)\n",
      "step 11534, loss is 4.8615288734436035\n",
      "(64, 33)\n",
      "step 11535, loss is 4.934309482574463\n",
      "(64, 33)\n",
      "step 11536, loss is 4.893063545227051\n",
      "(64, 33)\n",
      "step 11537, loss is 4.888517379760742\n",
      "(64, 33)\n",
      "step 11538, loss is 4.8374104499816895\n",
      "(64, 33)\n",
      "step 11539, loss is 4.877281188964844\n",
      "(64, 33)\n",
      "step 11540, loss is 4.6925787925720215\n",
      "(64, 33)\n",
      "step 11541, loss is 4.957961559295654\n",
      "(64, 33)\n",
      "step 11542, loss is 4.917459011077881\n",
      "(64, 33)\n",
      "step 11543, loss is 4.839352130889893\n",
      "(64, 33)\n",
      "step 11544, loss is 4.768089294433594\n",
      "(64, 33)\n",
      "step 11545, loss is 4.798164367675781\n",
      "(64, 33)\n",
      "step 11546, loss is 4.889675617218018\n",
      "(64, 33)\n",
      "step 11547, loss is 4.740091800689697\n",
      "(64, 33)\n",
      "step 11548, loss is 4.9332451820373535\n",
      "(64, 33)\n",
      "step 11549, loss is 4.762724876403809\n",
      "(64, 33)\n",
      "step 11550, loss is 4.975632667541504\n",
      "(64, 33)\n",
      "step 11551, loss is 4.929515361785889\n",
      "(64, 33)\n",
      "step 11552, loss is 4.750429153442383\n",
      "(64, 33)\n",
      "step 11553, loss is 4.8358564376831055\n",
      "(64, 33)\n",
      "step 11554, loss is 4.814345359802246\n",
      "(64, 33)\n",
      "step 11555, loss is 4.82601261138916\n",
      "(64, 33)\n",
      "step 11556, loss is 4.580417633056641\n",
      "(64, 33)\n",
      "step 11557, loss is 4.728538990020752\n",
      "(64, 33)\n",
      "step 11558, loss is 4.910987854003906\n",
      "(64, 33)\n",
      "step 11559, loss is 4.8302741050720215\n",
      "(64, 33)\n",
      "step 11560, loss is 4.734316349029541\n",
      "(64, 33)\n",
      "step 11561, loss is 4.743616104125977\n",
      "(64, 33)\n",
      "step 11562, loss is 4.742425441741943\n",
      "(64, 33)\n",
      "step 11563, loss is 4.7755537033081055\n",
      "(64, 33)\n",
      "step 11564, loss is 4.92307186126709\n",
      "(64, 33)\n",
      "step 11565, loss is 4.962787628173828\n",
      "(64, 33)\n",
      "step 11566, loss is 4.819706916809082\n",
      "(64, 33)\n",
      "step 11567, loss is 4.843086242675781\n",
      "(64, 33)\n",
      "step 11568, loss is 4.658798694610596\n",
      "(64, 33)\n",
      "step 11569, loss is 4.786793231964111\n",
      "(64, 33)\n",
      "step 11570, loss is 4.837947845458984\n",
      "(64, 33)\n",
      "step 11571, loss is 4.846163749694824\n",
      "(64, 33)\n",
      "step 11572, loss is 4.777103424072266\n",
      "(64, 33)\n",
      "step 11573, loss is 4.934981822967529\n",
      "(64, 33)\n",
      "step 11574, loss is 4.90069055557251\n",
      "(64, 33)\n",
      "step 11575, loss is 4.866521835327148\n",
      "(64, 33)\n",
      "step 11576, loss is 4.904709339141846\n",
      "(64, 33)\n",
      "step 11577, loss is 4.650201797485352\n",
      "(64, 33)\n",
      "step 11578, loss is 4.777751445770264\n",
      "(64, 33)\n",
      "step 11579, loss is 4.6490254402160645\n",
      "(64, 33)\n",
      "step 11580, loss is 4.9442644119262695\n",
      "(64, 33)\n",
      "step 11581, loss is 4.927983283996582\n",
      "(64, 33)\n",
      "step 11582, loss is 4.888640403747559\n",
      "(64, 33)\n",
      "step 11583, loss is 4.779110431671143\n",
      "(64, 33)\n",
      "step 11584, loss is 4.847314834594727\n",
      "(64, 33)\n",
      "step 11585, loss is 4.86351203918457\n",
      "(64, 33)\n",
      "step 11586, loss is 4.828470706939697\n",
      "(64, 33)\n",
      "step 11587, loss is 4.807849884033203\n",
      "(64, 33)\n",
      "step 11588, loss is 4.690585613250732\n",
      "(64, 33)\n",
      "step 11589, loss is 4.802910804748535\n",
      "(64, 33)\n",
      "step 11590, loss is 4.768814563751221\n",
      "(64, 33)\n",
      "step 11591, loss is 4.819406032562256\n",
      "(64, 33)\n",
      "step 11592, loss is 4.793842315673828\n",
      "(64, 33)\n",
      "step 11593, loss is 4.669180393218994\n",
      "(64, 33)\n",
      "step 11594, loss is 4.807931900024414\n",
      "(64, 33)\n",
      "step 11595, loss is 4.742435455322266\n",
      "(64, 33)\n",
      "step 11596, loss is 4.928936958312988\n",
      "(64, 33)\n",
      "step 11597, loss is 4.86708402633667\n",
      "(64, 33)\n",
      "step 11598, loss is 4.850284576416016\n",
      "(64, 33)\n",
      "step 11599, loss is 4.750118732452393\n",
      "(64, 33)\n",
      "step 11600, loss is 4.774421691894531\n",
      "(64, 33)\n",
      "step 11601, loss is 4.708272457122803\n",
      "(64, 33)\n",
      "step 11602, loss is 4.789730072021484\n",
      "(64, 33)\n",
      "step 11603, loss is 4.872914791107178\n",
      "(64, 33)\n",
      "step 11604, loss is 4.701356887817383\n",
      "(64, 33)\n",
      "step 11605, loss is 4.797062873840332\n",
      "(64, 33)\n",
      "step 11606, loss is 4.783836841583252\n",
      "(64, 33)\n",
      "step 11607, loss is 4.782398223876953\n",
      "(64, 33)\n",
      "step 11608, loss is 4.721568584442139\n",
      "(64, 33)\n",
      "step 11609, loss is 4.668263912200928\n",
      "(64, 33)\n",
      "step 11610, loss is 5.037121295928955\n",
      "(64, 33)\n",
      "step 11611, loss is 4.823734760284424\n",
      "(64, 33)\n",
      "step 11612, loss is 4.708939552307129\n",
      "(64, 33)\n",
      "step 11613, loss is 4.684109687805176\n",
      "(64, 33)\n",
      "step 11614, loss is 4.730606555938721\n",
      "(64, 33)\n",
      "step 11615, loss is 4.743560791015625\n",
      "(64, 33)\n",
      "step 11616, loss is 4.77105712890625\n",
      "(64, 33)\n",
      "step 11617, loss is 4.825986862182617\n",
      "(64, 33)\n",
      "step 11618, loss is 4.858696937561035\n",
      "(64, 33)\n",
      "step 11619, loss is 4.8525872230529785\n",
      "(64, 33)\n",
      "step 11620, loss is 4.7646708488464355\n",
      "(64, 33)\n",
      "step 11621, loss is 4.7278971672058105\n",
      "(64, 33)\n",
      "step 11622, loss is 4.972504138946533\n",
      "(64, 33)\n",
      "step 11623, loss is 4.796659469604492\n",
      "(64, 33)\n",
      "step 11624, loss is 4.908176898956299\n",
      "(64, 33)\n",
      "step 11625, loss is 4.782697677612305\n",
      "(64, 33)\n",
      "step 11626, loss is 4.81380033493042\n",
      "(64, 33)\n",
      "step 11627, loss is 4.706590175628662\n",
      "(64, 33)\n",
      "step 11628, loss is 4.759252548217773\n",
      "(64, 33)\n",
      "step 11629, loss is 4.822083473205566\n",
      "(64, 33)\n",
      "step 11630, loss is 4.789141654968262\n",
      "(64, 33)\n",
      "step 11631, loss is 4.946884632110596\n",
      "(64, 33)\n",
      "step 11632, loss is 4.73321533203125\n",
      "(64, 33)\n",
      "step 11633, loss is 4.703835964202881\n",
      "(64, 33)\n",
      "step 11634, loss is 4.890313625335693\n",
      "(64, 33)\n",
      "step 11635, loss is 4.841828346252441\n",
      "(64, 33)\n",
      "step 11636, loss is 4.6920037269592285\n",
      "(64, 33)\n",
      "step 11637, loss is 4.790524959564209\n",
      "(64, 33)\n",
      "step 11638, loss is 4.734517574310303\n",
      "(64, 33)\n",
      "step 11639, loss is 4.783261299133301\n",
      "(64, 33)\n",
      "step 11640, loss is 4.858702182769775\n",
      "(64, 33)\n",
      "step 11641, loss is 4.694881439208984\n",
      "(64, 33)\n",
      "step 11642, loss is 4.743047714233398\n",
      "(64, 33)\n",
      "step 11643, loss is 4.771623611450195\n",
      "(64, 33)\n",
      "step 11644, loss is 4.812808513641357\n",
      "(64, 33)\n",
      "step 11645, loss is 5.018852710723877\n",
      "(64, 33)\n",
      "step 11646, loss is 4.782602310180664\n",
      "(64, 33)\n",
      "step 11647, loss is 4.900899887084961\n",
      "(64, 33)\n",
      "step 11648, loss is 4.752590179443359\n",
      "(64, 33)\n",
      "step 11649, loss is 5.009398937225342\n",
      "(64, 33)\n",
      "step 11650, loss is 4.877906322479248\n",
      "(64, 33)\n",
      "step 11651, loss is 4.681547164916992\n",
      "(64, 33)\n",
      "step 11652, loss is 4.8922505378723145\n",
      "(64, 33)\n",
      "step 11653, loss is 4.813059329986572\n",
      "(64, 33)\n",
      "step 11654, loss is 4.91860818862915\n",
      "(64, 33)\n",
      "step 11655, loss is 4.823686122894287\n",
      "(64, 33)\n",
      "step 11656, loss is 4.541539192199707\n",
      "(64, 33)\n",
      "step 11657, loss is 4.736903190612793\n",
      "(64, 33)\n",
      "step 11658, loss is 4.754243850708008\n",
      "(64, 33)\n",
      "step 11659, loss is 4.814937591552734\n",
      "(64, 33)\n",
      "step 11660, loss is 4.661814212799072\n",
      "(64, 33)\n",
      "step 11661, loss is 4.8644256591796875\n",
      "(64, 33)\n",
      "step 11662, loss is 4.550563812255859\n",
      "(64, 33)\n",
      "step 11663, loss is 4.825324535369873\n",
      "(64, 33)\n",
      "step 11664, loss is 4.787754058837891\n",
      "(64, 33)\n",
      "step 11665, loss is 4.749567031860352\n",
      "(64, 33)\n",
      "step 11666, loss is 4.779340744018555\n",
      "(64, 33)\n",
      "step 11667, loss is 4.7826642990112305\n",
      "(64, 33)\n",
      "step 11668, loss is 4.9351806640625\n",
      "(64, 33)\n",
      "step 11669, loss is 4.812378883361816\n",
      "(64, 33)\n",
      "step 11670, loss is 4.674901008605957\n",
      "(64, 33)\n",
      "step 11671, loss is 4.780518054962158\n",
      "(64, 33)\n",
      "step 11672, loss is 4.94024658203125\n",
      "(64, 33)\n",
      "step 11673, loss is 4.805962562561035\n",
      "(64, 33)\n",
      "step 11674, loss is 4.8782877922058105\n",
      "(64, 33)\n",
      "step 11675, loss is 4.784955978393555\n",
      "(64, 33)\n",
      "step 11676, loss is 4.842123031616211\n",
      "(64, 33)\n",
      "step 11677, loss is 4.831955432891846\n",
      "(64, 33)\n",
      "step 11678, loss is 4.772245407104492\n",
      "(64, 33)\n",
      "step 11679, loss is 4.872618675231934\n",
      "(64, 33)\n",
      "step 11680, loss is 4.901020050048828\n",
      "(64, 33)\n",
      "step 11681, loss is 5.0134148597717285\n",
      "(64, 33)\n",
      "step 11682, loss is 4.866400718688965\n",
      "(64, 33)\n",
      "step 11683, loss is 4.741003036499023\n",
      "(64, 33)\n",
      "step 11684, loss is 4.8234782218933105\n",
      "(64, 33)\n",
      "step 11685, loss is 4.831514835357666\n",
      "(64, 33)\n",
      "step 11686, loss is 4.786434173583984\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11687, loss is 4.866245269775391\n",
      "(64, 33)\n",
      "step 11688, loss is 4.741726398468018\n",
      "(64, 33)\n",
      "step 11689, loss is 4.713981628417969\n",
      "(64, 33)\n",
      "step 11690, loss is 4.87162971496582\n",
      "(64, 33)\n",
      "step 11691, loss is 4.943159580230713\n",
      "(64, 33)\n",
      "step 11692, loss is 4.842819690704346\n",
      "(64, 33)\n",
      "step 11693, loss is 4.92087984085083\n",
      "(64, 33)\n",
      "step 11694, loss is 4.8465447425842285\n",
      "(64, 33)\n",
      "step 11695, loss is 4.712740898132324\n",
      "(64, 33)\n",
      "step 11696, loss is 4.883233547210693\n",
      "(64, 33)\n",
      "step 11697, loss is 4.776068210601807\n",
      "(64, 33)\n",
      "step 11698, loss is 4.974770545959473\n",
      "(64, 33)\n",
      "step 11699, loss is 4.701396942138672\n",
      "(64, 33)\n",
      "step 11700, loss is 4.860406875610352\n",
      "(64, 33)\n",
      "step 11701, loss is 4.820919036865234\n",
      "(64, 33)\n",
      "step 11702, loss is 5.005404472351074\n",
      "(64, 33)\n",
      "step 11703, loss is 4.8973612785339355\n",
      "(64, 33)\n",
      "step 11704, loss is 4.631680488586426\n",
      "(64, 33)\n",
      "step 11705, loss is 4.865225315093994\n",
      "(64, 33)\n",
      "step 11706, loss is 4.779171943664551\n",
      "(64, 33)\n",
      "step 11707, loss is 4.927496433258057\n",
      "(64, 33)\n",
      "step 11708, loss is 4.750679969787598\n",
      "(64, 33)\n",
      "step 11709, loss is 4.953503608703613\n",
      "(64, 33)\n",
      "step 11710, loss is 4.870588779449463\n",
      "(64, 33)\n",
      "step 11711, loss is 4.937467098236084\n",
      "(64, 33)\n",
      "step 11712, loss is 4.849362373352051\n",
      "(64, 33)\n",
      "step 11713, loss is 4.818836688995361\n",
      "(64, 33)\n",
      "step 11714, loss is 4.851076126098633\n",
      "(64, 33)\n",
      "step 11715, loss is 4.91250467300415\n",
      "(64, 33)\n",
      "step 11716, loss is 4.957578659057617\n",
      "(64, 33)\n",
      "step 11717, loss is 4.799078464508057\n",
      "(64, 33)\n",
      "step 11718, loss is 4.681816101074219\n",
      "(64, 33)\n",
      "step 11719, loss is 4.858458995819092\n",
      "(64, 33)\n",
      "step 11720, loss is 4.86940336227417\n",
      "(64, 33)\n",
      "step 11721, loss is 4.946050643920898\n",
      "(64, 33)\n",
      "step 11722, loss is 4.728141784667969\n",
      "(64, 33)\n",
      "step 11723, loss is 4.680764198303223\n",
      "(64, 33)\n",
      "step 11724, loss is 4.899709701538086\n",
      "(64, 33)\n",
      "step 11725, loss is 4.735686302185059\n",
      "(64, 33)\n",
      "step 11726, loss is 4.97076940536499\n",
      "(64, 33)\n",
      "step 11727, loss is 4.8671698570251465\n",
      "(64, 33)\n",
      "step 11728, loss is 4.81145715713501\n",
      "(64, 33)\n",
      "step 11729, loss is 4.774138450622559\n",
      "(64, 33)\n",
      "step 11730, loss is 4.9256792068481445\n",
      "(64, 33)\n",
      "step 11731, loss is 4.660069942474365\n",
      "(64, 33)\n",
      "step 11732, loss is 4.699472904205322\n",
      "(64, 33)\n",
      "step 11733, loss is 4.904951572418213\n",
      "(64, 33)\n",
      "step 11734, loss is 4.690258979797363\n",
      "(64, 33)\n",
      "step 11735, loss is 4.887632846832275\n",
      "(64, 33)\n",
      "step 11736, loss is 4.978987693786621\n",
      "(64, 33)\n",
      "step 11737, loss is 4.7640380859375\n",
      "(64, 33)\n",
      "step 11738, loss is 4.744699001312256\n",
      "(64, 33)\n",
      "step 11739, loss is 4.955922603607178\n",
      "(64, 33)\n",
      "step 11740, loss is 4.848177909851074\n",
      "(64, 33)\n",
      "step 11741, loss is 5.000163555145264\n",
      "(64, 33)\n",
      "step 11742, loss is 4.73320198059082\n",
      "(64, 33)\n",
      "step 11743, loss is 4.833492279052734\n",
      "(64, 33)\n",
      "step 11744, loss is 4.796443939208984\n",
      "(64, 33)\n",
      "step 11745, loss is 4.735584735870361\n",
      "(64, 33)\n",
      "step 11746, loss is 4.883631706237793\n",
      "(64, 33)\n",
      "step 11747, loss is 4.847007751464844\n",
      "(64, 33)\n",
      "step 11748, loss is 4.901325702667236\n",
      "(64, 33)\n",
      "step 11749, loss is 4.938353538513184\n",
      "(64, 33)\n",
      "step 11750, loss is 4.7085442543029785\n",
      "(64, 33)\n",
      "step 11751, loss is 4.848991870880127\n",
      "(64, 33)\n",
      "step 11752, loss is 4.833680152893066\n",
      "(64, 33)\n",
      "step 11753, loss is 4.9654693603515625\n",
      "(64, 33)\n",
      "step 11754, loss is 4.901522636413574\n",
      "(64, 33)\n",
      "step 11755, loss is 4.850598335266113\n",
      "(64, 33)\n",
      "step 11756, loss is 4.850353240966797\n",
      "(64, 33)\n",
      "step 11757, loss is 4.879542350769043\n",
      "(64, 33)\n",
      "step 11758, loss is 4.878542900085449\n",
      "(64, 33)\n",
      "step 11759, loss is 4.749388694763184\n",
      "(64, 33)\n",
      "step 11760, loss is 4.8815388679504395\n",
      "(64, 33)\n",
      "step 11761, loss is 4.84273624420166\n",
      "(64, 33)\n",
      "step 11762, loss is 4.847388744354248\n",
      "(64, 33)\n",
      "step 11763, loss is 4.913800239562988\n",
      "(64, 33)\n",
      "step 11764, loss is 4.889552116394043\n",
      "(64, 33)\n",
      "step 11765, loss is 4.711033344268799\n",
      "(64, 33)\n",
      "step 11766, loss is 5.023687362670898\n",
      "(64, 33)\n",
      "step 11767, loss is 5.022187232971191\n",
      "(64, 33)\n",
      "step 11768, loss is 4.688586711883545\n",
      "(64, 33)\n",
      "step 11769, loss is 4.813604831695557\n",
      "(64, 33)\n",
      "step 11770, loss is 4.976633548736572\n",
      "(64, 33)\n",
      "step 11771, loss is 4.711437702178955\n",
      "(64, 33)\n",
      "step 11772, loss is 4.950607776641846\n",
      "(64, 33)\n",
      "step 11773, loss is 4.97792387008667\n",
      "(64, 33)\n",
      "step 11774, loss is 4.870185852050781\n",
      "(64, 33)\n",
      "step 11775, loss is 4.949060440063477\n",
      "(64, 33)\n",
      "step 11776, loss is 4.780570983886719\n",
      "(64, 33)\n",
      "step 11777, loss is 4.7477240562438965\n",
      "(64, 33)\n",
      "step 11778, loss is 4.627748489379883\n",
      "(64, 33)\n",
      "step 11779, loss is 4.660287857055664\n",
      "(64, 33)\n",
      "step 11780, loss is 4.977434158325195\n",
      "(64, 33)\n",
      "step 11781, loss is 4.8188700675964355\n",
      "(64, 33)\n",
      "step 11782, loss is 4.7794599533081055\n",
      "(64, 33)\n",
      "step 11783, loss is 4.8145527839660645\n",
      "(64, 33)\n",
      "step 11784, loss is 4.984683036804199\n",
      "(64, 33)\n",
      "step 11785, loss is 4.980457305908203\n",
      "(64, 33)\n",
      "step 11786, loss is 4.913142681121826\n",
      "(64, 33)\n",
      "step 11787, loss is 4.817048072814941\n",
      "(64, 33)\n",
      "step 11788, loss is 4.86553955078125\n",
      "(64, 33)\n",
      "step 11789, loss is 4.847800254821777\n",
      "(64, 33)\n",
      "step 11790, loss is 4.877970218658447\n",
      "(64, 33)\n",
      "step 11791, loss is 4.834425449371338\n",
      "(64, 33)\n",
      "step 11792, loss is 4.876704692840576\n",
      "(64, 33)\n",
      "step 11793, loss is 4.62548303604126\n",
      "(64, 33)\n",
      "step 11794, loss is 4.7732253074646\n",
      "(64, 33)\n",
      "step 11795, loss is 4.808290004730225\n",
      "(64, 33)\n",
      "step 11796, loss is 4.892593860626221\n",
      "(64, 33)\n",
      "step 11797, loss is 4.662519931793213\n",
      "(64, 33)\n",
      "step 11798, loss is 4.922369003295898\n",
      "(64, 33)\n",
      "step 11799, loss is 4.7810869216918945\n",
      "(64, 33)\n",
      "step 11800, loss is 4.926486968994141\n",
      "(64, 33)\n",
      "step 11801, loss is 4.888240337371826\n",
      "(64, 33)\n",
      "step 11802, loss is 4.698678493499756\n",
      "(64, 33)\n",
      "step 11803, loss is 4.680532932281494\n",
      "(64, 33)\n",
      "step 11804, loss is 4.805889129638672\n",
      "(64, 33)\n",
      "step 11805, loss is 4.696351528167725\n",
      "(64, 33)\n",
      "step 11806, loss is 4.8756232261657715\n",
      "(64, 33)\n",
      "step 11807, loss is 4.788540840148926\n",
      "(64, 33)\n",
      "step 11808, loss is 4.782485485076904\n",
      "(64, 33)\n",
      "step 11809, loss is 4.911910533905029\n",
      "(64, 33)\n",
      "step 11810, loss is 4.941193103790283\n",
      "(64, 33)\n",
      "step 11811, loss is 4.993337631225586\n",
      "(64, 33)\n",
      "step 11812, loss is 4.863640308380127\n",
      "(64, 33)\n",
      "step 11813, loss is 5.058637619018555\n",
      "(64, 33)\n",
      "step 11814, loss is 4.843623161315918\n",
      "(64, 33)\n",
      "step 11815, loss is 4.9384331703186035\n",
      "(64, 33)\n",
      "step 11816, loss is 5.080018043518066\n",
      "(64, 33)\n",
      "step 11817, loss is 4.932868957519531\n",
      "(64, 33)\n",
      "step 11818, loss is 4.73564338684082\n",
      "(64, 33)\n",
      "step 11819, loss is 4.733908653259277\n",
      "(64, 33)\n",
      "step 11820, loss is 4.616424560546875\n",
      "(64, 33)\n",
      "step 11821, loss is 4.872087001800537\n",
      "(64, 33)\n",
      "step 11822, loss is 4.758973121643066\n",
      "(64, 33)\n",
      "step 11823, loss is 5.018324851989746\n",
      "(64, 33)\n",
      "step 11824, loss is 4.833146572113037\n",
      "(64, 33)\n",
      "step 11825, loss is 4.51286506652832\n",
      "(64, 33)\n",
      "step 11826, loss is 4.785848140716553\n",
      "(64, 33)\n",
      "step 11827, loss is 4.5854716300964355\n",
      "(64, 33)\n",
      "step 11828, loss is 4.742291450500488\n",
      "(64, 33)\n",
      "step 11829, loss is 4.70660924911499\n",
      "(64, 33)\n",
      "step 11830, loss is 4.852697372436523\n",
      "(64, 33)\n",
      "step 11831, loss is 4.8560614585876465\n",
      "(64, 33)\n",
      "step 11832, loss is 4.668232440948486\n",
      "(64, 33)\n",
      "step 11833, loss is 4.967646598815918\n",
      "(64, 33)\n",
      "step 11834, loss is 4.893343925476074\n",
      "(64, 33)\n",
      "step 11835, loss is 4.73764181137085\n",
      "(64, 33)\n",
      "step 11836, loss is 4.852250576019287\n",
      "(64, 33)\n",
      "step 11837, loss is 4.8899641036987305\n",
      "(64, 33)\n",
      "step 11838, loss is 4.934942722320557\n",
      "(64, 33)\n",
      "step 11839, loss is 4.811018466949463\n",
      "(64, 33)\n",
      "step 11840, loss is 4.773915767669678\n",
      "(64, 33)\n",
      "step 11841, loss is 4.971935749053955\n",
      "(64, 33)\n",
      "step 11842, loss is 4.810186386108398\n",
      "(64, 33)\n",
      "step 11843, loss is 4.8767781257629395\n",
      "(64, 33)\n",
      "step 11844, loss is 4.8828558921813965\n",
      "(64, 33)\n",
      "step 11845, loss is 4.784052848815918\n",
      "(64, 33)\n",
      "step 11846, loss is 4.956023216247559\n",
      "(64, 33)\n",
      "step 11847, loss is 5.066060543060303\n",
      "(64, 33)\n",
      "step 11848, loss is 4.755008697509766\n",
      "(64, 33)\n",
      "step 11849, loss is 4.713953018188477\n",
      "(64, 33)\n",
      "step 11850, loss is 4.834268569946289\n",
      "(64, 33)\n",
      "step 11851, loss is 4.763875484466553\n",
      "(64, 33)\n",
      "step 11852, loss is 4.704065322875977\n",
      "(64, 33)\n",
      "step 11853, loss is 4.660844802856445\n",
      "(64, 33)\n",
      "step 11854, loss is 4.597442150115967\n",
      "(64, 33)\n",
      "step 11855, loss is 4.678861618041992\n",
      "(64, 33)\n",
      "step 11856, loss is 4.814354419708252\n",
      "(64, 33)\n",
      "step 11857, loss is 4.645583629608154\n",
      "(64, 33)\n",
      "step 11858, loss is 4.748754501342773\n",
      "(64, 33)\n",
      "step 11859, loss is 4.708587646484375\n",
      "(64, 33)\n",
      "step 11860, loss is 4.8447370529174805\n",
      "(64, 33)\n",
      "step 11861, loss is 4.803666114807129\n",
      "(64, 33)\n",
      "step 11862, loss is 4.716753005981445\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11863, loss is 4.950443267822266\n",
      "(64, 33)\n",
      "step 11864, loss is 4.831568717956543\n",
      "(64, 33)\n",
      "step 11865, loss is 4.993119716644287\n",
      "(64, 33)\n",
      "step 11866, loss is 4.878821849822998\n",
      "(64, 33)\n",
      "step 11867, loss is 4.908134460449219\n",
      "(64, 33)\n",
      "step 11868, loss is 4.835385799407959\n",
      "(64, 33)\n",
      "step 11869, loss is 4.870483875274658\n",
      "(64, 33)\n",
      "step 11870, loss is 4.801420211791992\n",
      "(64, 33)\n",
      "step 11871, loss is 4.756063938140869\n",
      "(64, 33)\n",
      "step 11872, loss is 4.7396321296691895\n",
      "(64, 33)\n",
      "step 11873, loss is 4.627718448638916\n",
      "(64, 33)\n",
      "step 11874, loss is 4.529529094696045\n",
      "(64, 33)\n",
      "step 11875, loss is 4.846170902252197\n",
      "(64, 33)\n",
      "step 11876, loss is 4.692923069000244\n",
      "(64, 33)\n",
      "step 11877, loss is 4.843745231628418\n",
      "(64, 33)\n",
      "step 11878, loss is 4.900508880615234\n",
      "(64, 33)\n",
      "step 11879, loss is 4.801488876342773\n",
      "(64, 33)\n",
      "step 11880, loss is 4.585738182067871\n",
      "(64, 33)\n",
      "step 11881, loss is 4.745560646057129\n",
      "(64, 33)\n",
      "step 11882, loss is 4.864197254180908\n",
      "(64, 33)\n",
      "step 11883, loss is 4.698491096496582\n",
      "(64, 33)\n",
      "step 11884, loss is 4.855335712432861\n",
      "(64, 33)\n",
      "step 11885, loss is 4.696171760559082\n",
      "(64, 33)\n",
      "step 11886, loss is 4.8707756996154785\n",
      "(64, 33)\n",
      "step 11887, loss is 4.9488091468811035\n",
      "(64, 33)\n",
      "step 11888, loss is 4.872337818145752\n",
      "(64, 33)\n",
      "step 11889, loss is 4.883323669433594\n",
      "(64, 33)\n",
      "step 11890, loss is 4.754415035247803\n",
      "(64, 33)\n",
      "step 11891, loss is 4.689352512359619\n",
      "(64, 33)\n",
      "step 11892, loss is 4.8576860427856445\n",
      "(64, 33)\n",
      "step 11893, loss is 4.669875621795654\n",
      "(64, 33)\n",
      "step 11894, loss is 4.8647356033325195\n",
      "(64, 33)\n",
      "step 11895, loss is 4.664130210876465\n",
      "(64, 33)\n",
      "step 11896, loss is 4.863273620605469\n",
      "(64, 33)\n",
      "step 11897, loss is 4.725480556488037\n",
      "(64, 33)\n",
      "step 11898, loss is 4.848196506500244\n",
      "(64, 33)\n",
      "step 11899, loss is 4.9416728019714355\n",
      "(64, 33)\n",
      "step 11900, loss is 4.780672073364258\n",
      "(64, 33)\n",
      "step 11901, loss is 4.760398864746094\n",
      "(64, 33)\n",
      "step 11902, loss is 4.781087875366211\n",
      "(64, 33)\n",
      "step 11903, loss is 4.8874688148498535\n",
      "(64, 33)\n",
      "step 11904, loss is 4.89402961730957\n",
      "(64, 33)\n",
      "step 11905, loss is 4.817662239074707\n",
      "(64, 33)\n",
      "step 11906, loss is 4.708853244781494\n",
      "(64, 33)\n",
      "step 11907, loss is 4.519020080566406\n",
      "(64, 33)\n",
      "step 11908, loss is 5.049658298492432\n",
      "(64, 33)\n",
      "step 11909, loss is 4.907548904418945\n",
      "(64, 33)\n",
      "step 11910, loss is 4.6533284187316895\n",
      "(64, 33)\n",
      "step 11911, loss is 4.9153008460998535\n",
      "(64, 33)\n",
      "step 11912, loss is 4.683107376098633\n",
      "(64, 33)\n",
      "step 11913, loss is 4.897402286529541\n",
      "(64, 33)\n",
      "step 11914, loss is 4.764462471008301\n",
      "(64, 33)\n",
      "step 11915, loss is 4.814791679382324\n",
      "(64, 33)\n",
      "step 11916, loss is 4.729907989501953\n",
      "(64, 33)\n",
      "step 11917, loss is 4.919958591461182\n",
      "(64, 33)\n",
      "step 11918, loss is 4.847335338592529\n",
      "(64, 33)\n",
      "step 11919, loss is 4.925940036773682\n",
      "(64, 33)\n",
      "step 11920, loss is 4.77254056930542\n",
      "(64, 33)\n",
      "step 11921, loss is 4.839610576629639\n",
      "(64, 33)\n",
      "step 11922, loss is 4.7273077964782715\n",
      "(64, 33)\n",
      "step 11923, loss is 4.751121997833252\n",
      "(64, 33)\n",
      "step 11924, loss is 4.869056701660156\n",
      "(64, 33)\n",
      "step 11925, loss is 4.8504638671875\n",
      "(64, 33)\n",
      "step 11926, loss is 4.712212562561035\n",
      "(64, 33)\n",
      "step 11927, loss is 4.875576019287109\n",
      "(64, 33)\n",
      "step 11928, loss is 4.642757892608643\n",
      "(64, 33)\n",
      "step 11929, loss is 4.5775275230407715\n",
      "(64, 33)\n",
      "step 11930, loss is 4.85142707824707\n",
      "(64, 33)\n",
      "step 11931, loss is 4.816279888153076\n",
      "(64, 33)\n",
      "step 11932, loss is 4.834715843200684\n",
      "(64, 33)\n",
      "step 11933, loss is 4.6566572189331055\n",
      "(64, 33)\n",
      "step 11934, loss is 4.80776834487915\n",
      "(64, 33)\n",
      "step 11935, loss is 4.725215435028076\n",
      "(64, 33)\n",
      "step 11936, loss is 4.610174179077148\n",
      "(64, 33)\n",
      "step 11937, loss is 4.6956987380981445\n",
      "(64, 33)\n",
      "step 11938, loss is 4.842547416687012\n",
      "(64, 33)\n",
      "step 11939, loss is 4.832950115203857\n",
      "(64, 33)\n",
      "step 11940, loss is 4.798114776611328\n",
      "(64, 33)\n",
      "step 11941, loss is 4.699275016784668\n",
      "(64, 33)\n",
      "step 11942, loss is 4.740011692047119\n",
      "(64, 33)\n",
      "step 11943, loss is 4.8988237380981445\n",
      "(64, 33)\n",
      "step 11944, loss is 4.818816661834717\n",
      "(64, 33)\n",
      "step 11945, loss is 4.783621311187744\n",
      "(64, 33)\n",
      "step 11946, loss is 4.72466516494751\n",
      "(64, 33)\n",
      "step 11947, loss is 4.836964130401611\n",
      "(64, 33)\n",
      "step 11948, loss is 4.752773284912109\n",
      "(64, 33)\n",
      "step 11949, loss is 4.720465660095215\n",
      "(64, 33)\n",
      "step 11950, loss is 4.765105724334717\n",
      "(64, 33)\n",
      "step 11951, loss is 4.7304301261901855\n",
      "(64, 33)\n",
      "step 11952, loss is 4.900603294372559\n",
      "(64, 33)\n",
      "step 11953, loss is 4.688011169433594\n",
      "(64, 33)\n",
      "step 11954, loss is 4.901976585388184\n",
      "(64, 33)\n",
      "step 11955, loss is 4.839372634887695\n",
      "(64, 33)\n",
      "step 11956, loss is 4.848806858062744\n",
      "(64, 33)\n",
      "step 11957, loss is 4.826202392578125\n",
      "(64, 33)\n",
      "step 11958, loss is 4.704702854156494\n",
      "(64, 33)\n",
      "step 11959, loss is 4.885655879974365\n",
      "(64, 33)\n",
      "step 11960, loss is 4.8798651695251465\n",
      "(64, 33)\n",
      "step 11961, loss is 4.819403648376465\n",
      "(64, 33)\n",
      "step 11962, loss is 4.8029890060424805\n",
      "(64, 33)\n",
      "step 11963, loss is 5.00119686126709\n",
      "(64, 33)\n",
      "step 11964, loss is 4.873473167419434\n",
      "(64, 33)\n",
      "step 11965, loss is 4.803475856781006\n",
      "(64, 33)\n",
      "step 11966, loss is 4.767348766326904\n",
      "(64, 33)\n",
      "step 11967, loss is 4.810591220855713\n",
      "(64, 33)\n",
      "step 11968, loss is 4.666860103607178\n",
      "(64, 33)\n",
      "step 11969, loss is 4.929409980773926\n",
      "(64, 33)\n",
      "step 11970, loss is 4.676660537719727\n",
      "(64, 33)\n",
      "step 11971, loss is 4.856291770935059\n",
      "(64, 33)\n",
      "step 11972, loss is 4.898191452026367\n",
      "(64, 33)\n",
      "step 11973, loss is 4.989609241485596\n",
      "(64, 33)\n",
      "step 11974, loss is 4.78031063079834\n",
      "(64, 33)\n",
      "step 11975, loss is 4.785088539123535\n",
      "(64, 33)\n",
      "step 11976, loss is 4.7514472007751465\n",
      "(64, 33)\n",
      "step 11977, loss is 4.9780473709106445\n",
      "(64, 33)\n",
      "step 11978, loss is 4.792637825012207\n",
      "(64, 33)\n",
      "step 11979, loss is 4.8904242515563965\n",
      "(64, 33)\n",
      "step 11980, loss is 4.847970485687256\n",
      "(64, 33)\n",
      "step 11981, loss is 4.94217586517334\n",
      "(64, 33)\n",
      "step 11982, loss is 4.836344242095947\n",
      "(64, 33)\n",
      "step 11983, loss is 4.855973243713379\n",
      "(64, 33)\n",
      "step 11984, loss is 4.763680458068848\n",
      "(64, 33)\n",
      "step 11985, loss is 4.861170291900635\n",
      "(64, 33)\n",
      "step 11986, loss is 4.838440895080566\n",
      "(64, 33)\n",
      "step 11987, loss is 4.857050895690918\n",
      "(64, 33)\n",
      "step 11988, loss is 4.891505241394043\n",
      "(64, 33)\n",
      "step 11989, loss is 4.813272476196289\n",
      "(64, 33)\n",
      "step 11990, loss is 4.6832170486450195\n",
      "(64, 33)\n",
      "step 11991, loss is 4.869662761688232\n",
      "(64, 33)\n",
      "step 11992, loss is 4.6609954833984375\n",
      "(64, 33)\n",
      "step 11993, loss is 4.934770584106445\n",
      "(64, 33)\n",
      "step 11994, loss is 4.816615104675293\n",
      "(64, 33)\n",
      "step 11995, loss is 5.00918436050415\n",
      "(64, 33)\n",
      "step 11996, loss is 4.93100643157959\n",
      "(64, 33)\n",
      "step 11997, loss is 4.935605525970459\n",
      "(64, 33)\n",
      "step 11998, loss is 4.878353118896484\n",
      "(64, 33)\n",
      "step 11999, loss is 4.7092084884643555\n",
      "(64, 33)\n",
      "step 12000, loss is 4.738955020904541\n",
      "(64, 33)\n",
      "step 12001, loss is 4.946049690246582\n",
      "(64, 33)\n",
      "step 12002, loss is 4.836792945861816\n",
      "(64, 33)\n",
      "step 12003, loss is 4.789262294769287\n",
      "(64, 33)\n",
      "step 12004, loss is 4.592906951904297\n",
      "(64, 33)\n",
      "step 12005, loss is 4.914622783660889\n",
      "(64, 33)\n",
      "step 12006, loss is 4.66347599029541\n",
      "(64, 33)\n",
      "step 12007, loss is 4.542842864990234\n",
      "(64, 33)\n",
      "step 12008, loss is 4.823113918304443\n",
      "(64, 33)\n",
      "step 12009, loss is 4.816248893737793\n",
      "(64, 33)\n",
      "step 12010, loss is 4.846669673919678\n",
      "(64, 33)\n",
      "step 12011, loss is 4.841611862182617\n",
      "(64, 33)\n",
      "step 12012, loss is 4.965078353881836\n",
      "(64, 33)\n",
      "step 12013, loss is 5.011651515960693\n",
      "(64, 33)\n",
      "step 12014, loss is 4.889350414276123\n",
      "(64, 33)\n",
      "step 12015, loss is 4.794933319091797\n",
      "(64, 33)\n",
      "step 12016, loss is 4.815825462341309\n",
      "(64, 33)\n",
      "step 12017, loss is 4.731814861297607\n",
      "(64, 33)\n",
      "step 12018, loss is 4.756880760192871\n",
      "(64, 33)\n",
      "step 12019, loss is 4.929088592529297\n",
      "(64, 33)\n",
      "step 12020, loss is 4.969820022583008\n",
      "(64, 33)\n",
      "step 12021, loss is 4.659099102020264\n",
      "(64, 33)\n",
      "step 12022, loss is 4.663924694061279\n",
      "(64, 33)\n",
      "step 12023, loss is 4.868988037109375\n",
      "(64, 33)\n",
      "step 12024, loss is 4.8398027420043945\n",
      "(64, 33)\n",
      "step 12025, loss is 4.957186222076416\n",
      "(64, 33)\n",
      "step 12026, loss is 4.785617351531982\n",
      "(64, 33)\n",
      "step 12027, loss is 4.79820442199707\n",
      "(64, 33)\n",
      "step 12028, loss is 4.769416332244873\n",
      "(64, 33)\n",
      "step 12029, loss is 4.639681816101074\n",
      "(64, 33)\n",
      "step 12030, loss is 4.946227073669434\n",
      "(64, 33)\n",
      "step 12031, loss is 4.676174163818359\n",
      "(64, 33)\n",
      "step 12032, loss is 4.8579206466674805\n",
      "(64, 33)\n",
      "step 12033, loss is 4.829905033111572\n",
      "(64, 33)\n",
      "step 12034, loss is 4.867229461669922\n",
      "(64, 33)\n",
      "step 12035, loss is 4.691821575164795\n",
      "(64, 33)\n",
      "step 12036, loss is 4.805850028991699\n",
      "(64, 33)\n",
      "step 12037, loss is 4.747241497039795\n",
      "(64, 33)\n",
      "step 12038, loss is 4.709994316101074\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12039, loss is 4.821605205535889\n",
      "(64, 33)\n",
      "step 12040, loss is 4.842526435852051\n",
      "(64, 33)\n",
      "step 12041, loss is 4.94450569152832\n",
      "(64, 33)\n",
      "step 12042, loss is 4.8404459953308105\n",
      "(64, 33)\n",
      "step 12043, loss is 4.925046443939209\n",
      "(64, 33)\n",
      "step 12044, loss is 4.471376895904541\n",
      "(64, 33)\n",
      "step 12045, loss is 4.8263678550720215\n",
      "(64, 33)\n",
      "step 12046, loss is 4.767803192138672\n",
      "(64, 33)\n",
      "step 12047, loss is 4.773056983947754\n",
      "(64, 33)\n",
      "step 12048, loss is 4.72032356262207\n",
      "(64, 33)\n",
      "step 12049, loss is 4.7309250831604\n",
      "(64, 33)\n",
      "step 12050, loss is 4.736080169677734\n",
      "(64, 33)\n",
      "step 12051, loss is 4.873100757598877\n",
      "(64, 33)\n",
      "step 12052, loss is 4.8923869132995605\n",
      "(64, 33)\n",
      "step 12053, loss is 4.808680057525635\n",
      "(64, 33)\n",
      "step 12054, loss is 4.807180881500244\n",
      "(64, 33)\n",
      "step 12055, loss is 4.698688507080078\n",
      "(64, 33)\n",
      "step 12056, loss is 4.810493469238281\n",
      "(64, 33)\n",
      "step 12057, loss is 5.089583873748779\n",
      "(64, 33)\n",
      "step 12058, loss is 4.758368492126465\n",
      "(64, 33)\n",
      "step 12059, loss is 4.900889873504639\n",
      "(64, 33)\n",
      "step 12060, loss is 4.929112434387207\n",
      "(64, 33)\n",
      "step 12061, loss is 4.714304447174072\n",
      "(64, 33)\n",
      "step 12062, loss is 4.935797691345215\n",
      "(64, 33)\n",
      "step 12063, loss is 4.828409194946289\n",
      "(64, 33)\n",
      "step 12064, loss is 4.959860324859619\n",
      "(64, 33)\n",
      "step 12065, loss is 4.862722396850586\n",
      "(64, 33)\n",
      "step 12066, loss is 4.81564998626709\n",
      "(64, 33)\n",
      "step 12067, loss is 4.843196868896484\n",
      "(64, 33)\n",
      "step 12068, loss is 4.800804138183594\n",
      "(64, 33)\n",
      "step 12069, loss is 4.7953200340271\n",
      "(64, 33)\n",
      "step 12070, loss is 4.7596211433410645\n",
      "(64, 33)\n",
      "step 12071, loss is 4.789054870605469\n",
      "(64, 33)\n",
      "step 12072, loss is 4.822873592376709\n",
      "(64, 33)\n",
      "step 12073, loss is 4.932464599609375\n",
      "(64, 33)\n",
      "step 12074, loss is 4.848596096038818\n",
      "(64, 33)\n",
      "step 12075, loss is 4.6428375244140625\n",
      "(64, 33)\n",
      "step 12076, loss is 4.836831092834473\n",
      "(64, 33)\n",
      "step 12077, loss is 4.894803047180176\n",
      "(64, 33)\n",
      "step 12078, loss is 4.733663558959961\n",
      "(64, 33)\n",
      "step 12079, loss is 4.738846302032471\n",
      "(64, 33)\n",
      "step 12080, loss is 4.68466854095459\n",
      "(64, 33)\n",
      "step 12081, loss is 4.7680277824401855\n",
      "(64, 33)\n",
      "step 12082, loss is 4.749710559844971\n",
      "(64, 33)\n",
      "step 12083, loss is 4.9099507331848145\n",
      "(64, 33)\n",
      "step 12084, loss is 4.705222129821777\n",
      "(64, 33)\n",
      "step 12085, loss is 4.651340484619141\n",
      "(64, 33)\n",
      "step 12086, loss is 4.810067653656006\n",
      "(64, 33)\n",
      "step 12087, loss is 4.899043083190918\n",
      "(64, 33)\n",
      "step 12088, loss is 4.856503963470459\n",
      "(64, 33)\n",
      "step 12089, loss is 4.735762119293213\n",
      "(64, 33)\n",
      "step 12090, loss is 4.8900980949401855\n",
      "(64, 33)\n",
      "step 12091, loss is 4.844079494476318\n",
      "(64, 33)\n",
      "step 12092, loss is 4.608479976654053\n",
      "(64, 33)\n",
      "step 12093, loss is 4.732954502105713\n",
      "(64, 33)\n",
      "step 12094, loss is 4.934257507324219\n",
      "(64, 33)\n",
      "step 12095, loss is 4.954150199890137\n",
      "(64, 33)\n",
      "step 12096, loss is 4.902972221374512\n",
      "(64, 33)\n",
      "step 12097, loss is 4.723265647888184\n",
      "(64, 33)\n",
      "step 12098, loss is 4.9865031242370605\n",
      "(64, 33)\n",
      "step 12099, loss is 4.783890724182129\n",
      "(64, 33)\n",
      "step 12100, loss is 4.952821731567383\n",
      "(64, 33)\n",
      "step 12101, loss is 4.8805999755859375\n",
      "(64, 33)\n",
      "step 12102, loss is 4.76233434677124\n",
      "(64, 33)\n",
      "step 12103, loss is 4.853126525878906\n",
      "(64, 33)\n",
      "step 12104, loss is 4.7780680656433105\n",
      "(64, 33)\n",
      "step 12105, loss is 4.855888366699219\n",
      "(64, 33)\n",
      "step 12106, loss is 4.9363555908203125\n",
      "(64, 33)\n",
      "step 12107, loss is 4.788198471069336\n",
      "(64, 33)\n",
      "step 12108, loss is 4.811814785003662\n",
      "(64, 33)\n",
      "step 12109, loss is 4.83608865737915\n",
      "(64, 33)\n",
      "step 12110, loss is 4.8585100173950195\n",
      "(64, 33)\n",
      "step 12111, loss is 4.947605609893799\n",
      "(64, 33)\n",
      "step 12112, loss is 4.862596035003662\n",
      "(64, 33)\n",
      "step 12113, loss is 4.831136226654053\n",
      "(64, 33)\n",
      "step 12114, loss is 4.8029913902282715\n",
      "(64, 33)\n",
      "step 12115, loss is 4.978574752807617\n",
      "(64, 33)\n",
      "step 12116, loss is 4.834684371948242\n",
      "(64, 33)\n",
      "step 12117, loss is 4.748097896575928\n",
      "(64, 33)\n",
      "step 12118, loss is 4.880412578582764\n",
      "(64, 33)\n",
      "step 12119, loss is 4.881195545196533\n",
      "(64, 33)\n",
      "step 12120, loss is 4.758675575256348\n",
      "(64, 33)\n",
      "step 12121, loss is 4.80092191696167\n",
      "(64, 33)\n",
      "step 12122, loss is 4.755192756652832\n",
      "(64, 33)\n",
      "step 12123, loss is 4.954164981842041\n",
      "(64, 33)\n",
      "step 12124, loss is 4.721164703369141\n",
      "(64, 33)\n",
      "step 12125, loss is 4.8401970863342285\n",
      "(64, 33)\n",
      "step 12126, loss is 4.736756324768066\n",
      "(64, 33)\n",
      "step 12127, loss is 4.6051435470581055\n",
      "(64, 33)\n",
      "step 12128, loss is 4.895962715148926\n",
      "(64, 33)\n",
      "step 12129, loss is 4.657893657684326\n",
      "(64, 33)\n",
      "step 12130, loss is 4.804679870605469\n",
      "(64, 33)\n",
      "step 12131, loss is 4.649072170257568\n",
      "(64, 33)\n",
      "step 12132, loss is 5.01581335067749\n",
      "(64, 33)\n",
      "step 12133, loss is 4.754090785980225\n",
      "(64, 33)\n",
      "step 12134, loss is 4.931429386138916\n",
      "(64, 33)\n",
      "step 12135, loss is 4.6617751121521\n",
      "(64, 33)\n",
      "step 12136, loss is 4.801949501037598\n",
      "(64, 33)\n",
      "step 12137, loss is 4.797877311706543\n",
      "(64, 33)\n",
      "step 12138, loss is 4.770511627197266\n",
      "(64, 33)\n",
      "step 12139, loss is 4.7488532066345215\n",
      "(64, 33)\n",
      "step 12140, loss is 4.834876537322998\n",
      "(64, 33)\n",
      "step 12141, loss is 4.91757345199585\n",
      "(64, 33)\n",
      "step 12142, loss is 4.657517433166504\n",
      "(64, 33)\n",
      "step 12143, loss is 4.949987411499023\n",
      "(64, 33)\n",
      "step 12144, loss is 4.757974147796631\n",
      "(64, 33)\n",
      "step 12145, loss is 4.775782585144043\n",
      "(64, 33)\n",
      "step 12146, loss is 4.977133750915527\n",
      "(64, 33)\n",
      "step 12147, loss is 4.846712589263916\n",
      "(64, 33)\n",
      "step 12148, loss is 4.804394245147705\n",
      "(64, 33)\n",
      "step 12149, loss is 4.649380683898926\n",
      "(64, 33)\n",
      "step 12150, loss is 4.8948259353637695\n",
      "(64, 33)\n",
      "step 12151, loss is 4.875332355499268\n",
      "(64, 33)\n",
      "step 12152, loss is 4.856330871582031\n",
      "(64, 33)\n",
      "step 12153, loss is 4.766129016876221\n",
      "(64, 33)\n",
      "step 12154, loss is 4.764693737030029\n",
      "(64, 33)\n",
      "step 12155, loss is 4.650932312011719\n",
      "(64, 33)\n",
      "step 12156, loss is 4.80298376083374\n",
      "(64, 33)\n",
      "step 12157, loss is 4.815915584564209\n",
      "(64, 33)\n",
      "step 12158, loss is 4.993705749511719\n",
      "(64, 33)\n",
      "step 12159, loss is 4.811675071716309\n",
      "(64, 33)\n",
      "step 12160, loss is 4.863009452819824\n",
      "(64, 33)\n",
      "step 12161, loss is 4.687333106994629\n",
      "(64, 33)\n",
      "step 12162, loss is 4.885357856750488\n",
      "(64, 33)\n",
      "step 12163, loss is 4.79624605178833\n",
      "(64, 33)\n",
      "step 12164, loss is 4.918728351593018\n",
      "(64, 33)\n",
      "step 12165, loss is 4.856912136077881\n",
      "(64, 33)\n",
      "step 12166, loss is 4.783327102661133\n",
      "(64, 33)\n",
      "step 12167, loss is 4.833265781402588\n",
      "(64, 33)\n",
      "step 12168, loss is 4.538275718688965\n",
      "(64, 33)\n",
      "step 12169, loss is 4.908129692077637\n",
      "(64, 33)\n",
      "step 12170, loss is 4.790807723999023\n",
      "(64, 33)\n",
      "step 12171, loss is 4.891824722290039\n",
      "(64, 33)\n",
      "step 12172, loss is 4.653356552124023\n",
      "(64, 33)\n",
      "step 12173, loss is 4.927615165710449\n",
      "(64, 33)\n",
      "step 12174, loss is 4.73377799987793\n",
      "(64, 33)\n",
      "step 12175, loss is 4.957609176635742\n",
      "(64, 33)\n",
      "step 12176, loss is 4.695383548736572\n",
      "(64, 33)\n",
      "step 12177, loss is 4.876569747924805\n",
      "(64, 33)\n",
      "step 12178, loss is 4.581565856933594\n",
      "(64, 33)\n",
      "step 12179, loss is 4.758230686187744\n",
      "(64, 33)\n",
      "step 12180, loss is 4.842134952545166\n",
      "(64, 33)\n",
      "step 12181, loss is 4.892723083496094\n",
      "(64, 33)\n",
      "step 12182, loss is 4.814342498779297\n",
      "(64, 33)\n",
      "step 12183, loss is 4.8583526611328125\n",
      "(64, 33)\n",
      "step 12184, loss is 4.833812713623047\n",
      "(64, 33)\n",
      "step 12185, loss is 4.81308126449585\n",
      "(64, 33)\n",
      "step 12186, loss is 4.8294243812561035\n",
      "(64, 33)\n",
      "step 12187, loss is 4.592881679534912\n",
      "(64, 33)\n",
      "step 12188, loss is 4.809136390686035\n",
      "(64, 33)\n",
      "step 12189, loss is 4.854796886444092\n",
      "(64, 33)\n",
      "step 12190, loss is 4.779850482940674\n",
      "(64, 33)\n",
      "step 12191, loss is 4.884088516235352\n",
      "(64, 33)\n",
      "step 12192, loss is 4.712074279785156\n",
      "(64, 33)\n",
      "step 12193, loss is 4.649441719055176\n",
      "(64, 33)\n",
      "step 12194, loss is 4.760202884674072\n",
      "(64, 33)\n",
      "step 12195, loss is 5.103099346160889\n",
      "(64, 33)\n",
      "step 12196, loss is 4.858710765838623\n",
      "(64, 33)\n",
      "step 12197, loss is 4.829502105712891\n",
      "(64, 33)\n",
      "step 12198, loss is 4.549383163452148\n",
      "(64, 33)\n",
      "step 12199, loss is 5.0837836265563965\n",
      "(64, 33)\n",
      "step 12200, loss is 4.740209579467773\n",
      "(64, 33)\n",
      "step 12201, loss is 4.9592084884643555\n",
      "(64, 33)\n",
      "step 12202, loss is 4.814766883850098\n",
      "(64, 33)\n",
      "step 12203, loss is 5.008974552154541\n",
      "(64, 33)\n",
      "step 12204, loss is 4.846366882324219\n",
      "(64, 33)\n",
      "step 12205, loss is 4.795563697814941\n",
      "(64, 33)\n",
      "step 12206, loss is 4.880823612213135\n",
      "(64, 33)\n",
      "step 12207, loss is 4.559164047241211\n",
      "(64, 33)\n",
      "step 12208, loss is 4.680997848510742\n",
      "(64, 33)\n",
      "step 12209, loss is 4.803985595703125\n",
      "(64, 33)\n",
      "step 12210, loss is 4.805710792541504\n",
      "(64, 33)\n",
      "step 12211, loss is 4.710127353668213\n",
      "(64, 33)\n",
      "step 12212, loss is 4.747025966644287\n",
      "(64, 33)\n",
      "step 12213, loss is 4.871107578277588\n",
      "(64, 33)\n",
      "step 12214, loss is 4.7953901290893555\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12215, loss is 4.7786478996276855\n",
      "(64, 33)\n",
      "step 12216, loss is 4.6938090324401855\n",
      "(64, 33)\n",
      "step 12217, loss is 4.822854042053223\n",
      "(64, 33)\n",
      "step 12218, loss is 4.790980339050293\n",
      "(64, 33)\n",
      "step 12219, loss is 4.746563911437988\n",
      "(64, 33)\n",
      "step 12220, loss is 4.610650062561035\n",
      "(64, 33)\n",
      "step 12221, loss is 4.875943660736084\n",
      "(64, 33)\n",
      "step 12222, loss is 4.922101020812988\n",
      "(64, 33)\n",
      "step 12223, loss is 4.802809715270996\n",
      "(64, 33)\n",
      "step 12224, loss is 4.84440279006958\n",
      "(64, 33)\n",
      "step 12225, loss is 4.799115180969238\n",
      "(64, 33)\n",
      "step 12226, loss is 4.810626983642578\n",
      "(64, 33)\n",
      "step 12227, loss is 4.814347743988037\n",
      "(64, 33)\n",
      "step 12228, loss is 4.872753620147705\n",
      "(64, 33)\n",
      "step 12229, loss is 4.907829284667969\n",
      "(64, 33)\n",
      "step 12230, loss is 4.792302131652832\n",
      "(64, 33)\n",
      "step 12231, loss is 4.980878829956055\n",
      "(64, 33)\n",
      "step 12232, loss is 4.859474182128906\n",
      "(64, 33)\n",
      "step 12233, loss is 4.599220275878906\n",
      "(64, 33)\n",
      "step 12234, loss is 4.843879699707031\n",
      "(64, 33)\n",
      "step 12235, loss is 4.959796905517578\n",
      "(64, 33)\n",
      "step 12236, loss is 4.722345352172852\n",
      "(64, 33)\n",
      "step 12237, loss is 4.860889911651611\n",
      "(64, 33)\n",
      "step 12238, loss is 4.8240275382995605\n",
      "(64, 33)\n",
      "step 12239, loss is 4.816873073577881\n",
      "(64, 33)\n",
      "step 12240, loss is 4.60488748550415\n",
      "(64, 33)\n",
      "step 12241, loss is 4.8939104080200195\n",
      "(64, 33)\n",
      "step 12242, loss is 4.84088659286499\n",
      "(64, 33)\n",
      "step 12243, loss is 4.818504333496094\n",
      "(64, 33)\n",
      "step 12244, loss is 4.7084784507751465\n",
      "(64, 33)\n",
      "step 12245, loss is 4.772034645080566\n",
      "(64, 33)\n",
      "step 12246, loss is 4.726282119750977\n",
      "(64, 33)\n",
      "step 12247, loss is 4.862441062927246\n",
      "(64, 33)\n",
      "step 12248, loss is 4.653563976287842\n",
      "(64, 33)\n",
      "step 12249, loss is 5.018082618713379\n",
      "(64, 33)\n",
      "step 12250, loss is 4.7557373046875\n",
      "(64, 33)\n",
      "step 12251, loss is 4.976953506469727\n",
      "(64, 33)\n",
      "step 12252, loss is 4.893618583679199\n",
      "(64, 33)\n",
      "step 12253, loss is 4.817775726318359\n",
      "(64, 33)\n",
      "step 12254, loss is 4.818422317504883\n",
      "(64, 33)\n",
      "step 12255, loss is 4.909846782684326\n",
      "(64, 33)\n",
      "step 12256, loss is 4.664763927459717\n",
      "(64, 33)\n",
      "step 12257, loss is 4.909714221954346\n",
      "(64, 33)\n",
      "step 12258, loss is 4.836161136627197\n",
      "(64, 33)\n",
      "step 12259, loss is 5.01675271987915\n",
      "(64, 33)\n",
      "step 12260, loss is 4.739926338195801\n",
      "(64, 33)\n",
      "step 12261, loss is 4.820732116699219\n",
      "(64, 33)\n",
      "step 12262, loss is 4.828521728515625\n",
      "(64, 33)\n",
      "step 12263, loss is 4.959514617919922\n",
      "(64, 33)\n",
      "step 12264, loss is 4.86731481552124\n",
      "(64, 33)\n",
      "step 12265, loss is 4.750293731689453\n",
      "(64, 33)\n",
      "step 12266, loss is 4.7756805419921875\n",
      "(64, 33)\n",
      "step 12267, loss is 4.86679744720459\n",
      "(64, 33)\n",
      "step 12268, loss is 4.740021705627441\n",
      "(64, 33)\n",
      "step 12269, loss is 4.82957124710083\n",
      "(64, 33)\n",
      "step 12270, loss is 4.8831963539123535\n",
      "(64, 33)\n",
      "step 12271, loss is 4.796781539916992\n",
      "(64, 33)\n",
      "step 12272, loss is 4.704370498657227\n",
      "(64, 33)\n",
      "step 12273, loss is 4.984417915344238\n",
      "(64, 33)\n",
      "step 12274, loss is 5.052916049957275\n",
      "(64, 33)\n",
      "step 12275, loss is 4.697864055633545\n",
      "(64, 33)\n",
      "step 12276, loss is 4.850951671600342\n",
      "(64, 33)\n",
      "step 12277, loss is 4.724713325500488\n",
      "(64, 33)\n",
      "step 12278, loss is 4.861320495605469\n",
      "(64, 33)\n",
      "step 12279, loss is 4.9590983390808105\n",
      "(64, 33)\n",
      "step 12280, loss is 4.85011100769043\n",
      "(64, 33)\n",
      "step 12281, loss is 4.7361860275268555\n",
      "(64, 33)\n",
      "step 12282, loss is 4.634215831756592\n",
      "(64, 33)\n",
      "step 12283, loss is 4.712531089782715\n",
      "(64, 33)\n",
      "step 12284, loss is 4.791748046875\n",
      "(64, 33)\n",
      "step 12285, loss is 4.683957099914551\n",
      "(64, 33)\n",
      "step 12286, loss is 4.950618267059326\n",
      "(64, 33)\n",
      "step 12287, loss is 4.798091888427734\n",
      "(64, 33)\n",
      "step 12288, loss is 4.844331741333008\n",
      "(64, 33)\n",
      "step 12289, loss is 4.802270889282227\n",
      "(64, 33)\n",
      "step 12290, loss is 4.8369317054748535\n",
      "(64, 33)\n",
      "step 12291, loss is 4.871607780456543\n",
      "(64, 33)\n",
      "step 12292, loss is 4.733572483062744\n",
      "(64, 33)\n",
      "step 12293, loss is 4.799868106842041\n",
      "(64, 33)\n",
      "step 12294, loss is 4.907761096954346\n",
      "(64, 33)\n",
      "step 12295, loss is 4.862539768218994\n",
      "(64, 33)\n",
      "step 12296, loss is 4.708616733551025\n",
      "(64, 33)\n",
      "step 12297, loss is 4.741489410400391\n",
      "(64, 33)\n",
      "step 12298, loss is 4.796205043792725\n",
      "(64, 33)\n",
      "step 12299, loss is 4.802062034606934\n",
      "(64, 33)\n",
      "step 12300, loss is 4.716209411621094\n",
      "(64, 33)\n",
      "step 12301, loss is 4.837331295013428\n",
      "(64, 33)\n",
      "step 12302, loss is 4.894923210144043\n",
      "(64, 33)\n",
      "step 12303, loss is 4.802582740783691\n",
      "(64, 33)\n",
      "step 12304, loss is 4.918684005737305\n",
      "(64, 33)\n",
      "step 12305, loss is 4.732705593109131\n",
      "(64, 33)\n",
      "step 12306, loss is 4.520552635192871\n",
      "(64, 33)\n",
      "step 12307, loss is 4.833114147186279\n",
      "(64, 33)\n",
      "step 12308, loss is 4.881103038787842\n",
      "(64, 33)\n",
      "step 12309, loss is 4.79463529586792\n",
      "(64, 33)\n",
      "step 12310, loss is 4.742912292480469\n",
      "(64, 33)\n",
      "step 12311, loss is 4.817225456237793\n",
      "(64, 33)\n",
      "step 12312, loss is 4.718284606933594\n",
      "(64, 33)\n",
      "step 12313, loss is 4.715810775756836\n",
      "(64, 33)\n",
      "step 12314, loss is 4.931839942932129\n",
      "(64, 33)\n",
      "step 12315, loss is 4.79694938659668\n",
      "(64, 33)\n",
      "step 12316, loss is 4.818756103515625\n",
      "(64, 33)\n",
      "step 12317, loss is 4.756359577178955\n",
      "(64, 33)\n",
      "step 12318, loss is 4.837742328643799\n",
      "(64, 33)\n",
      "step 12319, loss is 4.899882793426514\n",
      "(64, 33)\n",
      "step 12320, loss is 4.807579040527344\n",
      "(64, 33)\n",
      "step 12321, loss is 4.636198997497559\n",
      "(64, 33)\n",
      "step 12322, loss is 4.840514659881592\n",
      "(64, 33)\n",
      "step 12323, loss is 4.907471179962158\n",
      "(64, 33)\n",
      "step 12324, loss is 4.93844747543335\n",
      "(64, 33)\n",
      "step 12325, loss is 4.83201265335083\n",
      "(64, 33)\n",
      "step 12326, loss is 4.836658000946045\n",
      "(64, 33)\n",
      "step 12327, loss is 4.76747989654541\n",
      "(64, 33)\n",
      "step 12328, loss is 4.900373935699463\n",
      "(64, 33)\n",
      "step 12329, loss is 4.841212749481201\n",
      "(64, 33)\n",
      "step 12330, loss is 4.95367431640625\n",
      "(64, 33)\n",
      "step 12331, loss is 4.81785774230957\n",
      "(64, 33)\n",
      "step 12332, loss is 4.746910572052002\n",
      "(64, 33)\n",
      "step 12333, loss is 4.753133773803711\n",
      "(64, 33)\n",
      "step 12334, loss is 4.81392240524292\n",
      "(64, 33)\n",
      "step 12335, loss is 4.833211898803711\n",
      "(64, 33)\n",
      "step 12336, loss is 4.7700910568237305\n",
      "(64, 33)\n",
      "step 12337, loss is 4.655528545379639\n",
      "(64, 33)\n",
      "step 12338, loss is 4.685500621795654\n",
      "(64, 33)\n",
      "step 12339, loss is 4.7305169105529785\n",
      "(64, 33)\n",
      "step 12340, loss is 4.794867038726807\n",
      "(64, 33)\n",
      "step 12341, loss is 4.608260631561279\n",
      "(64, 33)\n",
      "step 12342, loss is 4.894747734069824\n",
      "(64, 33)\n",
      "step 12343, loss is 4.882882118225098\n",
      "(64, 33)\n",
      "step 12344, loss is 5.027827262878418\n",
      "(64, 33)\n",
      "step 12345, loss is 4.831622123718262\n",
      "(64, 33)\n",
      "step 12346, loss is 4.817354202270508\n",
      "(64, 33)\n",
      "step 12347, loss is 4.844448566436768\n",
      "(64, 33)\n",
      "step 12348, loss is 4.906036376953125\n",
      "(64, 33)\n",
      "step 12349, loss is 5.024397850036621\n",
      "(64, 33)\n",
      "step 12350, loss is 4.701934814453125\n",
      "(64, 33)\n",
      "step 12351, loss is 4.814985752105713\n",
      "(64, 33)\n",
      "step 12352, loss is 4.821923732757568\n",
      "(64, 33)\n",
      "step 12353, loss is 4.893692970275879\n",
      "(64, 33)\n",
      "step 12354, loss is 4.812530517578125\n",
      "(64, 33)\n",
      "step 12355, loss is 4.756389617919922\n",
      "(64, 33)\n",
      "step 12356, loss is 4.766846179962158\n",
      "(64, 33)\n",
      "step 12357, loss is 5.065560817718506\n",
      "(64, 33)\n",
      "step 12358, loss is 4.751928329467773\n",
      "(64, 33)\n",
      "step 12359, loss is 4.8214592933654785\n",
      "(64, 33)\n",
      "step 12360, loss is 4.817268371582031\n",
      "(64, 33)\n",
      "step 12361, loss is 4.952266693115234\n",
      "(64, 33)\n",
      "step 12362, loss is 4.792089939117432\n",
      "(64, 33)\n",
      "step 12363, loss is 4.635188102722168\n",
      "(64, 33)\n",
      "step 12364, loss is 4.909361839294434\n",
      "(64, 33)\n",
      "step 12365, loss is 4.800466060638428\n",
      "(64, 33)\n",
      "step 12366, loss is 4.829052925109863\n",
      "(64, 33)\n",
      "step 12367, loss is 5.121668815612793\n",
      "(64, 33)\n",
      "step 12368, loss is 4.725429534912109\n",
      "(64, 33)\n",
      "step 12369, loss is 4.750113010406494\n",
      "(64, 33)\n",
      "step 12370, loss is 4.792265892028809\n",
      "(64, 33)\n",
      "step 12371, loss is 4.800849437713623\n",
      "(64, 33)\n",
      "step 12372, loss is 5.000668048858643\n",
      "(64, 33)\n",
      "step 12373, loss is 4.7576189041137695\n",
      "(64, 33)\n",
      "step 12374, loss is 4.772093772888184\n",
      "(64, 33)\n",
      "step 12375, loss is 4.722238063812256\n",
      "(64, 33)\n",
      "step 12376, loss is 4.727935791015625\n",
      "(64, 33)\n",
      "step 12377, loss is 4.607180595397949\n",
      "(64, 33)\n",
      "step 12378, loss is 4.903109550476074\n",
      "(64, 33)\n",
      "step 12379, loss is 4.728446960449219\n",
      "(64, 33)\n",
      "step 12380, loss is 4.839192867279053\n",
      "(64, 33)\n",
      "step 12381, loss is 4.850361347198486\n",
      "(64, 33)\n",
      "step 12382, loss is 4.813995361328125\n",
      "(64, 33)\n",
      "step 12383, loss is 4.707925319671631\n",
      "(64, 33)\n",
      "step 12384, loss is 4.8251214027404785\n",
      "(64, 33)\n",
      "step 12385, loss is 4.774503231048584\n",
      "(64, 33)\n",
      "step 12386, loss is 4.8414764404296875\n",
      "(64, 33)\n",
      "step 12387, loss is 4.899755954742432\n",
      "(64, 33)\n",
      "step 12388, loss is 5.017194747924805\n",
      "(64, 33)\n",
      "step 12389, loss is 4.898920059204102\n",
      "(64, 33)\n",
      "step 12390, loss is 4.909311771392822\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12391, loss is 4.8385796546936035\n",
      "(64, 33)\n",
      "step 12392, loss is 4.7768731117248535\n",
      "(64, 33)\n",
      "step 12393, loss is 4.605561256408691\n",
      "(64, 33)\n",
      "step 12394, loss is 4.840047836303711\n",
      "(64, 33)\n",
      "step 12395, loss is 4.813490867614746\n",
      "(64, 33)\n",
      "step 12396, loss is 4.650023937225342\n",
      "(64, 33)\n",
      "step 12397, loss is 4.770263671875\n",
      "(64, 33)\n",
      "step 12398, loss is 4.954245090484619\n",
      "(64, 33)\n",
      "step 12399, loss is 4.9119791984558105\n",
      "(64, 33)\n",
      "step 12400, loss is 4.742451190948486\n",
      "(64, 33)\n",
      "step 12401, loss is 4.654652118682861\n",
      "(64, 33)\n",
      "step 12402, loss is 4.773569583892822\n",
      "(64, 33)\n",
      "step 12403, loss is 4.723143577575684\n",
      "(64, 33)\n",
      "step 12404, loss is 4.804281234741211\n",
      "(64, 33)\n",
      "step 12405, loss is 4.8166375160217285\n",
      "(64, 33)\n",
      "step 12406, loss is 4.91506290435791\n",
      "(64, 33)\n",
      "step 12407, loss is 4.742561340332031\n",
      "(64, 33)\n",
      "step 12408, loss is 4.900278091430664\n",
      "(64, 33)\n",
      "step 12409, loss is 4.783571243286133\n",
      "(64, 33)\n",
      "step 12410, loss is 4.8944549560546875\n",
      "(64, 33)\n",
      "step 12411, loss is 4.851248741149902\n",
      "(64, 33)\n",
      "step 12412, loss is 4.781711101531982\n",
      "(64, 33)\n",
      "step 12413, loss is 4.87571382522583\n",
      "(64, 33)\n",
      "step 12414, loss is 4.955705165863037\n",
      "(64, 33)\n",
      "step 12415, loss is 4.87727689743042\n",
      "(64, 33)\n",
      "step 12416, loss is 4.763518810272217\n",
      "(64, 33)\n",
      "step 12417, loss is 4.896306037902832\n",
      "(64, 33)\n",
      "step 12418, loss is 4.8819475173950195\n",
      "(64, 33)\n",
      "step 12419, loss is 4.720293045043945\n",
      "(64, 33)\n",
      "step 12420, loss is 4.977723121643066\n",
      "(64, 33)\n",
      "step 12421, loss is 4.967586994171143\n",
      "(64, 33)\n",
      "step 12422, loss is 4.852965354919434\n",
      "(64, 33)\n",
      "step 12423, loss is 4.788729190826416\n",
      "(64, 33)\n",
      "step 12424, loss is 4.8216423988342285\n",
      "(64, 33)\n",
      "step 12425, loss is 4.670197486877441\n",
      "(64, 33)\n",
      "step 12426, loss is 4.913088321685791\n",
      "(64, 33)\n",
      "step 12427, loss is 4.765717506408691\n",
      "(64, 33)\n",
      "step 12428, loss is 4.747261047363281\n",
      "(64, 33)\n",
      "step 12429, loss is 5.008496284484863\n",
      "(64, 33)\n",
      "step 12430, loss is 4.700673580169678\n",
      "(64, 33)\n",
      "step 12431, loss is 4.81499719619751\n",
      "(64, 33)\n",
      "step 12432, loss is 4.778449535369873\n",
      "(64, 33)\n",
      "step 12433, loss is 4.727503776550293\n",
      "(64, 33)\n",
      "step 12434, loss is 4.768780708312988\n",
      "(64, 33)\n",
      "step 12435, loss is 5.033122539520264\n",
      "(64, 33)\n",
      "step 12436, loss is 4.848291397094727\n",
      "(64, 33)\n",
      "step 12437, loss is 4.757638931274414\n",
      "(64, 33)\n",
      "step 12438, loss is 4.758608818054199\n",
      "(64, 33)\n",
      "step 12439, loss is 4.857606887817383\n",
      "(64, 33)\n",
      "step 12440, loss is 4.538780212402344\n",
      "(64, 33)\n",
      "step 12441, loss is 4.589334487915039\n",
      "(64, 33)\n",
      "step 12442, loss is 5.032094478607178\n",
      "(64, 33)\n",
      "step 12443, loss is 4.746331214904785\n",
      "(64, 33)\n",
      "step 12444, loss is 4.848803997039795\n",
      "(64, 33)\n",
      "step 12445, loss is 4.770502090454102\n",
      "(64, 33)\n",
      "step 12446, loss is 4.893474102020264\n",
      "(64, 33)\n",
      "step 12447, loss is 4.521060943603516\n",
      "(64, 33)\n",
      "step 12448, loss is 4.835258960723877\n",
      "(64, 33)\n",
      "step 12449, loss is 5.073661804199219\n",
      "(64, 33)\n",
      "step 12450, loss is 4.739066123962402\n",
      "(64, 33)\n",
      "step 12451, loss is 5.071224689483643\n",
      "(64, 33)\n",
      "step 12452, loss is 4.816357135772705\n",
      "(64, 33)\n",
      "step 12453, loss is 4.842278480529785\n",
      "(64, 33)\n",
      "step 12454, loss is 4.696618556976318\n",
      "(64, 33)\n",
      "step 12455, loss is 4.648057460784912\n",
      "(64, 33)\n",
      "step 12456, loss is 4.951432228088379\n",
      "(64, 33)\n",
      "step 12457, loss is 4.891402721405029\n",
      "(64, 33)\n",
      "step 12458, loss is 4.762977600097656\n",
      "(64, 33)\n",
      "step 12459, loss is 4.886094093322754\n",
      "(64, 33)\n",
      "step 12460, loss is 4.766587257385254\n",
      "(64, 33)\n",
      "step 12461, loss is 4.83895206451416\n",
      "(64, 33)\n",
      "step 12462, loss is 4.573387622833252\n",
      "(64, 33)\n",
      "step 12463, loss is 4.972093105316162\n",
      "(64, 33)\n",
      "step 12464, loss is 4.940971374511719\n",
      "(64, 33)\n",
      "step 12465, loss is 4.730658054351807\n",
      "(64, 33)\n",
      "step 12466, loss is 4.62892484664917\n",
      "(64, 33)\n",
      "step 12467, loss is 5.076035499572754\n",
      "(64, 33)\n",
      "step 12468, loss is 4.595219612121582\n",
      "(64, 33)\n",
      "step 12469, loss is 4.835483551025391\n",
      "(64, 33)\n",
      "step 12470, loss is 4.730656623840332\n",
      "(64, 33)\n",
      "step 12471, loss is 4.859854221343994\n",
      "(64, 33)\n",
      "step 12472, loss is 4.8997297286987305\n",
      "(64, 33)\n",
      "step 12473, loss is 4.91878604888916\n",
      "(64, 33)\n",
      "step 12474, loss is 4.7741265296936035\n",
      "(64, 33)\n",
      "step 12475, loss is 4.754421234130859\n",
      "(64, 33)\n",
      "step 12476, loss is 4.733528137207031\n",
      "(64, 33)\n",
      "step 12477, loss is 4.862940788269043\n",
      "(64, 33)\n",
      "step 12478, loss is 4.756322860717773\n",
      "(64, 33)\n",
      "step 12479, loss is 4.941708087921143\n",
      "(64, 33)\n",
      "step 12480, loss is 4.868496894836426\n",
      "(64, 33)\n",
      "step 12481, loss is 4.967676162719727\n",
      "(64, 33)\n",
      "step 12482, loss is 4.905187606811523\n",
      "(64, 33)\n",
      "step 12483, loss is 4.703846454620361\n",
      "(64, 33)\n",
      "step 12484, loss is 4.949417591094971\n",
      "(64, 33)\n",
      "step 12485, loss is 4.963147163391113\n",
      "(64, 33)\n",
      "step 12486, loss is 4.805087089538574\n",
      "(64, 33)\n",
      "step 12487, loss is 4.68559455871582\n",
      "(64, 33)\n",
      "step 12488, loss is 4.745979309082031\n",
      "(64, 33)\n",
      "step 12489, loss is 4.762745380401611\n",
      "(64, 33)\n",
      "step 12490, loss is 4.7448225021362305\n",
      "(64, 33)\n",
      "step 12491, loss is 4.81424617767334\n",
      "(64, 33)\n",
      "step 12492, loss is 4.704346179962158\n",
      "(64, 33)\n",
      "step 12493, loss is 4.76071834564209\n",
      "(64, 33)\n",
      "step 12494, loss is 4.746791362762451\n",
      "(64, 33)\n",
      "step 12495, loss is 4.714564800262451\n",
      "(64, 33)\n",
      "step 12496, loss is 4.7730278968811035\n",
      "(64, 33)\n",
      "step 12497, loss is 4.804603576660156\n",
      "(64, 33)\n",
      "step 12498, loss is 4.914256572723389\n",
      "(64, 33)\n",
      "step 12499, loss is 4.650509357452393\n",
      "(64, 33)\n",
      "step 12500, loss is 4.695680141448975\n",
      "(64, 33)\n",
      "step 12501, loss is 5.000875473022461\n",
      "(64, 33)\n",
      "step 12502, loss is 5.00489616394043\n",
      "(64, 33)\n",
      "step 12503, loss is 4.862959861755371\n",
      "(64, 33)\n",
      "step 12504, loss is 4.880000591278076\n",
      "(64, 33)\n",
      "step 12505, loss is 4.726552963256836\n",
      "(64, 33)\n",
      "step 12506, loss is 4.861331939697266\n",
      "(64, 33)\n",
      "step 12507, loss is 4.742592811584473\n",
      "(64, 33)\n",
      "step 12508, loss is 4.847323417663574\n",
      "(64, 33)\n",
      "step 12509, loss is 4.778234481811523\n",
      "(64, 33)\n",
      "step 12510, loss is 4.9692888259887695\n",
      "(64, 33)\n",
      "step 12511, loss is 4.643009662628174\n",
      "(64, 33)\n",
      "step 12512, loss is 4.8072662353515625\n",
      "(64, 33)\n",
      "step 12513, loss is 4.711738109588623\n",
      "(64, 33)\n",
      "step 12514, loss is 4.934688568115234\n",
      "(64, 33)\n",
      "step 12515, loss is 4.736262798309326\n",
      "(64, 33)\n",
      "step 12516, loss is 4.739776134490967\n",
      "(64, 33)\n",
      "step 12517, loss is 4.846023082733154\n",
      "(64, 33)\n",
      "step 12518, loss is 4.672593593597412\n",
      "(64, 33)\n",
      "step 12519, loss is 4.961705684661865\n",
      "(64, 33)\n",
      "step 12520, loss is 4.875290870666504\n",
      "(64, 33)\n",
      "step 12521, loss is 4.927143573760986\n",
      "(64, 33)\n",
      "step 12522, loss is 4.730374336242676\n",
      "(64, 33)\n",
      "step 12523, loss is 4.942296504974365\n",
      "(64, 33)\n",
      "step 12524, loss is 4.808572292327881\n",
      "(64, 33)\n",
      "step 12525, loss is 4.802784442901611\n",
      "(64, 33)\n",
      "step 12526, loss is 4.798728942871094\n",
      "(64, 33)\n",
      "step 12527, loss is 4.944357395172119\n",
      "(64, 33)\n",
      "step 12528, loss is 4.771113395690918\n",
      "(64, 33)\n",
      "step 12529, loss is 4.751779556274414\n",
      "(64, 33)\n",
      "step 12530, loss is 4.955704212188721\n",
      "(64, 33)\n",
      "step 12531, loss is 4.815042972564697\n",
      "(64, 33)\n",
      "step 12532, loss is 4.843223571777344\n",
      "(64, 33)\n",
      "step 12533, loss is 4.854324817657471\n",
      "(64, 33)\n",
      "step 12534, loss is 4.782594680786133\n",
      "(64, 33)\n",
      "step 12535, loss is 4.896512508392334\n",
      "(64, 33)\n",
      "step 12536, loss is 5.024631977081299\n",
      "(64, 33)\n",
      "step 12537, loss is 4.802962779998779\n",
      "(64, 33)\n",
      "step 12538, loss is 4.842247486114502\n",
      "(64, 33)\n",
      "step 12539, loss is 4.707038879394531\n",
      "(64, 33)\n",
      "step 12540, loss is 4.8849101066589355\n",
      "(64, 33)\n",
      "step 12541, loss is 5.026485919952393\n",
      "(64, 33)\n",
      "step 12542, loss is 4.69010591506958\n",
      "(64, 33)\n",
      "step 12543, loss is 4.813547611236572\n",
      "(64, 33)\n",
      "step 12544, loss is 4.751143932342529\n",
      "(64, 33)\n",
      "step 12545, loss is 4.897268295288086\n",
      "(64, 33)\n",
      "step 12546, loss is 5.102063179016113\n",
      "(64, 33)\n",
      "step 12547, loss is 4.685998916625977\n",
      "(64, 33)\n",
      "step 12548, loss is 4.978548049926758\n",
      "(64, 33)\n",
      "step 12549, loss is 4.927438259124756\n",
      "(64, 33)\n",
      "step 12550, loss is 4.885778427124023\n",
      "(64, 33)\n",
      "step 12551, loss is 4.874296188354492\n",
      "(64, 33)\n",
      "step 12552, loss is 4.731808662414551\n",
      "(64, 33)\n",
      "step 12553, loss is 4.773547649383545\n",
      "(64, 33)\n",
      "step 12554, loss is 4.640766620635986\n",
      "(64, 33)\n",
      "step 12555, loss is 4.607748985290527\n",
      "(64, 33)\n",
      "step 12556, loss is 5.063114166259766\n",
      "(64, 33)\n",
      "step 12557, loss is 4.9584808349609375\n",
      "(64, 33)\n",
      "step 12558, loss is 4.810202121734619\n",
      "(64, 33)\n",
      "step 12559, loss is 4.771377086639404\n",
      "(64, 33)\n",
      "step 12560, loss is 4.904414176940918\n",
      "(64, 33)\n",
      "step 12561, loss is 4.782153606414795\n",
      "(64, 33)\n",
      "step 12562, loss is 4.806832790374756\n",
      "(64, 33)\n",
      "step 12563, loss is 4.810567378997803\n",
      "(64, 33)\n",
      "step 12564, loss is 4.879478454589844\n",
      "(64, 33)\n",
      "step 12565, loss is 4.775992393493652\n",
      "(64, 33)\n",
      "step 12566, loss is 4.6417741775512695\n",
      "(64, 33)\n",
      "step 12567, loss is 4.959897041320801\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12568, loss is 4.704246520996094\n",
      "(64, 33)\n",
      "step 12569, loss is 4.768220901489258\n",
      "(64, 33)\n",
      "step 12570, loss is 4.847796440124512\n",
      "(64, 33)\n",
      "step 12571, loss is 4.7790069580078125\n",
      "(64, 33)\n",
      "step 12572, loss is 4.816563129425049\n",
      "(64, 33)\n",
      "step 12573, loss is 4.776668071746826\n",
      "(64, 33)\n",
      "step 12574, loss is 5.037708759307861\n",
      "(64, 33)\n",
      "step 12575, loss is 4.7629313468933105\n",
      "(64, 33)\n",
      "step 12576, loss is 4.791110992431641\n",
      "(64, 33)\n",
      "step 12577, loss is 4.690098762512207\n",
      "(64, 33)\n",
      "step 12578, loss is 4.777579307556152\n",
      "(64, 33)\n",
      "step 12579, loss is 4.801502704620361\n",
      "(64, 33)\n",
      "step 12580, loss is 4.748574733734131\n",
      "(64, 33)\n",
      "step 12581, loss is 4.741842746734619\n",
      "(64, 33)\n",
      "step 12582, loss is 4.8511481285095215\n",
      "(64, 33)\n",
      "step 12583, loss is 4.89445161819458\n",
      "(64, 33)\n",
      "step 12584, loss is 4.697896957397461\n",
      "(64, 33)\n",
      "step 12585, loss is 4.882036209106445\n",
      "(64, 33)\n",
      "step 12586, loss is 4.596375942230225\n",
      "(64, 33)\n",
      "step 12587, loss is 4.823943138122559\n",
      "(64, 33)\n",
      "step 12588, loss is 4.8759002685546875\n",
      "(64, 33)\n",
      "step 12589, loss is 4.876115322113037\n",
      "(64, 33)\n",
      "step 12590, loss is 4.739365100860596\n",
      "(64, 33)\n",
      "step 12591, loss is 4.829574108123779\n",
      "(64, 33)\n",
      "step 12592, loss is 4.884708404541016\n",
      "(64, 33)\n",
      "step 12593, loss is 4.875351905822754\n",
      "(64, 33)\n",
      "step 12594, loss is 4.855859279632568\n",
      "(64, 33)\n",
      "step 12595, loss is 4.910947322845459\n",
      "(64, 33)\n",
      "step 12596, loss is 4.810965538024902\n",
      "(64, 33)\n",
      "step 12597, loss is 4.999585151672363\n",
      "(64, 33)\n",
      "step 12598, loss is 4.918209075927734\n",
      "(64, 33)\n",
      "step 12599, loss is 4.957124710083008\n",
      "(64, 33)\n",
      "step 12600, loss is 4.769777297973633\n",
      "(64, 33)\n",
      "step 12601, loss is 4.687633991241455\n",
      "(64, 33)\n",
      "step 12602, loss is 4.871025562286377\n",
      "(64, 33)\n",
      "step 12603, loss is 4.903827667236328\n",
      "(64, 33)\n",
      "step 12604, loss is 4.69474983215332\n",
      "(64, 33)\n",
      "step 12605, loss is 4.726373195648193\n",
      "(64, 33)\n",
      "step 12606, loss is 4.736403942108154\n",
      "(64, 33)\n",
      "step 12607, loss is 4.929111480712891\n",
      "(64, 33)\n",
      "step 12608, loss is 4.9984869956970215\n",
      "(64, 33)\n",
      "step 12609, loss is 4.815680027008057\n",
      "(64, 33)\n",
      "step 12610, loss is 4.772511959075928\n",
      "(64, 33)\n",
      "step 12611, loss is 4.830228328704834\n",
      "(64, 33)\n",
      "step 12612, loss is 4.814173698425293\n",
      "(64, 33)\n",
      "step 12613, loss is 4.88470458984375\n",
      "(64, 33)\n",
      "step 12614, loss is 4.768033027648926\n",
      "(64, 33)\n",
      "step 12615, loss is 4.917356491088867\n",
      "(64, 33)\n",
      "step 12616, loss is 4.881952285766602\n",
      "(64, 33)\n",
      "step 12617, loss is 4.635584354400635\n",
      "(64, 33)\n",
      "step 12618, loss is 4.925234317779541\n",
      "(64, 33)\n",
      "step 12619, loss is 4.670312404632568\n",
      "(64, 33)\n",
      "step 12620, loss is 4.896046161651611\n",
      "(64, 33)\n",
      "step 12621, loss is 4.938206672668457\n",
      "(64, 33)\n",
      "step 12622, loss is 4.877161979675293\n",
      "(64, 33)\n",
      "step 12623, loss is 4.797957897186279\n",
      "(64, 33)\n",
      "step 12624, loss is 4.812243938446045\n",
      "(64, 33)\n",
      "step 12625, loss is 4.8391594886779785\n",
      "(64, 33)\n",
      "step 12626, loss is 4.916524410247803\n",
      "(64, 33)\n",
      "step 12627, loss is 4.620800495147705\n",
      "(64, 33)\n",
      "step 12628, loss is 4.798542022705078\n",
      "(64, 33)\n",
      "step 12629, loss is 5.034187316894531\n",
      "(64, 33)\n",
      "step 12630, loss is 4.884779453277588\n",
      "(64, 33)\n",
      "step 12631, loss is 4.8910231590271\n",
      "(64, 33)\n",
      "step 12632, loss is 4.7905449867248535\n",
      "(64, 33)\n",
      "step 12633, loss is 4.664244651794434\n",
      "(64, 33)\n",
      "step 12634, loss is 4.690546989440918\n",
      "(64, 33)\n",
      "step 12635, loss is 5.0718770027160645\n",
      "(64, 33)\n",
      "step 12636, loss is 4.810327053070068\n",
      "(64, 33)\n",
      "step 12637, loss is 4.813040733337402\n",
      "(64, 33)\n",
      "step 12638, loss is 4.805345058441162\n",
      "(64, 33)\n",
      "step 12639, loss is 4.90598201751709\n",
      "(64, 33)\n",
      "step 12640, loss is 4.909811019897461\n",
      "(64, 33)\n",
      "step 12641, loss is 4.883116245269775\n",
      "(64, 33)\n",
      "step 12642, loss is 4.91282320022583\n",
      "(64, 33)\n",
      "step 12643, loss is 4.774701118469238\n",
      "(64, 33)\n",
      "step 12644, loss is 4.526007652282715\n",
      "(64, 33)\n",
      "step 12645, loss is 4.8614654541015625\n",
      "(64, 33)\n",
      "step 12646, loss is 4.867960453033447\n",
      "(64, 33)\n",
      "step 12647, loss is 4.952453136444092\n",
      "(64, 33)\n",
      "step 12648, loss is 4.888200759887695\n",
      "(64, 33)\n",
      "step 12649, loss is 4.667699337005615\n",
      "(64, 33)\n",
      "step 12650, loss is 4.931483745574951\n",
      "(64, 33)\n",
      "step 12651, loss is 4.732586860656738\n",
      "(64, 33)\n",
      "step 12652, loss is 4.759763717651367\n",
      "(64, 33)\n",
      "step 12653, loss is 4.846001625061035\n",
      "(64, 33)\n",
      "step 12654, loss is 4.772500038146973\n",
      "(64, 33)\n",
      "step 12655, loss is 4.741182327270508\n",
      "(64, 33)\n",
      "step 12656, loss is 4.715520858764648\n",
      "(64, 33)\n",
      "step 12657, loss is 5.021594524383545\n",
      "(64, 33)\n",
      "step 12658, loss is 4.8384833335876465\n",
      "(64, 33)\n",
      "step 12659, loss is 4.851716995239258\n",
      "(64, 33)\n",
      "step 12660, loss is 4.885652542114258\n",
      "(64, 33)\n",
      "step 12661, loss is 4.81857967376709\n",
      "(64, 33)\n",
      "step 12662, loss is 4.832579135894775\n",
      "(64, 33)\n",
      "step 12663, loss is 4.94923734664917\n",
      "(64, 33)\n",
      "step 12664, loss is 4.834018230438232\n",
      "(64, 33)\n",
      "step 12665, loss is 4.890610694885254\n",
      "(64, 33)\n",
      "step 12666, loss is 4.81561803817749\n",
      "(64, 33)\n",
      "step 12667, loss is 4.802982330322266\n",
      "(64, 33)\n",
      "step 12668, loss is 4.722794055938721\n",
      "(64, 33)\n",
      "step 12669, loss is 4.789931297302246\n",
      "(64, 33)\n",
      "step 12670, loss is 4.758778095245361\n",
      "(64, 33)\n",
      "step 12671, loss is 4.762793064117432\n",
      "(64, 33)\n",
      "step 12672, loss is 4.836372375488281\n",
      "(64, 33)\n",
      "step 12673, loss is 4.8484368324279785\n",
      "(64, 33)\n",
      "step 12674, loss is 4.941749572753906\n",
      "(64, 33)\n",
      "step 12675, loss is 4.556694507598877\n",
      "(64, 33)\n",
      "step 12676, loss is 4.680115699768066\n",
      "(64, 33)\n",
      "step 12677, loss is 4.831090450286865\n",
      "(64, 33)\n",
      "step 12678, loss is 4.802328586578369\n",
      "(64, 33)\n",
      "step 12679, loss is 4.948007583618164\n",
      "(64, 33)\n",
      "step 12680, loss is 4.713822841644287\n",
      "(64, 33)\n",
      "step 12681, loss is 5.000767230987549\n",
      "(64, 33)\n",
      "step 12682, loss is 4.723381519317627\n",
      "(64, 33)\n",
      "step 12683, loss is 4.638566493988037\n",
      "(64, 33)\n",
      "step 12684, loss is 4.788618087768555\n",
      "(64, 33)\n",
      "step 12685, loss is 4.796484470367432\n",
      "(64, 33)\n",
      "step 12686, loss is 4.755472660064697\n",
      "(64, 33)\n",
      "step 12687, loss is 4.857968807220459\n",
      "(64, 33)\n",
      "step 12688, loss is 4.866405010223389\n",
      "(64, 33)\n",
      "step 12689, loss is 4.999494552612305\n",
      "(64, 33)\n",
      "step 12690, loss is 4.82460880279541\n",
      "(64, 33)\n",
      "step 12691, loss is 4.811613082885742\n",
      "(64, 33)\n",
      "step 12692, loss is 4.767248153686523\n",
      "(64, 33)\n",
      "step 12693, loss is 4.737814903259277\n",
      "(64, 33)\n",
      "step 12694, loss is 4.650585174560547\n",
      "(64, 33)\n",
      "step 12695, loss is 4.8116021156311035\n",
      "(64, 33)\n",
      "step 12696, loss is 4.651863098144531\n",
      "(64, 33)\n",
      "step 12697, loss is 4.832122802734375\n",
      "(64, 33)\n",
      "step 12698, loss is 4.833813667297363\n",
      "(64, 33)\n",
      "step 12699, loss is 4.856536865234375\n",
      "(64, 33)\n",
      "step 12700, loss is 4.928533554077148\n",
      "(64, 33)\n",
      "step 12701, loss is 4.913632392883301\n",
      "(64, 33)\n",
      "step 12702, loss is 4.811807155609131\n",
      "(64, 33)\n",
      "step 12703, loss is 4.898871421813965\n",
      "(64, 33)\n",
      "step 12704, loss is 4.74986457824707\n",
      "(64, 33)\n",
      "step 12705, loss is 4.943009853363037\n",
      "(64, 33)\n",
      "step 12706, loss is 4.840321063995361\n",
      "(64, 33)\n",
      "step 12707, loss is 4.727384090423584\n",
      "(64, 33)\n",
      "step 12708, loss is 4.846938610076904\n",
      "(64, 33)\n",
      "step 12709, loss is 4.980778694152832\n",
      "(64, 33)\n",
      "step 12710, loss is 4.946800231933594\n",
      "(64, 33)\n",
      "step 12711, loss is 4.79693078994751\n",
      "(64, 33)\n",
      "step 12712, loss is 4.723006248474121\n",
      "(64, 33)\n",
      "step 12713, loss is 4.77020263671875\n",
      "(64, 33)\n",
      "step 12714, loss is 4.936972618103027\n",
      "(64, 33)\n",
      "step 12715, loss is 4.785149097442627\n",
      "(64, 33)\n",
      "step 12716, loss is 4.7090535163879395\n",
      "(64, 33)\n",
      "step 12717, loss is 4.75104284286499\n",
      "(64, 33)\n",
      "step 12718, loss is 4.842236042022705\n",
      "(64, 33)\n",
      "step 12719, loss is 5.117021083831787\n",
      "(64, 33)\n",
      "step 12720, loss is 4.7797017097473145\n",
      "(64, 33)\n",
      "step 12721, loss is 4.868474006652832\n",
      "(64, 33)\n",
      "step 12722, loss is 4.707046031951904\n",
      "(64, 33)\n",
      "step 12723, loss is 4.806092262268066\n",
      "(64, 33)\n",
      "step 12724, loss is 4.743534564971924\n",
      "(64, 33)\n",
      "step 12725, loss is 4.699405670166016\n",
      "(64, 33)\n",
      "step 12726, loss is 4.740779399871826\n",
      "(64, 33)\n",
      "step 12727, loss is 4.807434558868408\n",
      "(64, 33)\n",
      "step 12728, loss is 4.855849742889404\n",
      "(64, 33)\n",
      "step 12729, loss is 4.914270401000977\n",
      "(64, 33)\n",
      "step 12730, loss is 4.758033752441406\n",
      "(64, 33)\n",
      "step 12731, loss is 4.7778096199035645\n",
      "(64, 33)\n",
      "step 12732, loss is 4.767480850219727\n",
      "(64, 33)\n",
      "step 12733, loss is 4.737705230712891\n",
      "(64, 33)\n",
      "step 12734, loss is 4.744878768920898\n",
      "(64, 33)\n",
      "step 12735, loss is 4.706132888793945\n",
      "(64, 33)\n",
      "step 12736, loss is 4.694395542144775\n",
      "(64, 33)\n",
      "step 12737, loss is 4.920607089996338\n",
      "(64, 33)\n",
      "step 12738, loss is 4.869362831115723\n",
      "(64, 33)\n",
      "step 12739, loss is 4.666464805603027\n",
      "(64, 33)\n",
      "step 12740, loss is 4.905989646911621\n",
      "(64, 33)\n",
      "step 12741, loss is 4.715737819671631\n",
      "(64, 33)\n",
      "step 12742, loss is 4.714107513427734\n",
      "(64, 33)\n",
      "step 12743, loss is 4.841001033782959\n",
      "(64, 33)\n",
      "step 12744, loss is 4.762115955352783\n",
      "(64, 33)\n",
      "step 12745, loss is 4.792149543762207\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12746, loss is 4.865628719329834\n",
      "(64, 33)\n",
      "step 12747, loss is 4.857595920562744\n",
      "(64, 33)\n",
      "step 12748, loss is 4.892301559448242\n",
      "(64, 33)\n",
      "step 12749, loss is 4.795314788818359\n",
      "(64, 33)\n",
      "step 12750, loss is 4.653222560882568\n",
      "(64, 33)\n",
      "step 12751, loss is 4.917439937591553\n",
      "(64, 33)\n",
      "step 12752, loss is 4.7541117668151855\n",
      "(64, 33)\n",
      "step 12753, loss is 4.830810546875\n",
      "(64, 33)\n",
      "step 12754, loss is 4.7046027183532715\n",
      "(64, 33)\n",
      "step 12755, loss is 4.919278144836426\n",
      "(64, 33)\n",
      "step 12756, loss is 4.776031970977783\n",
      "(64, 33)\n",
      "step 12757, loss is 4.872145175933838\n",
      "(64, 33)\n",
      "step 12758, loss is 4.857307434082031\n",
      "(64, 33)\n",
      "step 12759, loss is 4.945402145385742\n",
      "(64, 33)\n",
      "step 12760, loss is 4.6810431480407715\n",
      "(64, 33)\n",
      "step 12761, loss is 4.847788333892822\n",
      "(64, 33)\n",
      "step 12762, loss is 4.850102424621582\n",
      "(64, 33)\n",
      "step 12763, loss is 4.7013773918151855\n",
      "(64, 33)\n",
      "step 12764, loss is 4.849621772766113\n",
      "(64, 33)\n",
      "step 12765, loss is 5.052121162414551\n",
      "(64, 33)\n",
      "step 12766, loss is 4.833728790283203\n",
      "(64, 33)\n",
      "step 12767, loss is 4.726945400238037\n",
      "(64, 33)\n",
      "step 12768, loss is 4.797027111053467\n",
      "(64, 33)\n",
      "step 12769, loss is 4.818206310272217\n",
      "(64, 33)\n",
      "step 12770, loss is 4.753627300262451\n",
      "(64, 33)\n",
      "step 12771, loss is 4.670826435089111\n",
      "(64, 33)\n",
      "step 12772, loss is 4.881476879119873\n",
      "(64, 33)\n",
      "step 12773, loss is 4.6676716804504395\n",
      "(64, 33)\n",
      "step 12774, loss is 4.91762113571167\n",
      "(64, 33)\n",
      "step 12775, loss is 4.8583455085754395\n",
      "(64, 33)\n",
      "step 12776, loss is 4.7845306396484375\n",
      "(64, 33)\n",
      "step 12777, loss is 4.806589603424072\n",
      "(64, 33)\n",
      "step 12778, loss is 4.8258538246154785\n",
      "(64, 33)\n",
      "step 12779, loss is 4.741726875305176\n",
      "(64, 33)\n",
      "step 12780, loss is 4.746908664703369\n",
      "(64, 33)\n",
      "step 12781, loss is 4.8106279373168945\n",
      "(64, 33)\n",
      "step 12782, loss is 4.990794658660889\n",
      "(64, 33)\n",
      "step 12783, loss is 4.94661283493042\n",
      "(64, 33)\n",
      "step 12784, loss is 4.599828720092773\n",
      "(64, 33)\n",
      "step 12785, loss is 4.893026351928711\n",
      "(64, 33)\n",
      "step 12786, loss is 4.961941719055176\n",
      "(64, 33)\n",
      "step 12787, loss is 4.732234954833984\n",
      "(64, 33)\n",
      "step 12788, loss is 4.772797107696533\n",
      "(64, 33)\n",
      "step 12789, loss is 4.8878493309021\n",
      "(64, 33)\n",
      "step 12790, loss is 4.904565334320068\n",
      "(64, 33)\n",
      "step 12791, loss is 4.915404319763184\n",
      "(64, 33)\n",
      "step 12792, loss is 5.030852317810059\n",
      "(64, 33)\n",
      "step 12793, loss is 4.961801052093506\n",
      "(64, 33)\n",
      "step 12794, loss is 4.857553005218506\n",
      "(64, 33)\n",
      "step 12795, loss is 4.817825794219971\n",
      "(64, 33)\n",
      "step 12796, loss is 4.823665618896484\n",
      "(64, 33)\n",
      "step 12797, loss is 4.808927059173584\n",
      "(64, 33)\n",
      "step 12798, loss is 4.766400337219238\n",
      "(64, 33)\n",
      "step 12799, loss is 4.945274353027344\n",
      "(64, 33)\n",
      "step 12800, loss is 4.828976631164551\n",
      "(64, 33)\n",
      "step 12801, loss is 4.827830791473389\n",
      "(64, 33)\n",
      "step 12802, loss is 4.920923709869385\n",
      "(64, 33)\n",
      "step 12803, loss is 4.6697163581848145\n",
      "(64, 33)\n",
      "step 12804, loss is 4.872627258300781\n",
      "(64, 33)\n",
      "step 12805, loss is 4.792643070220947\n",
      "(64, 33)\n",
      "step 12806, loss is 4.787450790405273\n",
      "(64, 33)\n",
      "step 12807, loss is 4.777737617492676\n",
      "(64, 33)\n",
      "step 12808, loss is 4.700507164001465\n",
      "(64, 33)\n",
      "step 12809, loss is 4.796675205230713\n",
      "(64, 33)\n",
      "step 12810, loss is 4.771076202392578\n",
      "(64, 33)\n",
      "step 12811, loss is 4.916072368621826\n",
      "(64, 33)\n",
      "step 12812, loss is 4.966890335083008\n",
      "(64, 33)\n",
      "step 12813, loss is 4.746825218200684\n",
      "(64, 33)\n",
      "step 12814, loss is 4.787372589111328\n",
      "(64, 33)\n",
      "step 12815, loss is 5.021474361419678\n",
      "(64, 33)\n",
      "step 12816, loss is 4.776850700378418\n",
      "(64, 33)\n",
      "step 12817, loss is 4.838223934173584\n",
      "(64, 33)\n",
      "step 12818, loss is 4.6544575691223145\n",
      "(64, 33)\n",
      "step 12819, loss is 4.704065799713135\n",
      "(64, 33)\n",
      "step 12820, loss is 4.733107089996338\n",
      "(64, 33)\n",
      "step 12821, loss is 4.770594596862793\n",
      "(64, 33)\n",
      "step 12822, loss is 4.935795307159424\n",
      "(64, 33)\n",
      "step 12823, loss is 4.774848937988281\n",
      "(64, 33)\n",
      "step 12824, loss is 4.918668746948242\n",
      "(64, 33)\n",
      "step 12825, loss is 4.672304630279541\n",
      "(64, 33)\n",
      "step 12826, loss is 4.960336685180664\n",
      "(64, 33)\n",
      "step 12827, loss is 4.935895919799805\n",
      "(64, 33)\n",
      "step 12828, loss is 4.993511199951172\n",
      "(64, 33)\n",
      "step 12829, loss is 4.738517761230469\n",
      "(64, 33)\n",
      "step 12830, loss is 4.876930236816406\n",
      "(64, 33)\n",
      "step 12831, loss is 4.869706630706787\n",
      "(64, 33)\n",
      "step 12832, loss is 4.892355918884277\n",
      "(64, 33)\n",
      "step 12833, loss is 4.809813976287842\n",
      "(64, 33)\n",
      "step 12834, loss is 4.781446933746338\n",
      "(64, 33)\n",
      "step 12835, loss is 4.806079864501953\n",
      "(64, 33)\n",
      "step 12836, loss is 4.783087730407715\n",
      "(64, 33)\n",
      "step 12837, loss is 4.870516300201416\n",
      "(64, 33)\n",
      "step 12838, loss is 4.795022964477539\n",
      "(64, 33)\n",
      "step 12839, loss is 4.802652359008789\n",
      "(64, 33)\n",
      "step 12840, loss is 4.833478927612305\n",
      "(64, 33)\n",
      "step 12841, loss is 4.7754340171813965\n",
      "(64, 33)\n",
      "step 12842, loss is 4.761492729187012\n",
      "(64, 33)\n",
      "step 12843, loss is 4.648922920227051\n",
      "(64, 33)\n",
      "step 12844, loss is 4.841520309448242\n",
      "(64, 33)\n",
      "step 12845, loss is 4.841026306152344\n",
      "(64, 33)\n",
      "step 12846, loss is 4.802925109863281\n",
      "(64, 33)\n",
      "step 12847, loss is 4.637367248535156\n",
      "(64, 33)\n",
      "step 12848, loss is 4.679449081420898\n",
      "(64, 33)\n",
      "step 12849, loss is 4.778059005737305\n",
      "(64, 33)\n",
      "step 12850, loss is 4.879781723022461\n",
      "(64, 33)\n",
      "step 12851, loss is 4.862245559692383\n",
      "(64, 33)\n",
      "step 12852, loss is 4.619605541229248\n",
      "(64, 33)\n",
      "step 12853, loss is 4.804654121398926\n",
      "(64, 33)\n",
      "step 12854, loss is 4.963251113891602\n",
      "(64, 33)\n",
      "step 12855, loss is 4.709752559661865\n",
      "(64, 33)\n",
      "step 12856, loss is 5.139405727386475\n",
      "(64, 33)\n",
      "step 12857, loss is 4.750511169433594\n",
      "(64, 33)\n",
      "step 12858, loss is 4.6473541259765625\n",
      "(64, 33)\n",
      "step 12859, loss is 4.881878852844238\n",
      "(64, 33)\n",
      "step 12860, loss is 4.649209976196289\n",
      "(64, 33)\n",
      "step 12861, loss is 5.092117786407471\n",
      "(64, 33)\n",
      "step 12862, loss is 4.794045925140381\n",
      "(64, 33)\n",
      "step 12863, loss is 4.845211982727051\n",
      "(64, 33)\n",
      "step 12864, loss is 5.003757953643799\n",
      "(64, 33)\n",
      "step 12865, loss is 5.005285739898682\n",
      "(64, 33)\n",
      "step 12866, loss is 4.749144554138184\n",
      "(64, 33)\n",
      "step 12867, loss is 4.8234944343566895\n",
      "(64, 33)\n",
      "step 12868, loss is 4.864101409912109\n",
      "(64, 33)\n",
      "step 12869, loss is 4.531088829040527\n",
      "(64, 33)\n",
      "step 12870, loss is 4.983806133270264\n",
      "(64, 33)\n",
      "step 12871, loss is 4.839408874511719\n",
      "(64, 33)\n",
      "step 12872, loss is 5.048958778381348\n",
      "(64, 33)\n",
      "step 12873, loss is 4.867451190948486\n",
      "(64, 33)\n",
      "step 12874, loss is 4.708315849304199\n",
      "(64, 33)\n",
      "step 12875, loss is 4.767726898193359\n",
      "(64, 33)\n",
      "step 12876, loss is 4.792722702026367\n",
      "(64, 33)\n",
      "step 12877, loss is 4.8464837074279785\n",
      "(64, 33)\n",
      "step 12878, loss is 4.753362655639648\n",
      "(64, 33)\n",
      "step 12879, loss is 4.9807305335998535\n",
      "(64, 33)\n",
      "step 12880, loss is 4.591068267822266\n",
      "(64, 33)\n",
      "step 12881, loss is 4.902117729187012\n",
      "(64, 33)\n",
      "step 12882, loss is 4.919702529907227\n",
      "(64, 33)\n",
      "step 12883, loss is 4.759515285491943\n",
      "(64, 33)\n",
      "step 12884, loss is 4.636611461639404\n",
      "(64, 33)\n",
      "step 12885, loss is 4.955783367156982\n",
      "(64, 33)\n",
      "step 12886, loss is 4.795134544372559\n",
      "(64, 33)\n",
      "step 12887, loss is 4.571014881134033\n",
      "(64, 33)\n",
      "step 12888, loss is 4.6308417320251465\n",
      "(64, 33)\n",
      "step 12889, loss is 4.703139305114746\n",
      "(64, 33)\n",
      "step 12890, loss is 4.775362491607666\n",
      "(64, 33)\n",
      "step 12891, loss is 4.8187408447265625\n",
      "(64, 33)\n",
      "step 12892, loss is 4.73148250579834\n",
      "(64, 33)\n",
      "step 12893, loss is 4.911245346069336\n",
      "(64, 33)\n",
      "step 12894, loss is 4.807868957519531\n",
      "(64, 33)\n",
      "step 12895, loss is 4.885054111480713\n",
      "(64, 33)\n",
      "step 12896, loss is 4.8225016593933105\n",
      "(64, 33)\n",
      "step 12897, loss is 4.9568328857421875\n",
      "(64, 33)\n",
      "step 12898, loss is 4.881487846374512\n",
      "(64, 33)\n",
      "step 12899, loss is 4.733591079711914\n",
      "(64, 33)\n",
      "step 12900, loss is 4.7226481437683105\n",
      "(64, 33)\n",
      "step 12901, loss is 4.623219013214111\n",
      "(64, 33)\n",
      "step 12902, loss is 4.841139316558838\n",
      "(64, 33)\n",
      "step 12903, loss is 4.730660915374756\n",
      "(64, 33)\n",
      "step 12904, loss is 4.736320495605469\n",
      "(64, 33)\n",
      "step 12905, loss is 4.745384216308594\n",
      "(64, 33)\n",
      "step 12906, loss is 5.0745720863342285\n",
      "(64, 33)\n",
      "step 12907, loss is 4.835515022277832\n",
      "(64, 33)\n",
      "step 12908, loss is 4.890951156616211\n",
      "(64, 33)\n",
      "step 12909, loss is 4.806789398193359\n",
      "(64, 33)\n",
      "step 12910, loss is 4.634788513183594\n",
      "(64, 33)\n",
      "step 12911, loss is 4.850639343261719\n",
      "(64, 33)\n",
      "step 12912, loss is 4.873498916625977\n",
      "(64, 33)\n",
      "step 12913, loss is 4.638807773590088\n",
      "(64, 33)\n",
      "step 12914, loss is 4.836374282836914\n",
      "(64, 33)\n",
      "step 12915, loss is 4.822052955627441\n",
      "(64, 33)\n",
      "step 12916, loss is 4.827181816101074\n",
      "(64, 33)\n",
      "step 12917, loss is 4.803385257720947\n",
      "(64, 33)\n",
      "step 12918, loss is 4.890242099761963\n",
      "(64, 33)\n",
      "step 12919, loss is 4.7979960441589355\n",
      "(64, 33)\n",
      "step 12920, loss is 4.808516502380371\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12921, loss is 5.070303916931152\n",
      "(64, 33)\n",
      "step 12922, loss is 4.761173248291016\n",
      "(64, 33)\n",
      "step 12923, loss is 4.837771892547607\n",
      "(64, 33)\n",
      "step 12924, loss is 4.754715919494629\n",
      "(64, 33)\n",
      "step 12925, loss is 4.631877422332764\n",
      "(64, 33)\n",
      "step 12926, loss is 4.853297233581543\n",
      "(64, 33)\n",
      "step 12927, loss is 4.798083782196045\n",
      "(64, 33)\n",
      "step 12928, loss is 4.6744232177734375\n",
      "(64, 33)\n",
      "step 12929, loss is 4.861664295196533\n",
      "(64, 33)\n",
      "step 12930, loss is 4.638949394226074\n",
      "(64, 33)\n",
      "step 12931, loss is 4.845888137817383\n",
      "(64, 33)\n",
      "step 12932, loss is 4.649313449859619\n",
      "(64, 33)\n",
      "step 12933, loss is 4.970795631408691\n",
      "(64, 33)\n",
      "step 12934, loss is 4.794061183929443\n",
      "(64, 33)\n",
      "step 12935, loss is 4.893399238586426\n",
      "(64, 33)\n",
      "step 12936, loss is 4.667527675628662\n",
      "(64, 33)\n",
      "step 12937, loss is 4.8843302726745605\n",
      "(64, 33)\n",
      "step 12938, loss is 4.929973125457764\n",
      "(64, 33)\n",
      "step 12939, loss is 4.951044082641602\n",
      "(64, 33)\n",
      "step 12940, loss is 4.833580493927002\n",
      "(64, 33)\n",
      "step 12941, loss is 4.874472618103027\n",
      "(64, 33)\n",
      "step 12942, loss is 4.732382297515869\n",
      "(64, 33)\n",
      "step 12943, loss is 4.833431243896484\n",
      "(64, 33)\n",
      "step 12944, loss is 4.925772666931152\n",
      "(64, 33)\n",
      "step 12945, loss is 4.86439323425293\n",
      "(64, 33)\n",
      "step 12946, loss is 4.923468589782715\n",
      "(64, 33)\n",
      "step 12947, loss is 4.69374942779541\n",
      "(64, 33)\n",
      "step 12948, loss is 4.939975261688232\n",
      "(64, 33)\n",
      "step 12949, loss is 4.799441814422607\n",
      "(64, 33)\n",
      "step 12950, loss is 4.704967975616455\n",
      "(64, 33)\n",
      "step 12951, loss is 4.822572231292725\n",
      "(64, 33)\n",
      "step 12952, loss is 4.620976448059082\n",
      "(64, 33)\n",
      "step 12953, loss is 4.691612243652344\n",
      "(64, 33)\n",
      "step 12954, loss is 4.761724948883057\n",
      "(64, 33)\n",
      "step 12955, loss is 4.769903659820557\n",
      "(64, 33)\n",
      "step 12956, loss is 4.938880443572998\n",
      "(64, 33)\n",
      "step 12957, loss is 4.764140605926514\n",
      "(64, 33)\n",
      "step 12958, loss is 4.747555732727051\n",
      "(64, 33)\n",
      "step 12959, loss is 4.668275356292725\n",
      "(64, 33)\n",
      "step 12960, loss is 4.760565280914307\n",
      "(64, 33)\n",
      "step 12961, loss is 4.933650016784668\n",
      "(64, 33)\n",
      "step 12962, loss is 4.737260818481445\n",
      "(64, 33)\n",
      "step 12963, loss is 4.7247138023376465\n",
      "(64, 33)\n",
      "step 12964, loss is 4.7252302169799805\n",
      "(64, 33)\n",
      "step 12965, loss is 4.627247333526611\n",
      "(64, 33)\n",
      "step 12966, loss is 4.928507328033447\n",
      "(64, 33)\n",
      "step 12967, loss is 4.805665969848633\n",
      "(64, 33)\n",
      "step 12968, loss is 5.079285621643066\n",
      "(64, 33)\n",
      "step 12969, loss is 4.773166656494141\n",
      "(64, 33)\n",
      "step 12970, loss is 4.92943000793457\n",
      "(64, 33)\n",
      "step 12971, loss is 4.754624366760254\n",
      "(64, 33)\n",
      "step 12972, loss is 4.736908435821533\n",
      "(64, 33)\n",
      "step 12973, loss is 4.697822570800781\n",
      "(64, 33)\n",
      "step 12974, loss is 4.749964714050293\n",
      "(64, 33)\n",
      "step 12975, loss is 4.941374778747559\n",
      "(64, 33)\n",
      "step 12976, loss is 4.869908809661865\n",
      "(64, 33)\n",
      "step 12977, loss is 4.658511638641357\n",
      "(64, 33)\n",
      "step 12978, loss is 4.788197994232178\n",
      "(64, 33)\n",
      "step 12979, loss is 4.708709716796875\n",
      "(64, 33)\n",
      "step 12980, loss is 4.825673580169678\n",
      "(64, 33)\n",
      "step 12981, loss is 4.8662872314453125\n",
      "(64, 33)\n",
      "step 12982, loss is 4.815260410308838\n",
      "(64, 33)\n",
      "step 12983, loss is 4.831442356109619\n",
      "(64, 33)\n",
      "step 12984, loss is 4.886556148529053\n",
      "(64, 33)\n",
      "step 12985, loss is 4.7490010261535645\n",
      "(64, 33)\n",
      "step 12986, loss is 4.907821178436279\n",
      "(64, 33)\n",
      "step 12987, loss is 4.717716217041016\n",
      "(64, 33)\n",
      "step 12988, loss is 4.78679084777832\n",
      "(64, 33)\n",
      "step 12989, loss is 5.013588905334473\n",
      "(64, 33)\n",
      "step 12990, loss is 4.69033670425415\n",
      "(64, 33)\n",
      "step 12991, loss is 4.796889781951904\n",
      "(64, 33)\n",
      "step 12992, loss is 4.615469455718994\n",
      "(64, 33)\n",
      "step 12993, loss is 4.7855143547058105\n",
      "(64, 33)\n",
      "step 12994, loss is 5.027681827545166\n",
      "(64, 33)\n",
      "step 12995, loss is 4.939880847930908\n",
      "(64, 33)\n",
      "step 12996, loss is 4.647744655609131\n",
      "(64, 33)\n",
      "step 12997, loss is 4.726193904876709\n",
      "(64, 33)\n",
      "step 12998, loss is 4.887285232543945\n",
      "(64, 33)\n",
      "step 12999, loss is 4.849246025085449\n",
      "(64, 33)\n",
      "step 13000, loss is 4.491322994232178\n",
      "(64, 33)\n",
      "step 13001, loss is 4.8508992195129395\n",
      "(64, 33)\n",
      "step 13002, loss is 4.770646572113037\n",
      "(64, 33)\n",
      "step 13003, loss is 4.659486770629883\n",
      "(64, 33)\n",
      "step 13004, loss is 4.754672527313232\n",
      "(64, 33)\n",
      "step 13005, loss is 4.733200550079346\n",
      "(64, 33)\n",
      "step 13006, loss is 4.861587047576904\n",
      "(64, 33)\n",
      "step 13007, loss is 4.975131511688232\n",
      "(64, 33)\n",
      "step 13008, loss is 4.767906188964844\n",
      "(64, 33)\n",
      "step 13009, loss is 4.6608757972717285\n",
      "(64, 33)\n",
      "step 13010, loss is 4.782315254211426\n",
      "(64, 33)\n",
      "step 13011, loss is 4.831665992736816\n",
      "(64, 33)\n",
      "step 13012, loss is 4.925663471221924\n",
      "(64, 33)\n",
      "step 13013, loss is 4.765950679779053\n",
      "(64, 33)\n",
      "step 13014, loss is 4.793651580810547\n",
      "(64, 33)\n",
      "step 13015, loss is 4.723647117614746\n",
      "(64, 33)\n",
      "step 13016, loss is 4.902719497680664\n",
      "(64, 33)\n",
      "step 13017, loss is 4.710476875305176\n",
      "(64, 33)\n",
      "step 13018, loss is 4.799858093261719\n",
      "(64, 33)\n",
      "step 13019, loss is 4.898787975311279\n",
      "(64, 33)\n",
      "step 13020, loss is 4.770668029785156\n",
      "(64, 33)\n",
      "step 13021, loss is 4.78190803527832\n",
      "(64, 33)\n",
      "step 13022, loss is 4.819825172424316\n",
      "(64, 33)\n",
      "step 13023, loss is 4.866298198699951\n",
      "(64, 33)\n",
      "step 13024, loss is 4.676244735717773\n",
      "(64, 33)\n",
      "step 13025, loss is 4.896720886230469\n",
      "(64, 33)\n",
      "step 13026, loss is 4.8143439292907715\n",
      "(64, 33)\n",
      "step 13027, loss is 4.988263130187988\n",
      "(64, 33)\n",
      "step 13028, loss is 4.702794551849365\n",
      "(64, 33)\n",
      "step 13029, loss is 4.9718241691589355\n",
      "(64, 33)\n",
      "step 13030, loss is 4.746342182159424\n",
      "(64, 33)\n",
      "step 13031, loss is 4.934257507324219\n",
      "(64, 33)\n",
      "step 13032, loss is 4.76682710647583\n",
      "(64, 33)\n",
      "step 13033, loss is 4.773346424102783\n",
      "(64, 33)\n",
      "step 13034, loss is 4.713226318359375\n",
      "(64, 33)\n",
      "step 13035, loss is 4.765180587768555\n",
      "(64, 33)\n",
      "step 13036, loss is 4.659450531005859\n",
      "(64, 33)\n",
      "step 13037, loss is 4.631072521209717\n",
      "(64, 33)\n",
      "step 13038, loss is 4.982912063598633\n",
      "(64, 33)\n",
      "step 13039, loss is 4.823262691497803\n",
      "(64, 33)\n",
      "step 13040, loss is 4.750588417053223\n",
      "(64, 33)\n",
      "step 13041, loss is 4.954872131347656\n",
      "(64, 33)\n",
      "step 13042, loss is 4.795237064361572\n",
      "(64, 33)\n",
      "step 13043, loss is 4.771433353424072\n",
      "(64, 33)\n",
      "step 13044, loss is 4.670809745788574\n",
      "(64, 33)\n",
      "step 13045, loss is 4.759943008422852\n",
      "(64, 33)\n",
      "step 13046, loss is 4.728692054748535\n",
      "(64, 33)\n",
      "step 13047, loss is 4.83284854888916\n",
      "(64, 33)\n",
      "step 13048, loss is 4.8257598876953125\n",
      "(64, 33)\n",
      "step 13049, loss is 4.922285079956055\n",
      "(64, 33)\n",
      "step 13050, loss is 4.671872615814209\n",
      "(64, 33)\n",
      "step 13051, loss is 4.734898090362549\n",
      "(64, 33)\n",
      "step 13052, loss is 4.7882080078125\n",
      "(64, 33)\n",
      "step 13053, loss is 4.678910255432129\n",
      "(64, 33)\n",
      "step 13054, loss is 4.917644023895264\n",
      "(64, 33)\n",
      "step 13055, loss is 4.834622383117676\n",
      "(64, 33)\n",
      "step 13056, loss is 4.77220344543457\n",
      "(64, 33)\n",
      "step 13057, loss is 4.900326728820801\n",
      "(64, 33)\n",
      "step 13058, loss is 4.7606964111328125\n",
      "(64, 33)\n",
      "step 13059, loss is 4.772735118865967\n",
      "(64, 33)\n",
      "step 13060, loss is 4.704759120941162\n",
      "(64, 33)\n",
      "step 13061, loss is 4.771450042724609\n",
      "(64, 33)\n",
      "step 13062, loss is 4.82366943359375\n",
      "(64, 33)\n",
      "step 13063, loss is 4.65894889831543\n",
      "(64, 33)\n",
      "step 13064, loss is 4.7898173332214355\n",
      "(64, 33)\n",
      "step 13065, loss is 4.7355637550354\n",
      "(64, 33)\n",
      "step 13066, loss is 4.740728378295898\n",
      "(64, 33)\n",
      "step 13067, loss is 4.771590232849121\n",
      "(64, 33)\n",
      "step 13068, loss is 4.7915730476379395\n",
      "(64, 33)\n",
      "step 13069, loss is 4.825967311859131\n",
      "(64, 33)\n",
      "step 13070, loss is 4.825941562652588\n",
      "(64, 33)\n",
      "step 13071, loss is 4.745739936828613\n",
      "(64, 33)\n",
      "step 13072, loss is 4.892426013946533\n",
      "(64, 33)\n",
      "step 13073, loss is 4.965795516967773\n",
      "(64, 33)\n",
      "step 13074, loss is 4.687253475189209\n",
      "(64, 33)\n",
      "step 13075, loss is 4.884969234466553\n",
      "(64, 33)\n",
      "step 13076, loss is 4.549915790557861\n",
      "(64, 33)\n",
      "step 13077, loss is 4.987969875335693\n",
      "(64, 33)\n",
      "step 13078, loss is 4.764402389526367\n",
      "(64, 33)\n",
      "step 13079, loss is 4.714660167694092\n",
      "(64, 33)\n",
      "step 13080, loss is 4.823544502258301\n",
      "(64, 33)\n",
      "step 13081, loss is 4.798011302947998\n",
      "(64, 33)\n",
      "step 13082, loss is 4.798557281494141\n",
      "(64, 33)\n",
      "step 13083, loss is 4.96732759475708\n",
      "(64, 33)\n",
      "step 13084, loss is 4.762999057769775\n",
      "(64, 33)\n",
      "step 13085, loss is 4.85650634765625\n",
      "(64, 33)\n",
      "step 13086, loss is 4.885958194732666\n",
      "(64, 33)\n",
      "step 13087, loss is 4.732457637786865\n",
      "(64, 33)\n",
      "step 13088, loss is 4.769819259643555\n",
      "(64, 33)\n",
      "step 13089, loss is 4.764806270599365\n",
      "(64, 33)\n",
      "step 13090, loss is 4.89739990234375\n",
      "(64, 33)\n",
      "step 13091, loss is 4.7492289543151855\n",
      "(64, 33)\n",
      "step 13092, loss is 4.530685901641846\n",
      "(64, 33)\n",
      "step 13093, loss is 4.828507900238037\n",
      "(64, 33)\n",
      "step 13094, loss is 4.805025577545166\n",
      "(64, 33)\n",
      "step 13095, loss is 4.985637664794922\n",
      "(64, 33)\n",
      "step 13096, loss is 4.890539646148682\n",
      "(64, 33)\n",
      "step 13097, loss is 4.8222784996032715\n",
      "(64, 33)\n",
      "step 13098, loss is 4.784876823425293\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13099, loss is 4.787001132965088\n",
      "(64, 33)\n",
      "step 13100, loss is 4.7214765548706055\n",
      "(64, 33)\n",
      "step 13101, loss is 4.823147773742676\n",
      "(64, 33)\n",
      "step 13102, loss is 4.802441120147705\n",
      "(64, 33)\n",
      "step 13103, loss is 4.923981189727783\n",
      "(64, 33)\n",
      "step 13104, loss is 4.773651123046875\n",
      "(64, 33)\n",
      "step 13105, loss is 4.7870049476623535\n",
      "(64, 33)\n",
      "step 13106, loss is 4.688104629516602\n",
      "(64, 33)\n",
      "step 13107, loss is 4.859941005706787\n",
      "(64, 33)\n",
      "step 13108, loss is 4.640483379364014\n",
      "(64, 33)\n",
      "step 13109, loss is 4.912839889526367\n",
      "(64, 33)\n",
      "step 13110, loss is 4.777257442474365\n",
      "(64, 33)\n",
      "step 13111, loss is 4.735402584075928\n",
      "(64, 33)\n",
      "step 13112, loss is 5.052412033081055\n",
      "(64, 33)\n",
      "step 13113, loss is 4.7891764640808105\n",
      "(64, 33)\n",
      "step 13114, loss is 4.98970365524292\n",
      "(64, 33)\n",
      "step 13115, loss is 4.860379695892334\n",
      "(64, 33)\n",
      "step 13116, loss is 4.953550338745117\n",
      "(64, 33)\n",
      "step 13117, loss is 4.650656223297119\n",
      "(64, 33)\n",
      "step 13118, loss is 4.95288610458374\n",
      "(64, 33)\n",
      "step 13119, loss is 4.666243553161621\n",
      "(64, 33)\n",
      "step 13120, loss is 4.717174053192139\n",
      "(64, 33)\n",
      "step 13121, loss is 4.7678351402282715\n",
      "(64, 33)\n",
      "step 13122, loss is 4.731374263763428\n",
      "(64, 33)\n",
      "step 13123, loss is 5.079129219055176\n",
      "(64, 33)\n",
      "step 13124, loss is 4.986661911010742\n",
      "(64, 33)\n",
      "step 13125, loss is 4.940542697906494\n",
      "(64, 33)\n",
      "step 13126, loss is 4.797174453735352\n",
      "(64, 33)\n",
      "step 13127, loss is 4.885923862457275\n",
      "(64, 33)\n",
      "step 13128, loss is 4.84768009185791\n",
      "(64, 33)\n",
      "step 13129, loss is 4.656831741333008\n",
      "(64, 33)\n",
      "step 13130, loss is 4.629530906677246\n",
      "(64, 33)\n",
      "step 13131, loss is 4.492875576019287\n",
      "(64, 33)\n",
      "step 13132, loss is 4.931008815765381\n",
      "(64, 33)\n",
      "step 13133, loss is 4.8313093185424805\n",
      "(64, 33)\n",
      "step 13134, loss is 4.801490306854248\n",
      "(64, 33)\n",
      "step 13135, loss is 4.9229302406311035\n",
      "(64, 33)\n",
      "step 13136, loss is 4.875451564788818\n",
      "(64, 33)\n",
      "step 13137, loss is 4.749111175537109\n",
      "(64, 33)\n",
      "step 13138, loss is 4.6561102867126465\n",
      "(64, 33)\n",
      "step 13139, loss is 4.754716396331787\n",
      "(64, 33)\n",
      "step 13140, loss is 4.672872543334961\n",
      "(64, 33)\n",
      "step 13141, loss is 4.89573335647583\n",
      "(64, 33)\n",
      "step 13142, loss is 4.9241437911987305\n",
      "(64, 33)\n",
      "step 13143, loss is 4.685791969299316\n",
      "(64, 33)\n",
      "step 13144, loss is 4.794865131378174\n",
      "(64, 33)\n",
      "step 13145, loss is 4.795976161956787\n",
      "(64, 33)\n",
      "step 13146, loss is 4.75664758682251\n",
      "(64, 33)\n",
      "step 13147, loss is 4.810342788696289\n",
      "(64, 33)\n",
      "step 13148, loss is 4.967076301574707\n",
      "(64, 33)\n",
      "step 13149, loss is 4.785279273986816\n",
      "(64, 33)\n",
      "step 13150, loss is 4.8487019538879395\n",
      "(64, 33)\n",
      "step 13151, loss is 4.792789459228516\n",
      "(64, 33)\n",
      "step 13152, loss is 4.7206645011901855\n",
      "(64, 33)\n",
      "step 13153, loss is 4.91159725189209\n",
      "(64, 33)\n",
      "step 13154, loss is 4.8888702392578125\n",
      "(64, 33)\n",
      "step 13155, loss is 4.72836446762085\n",
      "(64, 33)\n",
      "step 13156, loss is 4.744529724121094\n",
      "(64, 33)\n",
      "step 13157, loss is 4.913738250732422\n",
      "(64, 33)\n",
      "step 13158, loss is 4.712247848510742\n",
      "(64, 33)\n",
      "step 13159, loss is 4.8834228515625\n",
      "(64, 33)\n",
      "step 13160, loss is 4.81554651260376\n",
      "(64, 33)\n",
      "step 13161, loss is 4.780215263366699\n",
      "(64, 33)\n",
      "step 13162, loss is 4.78598690032959\n",
      "(64, 33)\n",
      "step 13163, loss is 4.70620059967041\n",
      "(64, 33)\n",
      "step 13164, loss is 4.71241569519043\n",
      "(64, 33)\n",
      "step 13165, loss is 4.999534606933594\n",
      "(64, 33)\n",
      "step 13166, loss is 4.889368057250977\n",
      "(64, 33)\n",
      "step 13167, loss is 4.828267574310303\n",
      "(64, 33)\n",
      "step 13168, loss is 4.942486763000488\n",
      "(64, 33)\n",
      "step 13169, loss is 4.944427967071533\n",
      "(64, 33)\n",
      "step 13170, loss is 4.808048725128174\n",
      "(64, 33)\n",
      "step 13171, loss is 4.624181747436523\n",
      "(64, 33)\n",
      "step 13172, loss is 4.639284610748291\n",
      "(64, 33)\n",
      "step 13173, loss is 4.823151588439941\n",
      "(64, 33)\n",
      "step 13174, loss is 4.857633590698242\n",
      "(64, 33)\n",
      "step 13175, loss is 4.861514091491699\n",
      "(64, 33)\n",
      "step 13176, loss is 4.551568031311035\n",
      "(64, 33)\n",
      "step 13177, loss is 4.979135990142822\n",
      "(64, 33)\n",
      "step 13178, loss is 4.853522300720215\n",
      "(64, 33)\n",
      "step 13179, loss is 4.7810444831848145\n",
      "(64, 33)\n",
      "step 13180, loss is 4.998007297515869\n",
      "(64, 33)\n",
      "step 13181, loss is 4.7903008460998535\n",
      "(64, 33)\n",
      "step 13182, loss is 5.015585899353027\n",
      "(64, 33)\n",
      "step 13183, loss is 4.859316825866699\n",
      "(64, 33)\n",
      "step 13184, loss is 4.808409214019775\n",
      "(64, 33)\n",
      "step 13185, loss is 4.821156024932861\n",
      "(64, 33)\n",
      "step 13186, loss is 4.802891731262207\n",
      "(64, 33)\n",
      "step 13187, loss is 4.8831095695495605\n",
      "(64, 33)\n",
      "step 13188, loss is 4.8564066886901855\n",
      "(64, 33)\n",
      "step 13189, loss is 5.05403470993042\n",
      "(64, 33)\n",
      "step 13190, loss is 4.808652877807617\n",
      "(64, 33)\n",
      "step 13191, loss is 4.738403797149658\n",
      "(64, 33)\n",
      "step 13192, loss is 4.837963104248047\n",
      "(64, 33)\n",
      "step 13193, loss is 4.874536991119385\n",
      "(64, 33)\n",
      "step 13194, loss is 4.815647125244141\n",
      "(64, 33)\n",
      "step 13195, loss is 4.836749076843262\n",
      "(64, 33)\n",
      "step 13196, loss is 4.972648620605469\n",
      "(64, 33)\n",
      "step 13197, loss is 4.748274803161621\n",
      "(64, 33)\n",
      "step 13198, loss is 4.709226608276367\n",
      "(64, 33)\n",
      "step 13199, loss is 4.837656497955322\n",
      "(64, 33)\n",
      "step 13200, loss is 4.678736686706543\n",
      "(64, 33)\n",
      "step 13201, loss is 4.943800449371338\n",
      "(64, 33)\n",
      "step 13202, loss is 4.782878398895264\n",
      "(64, 33)\n",
      "step 13203, loss is 4.761198043823242\n",
      "(64, 33)\n",
      "step 13204, loss is 4.768056869506836\n",
      "(64, 33)\n",
      "step 13205, loss is 4.696405410766602\n",
      "(64, 33)\n",
      "step 13206, loss is 4.905370712280273\n",
      "(64, 33)\n",
      "step 13207, loss is 4.857450485229492\n",
      "(64, 33)\n",
      "step 13208, loss is 4.950392246246338\n",
      "(64, 33)\n",
      "step 13209, loss is 4.631096839904785\n",
      "(64, 33)\n",
      "step 13210, loss is 4.856101036071777\n",
      "(64, 33)\n",
      "step 13211, loss is 4.83999490737915\n",
      "(64, 33)\n",
      "step 13212, loss is 4.633677005767822\n",
      "(64, 33)\n",
      "step 13213, loss is 4.633835792541504\n",
      "(64, 33)\n",
      "step 13214, loss is 4.76142692565918\n",
      "(64, 33)\n",
      "step 13215, loss is 4.751319408416748\n",
      "(64, 33)\n",
      "step 13216, loss is 4.726962566375732\n",
      "(64, 33)\n",
      "step 13217, loss is 4.7850494384765625\n",
      "(64, 33)\n",
      "step 13218, loss is 4.7685770988464355\n",
      "(64, 33)\n",
      "step 13219, loss is 4.741305828094482\n",
      "(64, 33)\n",
      "step 13220, loss is 4.85502815246582\n",
      "(64, 33)\n",
      "step 13221, loss is 4.8477349281311035\n",
      "(64, 33)\n",
      "step 13222, loss is 5.084697246551514\n",
      "(64, 33)\n",
      "step 13223, loss is 4.898830413818359\n",
      "(64, 33)\n",
      "step 13224, loss is 4.885766983032227\n",
      "(64, 33)\n",
      "step 13225, loss is 4.711799144744873\n",
      "(64, 33)\n",
      "step 13226, loss is 4.774378299713135\n",
      "(64, 33)\n",
      "step 13227, loss is 4.841658592224121\n",
      "(64, 33)\n",
      "step 13228, loss is 4.956800937652588\n",
      "(64, 33)\n",
      "step 13229, loss is 4.945760726928711\n",
      "(64, 33)\n",
      "step 13230, loss is 4.879545211791992\n",
      "(64, 33)\n",
      "step 13231, loss is 4.869448184967041\n",
      "(64, 33)\n",
      "step 13232, loss is 4.828502655029297\n",
      "(64, 33)\n",
      "step 13233, loss is 4.800337314605713\n",
      "(64, 33)\n",
      "step 13234, loss is 4.884206771850586\n",
      "(64, 33)\n",
      "step 13235, loss is 4.767499923706055\n",
      "(64, 33)\n",
      "step 13236, loss is 4.786516189575195\n",
      "(64, 33)\n",
      "step 13237, loss is 4.716146945953369\n",
      "(64, 33)\n",
      "step 13238, loss is 4.959190845489502\n",
      "(64, 33)\n",
      "step 13239, loss is 4.7861456871032715\n",
      "(64, 33)\n",
      "step 13240, loss is 4.884913444519043\n",
      "(64, 33)\n",
      "step 13241, loss is 4.745962619781494\n",
      "(64, 33)\n",
      "step 13242, loss is 4.691955089569092\n",
      "(64, 33)\n",
      "step 13243, loss is 4.832025051116943\n",
      "(64, 33)\n",
      "step 13244, loss is 4.706974029541016\n",
      "(64, 33)\n",
      "step 13245, loss is 4.888355731964111\n",
      "(64, 33)\n",
      "step 13246, loss is 4.729954719543457\n",
      "(64, 33)\n",
      "step 13247, loss is 4.775667190551758\n",
      "(64, 33)\n",
      "step 13248, loss is 4.843627452850342\n",
      "(64, 33)\n",
      "step 13249, loss is 4.823652744293213\n",
      "(64, 33)\n",
      "step 13250, loss is 4.696859836578369\n",
      "(64, 33)\n",
      "step 13251, loss is 4.966734886169434\n",
      "(64, 33)\n",
      "step 13252, loss is 4.798838138580322\n",
      "(64, 33)\n",
      "step 13253, loss is 4.935554027557373\n",
      "(64, 33)\n",
      "step 13254, loss is 4.691372394561768\n",
      "(64, 33)\n",
      "step 13255, loss is 4.870211601257324\n",
      "(64, 33)\n",
      "step 13256, loss is 4.797307014465332\n",
      "(64, 33)\n",
      "step 13257, loss is 4.755677223205566\n",
      "(64, 33)\n",
      "step 13258, loss is 5.121814250946045\n",
      "(64, 33)\n",
      "step 13259, loss is 4.501030445098877\n",
      "(64, 33)\n",
      "step 13260, loss is 4.7704267501831055\n",
      "(64, 33)\n",
      "step 13261, loss is 4.693633556365967\n",
      "(64, 33)\n",
      "step 13262, loss is 4.894526958465576\n",
      "(64, 33)\n",
      "step 13263, loss is 4.723879337310791\n",
      "(64, 33)\n",
      "step 13264, loss is 4.872661113739014\n",
      "(64, 33)\n",
      "step 13265, loss is 4.697266578674316\n",
      "(64, 33)\n",
      "step 13266, loss is 4.80090856552124\n",
      "(64, 33)\n",
      "step 13267, loss is 4.7669477462768555\n",
      "(64, 33)\n",
      "step 13268, loss is 4.772995948791504\n",
      "(64, 33)\n",
      "step 13269, loss is 4.711211681365967\n",
      "(64, 33)\n",
      "step 13270, loss is 4.7021403312683105\n",
      "(64, 33)\n",
      "step 13271, loss is 4.974412441253662\n",
      "(64, 33)\n",
      "step 13272, loss is 4.872337341308594\n",
      "(64, 33)\n",
      "step 13273, loss is 4.9529337882995605\n",
      "(64, 33)\n",
      "step 13274, loss is 4.743404388427734\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13275, loss is 4.8743414878845215\n",
      "(64, 33)\n",
      "step 13276, loss is 4.929111003875732\n",
      "(64, 33)\n",
      "step 13277, loss is 4.8661723136901855\n",
      "(64, 33)\n",
      "step 13278, loss is 4.9483771324157715\n",
      "(64, 33)\n",
      "step 13279, loss is 4.787125587463379\n",
      "(64, 33)\n",
      "step 13280, loss is 5.007132530212402\n",
      "(64, 33)\n",
      "step 13281, loss is 4.7969536781311035\n",
      "(64, 33)\n",
      "step 13282, loss is 4.943026542663574\n",
      "(64, 33)\n",
      "step 13283, loss is 4.8569183349609375\n",
      "(64, 33)\n",
      "step 13284, loss is 4.725854873657227\n",
      "(64, 33)\n",
      "step 13285, loss is 5.025090217590332\n",
      "(64, 33)\n",
      "step 13286, loss is 4.828486442565918\n",
      "(64, 33)\n",
      "step 13287, loss is 4.901632785797119\n",
      "(64, 33)\n",
      "step 13288, loss is 4.776509761810303\n",
      "(64, 33)\n",
      "step 13289, loss is 4.8449320793151855\n",
      "(64, 33)\n",
      "step 13290, loss is 4.869479656219482\n",
      "(64, 33)\n",
      "step 13291, loss is 4.862302780151367\n",
      "(64, 33)\n",
      "step 13292, loss is 4.852126121520996\n",
      "(64, 33)\n",
      "step 13293, loss is 4.809182167053223\n",
      "(64, 33)\n",
      "step 13294, loss is 4.7133402824401855\n",
      "(64, 33)\n",
      "step 13295, loss is 4.648825168609619\n",
      "(64, 33)\n",
      "step 13296, loss is 4.836172580718994\n",
      "(64, 33)\n",
      "step 13297, loss is 5.0722270011901855\n",
      "(64, 33)\n",
      "step 13298, loss is 4.745298862457275\n",
      "(64, 33)\n",
      "step 13299, loss is 4.773278713226318\n",
      "(64, 33)\n",
      "step 13300, loss is 4.838006496429443\n",
      "(64, 33)\n",
      "step 13301, loss is 4.712210178375244\n",
      "(64, 33)\n",
      "step 13302, loss is 4.760381698608398\n",
      "(64, 33)\n",
      "step 13303, loss is 4.913763999938965\n",
      "(64, 33)\n",
      "step 13304, loss is 4.886163711547852\n",
      "(64, 33)\n",
      "step 13305, loss is 4.725192070007324\n",
      "(64, 33)\n",
      "step 13306, loss is 4.662993907928467\n",
      "(64, 33)\n",
      "step 13307, loss is 4.766063213348389\n",
      "(64, 33)\n",
      "step 13308, loss is 4.925363540649414\n",
      "(64, 33)\n",
      "step 13309, loss is 4.7030487060546875\n",
      "(64, 33)\n",
      "step 13310, loss is 4.843404769897461\n",
      "(64, 33)\n",
      "step 13311, loss is 4.842842102050781\n",
      "(64, 33)\n",
      "step 13312, loss is 4.731738090515137\n",
      "(64, 33)\n",
      "step 13313, loss is 4.745208740234375\n",
      "(64, 33)\n",
      "step 13314, loss is 4.923709869384766\n",
      "(64, 33)\n",
      "step 13315, loss is 4.756690979003906\n",
      "(64, 33)\n",
      "step 13316, loss is 4.733900547027588\n",
      "(64, 33)\n",
      "step 13317, loss is 4.877780914306641\n",
      "(64, 33)\n",
      "step 13318, loss is 4.907731056213379\n",
      "(64, 33)\n",
      "step 13319, loss is 4.800590991973877\n",
      "(64, 33)\n",
      "step 13320, loss is 4.860086917877197\n",
      "(64, 33)\n",
      "step 13321, loss is 4.791317939758301\n",
      "(64, 33)\n",
      "step 13322, loss is 4.835892200469971\n",
      "(64, 33)\n",
      "step 13323, loss is 4.739189147949219\n",
      "(64, 33)\n",
      "step 13324, loss is 5.052614212036133\n",
      "(64, 33)\n",
      "step 13325, loss is 4.792765140533447\n",
      "(64, 33)\n",
      "step 13326, loss is 4.8229804039001465\n",
      "(64, 33)\n",
      "step 13327, loss is 4.655862331390381\n",
      "(64, 33)\n",
      "step 13328, loss is 4.957983016967773\n",
      "(64, 33)\n",
      "step 13329, loss is 4.867242336273193\n",
      "(64, 33)\n",
      "step 13330, loss is 4.838956832885742\n",
      "(64, 33)\n",
      "step 13331, loss is 4.732668876647949\n",
      "(64, 33)\n",
      "step 13332, loss is 4.859889030456543\n",
      "(64, 33)\n",
      "step 13333, loss is 4.986207008361816\n",
      "(64, 33)\n",
      "step 13334, loss is 4.876559257507324\n",
      "(64, 33)\n",
      "step 13335, loss is 4.855918884277344\n",
      "(64, 33)\n",
      "step 13336, loss is 4.817674160003662\n",
      "(64, 33)\n",
      "step 13337, loss is 4.867855548858643\n",
      "(64, 33)\n",
      "step 13338, loss is 4.837611675262451\n",
      "(64, 33)\n",
      "step 13339, loss is 4.820992946624756\n",
      "(64, 33)\n",
      "step 13340, loss is 4.678107738494873\n",
      "(64, 33)\n",
      "step 13341, loss is 4.690854072570801\n",
      "(64, 33)\n",
      "step 13342, loss is 4.751186370849609\n",
      "(64, 33)\n",
      "step 13343, loss is 4.837377548217773\n",
      "(64, 33)\n",
      "step 13344, loss is 5.01857852935791\n",
      "(64, 33)\n",
      "step 13345, loss is 4.860799789428711\n",
      "(64, 33)\n",
      "step 13346, loss is 5.027914047241211\n",
      "(64, 33)\n",
      "step 13347, loss is 4.819948196411133\n",
      "(64, 33)\n",
      "step 13348, loss is 4.9055304527282715\n",
      "(64, 33)\n",
      "step 13349, loss is 4.737854957580566\n",
      "(64, 33)\n",
      "step 13350, loss is 4.837556838989258\n",
      "(64, 33)\n",
      "step 13351, loss is 4.718544960021973\n",
      "(64, 33)\n",
      "step 13352, loss is 4.660416603088379\n",
      "(64, 33)\n",
      "step 13353, loss is 4.779350280761719\n",
      "(64, 33)\n",
      "step 13354, loss is 4.80360746383667\n",
      "(64, 33)\n",
      "step 13355, loss is 4.718921661376953\n",
      "(64, 33)\n",
      "step 13356, loss is 4.775866508483887\n",
      "(64, 33)\n",
      "step 13357, loss is 4.797749042510986\n",
      "(64, 33)\n",
      "step 13358, loss is 4.778172969818115\n",
      "(64, 33)\n",
      "step 13359, loss is 4.912333011627197\n",
      "(64, 33)\n",
      "step 13360, loss is 4.842465400695801\n",
      "(64, 33)\n",
      "step 13361, loss is 4.891147136688232\n",
      "(64, 33)\n",
      "step 13362, loss is 4.8302435874938965\n",
      "(64, 33)\n",
      "step 13363, loss is 4.780645370483398\n",
      "(64, 33)\n",
      "step 13364, loss is 4.788370609283447\n",
      "(64, 33)\n",
      "step 13365, loss is 4.969720363616943\n",
      "(64, 33)\n",
      "step 13366, loss is 4.957683563232422\n",
      "(64, 33)\n",
      "step 13367, loss is 4.727738380432129\n",
      "(64, 33)\n",
      "step 13368, loss is 4.936249732971191\n",
      "(64, 33)\n",
      "step 13369, loss is 4.793140411376953\n",
      "(64, 33)\n",
      "step 13370, loss is 4.736112594604492\n",
      "(64, 33)\n",
      "step 13371, loss is 4.723593235015869\n",
      "(64, 33)\n",
      "step 13372, loss is 4.673223972320557\n",
      "(64, 33)\n",
      "step 13373, loss is 4.866481781005859\n",
      "(64, 33)\n",
      "step 13374, loss is 4.946323871612549\n",
      "(64, 33)\n",
      "step 13375, loss is 4.606250762939453\n",
      "(64, 33)\n",
      "step 13376, loss is 4.721640110015869\n",
      "(64, 33)\n",
      "step 13377, loss is 4.913505554199219\n",
      "(64, 33)\n",
      "step 13378, loss is 4.842615127563477\n",
      "(64, 33)\n",
      "step 13379, loss is 4.945207118988037\n",
      "(64, 33)\n",
      "step 13380, loss is 4.861485004425049\n",
      "(64, 33)\n",
      "step 13381, loss is 4.73945951461792\n",
      "(64, 33)\n",
      "step 13382, loss is 5.031610488891602\n",
      "(64, 33)\n",
      "step 13383, loss is 4.890373229980469\n",
      "(64, 33)\n",
      "step 13384, loss is 4.775799751281738\n",
      "(64, 33)\n",
      "step 13385, loss is 4.6976752281188965\n",
      "(64, 33)\n",
      "step 13386, loss is 4.806567192077637\n",
      "(64, 33)\n",
      "step 13387, loss is 4.6775407791137695\n",
      "(64, 33)\n",
      "step 13388, loss is 4.904984951019287\n",
      "(64, 33)\n",
      "step 13389, loss is 4.641093730926514\n",
      "(64, 33)\n",
      "step 13390, loss is 4.804360866546631\n",
      "(64, 33)\n",
      "step 13391, loss is 4.853309631347656\n",
      "(64, 33)\n",
      "step 13392, loss is 4.8453192710876465\n",
      "(64, 33)\n",
      "step 13393, loss is 4.7725911140441895\n",
      "(64, 33)\n",
      "step 13394, loss is 4.736169815063477\n",
      "(64, 33)\n",
      "step 13395, loss is 4.798300266265869\n",
      "(64, 33)\n",
      "step 13396, loss is 4.838157653808594\n",
      "(64, 33)\n",
      "step 13397, loss is 4.825491428375244\n",
      "(64, 33)\n",
      "step 13398, loss is 4.760236740112305\n",
      "(64, 33)\n",
      "step 13399, loss is 4.925270080566406\n",
      "(64, 33)\n",
      "step 13400, loss is 4.722394943237305\n",
      "(64, 33)\n",
      "step 13401, loss is 4.84865140914917\n",
      "(64, 33)\n",
      "step 13402, loss is 4.800529479980469\n",
      "(64, 33)\n",
      "step 13403, loss is 4.919810771942139\n",
      "(64, 33)\n",
      "step 13404, loss is 4.8334784507751465\n",
      "(64, 33)\n",
      "step 13405, loss is 4.833025932312012\n",
      "(64, 33)\n",
      "step 13406, loss is 4.972732067108154\n",
      "(64, 33)\n",
      "step 13407, loss is 4.777998447418213\n",
      "(64, 33)\n",
      "step 13408, loss is 4.8062357902526855\n",
      "(64, 33)\n",
      "step 13409, loss is 4.675868511199951\n",
      "(64, 33)\n",
      "step 13410, loss is 4.850285530090332\n",
      "(64, 33)\n",
      "step 13411, loss is 4.678919315338135\n",
      "(64, 33)\n",
      "step 13412, loss is 4.644914150238037\n",
      "(64, 33)\n",
      "step 13413, loss is 4.738892555236816\n",
      "(64, 33)\n",
      "step 13414, loss is 4.859141826629639\n",
      "(64, 33)\n",
      "step 13415, loss is 4.758072376251221\n",
      "(64, 33)\n",
      "step 13416, loss is 4.8776116371154785\n",
      "(64, 33)\n",
      "step 13417, loss is 4.927637577056885\n",
      "(64, 33)\n",
      "step 13418, loss is 4.797244071960449\n",
      "(64, 33)\n",
      "step 13419, loss is 4.730747699737549\n",
      "(64, 33)\n",
      "step 13420, loss is 4.867373466491699\n",
      "(64, 33)\n",
      "step 13421, loss is 4.782378673553467\n",
      "(64, 33)\n",
      "step 13422, loss is 4.787942886352539\n",
      "(64, 33)\n",
      "step 13423, loss is 4.722832679748535\n",
      "(64, 33)\n",
      "step 13424, loss is 4.799666881561279\n",
      "(64, 33)\n",
      "step 13425, loss is 4.783144474029541\n",
      "(64, 33)\n",
      "step 13426, loss is 4.8955793380737305\n",
      "(64, 33)\n",
      "step 13427, loss is 4.849754333496094\n",
      "(64, 33)\n",
      "step 13428, loss is 4.924764156341553\n",
      "(64, 33)\n",
      "step 13429, loss is 4.870372772216797\n",
      "(64, 33)\n",
      "step 13430, loss is 4.9050984382629395\n",
      "(64, 33)\n",
      "step 13431, loss is 4.889311790466309\n",
      "(64, 33)\n",
      "step 13432, loss is 4.873257637023926\n",
      "(64, 33)\n",
      "step 13433, loss is 4.681419372558594\n",
      "(64, 33)\n",
      "step 13434, loss is 4.871730804443359\n",
      "(64, 33)\n",
      "step 13435, loss is 4.740667819976807\n",
      "(64, 33)\n",
      "step 13436, loss is 4.971344947814941\n",
      "(64, 33)\n",
      "step 13437, loss is 4.779176712036133\n",
      "(64, 33)\n",
      "step 13438, loss is 4.825437545776367\n",
      "(64, 33)\n",
      "step 13439, loss is 4.745092868804932\n",
      "(64, 33)\n",
      "step 13440, loss is 4.732123851776123\n",
      "(64, 33)\n",
      "step 13441, loss is 4.927324295043945\n",
      "(64, 33)\n",
      "step 13442, loss is 4.777621269226074\n",
      "(64, 33)\n",
      "step 13443, loss is 4.76202392578125\n",
      "(64, 33)\n",
      "step 13444, loss is 4.7207818031311035\n",
      "(64, 33)\n",
      "step 13445, loss is 4.893894195556641\n",
      "(64, 33)\n",
      "step 13446, loss is 4.738802909851074\n",
      "(64, 33)\n",
      "step 13447, loss is 4.863989353179932\n",
      "(64, 33)\n",
      "step 13448, loss is 4.857929229736328\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13449, loss is 4.79586124420166\n",
      "(64, 33)\n",
      "step 13450, loss is 4.787123203277588\n",
      "(64, 33)\n",
      "step 13451, loss is 4.792384624481201\n",
      "(64, 33)\n",
      "step 13452, loss is 4.948399066925049\n",
      "(64, 33)\n",
      "step 13453, loss is 4.960175514221191\n",
      "(64, 33)\n",
      "step 13454, loss is 4.899636268615723\n",
      "(64, 33)\n",
      "step 13455, loss is 4.952568054199219\n",
      "(64, 33)\n",
      "step 13456, loss is 4.774591445922852\n",
      "(64, 33)\n",
      "step 13457, loss is 4.833140850067139\n",
      "(64, 33)\n",
      "step 13458, loss is 4.770572662353516\n",
      "(64, 33)\n",
      "step 13459, loss is 4.947198390960693\n",
      "(64, 33)\n",
      "step 13460, loss is 4.859298229217529\n",
      "(64, 33)\n",
      "step 13461, loss is 4.689056396484375\n",
      "(64, 33)\n",
      "step 13462, loss is 4.695559024810791\n",
      "(64, 33)\n",
      "step 13463, loss is 4.715023994445801\n",
      "(64, 33)\n",
      "step 13464, loss is 4.765707969665527\n",
      "(64, 33)\n",
      "step 13465, loss is 4.824589252471924\n",
      "(64, 33)\n",
      "step 13466, loss is 4.788383960723877\n",
      "(64, 33)\n",
      "step 13467, loss is 4.919315814971924\n",
      "(64, 33)\n",
      "step 13468, loss is 4.674703121185303\n",
      "(64, 33)\n",
      "step 13469, loss is 4.780958652496338\n",
      "(64, 33)\n",
      "step 13470, loss is 4.880640983581543\n",
      "(64, 33)\n",
      "step 13471, loss is 4.933043479919434\n",
      "(64, 33)\n",
      "step 13472, loss is 4.7356648445129395\n",
      "(64, 33)\n",
      "step 13473, loss is 4.679740905761719\n",
      "(64, 33)\n",
      "step 13474, loss is 4.810741901397705\n",
      "(64, 33)\n",
      "step 13475, loss is 4.916632175445557\n",
      "(64, 33)\n",
      "step 13476, loss is 4.920328140258789\n",
      "(64, 33)\n",
      "step 13477, loss is 4.746656894683838\n",
      "(64, 33)\n",
      "step 13478, loss is 4.787666320800781\n",
      "(64, 33)\n",
      "step 13479, loss is 4.763615608215332\n",
      "(64, 33)\n",
      "step 13480, loss is 4.806116104125977\n",
      "(64, 33)\n",
      "step 13481, loss is 4.788845062255859\n",
      "(64, 33)\n",
      "step 13482, loss is 4.8458251953125\n",
      "(64, 33)\n",
      "step 13483, loss is 4.628875255584717\n",
      "(64, 33)\n",
      "step 13484, loss is 4.657086372375488\n",
      "(64, 33)\n",
      "step 13485, loss is 4.641107559204102\n",
      "(64, 33)\n",
      "step 13486, loss is 4.860441207885742\n",
      "(64, 33)\n",
      "step 13487, loss is 4.855481147766113\n",
      "(64, 33)\n",
      "step 13488, loss is 4.966124057769775\n",
      "(64, 33)\n",
      "step 13489, loss is 4.72689962387085\n",
      "(64, 33)\n",
      "step 13490, loss is 4.880699157714844\n",
      "(64, 33)\n",
      "step 13491, loss is 4.971584320068359\n",
      "(64, 33)\n",
      "step 13492, loss is 4.800695896148682\n",
      "(64, 33)\n",
      "step 13493, loss is 4.6333746910095215\n",
      "(64, 33)\n",
      "step 13494, loss is 4.893878936767578\n",
      "(64, 33)\n",
      "step 13495, loss is 4.740966320037842\n",
      "(64, 33)\n",
      "step 13496, loss is 4.829225063323975\n",
      "(64, 33)\n",
      "step 13497, loss is 4.880237579345703\n",
      "(64, 33)\n",
      "step 13498, loss is 4.651295185089111\n",
      "(64, 33)\n",
      "step 13499, loss is 4.8902506828308105\n",
      "(64, 33)\n",
      "step 13500, loss is 4.81917142868042\n",
      "(64, 33)\n",
      "step 13501, loss is 4.909139633178711\n",
      "(64, 33)\n",
      "step 13502, loss is 4.58139181137085\n",
      "(64, 33)\n",
      "step 13503, loss is 4.81564998626709\n",
      "(64, 33)\n",
      "step 13504, loss is 4.992393493652344\n",
      "(64, 33)\n",
      "step 13505, loss is 5.079390048980713\n",
      "(64, 33)\n",
      "step 13506, loss is 4.711784839630127\n",
      "(64, 33)\n",
      "step 13507, loss is 4.642939567565918\n",
      "(64, 33)\n",
      "step 13508, loss is 4.562002658843994\n",
      "(64, 33)\n",
      "step 13509, loss is 4.81770133972168\n",
      "(64, 33)\n",
      "step 13510, loss is 4.747138977050781\n",
      "(64, 33)\n",
      "step 13511, loss is 4.898712158203125\n",
      "(64, 33)\n",
      "step 13512, loss is 4.877450942993164\n",
      "(64, 33)\n",
      "step 13513, loss is 4.742402076721191\n",
      "(64, 33)\n",
      "step 13514, loss is 4.781315326690674\n",
      "(64, 33)\n",
      "step 13515, loss is 4.636066436767578\n",
      "(64, 33)\n",
      "step 13516, loss is 4.859889030456543\n",
      "(64, 33)\n",
      "step 13517, loss is 4.926813125610352\n",
      "(64, 33)\n",
      "step 13518, loss is 4.890992164611816\n",
      "(64, 33)\n",
      "step 13519, loss is 4.97725772857666\n",
      "(64, 33)\n",
      "step 13520, loss is 4.643673896789551\n",
      "(64, 33)\n",
      "step 13521, loss is 4.724445819854736\n",
      "(64, 33)\n",
      "step 13522, loss is 4.850414752960205\n",
      "(64, 33)\n",
      "step 13523, loss is 4.738170146942139\n",
      "(64, 33)\n",
      "step 13524, loss is 4.827366828918457\n",
      "(64, 33)\n",
      "step 13525, loss is 4.847504138946533\n",
      "(64, 33)\n",
      "step 13526, loss is 4.795607089996338\n",
      "(64, 33)\n",
      "step 13527, loss is 4.939587116241455\n",
      "(64, 33)\n",
      "step 13528, loss is 4.769607067108154\n",
      "(64, 33)\n",
      "step 13529, loss is 4.94377326965332\n",
      "(64, 33)\n",
      "step 13530, loss is 4.77750301361084\n",
      "(64, 33)\n",
      "step 13531, loss is 4.889986515045166\n",
      "(64, 33)\n",
      "step 13532, loss is 4.792835712432861\n",
      "(64, 33)\n",
      "step 13533, loss is 5.042056083679199\n",
      "(64, 33)\n",
      "step 13534, loss is 4.750807762145996\n",
      "(64, 33)\n",
      "step 13535, loss is 5.056228160858154\n",
      "(64, 33)\n",
      "step 13536, loss is 4.749509334564209\n",
      "(64, 33)\n",
      "step 13537, loss is 4.734227180480957\n",
      "(64, 33)\n",
      "step 13538, loss is 4.838913917541504\n",
      "(64, 33)\n",
      "step 13539, loss is 4.641472816467285\n",
      "(64, 33)\n",
      "step 13540, loss is 5.0577311515808105\n",
      "(64, 33)\n",
      "step 13541, loss is 4.894026756286621\n",
      "(64, 33)\n",
      "step 13542, loss is 4.734967231750488\n",
      "(64, 33)\n",
      "step 13543, loss is 4.837411880493164\n",
      "(64, 33)\n",
      "step 13544, loss is 4.835361003875732\n",
      "(64, 33)\n",
      "step 13545, loss is 4.780696868896484\n",
      "(64, 33)\n",
      "step 13546, loss is 4.812642574310303\n",
      "(64, 33)\n",
      "step 13547, loss is 4.781247138977051\n",
      "(64, 33)\n",
      "step 13548, loss is 4.705799102783203\n",
      "(64, 33)\n",
      "step 13549, loss is 4.8331828117370605\n",
      "(64, 33)\n",
      "step 13550, loss is 4.921700477600098\n",
      "(64, 33)\n",
      "step 13551, loss is 4.769766330718994\n",
      "(64, 33)\n",
      "step 13552, loss is 5.006221294403076\n",
      "(64, 33)\n",
      "step 13553, loss is 4.7396392822265625\n",
      "(64, 33)\n",
      "step 13554, loss is 4.903878211975098\n",
      "(64, 33)\n",
      "step 13555, loss is 4.767667293548584\n",
      "(64, 33)\n",
      "step 13556, loss is 4.525579452514648\n",
      "(64, 33)\n",
      "step 13557, loss is 4.798122406005859\n",
      "(64, 33)\n",
      "step 13558, loss is 4.785171031951904\n",
      "(64, 33)\n",
      "step 13559, loss is 4.756147861480713\n",
      "(64, 33)\n",
      "step 13560, loss is 4.770335674285889\n",
      "(64, 33)\n",
      "step 13561, loss is 4.914144515991211\n",
      "(64, 33)\n",
      "step 13562, loss is 4.899968147277832\n",
      "(64, 33)\n",
      "step 13563, loss is 4.71126651763916\n",
      "(64, 33)\n",
      "step 13564, loss is 4.8461198806762695\n",
      "(64, 33)\n",
      "step 13565, loss is 4.786363124847412\n",
      "(64, 33)\n",
      "step 13566, loss is 4.941778659820557\n",
      "(64, 33)\n",
      "step 13567, loss is 4.811079502105713\n",
      "(64, 33)\n",
      "step 13568, loss is 4.902473449707031\n",
      "(64, 33)\n",
      "step 13569, loss is 4.707209587097168\n",
      "(64, 33)\n",
      "step 13570, loss is 4.73724889755249\n",
      "(64, 33)\n",
      "step 13571, loss is 4.706643581390381\n",
      "(64, 33)\n",
      "step 13572, loss is 4.895763874053955\n",
      "(64, 33)\n",
      "step 13573, loss is 4.7803874015808105\n",
      "(64, 33)\n",
      "step 13574, loss is 4.883237838745117\n",
      "(64, 33)\n",
      "step 13575, loss is 4.67609977722168\n",
      "(64, 33)\n",
      "step 13576, loss is 4.788588047027588\n",
      "(64, 33)\n",
      "step 13577, loss is 4.885326862335205\n",
      "(64, 33)\n",
      "step 13578, loss is 4.73543119430542\n",
      "(64, 33)\n",
      "step 13579, loss is 4.7947893142700195\n",
      "(64, 33)\n",
      "step 13580, loss is 4.766000270843506\n",
      "(64, 33)\n",
      "step 13581, loss is 4.911288261413574\n",
      "(64, 33)\n",
      "step 13582, loss is 4.752050399780273\n",
      "(64, 33)\n",
      "step 13583, loss is 4.792668342590332\n",
      "(64, 33)\n",
      "step 13584, loss is 4.954636573791504\n",
      "(64, 33)\n",
      "step 13585, loss is 4.901354789733887\n",
      "(64, 33)\n",
      "step 13586, loss is 4.729343414306641\n",
      "(64, 33)\n",
      "step 13587, loss is 4.8637800216674805\n",
      "(64, 33)\n",
      "step 13588, loss is 4.9083075523376465\n",
      "(64, 33)\n",
      "step 13589, loss is 4.749758720397949\n",
      "(64, 33)\n",
      "step 13590, loss is 4.7637481689453125\n",
      "(64, 33)\n",
      "step 13591, loss is 5.002384185791016\n",
      "(64, 33)\n",
      "step 13592, loss is 4.907898426055908\n",
      "(64, 33)\n",
      "step 13593, loss is 4.881011962890625\n",
      "(64, 33)\n",
      "step 13594, loss is 4.964570999145508\n",
      "(64, 33)\n",
      "step 13595, loss is 4.978989124298096\n",
      "(64, 33)\n",
      "step 13596, loss is 4.79407262802124\n",
      "(64, 33)\n",
      "step 13597, loss is 4.609373092651367\n",
      "(64, 33)\n",
      "step 13598, loss is 4.866793632507324\n",
      "(64, 33)\n",
      "step 13599, loss is 4.712378978729248\n",
      "(64, 33)\n",
      "step 13600, loss is 4.946950435638428\n",
      "(64, 33)\n",
      "step 13601, loss is 4.942828178405762\n",
      "(64, 33)\n",
      "step 13602, loss is 4.977560520172119\n",
      "(64, 33)\n",
      "step 13603, loss is 4.902027130126953\n",
      "(64, 33)\n",
      "step 13604, loss is 4.737828254699707\n",
      "(64, 33)\n",
      "step 13605, loss is 4.766067028045654\n",
      "(64, 33)\n",
      "step 13606, loss is 4.898985862731934\n",
      "(64, 33)\n",
      "step 13607, loss is 4.845867156982422\n",
      "(64, 33)\n",
      "step 13608, loss is 4.812093734741211\n",
      "(64, 33)\n",
      "step 13609, loss is 4.87063455581665\n",
      "(64, 33)\n",
      "step 13610, loss is 4.669276237487793\n",
      "(64, 33)\n",
      "step 13611, loss is 4.834247589111328\n",
      "(64, 33)\n",
      "step 13612, loss is 4.841777324676514\n",
      "(64, 33)\n",
      "step 13613, loss is 4.648911952972412\n",
      "(64, 33)\n",
      "step 13614, loss is 4.839975833892822\n",
      "(64, 33)\n",
      "step 13615, loss is 4.676358699798584\n",
      "(64, 33)\n",
      "step 13616, loss is 4.757540702819824\n",
      "(64, 33)\n",
      "step 13617, loss is 4.6633710861206055\n",
      "(64, 33)\n",
      "step 13618, loss is 4.785640239715576\n",
      "(64, 33)\n",
      "step 13619, loss is 4.967397212982178\n",
      "(64, 33)\n",
      "step 13620, loss is 4.804422855377197\n",
      "(64, 33)\n",
      "step 13621, loss is 4.830052375793457\n",
      "(64, 33)\n",
      "step 13622, loss is 4.752630710601807\n",
      "(64, 33)\n",
      "step 13623, loss is 4.645584583282471\n",
      "(64, 33)\n",
      "step 13624, loss is 4.767074108123779\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13625, loss is 4.798301696777344\n",
      "(64, 33)\n",
      "step 13626, loss is 4.924424171447754\n",
      "(64, 33)\n",
      "step 13627, loss is 4.754544258117676\n",
      "(64, 33)\n",
      "step 13628, loss is 4.7603068351745605\n",
      "(64, 33)\n",
      "step 13629, loss is 4.9000396728515625\n",
      "(64, 33)\n",
      "step 13630, loss is 4.70811128616333\n",
      "(64, 33)\n",
      "step 13631, loss is 4.830918312072754\n",
      "(64, 33)\n",
      "step 13632, loss is 4.761059284210205\n",
      "(64, 33)\n",
      "step 13633, loss is 4.758785724639893\n",
      "(64, 33)\n",
      "step 13634, loss is 4.951374530792236\n",
      "(64, 33)\n",
      "step 13635, loss is 4.654082775115967\n",
      "(64, 33)\n",
      "step 13636, loss is 4.705029487609863\n",
      "(64, 33)\n",
      "step 13637, loss is 4.798493385314941\n",
      "(64, 33)\n",
      "step 13638, loss is 4.847865104675293\n",
      "(64, 33)\n",
      "step 13639, loss is 4.877871036529541\n",
      "(64, 33)\n",
      "step 13640, loss is 4.856298446655273\n",
      "(64, 33)\n",
      "step 13641, loss is 4.877941608428955\n",
      "(64, 33)\n",
      "step 13642, loss is 4.644089221954346\n",
      "(64, 33)\n",
      "step 13643, loss is 4.839855194091797\n",
      "(64, 33)\n",
      "step 13644, loss is 4.93010139465332\n",
      "(64, 33)\n",
      "step 13645, loss is 4.969794273376465\n",
      "(64, 33)\n",
      "step 13646, loss is 4.83150053024292\n",
      "(64, 33)\n",
      "step 13647, loss is 4.7784295082092285\n",
      "(64, 33)\n",
      "step 13648, loss is 4.583303928375244\n",
      "(64, 33)\n",
      "step 13649, loss is 4.706034183502197\n",
      "(64, 33)\n",
      "step 13650, loss is 4.717567443847656\n",
      "(64, 33)\n",
      "step 13651, loss is 4.874418258666992\n",
      "(64, 33)\n",
      "step 13652, loss is 4.937039375305176\n",
      "(64, 33)\n",
      "step 13653, loss is 4.7382049560546875\n",
      "(64, 33)\n",
      "step 13654, loss is 4.848694801330566\n",
      "(64, 33)\n",
      "step 13655, loss is 4.734564781188965\n",
      "(64, 33)\n",
      "step 13656, loss is 4.760823726654053\n",
      "(64, 33)\n",
      "step 13657, loss is 4.881017208099365\n",
      "(64, 33)\n",
      "step 13658, loss is 5.020090579986572\n",
      "(64, 33)\n",
      "step 13659, loss is 4.697437286376953\n",
      "(64, 33)\n",
      "step 13660, loss is 4.787230014801025\n",
      "(64, 33)\n",
      "step 13661, loss is 4.818137168884277\n",
      "(64, 33)\n",
      "step 13662, loss is 4.968486309051514\n",
      "(64, 33)\n",
      "step 13663, loss is 4.7500319480896\n",
      "(64, 33)\n",
      "step 13664, loss is 4.728482246398926\n",
      "(64, 33)\n",
      "step 13665, loss is 4.832413196563721\n",
      "(64, 33)\n",
      "step 13666, loss is 5.00811767578125\n",
      "(64, 33)\n",
      "step 13667, loss is 4.786585807800293\n",
      "(64, 33)\n",
      "step 13668, loss is 4.894554138183594\n",
      "(64, 33)\n",
      "step 13669, loss is 4.9368133544921875\n",
      "(64, 33)\n",
      "step 13670, loss is 4.925222873687744\n",
      "(64, 33)\n",
      "step 13671, loss is 4.892004489898682\n",
      "(64, 33)\n",
      "step 13672, loss is 4.772507667541504\n",
      "(64, 33)\n",
      "step 13673, loss is 4.627299785614014\n",
      "(64, 33)\n",
      "step 13674, loss is 4.721734046936035\n",
      "(64, 33)\n",
      "step 13675, loss is 4.742319583892822\n",
      "(64, 33)\n",
      "step 13676, loss is 4.878000259399414\n",
      "(64, 33)\n",
      "step 13677, loss is 4.676151275634766\n",
      "(64, 33)\n",
      "step 13678, loss is 4.733433723449707\n",
      "(64, 33)\n",
      "step 13679, loss is 4.822499752044678\n",
      "(64, 33)\n",
      "step 13680, loss is 5.0716094970703125\n",
      "(64, 33)\n",
      "step 13681, loss is 4.9331464767456055\n",
      "(64, 33)\n",
      "step 13682, loss is 4.917985439300537\n",
      "(64, 33)\n",
      "step 13683, loss is 4.807952880859375\n",
      "(64, 33)\n",
      "step 13684, loss is 4.8208231925964355\n",
      "(64, 33)\n",
      "step 13685, loss is 4.852242469787598\n",
      "(64, 33)\n",
      "step 13686, loss is 5.019323348999023\n",
      "(64, 33)\n",
      "step 13687, loss is 4.872548580169678\n",
      "(64, 33)\n",
      "step 13688, loss is 4.845896244049072\n",
      "(64, 33)\n",
      "step 13689, loss is 5.009681701660156\n",
      "(64, 33)\n",
      "step 13690, loss is 4.807799339294434\n",
      "(64, 33)\n",
      "step 13691, loss is 4.831973552703857\n",
      "(64, 33)\n",
      "step 13692, loss is 4.87540864944458\n",
      "(64, 33)\n",
      "step 13693, loss is 4.945431709289551\n",
      "(64, 33)\n",
      "step 13694, loss is 4.951971530914307\n",
      "(64, 33)\n",
      "step 13695, loss is 4.771373271942139\n",
      "(64, 33)\n",
      "step 13696, loss is 4.839783668518066\n",
      "(64, 33)\n",
      "step 13697, loss is 4.790745258331299\n",
      "(64, 33)\n",
      "step 13698, loss is 4.846806526184082\n",
      "(64, 33)\n",
      "step 13699, loss is 4.930285930633545\n",
      "(64, 33)\n",
      "step 13700, loss is 4.799473762512207\n",
      "(64, 33)\n",
      "step 13701, loss is 4.919787883758545\n",
      "(64, 33)\n",
      "step 13702, loss is 4.839775085449219\n",
      "(64, 33)\n",
      "step 13703, loss is 4.844097137451172\n",
      "(64, 33)\n",
      "step 13704, loss is 4.958412170410156\n",
      "(64, 33)\n",
      "step 13705, loss is 4.882040023803711\n",
      "(64, 33)\n",
      "step 13706, loss is 4.944308280944824\n",
      "(64, 33)\n",
      "step 13707, loss is 4.850947380065918\n",
      "(64, 33)\n",
      "step 13708, loss is 4.909745693206787\n",
      "(64, 33)\n",
      "step 13709, loss is 4.8624491691589355\n",
      "(64, 33)\n",
      "step 13710, loss is 4.817223072052002\n",
      "(64, 33)\n",
      "step 13711, loss is 4.6501030921936035\n",
      "(64, 33)\n",
      "step 13712, loss is 4.973679542541504\n",
      "(64, 33)\n",
      "step 13713, loss is 4.933706283569336\n",
      "(64, 33)\n",
      "step 13714, loss is 4.800316333770752\n",
      "(64, 33)\n",
      "step 13715, loss is 4.858649730682373\n",
      "(64, 33)\n",
      "step 13716, loss is 4.805684566497803\n",
      "(64, 33)\n",
      "step 13717, loss is 4.870581150054932\n",
      "(64, 33)\n",
      "step 13718, loss is 4.760684490203857\n",
      "(64, 33)\n",
      "step 13719, loss is 5.075751304626465\n",
      "(64, 33)\n",
      "step 13720, loss is 4.861285209655762\n",
      "(64, 33)\n",
      "step 13721, loss is 4.797706127166748\n",
      "(64, 33)\n",
      "step 13722, loss is 4.877403736114502\n",
      "(64, 33)\n",
      "step 13723, loss is 4.9914164543151855\n",
      "(64, 33)\n",
      "step 13724, loss is 4.680049896240234\n",
      "(64, 33)\n",
      "step 13725, loss is 4.8687052726745605\n",
      "(64, 33)\n",
      "step 13726, loss is 4.757054805755615\n",
      "(64, 33)\n",
      "step 13727, loss is 4.842436790466309\n",
      "(64, 33)\n",
      "step 13728, loss is 5.040069103240967\n",
      "(64, 33)\n",
      "step 13729, loss is 4.852985858917236\n",
      "(64, 33)\n",
      "step 13730, loss is 4.644845485687256\n",
      "(64, 33)\n",
      "step 13731, loss is 4.8097758293151855\n",
      "(64, 33)\n",
      "step 13732, loss is 5.011265754699707\n",
      "(64, 33)\n",
      "step 13733, loss is 4.786460876464844\n",
      "(64, 33)\n",
      "step 13734, loss is 4.768723964691162\n",
      "(64, 33)\n",
      "step 13735, loss is 4.736745357513428\n",
      "(64, 33)\n",
      "step 13736, loss is 4.8284125328063965\n",
      "(64, 33)\n",
      "step 13737, loss is 4.910243988037109\n",
      "(64, 33)\n",
      "step 13738, loss is 4.856377124786377\n",
      "(64, 33)\n",
      "step 13739, loss is 4.685368061065674\n",
      "(64, 33)\n",
      "step 13740, loss is 4.80861234664917\n",
      "(64, 33)\n",
      "step 13741, loss is 4.98612642288208\n",
      "(64, 33)\n",
      "step 13742, loss is 4.780807018280029\n",
      "(64, 33)\n",
      "step 13743, loss is 4.880428791046143\n",
      "(64, 33)\n",
      "step 13744, loss is 4.751158237457275\n",
      "(64, 33)\n",
      "step 13745, loss is 4.800147533416748\n",
      "(64, 33)\n",
      "step 13746, loss is 4.759642124176025\n",
      "(64, 33)\n",
      "step 13747, loss is 4.746241569519043\n",
      "(64, 33)\n",
      "step 13748, loss is 4.806582927703857\n",
      "(64, 33)\n",
      "step 13749, loss is 4.4917168617248535\n",
      "(64, 33)\n",
      "step 13750, loss is 4.862549781799316\n",
      "(64, 33)\n",
      "step 13751, loss is 4.632142543792725\n",
      "(64, 33)\n",
      "step 13752, loss is 4.9406561851501465\n",
      "(64, 33)\n",
      "step 13753, loss is 4.812330722808838\n",
      "(64, 33)\n",
      "step 13754, loss is 5.1572184562683105\n",
      "(64, 33)\n",
      "step 13755, loss is 4.8023271560668945\n",
      "(64, 33)\n",
      "step 13756, loss is 4.935203552246094\n",
      "(64, 33)\n",
      "step 13757, loss is 4.85416841506958\n",
      "(64, 33)\n",
      "step 13758, loss is 4.781197547912598\n",
      "(64, 33)\n",
      "step 13759, loss is 4.88655424118042\n",
      "(64, 33)\n",
      "step 13760, loss is 4.875753879547119\n",
      "(64, 33)\n",
      "step 13761, loss is 4.919083118438721\n",
      "(64, 33)\n",
      "step 13762, loss is 4.545425891876221\n",
      "(64, 33)\n",
      "step 13763, loss is 4.827780723571777\n",
      "(64, 33)\n",
      "step 13764, loss is 4.892453193664551\n",
      "(64, 33)\n",
      "step 13765, loss is 4.694872856140137\n",
      "(64, 33)\n",
      "step 13766, loss is 4.826270580291748\n",
      "(64, 33)\n",
      "step 13767, loss is 4.897270679473877\n",
      "(64, 33)\n",
      "step 13768, loss is 4.785011291503906\n",
      "(64, 33)\n",
      "step 13769, loss is 4.8925089836120605\n",
      "(64, 33)\n",
      "step 13770, loss is 4.828774452209473\n",
      "(64, 33)\n",
      "step 13771, loss is 4.827507972717285\n",
      "(64, 33)\n",
      "step 13772, loss is 4.957896709442139\n",
      "(64, 33)\n",
      "step 13773, loss is 4.912057876586914\n",
      "(64, 33)\n",
      "step 13774, loss is 4.899910926818848\n",
      "(64, 33)\n",
      "step 13775, loss is 4.904221057891846\n",
      "(64, 33)\n",
      "step 13776, loss is 4.79755973815918\n",
      "(64, 33)\n",
      "step 13777, loss is 4.868984222412109\n",
      "(64, 33)\n",
      "step 13778, loss is 4.945946216583252\n",
      "(64, 33)\n",
      "step 13779, loss is 4.6477155685424805\n",
      "(64, 33)\n",
      "step 13780, loss is 4.902114391326904\n",
      "(64, 33)\n",
      "step 13781, loss is 4.917459964752197\n",
      "(64, 33)\n",
      "step 13782, loss is 4.906941890716553\n",
      "(64, 33)\n",
      "step 13783, loss is 5.020013332366943\n",
      "(64, 33)\n",
      "step 13784, loss is 4.641337871551514\n",
      "(64, 33)\n",
      "step 13785, loss is 4.778834819793701\n",
      "(64, 33)\n",
      "step 13786, loss is 4.7646708488464355\n",
      "(64, 33)\n",
      "step 13787, loss is 4.767824649810791\n",
      "(64, 33)\n",
      "step 13788, loss is 4.832822322845459\n",
      "(64, 33)\n",
      "step 13789, loss is 5.0082783699035645\n",
      "(64, 33)\n",
      "step 13790, loss is 4.685758590698242\n",
      "(64, 33)\n",
      "step 13791, loss is 4.818591594696045\n",
      "(64, 33)\n",
      "step 13792, loss is 4.792328357696533\n",
      "(64, 33)\n",
      "step 13793, loss is 4.712663173675537\n",
      "(64, 33)\n",
      "step 13794, loss is 4.7260942459106445\n",
      "(64, 33)\n",
      "step 13795, loss is 4.832762718200684\n",
      "(64, 33)\n",
      "step 13796, loss is 4.9801788330078125\n",
      "(64, 33)\n",
      "step 13797, loss is 4.816145896911621\n",
      "(64, 33)\n",
      "step 13798, loss is 4.667107105255127\n",
      "(64, 33)\n",
      "step 13799, loss is 4.885366916656494\n",
      "(64, 33)\n",
      "step 13800, loss is 4.57684326171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 13801, loss is 4.844166278839111\n",
      "(64, 33)\n",
      "step 13802, loss is 4.759037017822266\n",
      "(64, 33)\n",
      "step 13803, loss is 4.766003608703613\n",
      "(64, 33)\n",
      "step 13804, loss is 4.575653553009033\n",
      "(64, 33)\n",
      "step 13805, loss is 4.754836082458496\n",
      "(64, 33)\n",
      "step 13806, loss is 4.797977447509766\n",
      "(64, 33)\n",
      "step 13807, loss is 4.849728107452393\n",
      "(64, 33)\n",
      "step 13808, loss is 4.6537184715271\n",
      "(64, 33)\n",
      "step 13809, loss is 4.851315498352051\n",
      "(64, 33)\n",
      "step 13810, loss is 4.8061017990112305\n",
      "(64, 33)\n",
      "step 13811, loss is 4.7473368644714355\n",
      "(64, 33)\n",
      "step 13812, loss is 4.854109287261963\n",
      "(64, 33)\n",
      "step 13813, loss is 4.849555969238281\n",
      "(64, 33)\n",
      "step 13814, loss is 4.667653560638428\n",
      "(64, 33)\n",
      "step 13815, loss is 4.784374713897705\n",
      "(64, 33)\n",
      "step 13816, loss is 4.7168803215026855\n",
      "(64, 33)\n",
      "step 13817, loss is 4.906479835510254\n",
      "(64, 33)\n",
      "step 13818, loss is 4.835827827453613\n",
      "(64, 33)\n",
      "step 13819, loss is 4.893938064575195\n",
      "(64, 33)\n",
      "step 13820, loss is 4.7445573806762695\n",
      "(64, 33)\n",
      "step 13821, loss is 4.659318447113037\n",
      "(64, 33)\n",
      "step 13822, loss is 4.956460475921631\n",
      "(64, 33)\n",
      "step 13823, loss is 4.887734889984131\n",
      "(64, 33)\n",
      "step 13824, loss is 4.665306091308594\n",
      "(64, 33)\n",
      "step 13825, loss is 4.742605686187744\n",
      "(64, 33)\n",
      "step 13826, loss is 4.909061431884766\n",
      "(64, 33)\n",
      "step 13827, loss is 4.937277317047119\n",
      "(64, 33)\n",
      "step 13828, loss is 4.864786624908447\n",
      "(64, 33)\n",
      "step 13829, loss is 4.804074764251709\n",
      "(64, 33)\n",
      "step 13830, loss is 4.772925853729248\n",
      "(64, 33)\n",
      "step 13831, loss is 4.655156135559082\n",
      "(64, 33)\n",
      "step 13832, loss is 4.912577152252197\n",
      "(64, 33)\n",
      "step 13833, loss is 4.870591640472412\n",
      "(64, 33)\n",
      "step 13834, loss is 4.86472749710083\n",
      "(64, 33)\n",
      "step 13835, loss is 4.989563465118408\n",
      "(64, 33)\n",
      "step 13836, loss is 4.745632648468018\n",
      "(64, 33)\n",
      "step 13837, loss is 4.829960823059082\n",
      "(64, 33)\n",
      "step 13838, loss is 4.810879707336426\n",
      "(64, 33)\n",
      "step 13839, loss is 4.736341953277588\n",
      "(64, 33)\n",
      "step 13840, loss is 4.808512210845947\n",
      "(64, 33)\n",
      "step 13841, loss is 4.768823623657227\n",
      "(64, 33)\n",
      "step 13842, loss is 4.708688735961914\n",
      "(64, 33)\n",
      "step 13843, loss is 4.760464668273926\n",
      "(64, 33)\n",
      "step 13844, loss is 5.019771575927734\n",
      "(64, 33)\n",
      "step 13845, loss is 4.8139801025390625\n",
      "(64, 33)\n",
      "step 13846, loss is 4.747161388397217\n",
      "(64, 33)\n",
      "step 13847, loss is 4.793989658355713\n",
      "(64, 33)\n",
      "step 13848, loss is 4.882794380187988\n",
      "(64, 33)\n",
      "step 13849, loss is 4.8706583976745605\n",
      "(64, 33)\n",
      "step 13850, loss is 4.911781311035156\n",
      "(64, 33)\n",
      "step 13851, loss is 4.646871089935303\n",
      "(64, 33)\n",
      "step 13852, loss is 4.734256744384766\n",
      "(64, 33)\n",
      "step 13853, loss is 4.999186992645264\n",
      "(64, 33)\n",
      "step 13854, loss is 4.860457420349121\n",
      "(64, 33)\n",
      "step 13855, loss is 4.894199371337891\n",
      "(64, 33)\n",
      "step 13856, loss is 4.983904838562012\n",
      "(64, 33)\n",
      "step 13857, loss is 4.717529296875\n",
      "(64, 33)\n",
      "step 13858, loss is 4.739245891571045\n",
      "(64, 33)\n",
      "step 13859, loss is 5.008650779724121\n",
      "(64, 33)\n",
      "step 13860, loss is 5.0190815925598145\n",
      "(64, 33)\n",
      "step 13861, loss is 4.783842086791992\n",
      "(64, 33)\n",
      "step 13862, loss is 4.7340192794799805\n",
      "(64, 33)\n",
      "step 13863, loss is 4.828011989593506\n",
      "(64, 33)\n",
      "step 13864, loss is 4.68962287902832\n",
      "(64, 33)\n",
      "step 13865, loss is 5.087691307067871\n",
      "(64, 33)\n",
      "step 13866, loss is 4.867201805114746\n",
      "(64, 33)\n",
      "step 13867, loss is 4.955311298370361\n",
      "(64, 33)\n",
      "step 13868, loss is 4.5816144943237305\n",
      "(64, 33)\n",
      "step 13869, loss is 4.811017036437988\n",
      "(64, 33)\n",
      "step 13870, loss is 4.749961853027344\n",
      "(64, 33)\n",
      "step 13871, loss is 4.899331092834473\n",
      "(64, 33)\n",
      "step 13872, loss is 4.74467134475708\n",
      "(64, 33)\n",
      "step 13873, loss is 4.706016540527344\n",
      "(64, 33)\n",
      "step 13874, loss is 4.6675920486450195\n",
      "(64, 33)\n",
      "step 13875, loss is 4.940493106842041\n",
      "(64, 33)\n",
      "step 13876, loss is 5.0358991622924805\n",
      "(64, 33)\n",
      "step 13877, loss is 4.7204766273498535\n",
      "(64, 33)\n",
      "step 13878, loss is 4.734714508056641\n",
      "(64, 33)\n",
      "step 13879, loss is 4.782878875732422\n",
      "(64, 33)\n",
      "step 13880, loss is 4.794825553894043\n",
      "(64, 33)\n",
      "step 13881, loss is 4.937348365783691\n",
      "(64, 33)\n",
      "step 13882, loss is 4.756211757659912\n",
      "(64, 33)\n",
      "step 13883, loss is 4.803251266479492\n",
      "(64, 33)\n",
      "step 13884, loss is 4.837791919708252\n",
      "(64, 33)\n",
      "step 13885, loss is 4.787660121917725\n",
      "(64, 33)\n",
      "step 13886, loss is 5.026784896850586\n",
      "(64, 33)\n",
      "step 13887, loss is 4.684335231781006\n",
      "(64, 33)\n",
      "step 13888, loss is 4.889063835144043\n",
      "(64, 33)\n",
      "step 13889, loss is 4.684080123901367\n",
      "(64, 33)\n",
      "step 13890, loss is 4.7844061851501465\n",
      "(64, 33)\n",
      "step 13891, loss is 4.743614196777344\n",
      "(64, 33)\n",
      "step 13892, loss is 4.791634559631348\n",
      "(64, 33)\n",
      "step 13893, loss is 4.776972770690918\n",
      "(64, 33)\n",
      "step 13894, loss is 4.768852233886719\n",
      "(64, 33)\n",
      "step 13895, loss is 4.9162492752075195\n",
      "(64, 33)\n",
      "step 13896, loss is 4.616296291351318\n",
      "(64, 33)\n",
      "step 13897, loss is 4.910703659057617\n",
      "(64, 33)\n",
      "step 13898, loss is 4.638898849487305\n",
      "(64, 33)\n",
      "step 13899, loss is 4.600368976593018\n",
      "(64, 33)\n",
      "step 13900, loss is 4.857584476470947\n",
      "(64, 33)\n",
      "step 13901, loss is 4.789608955383301\n",
      "(64, 33)\n",
      "step 13902, loss is 4.809467792510986\n",
      "(64, 33)\n",
      "step 13903, loss is 4.731157302856445\n",
      "(64, 33)\n",
      "step 13904, loss is 4.906485080718994\n",
      "(64, 33)\n",
      "step 13905, loss is 4.872038841247559\n",
      "(64, 33)\n",
      "step 13906, loss is 4.908277988433838\n",
      "(64, 33)\n",
      "step 13907, loss is 4.783847808837891\n",
      "(64, 33)\n",
      "step 13908, loss is 4.595682621002197\n",
      "(64, 33)\n",
      "step 13909, loss is 4.594551086425781\n",
      "(64, 33)\n",
      "step 13910, loss is 4.971638202667236\n",
      "(64, 33)\n",
      "step 13911, loss is 4.89790678024292\n",
      "(64, 33)\n",
      "step 13912, loss is 4.938689231872559\n",
      "(64, 33)\n",
      "step 13913, loss is 4.778868198394775\n",
      "(64, 33)\n",
      "step 13914, loss is 4.690901279449463\n",
      "(64, 33)\n",
      "step 13915, loss is 4.705326080322266\n",
      "(64, 33)\n",
      "step 13916, loss is 4.767011642456055\n",
      "(64, 33)\n",
      "step 13917, loss is 4.801749229431152\n",
      "(64, 33)\n",
      "step 13918, loss is 4.7697296142578125\n",
      "(64, 33)\n",
      "step 13919, loss is 4.917121887207031\n",
      "(64, 33)\n",
      "step 13920, loss is 4.922381401062012\n",
      "(64, 33)\n",
      "step 13921, loss is 4.603601455688477\n",
      "(64, 33)\n",
      "step 13922, loss is 4.873565673828125\n",
      "(64, 33)\n",
      "step 13923, loss is 4.889773368835449\n",
      "(64, 33)\n",
      "step 13924, loss is 4.638830661773682\n",
      "(64, 33)\n",
      "step 13925, loss is 4.990179538726807\n",
      "(64, 33)\n",
      "step 13926, loss is 4.771204948425293\n",
      "(64, 33)\n",
      "step 13927, loss is 4.7215681076049805\n",
      "(64, 33)\n",
      "step 13928, loss is 4.884019374847412\n",
      "(64, 33)\n",
      "step 13929, loss is 5.048412322998047\n",
      "(64, 33)\n",
      "step 13930, loss is 4.775261402130127\n",
      "(64, 33)\n",
      "step 13931, loss is 4.9132561683654785\n",
      "(64, 33)\n",
      "step 13932, loss is 4.848686218261719\n",
      "(64, 33)\n",
      "step 13933, loss is 4.868747234344482\n",
      "(64, 33)\n",
      "step 13934, loss is 4.789844036102295\n",
      "(64, 33)\n",
      "step 13935, loss is 4.860660076141357\n",
      "(64, 33)\n",
      "step 13936, loss is 4.728590488433838\n",
      "(64, 33)\n",
      "step 13937, loss is 4.705367088317871\n",
      "(64, 33)\n",
      "step 13938, loss is 4.856472492218018\n",
      "(64, 33)\n",
      "step 13939, loss is 5.003055572509766\n",
      "(64, 33)\n",
      "step 13940, loss is 4.715209484100342\n",
      "(64, 33)\n",
      "step 13941, loss is 4.612163066864014\n",
      "(64, 33)\n",
      "step 13942, loss is 4.781702518463135\n",
      "(64, 33)\n",
      "step 13943, loss is 4.711113452911377\n",
      "(64, 33)\n",
      "step 13944, loss is 4.789265155792236\n",
      "(64, 33)\n",
      "step 13945, loss is 4.888376712799072\n",
      "(64, 33)\n",
      "step 13946, loss is 4.902524948120117\n",
      "(64, 33)\n",
      "step 13947, loss is 4.945815563201904\n",
      "(64, 33)\n",
      "step 13948, loss is 4.6893792152404785\n",
      "(64, 33)\n",
      "step 13949, loss is 4.8834662437438965\n",
      "(64, 33)\n",
      "step 13950, loss is 4.6936845779418945\n",
      "(64, 33)\n",
      "step 13951, loss is 4.757194519042969\n",
      "(64, 33)\n",
      "step 13952, loss is 4.951167583465576\n",
      "(64, 33)\n",
      "step 13953, loss is 4.768206596374512\n",
      "(64, 33)\n",
      "step 13954, loss is 4.896595001220703\n",
      "(64, 33)\n",
      "step 13955, loss is 4.92056941986084\n",
      "(64, 33)\n",
      "step 13956, loss is 4.768794059753418\n",
      "(64, 33)\n",
      "step 13957, loss is 4.791209697723389\n",
      "(64, 33)\n",
      "step 13958, loss is 4.826932907104492\n",
      "(64, 33)\n",
      "step 13959, loss is 4.716330051422119\n",
      "(64, 33)\n",
      "step 13960, loss is 4.712484836578369\n",
      "(64, 33)\n",
      "step 13961, loss is 4.921786785125732\n",
      "(64, 33)\n",
      "step 13962, loss is 4.7364277839660645\n",
      "(64, 33)\n",
      "step 13963, loss is 4.906495094299316\n",
      "(64, 33)\n",
      "step 13964, loss is 4.824813365936279\n",
      "(64, 33)\n",
      "step 13965, loss is 4.6781229972839355\n",
      "(64, 33)\n",
      "step 13966, loss is 4.767317771911621\n",
      "(64, 33)\n",
      "step 13967, loss is 4.863028049468994\n",
      "(64, 33)\n",
      "step 13968, loss is 4.769073009490967\n",
      "(64, 33)\n",
      "step 13969, loss is 4.590803623199463\n",
      "(64, 33)\n",
      "step 13970, loss is 4.742901802062988\n",
      "(64, 33)\n",
      "step 13971, loss is 4.904818058013916\n",
      "(64, 33)\n",
      "step 13972, loss is 4.670218467712402\n",
      "(64, 33)\n",
      "step 13973, loss is 4.787941932678223\n",
      "(64, 33)\n",
      "step 13974, loss is 4.630241394042969\n",
      "(64, 33)\n",
      "step 13975, loss is 4.7718586921691895\n",
      "(64, 33)\n",
      "step 13976, loss is 4.818513870239258\n",
      "(64, 33)\n",
      "step 13977, loss is 4.850547790527344\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13978, loss is 5.027582168579102\n",
      "(64, 33)\n",
      "step 13979, loss is 4.889665603637695\n",
      "(64, 33)\n",
      "step 13980, loss is 4.852288246154785\n",
      "(64, 33)\n",
      "step 13981, loss is 4.836556434631348\n",
      "(64, 33)\n",
      "step 13982, loss is 4.701720237731934\n",
      "(64, 33)\n",
      "step 13983, loss is 4.592169761657715\n",
      "(64, 33)\n",
      "step 13984, loss is 4.867825984954834\n",
      "(64, 33)\n",
      "step 13985, loss is 4.659982681274414\n",
      "(64, 33)\n",
      "step 13986, loss is 4.785378456115723\n",
      "(64, 33)\n",
      "step 13987, loss is 4.786899566650391\n",
      "(64, 33)\n",
      "step 13988, loss is 4.673573017120361\n",
      "(64, 33)\n",
      "step 13989, loss is 4.693859577178955\n",
      "(64, 33)\n",
      "step 13990, loss is 4.796169281005859\n",
      "(64, 33)\n",
      "step 13991, loss is 4.863936901092529\n",
      "(64, 33)\n",
      "step 13992, loss is 4.9063639640808105\n",
      "(64, 33)\n",
      "step 13993, loss is 4.856785297393799\n",
      "(64, 33)\n",
      "step 13994, loss is 4.654438495635986\n",
      "(64, 33)\n",
      "step 13995, loss is 4.736078262329102\n",
      "(64, 33)\n",
      "step 13996, loss is 4.680723667144775\n",
      "(64, 33)\n",
      "step 13997, loss is 4.913056373596191\n",
      "(64, 33)\n",
      "step 13998, loss is 4.805432319641113\n",
      "(64, 33)\n",
      "step 13999, loss is 4.753438472747803\n",
      "(64, 33)\n",
      "step 14000, loss is 4.834268569946289\n",
      "(64, 33)\n",
      "step 14001, loss is 4.78817081451416\n",
      "(64, 33)\n",
      "step 14002, loss is 4.72070837020874\n",
      "(64, 33)\n",
      "step 14003, loss is 4.8145060539245605\n",
      "(64, 33)\n",
      "step 14004, loss is 4.715298175811768\n",
      "(64, 33)\n",
      "step 14005, loss is 4.667480945587158\n",
      "(64, 33)\n",
      "step 14006, loss is 4.838212490081787\n",
      "(64, 33)\n",
      "step 14007, loss is 4.794482231140137\n",
      "(64, 33)\n",
      "step 14008, loss is 4.892834186553955\n",
      "(64, 33)\n",
      "step 14009, loss is 4.926319599151611\n",
      "(64, 33)\n",
      "step 14010, loss is 4.712004661560059\n",
      "(64, 33)\n",
      "step 14011, loss is 4.849586486816406\n",
      "(64, 33)\n",
      "step 14012, loss is 4.814255237579346\n",
      "(64, 33)\n",
      "step 14013, loss is 4.931355953216553\n",
      "(64, 33)\n",
      "step 14014, loss is 4.891303062438965\n",
      "(64, 33)\n",
      "step 14015, loss is 4.850347518920898\n",
      "(64, 33)\n",
      "step 14016, loss is 4.747946262359619\n",
      "(64, 33)\n",
      "step 14017, loss is 5.022225856781006\n",
      "(64, 33)\n",
      "step 14018, loss is 4.719619274139404\n",
      "(64, 33)\n",
      "step 14019, loss is 4.864068031311035\n",
      "(64, 33)\n",
      "step 14020, loss is 4.806598663330078\n",
      "(64, 33)\n",
      "step 14021, loss is 4.8958048820495605\n",
      "(64, 33)\n",
      "step 14022, loss is 4.756877899169922\n",
      "(64, 33)\n",
      "step 14023, loss is 4.8299126625061035\n",
      "(64, 33)\n",
      "step 14024, loss is 4.782432556152344\n",
      "(64, 33)\n",
      "step 14025, loss is 4.873867034912109\n",
      "(64, 33)\n",
      "step 14026, loss is 4.791723251342773\n",
      "(64, 33)\n",
      "step 14027, loss is 4.705345153808594\n",
      "(64, 33)\n",
      "step 14028, loss is 4.624145030975342\n",
      "(64, 33)\n",
      "step 14029, loss is 4.764497756958008\n",
      "(64, 33)\n",
      "step 14030, loss is 4.694025039672852\n",
      "(64, 33)\n",
      "step 14031, loss is 4.791978359222412\n",
      "(64, 33)\n",
      "step 14032, loss is 4.759055137634277\n",
      "(64, 33)\n",
      "step 14033, loss is 4.830204963684082\n",
      "(64, 33)\n",
      "step 14034, loss is 4.945915222167969\n",
      "(64, 33)\n",
      "step 14035, loss is 5.005937576293945\n",
      "(64, 33)\n",
      "step 14036, loss is 4.60978364944458\n",
      "(64, 33)\n",
      "step 14037, loss is 4.797458171844482\n",
      "(64, 33)\n",
      "step 14038, loss is 4.671693801879883\n",
      "(64, 33)\n",
      "step 14039, loss is 4.698951721191406\n",
      "(64, 33)\n",
      "step 14040, loss is 4.941435813903809\n",
      "(64, 33)\n",
      "step 14041, loss is 4.542093753814697\n",
      "(64, 33)\n",
      "step 14042, loss is 4.644281387329102\n",
      "(64, 33)\n",
      "step 14043, loss is 4.932427883148193\n",
      "(64, 33)\n",
      "step 14044, loss is 4.615400791168213\n",
      "(64, 33)\n",
      "step 14045, loss is 4.757055759429932\n",
      "(64, 33)\n",
      "step 14046, loss is 4.733924388885498\n",
      "(64, 33)\n",
      "step 14047, loss is 4.690803527832031\n",
      "(64, 33)\n",
      "step 14048, loss is 4.830244064331055\n",
      "(64, 33)\n",
      "step 14049, loss is 4.719672203063965\n",
      "(64, 33)\n",
      "step 14050, loss is 4.645584583282471\n",
      "(64, 33)\n",
      "step 14051, loss is 4.893866539001465\n",
      "(64, 33)\n",
      "step 14052, loss is 4.796567916870117\n",
      "(64, 33)\n",
      "step 14053, loss is 4.902636528015137\n",
      "(64, 33)\n",
      "step 14054, loss is 4.925760746002197\n",
      "(64, 33)\n",
      "step 14055, loss is 4.843743801116943\n",
      "(64, 33)\n",
      "step 14056, loss is 4.763879776000977\n",
      "(64, 33)\n",
      "step 14057, loss is 4.895530700683594\n",
      "(64, 33)\n",
      "step 14058, loss is 4.95468807220459\n",
      "(64, 33)\n",
      "step 14059, loss is 4.703353404998779\n",
      "(64, 33)\n",
      "step 14060, loss is 4.666983604431152\n",
      "(64, 33)\n",
      "step 14061, loss is 4.7906575202941895\n",
      "(64, 33)\n",
      "step 14062, loss is 4.9581074714660645\n",
      "(64, 33)\n",
      "step 14063, loss is 4.733328342437744\n",
      "(64, 33)\n",
      "step 14064, loss is 4.7688307762146\n",
      "(64, 33)\n",
      "step 14065, loss is 4.840170383453369\n",
      "(64, 33)\n",
      "step 14066, loss is 4.842166423797607\n",
      "(64, 33)\n",
      "step 14067, loss is 4.877164840698242\n",
      "(64, 33)\n",
      "step 14068, loss is 4.695545673370361\n",
      "(64, 33)\n",
      "step 14069, loss is 4.768102645874023\n",
      "(64, 33)\n",
      "step 14070, loss is 4.868590831756592\n",
      "(64, 33)\n",
      "step 14071, loss is 4.8888373374938965\n",
      "(64, 33)\n",
      "step 14072, loss is 4.632413864135742\n",
      "(64, 33)\n",
      "step 14073, loss is 4.850424289703369\n",
      "(64, 33)\n",
      "step 14074, loss is 4.942575454711914\n",
      "(64, 33)\n",
      "step 14075, loss is 4.8836236000061035\n",
      "(64, 33)\n",
      "step 14076, loss is 4.796885013580322\n",
      "(64, 33)\n",
      "step 14077, loss is 4.818171501159668\n",
      "(64, 33)\n",
      "step 14078, loss is 4.80204439163208\n",
      "(64, 33)\n",
      "step 14079, loss is 4.740703105926514\n",
      "(64, 33)\n",
      "step 14080, loss is 4.759067058563232\n",
      "(64, 33)\n",
      "step 14081, loss is 4.864369869232178\n",
      "(64, 33)\n",
      "step 14082, loss is 4.891936302185059\n",
      "(64, 33)\n",
      "step 14083, loss is 4.589745998382568\n",
      "(64, 33)\n",
      "step 14084, loss is 4.716682434082031\n",
      "(64, 33)\n",
      "step 14085, loss is 4.954547882080078\n",
      "(64, 33)\n",
      "step 14086, loss is 4.87139368057251\n",
      "(64, 33)\n",
      "step 14087, loss is 4.624477386474609\n",
      "(64, 33)\n",
      "step 14088, loss is 4.880298137664795\n",
      "(64, 33)\n",
      "step 14089, loss is 4.862166404724121\n",
      "(64, 33)\n",
      "step 14090, loss is 4.723517417907715\n",
      "(64, 33)\n",
      "step 14091, loss is 4.9044013023376465\n",
      "(64, 33)\n",
      "step 14092, loss is 4.719878673553467\n",
      "(64, 33)\n",
      "step 14093, loss is 4.713596820831299\n",
      "(64, 33)\n",
      "step 14094, loss is 4.686489105224609\n",
      "(64, 33)\n",
      "step 14095, loss is 4.853723526000977\n",
      "(64, 33)\n",
      "step 14096, loss is 4.726301193237305\n",
      "(64, 33)\n",
      "step 14097, loss is 4.851437568664551\n",
      "(64, 33)\n",
      "step 14098, loss is 4.773843765258789\n",
      "(64, 33)\n",
      "step 14099, loss is 4.87861967086792\n",
      "(64, 33)\n",
      "step 14100, loss is 4.830610275268555\n",
      "(64, 33)\n",
      "step 14101, loss is 4.834914207458496\n",
      "(64, 33)\n",
      "step 14102, loss is 4.835506916046143\n",
      "(64, 33)\n",
      "step 14103, loss is 4.656162738800049\n",
      "(64, 33)\n",
      "step 14104, loss is 4.760016918182373\n",
      "(64, 33)\n",
      "step 14105, loss is 4.694792747497559\n",
      "(64, 33)\n",
      "step 14106, loss is 4.819128513336182\n",
      "(64, 33)\n",
      "step 14107, loss is 4.831628799438477\n",
      "(64, 33)\n",
      "step 14108, loss is 4.914832592010498\n",
      "(64, 33)\n",
      "step 14109, loss is 4.823238372802734\n",
      "(64, 33)\n",
      "step 14110, loss is 4.820413589477539\n",
      "(64, 33)\n",
      "step 14111, loss is 4.74574613571167\n",
      "(64, 33)\n",
      "step 14112, loss is 4.910112380981445\n",
      "(64, 33)\n",
      "step 14113, loss is 4.800627708435059\n",
      "(64, 33)\n",
      "step 14114, loss is 4.701765537261963\n",
      "(64, 33)\n",
      "step 14115, loss is 4.655196189880371\n",
      "(64, 33)\n",
      "step 14116, loss is 4.887955188751221\n",
      "(64, 33)\n",
      "step 14117, loss is 4.8969244956970215\n",
      "(64, 33)\n",
      "step 14118, loss is 4.807187080383301\n",
      "(64, 33)\n",
      "step 14119, loss is 4.716306686401367\n",
      "(64, 33)\n",
      "step 14120, loss is 4.834467887878418\n",
      "(64, 33)\n",
      "step 14121, loss is 4.7914252281188965\n",
      "(64, 33)\n",
      "step 14122, loss is 4.616661071777344\n",
      "(64, 33)\n",
      "step 14123, loss is 4.894412040710449\n",
      "(64, 33)\n",
      "step 14124, loss is 4.779516220092773\n",
      "(64, 33)\n",
      "step 14125, loss is 4.820258140563965\n",
      "(64, 33)\n",
      "step 14126, loss is 4.7746734619140625\n",
      "(64, 33)\n",
      "step 14127, loss is 4.71720552444458\n",
      "(64, 33)\n",
      "step 14128, loss is 4.95539665222168\n",
      "(64, 33)\n",
      "step 14129, loss is 4.910567760467529\n",
      "(64, 33)\n",
      "step 14130, loss is 4.744353771209717\n",
      "(64, 33)\n",
      "step 14131, loss is 4.732050895690918\n",
      "(64, 33)\n",
      "step 14132, loss is 4.873479843139648\n",
      "(64, 33)\n",
      "step 14133, loss is 4.732547283172607\n",
      "(64, 33)\n",
      "step 14134, loss is 4.8203535079956055\n",
      "(64, 33)\n",
      "step 14135, loss is 4.975816249847412\n",
      "(64, 33)\n",
      "step 14136, loss is 4.81555700302124\n",
      "(64, 33)\n",
      "step 14137, loss is 4.732210636138916\n",
      "(64, 33)\n",
      "step 14138, loss is 4.82169771194458\n",
      "(64, 33)\n",
      "step 14139, loss is 4.801058292388916\n",
      "(64, 33)\n",
      "step 14140, loss is 4.796198844909668\n",
      "(64, 33)\n",
      "step 14141, loss is 4.939621448516846\n",
      "(64, 33)\n",
      "step 14142, loss is 4.870070934295654\n",
      "(64, 33)\n",
      "step 14143, loss is 4.695742607116699\n",
      "(64, 33)\n",
      "step 14144, loss is 4.614428520202637\n",
      "(64, 33)\n",
      "step 14145, loss is 4.89508581161499\n",
      "(64, 33)\n",
      "step 14146, loss is 4.934775352478027\n",
      "(64, 33)\n",
      "step 14147, loss is 4.835537910461426\n",
      "(64, 33)\n",
      "step 14148, loss is 4.955726623535156\n",
      "(64, 33)\n",
      "step 14149, loss is 4.799110412597656\n",
      "(64, 33)\n",
      "step 14150, loss is 4.655425548553467\n",
      "(64, 33)\n",
      "step 14151, loss is 4.8288187980651855\n",
      "(64, 33)\n",
      "step 14152, loss is 4.907297134399414\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14153, loss is 4.83516263961792\n",
      "(64, 33)\n",
      "step 14154, loss is 4.754209518432617\n",
      "(64, 33)\n",
      "step 14155, loss is 4.795373439788818\n",
      "(64, 33)\n",
      "step 14156, loss is 4.78443717956543\n",
      "(64, 33)\n",
      "step 14157, loss is 4.792728900909424\n",
      "(64, 33)\n",
      "step 14158, loss is 4.869779109954834\n",
      "(64, 33)\n",
      "step 14159, loss is 4.9153242111206055\n",
      "(64, 33)\n",
      "step 14160, loss is 4.739457607269287\n",
      "(64, 33)\n",
      "step 14161, loss is 4.826505184173584\n",
      "(64, 33)\n",
      "step 14162, loss is 4.729044437408447\n",
      "(64, 33)\n",
      "step 14163, loss is 4.673386096954346\n",
      "(64, 33)\n",
      "step 14164, loss is 4.823233604431152\n",
      "(64, 33)\n",
      "step 14165, loss is 4.715109825134277\n",
      "(64, 33)\n",
      "step 14166, loss is 4.748476028442383\n",
      "(64, 33)\n",
      "step 14167, loss is 4.74479341506958\n",
      "(64, 33)\n",
      "step 14168, loss is 4.871084690093994\n",
      "(64, 33)\n",
      "step 14169, loss is 4.997809410095215\n",
      "(64, 33)\n",
      "step 14170, loss is 4.863025188446045\n",
      "(64, 33)\n",
      "step 14171, loss is 4.918926239013672\n",
      "(64, 33)\n",
      "step 14172, loss is 4.873795986175537\n",
      "(64, 33)\n",
      "step 14173, loss is 4.652854919433594\n",
      "(64, 33)\n",
      "step 14174, loss is 4.905521392822266\n",
      "(64, 33)\n",
      "step 14175, loss is 4.738949775695801\n",
      "(64, 33)\n",
      "step 14176, loss is 4.734343528747559\n",
      "(64, 33)\n",
      "step 14177, loss is 4.957908630371094\n",
      "(64, 33)\n",
      "step 14178, loss is 4.815880298614502\n",
      "(64, 33)\n",
      "step 14179, loss is 4.806205749511719\n",
      "(64, 33)\n",
      "step 14180, loss is 4.896839618682861\n",
      "(64, 33)\n",
      "step 14181, loss is 4.75643253326416\n",
      "(64, 33)\n",
      "step 14182, loss is 4.730364799499512\n",
      "(64, 33)\n",
      "step 14183, loss is 4.820384502410889\n",
      "(64, 33)\n",
      "step 14184, loss is 4.782835483551025\n",
      "(64, 33)\n",
      "step 14185, loss is 4.755126476287842\n",
      "(64, 33)\n",
      "step 14186, loss is 4.894376277923584\n",
      "(64, 33)\n",
      "step 14187, loss is 4.9418206214904785\n",
      "(64, 33)\n",
      "step 14188, loss is 4.795158386230469\n",
      "(64, 33)\n",
      "step 14189, loss is 4.865729331970215\n",
      "(64, 33)\n",
      "step 14190, loss is 4.512711048126221\n",
      "(64, 33)\n",
      "step 14191, loss is 4.739036560058594\n",
      "(64, 33)\n",
      "step 14192, loss is 4.698770523071289\n",
      "(64, 33)\n",
      "step 14193, loss is 4.760447978973389\n",
      "(64, 33)\n",
      "step 14194, loss is 4.939717769622803\n",
      "(64, 33)\n",
      "step 14195, loss is 4.9041571617126465\n",
      "(64, 33)\n",
      "step 14196, loss is 4.957305908203125\n",
      "(64, 33)\n",
      "step 14197, loss is 4.846390247344971\n",
      "(64, 33)\n",
      "step 14198, loss is 4.808873176574707\n",
      "(64, 33)\n",
      "step 14199, loss is 4.805446624755859\n",
      "(64, 33)\n",
      "step 14200, loss is 4.9268798828125\n",
      "(64, 33)\n",
      "step 14201, loss is 4.700786590576172\n",
      "(64, 33)\n",
      "step 14202, loss is 4.827260494232178\n",
      "(64, 33)\n",
      "step 14203, loss is 4.930458068847656\n",
      "(64, 33)\n",
      "step 14204, loss is 4.882552623748779\n",
      "(64, 33)\n",
      "step 14205, loss is 4.654913425445557\n",
      "(64, 33)\n",
      "step 14206, loss is 4.736016273498535\n",
      "(64, 33)\n",
      "step 14207, loss is 4.876001834869385\n",
      "(64, 33)\n",
      "step 14208, loss is 4.703659534454346\n",
      "(64, 33)\n",
      "step 14209, loss is 4.7835588455200195\n",
      "(64, 33)\n",
      "step 14210, loss is 4.66666841506958\n",
      "(64, 33)\n",
      "step 14211, loss is 4.817649841308594\n",
      "(64, 33)\n",
      "step 14212, loss is 4.683197021484375\n",
      "(64, 33)\n",
      "step 14213, loss is 4.807157516479492\n",
      "(64, 33)\n",
      "step 14214, loss is 4.682158470153809\n",
      "(64, 33)\n",
      "step 14215, loss is 4.7500319480896\n",
      "(64, 33)\n",
      "step 14216, loss is 4.674488544464111\n",
      "(64, 33)\n",
      "step 14217, loss is 4.830774307250977\n",
      "(64, 33)\n",
      "step 14218, loss is 4.916954040527344\n",
      "(64, 33)\n",
      "step 14219, loss is 4.812979221343994\n",
      "(64, 33)\n",
      "step 14220, loss is 4.7368011474609375\n",
      "(64, 33)\n",
      "step 14221, loss is 4.6556549072265625\n",
      "(64, 33)\n",
      "step 14222, loss is 4.8971405029296875\n",
      "(64, 33)\n",
      "step 14223, loss is 4.735161781311035\n",
      "(64, 33)\n",
      "step 14224, loss is 4.7712836265563965\n",
      "(64, 33)\n",
      "step 14225, loss is 4.638176918029785\n",
      "(64, 33)\n",
      "step 14226, loss is 4.92709493637085\n",
      "(64, 33)\n",
      "step 14227, loss is 4.951742172241211\n",
      "(64, 33)\n",
      "step 14228, loss is 4.980983734130859\n",
      "(64, 33)\n",
      "step 14229, loss is 4.752856254577637\n",
      "(64, 33)\n",
      "step 14230, loss is 4.922590732574463\n",
      "(64, 33)\n",
      "step 14231, loss is 4.707258224487305\n",
      "(64, 33)\n",
      "step 14232, loss is 4.834500789642334\n",
      "(64, 33)\n",
      "step 14233, loss is 5.003912925720215\n",
      "(64, 33)\n",
      "step 14234, loss is 4.725722789764404\n",
      "(64, 33)\n",
      "step 14235, loss is 4.749330520629883\n",
      "(64, 33)\n",
      "step 14236, loss is 4.898324489593506\n",
      "(64, 33)\n",
      "step 14237, loss is 4.722456932067871\n",
      "(64, 33)\n",
      "step 14238, loss is 4.835500717163086\n",
      "(64, 33)\n",
      "step 14239, loss is 4.702495098114014\n",
      "(64, 33)\n",
      "step 14240, loss is 4.763620376586914\n",
      "(64, 33)\n",
      "step 14241, loss is 4.8969879150390625\n",
      "(64, 33)\n",
      "step 14242, loss is 4.980960845947266\n",
      "(64, 33)\n",
      "step 14243, loss is 4.884222984313965\n",
      "(64, 33)\n",
      "step 14244, loss is 4.944243907928467\n",
      "(64, 33)\n",
      "step 14245, loss is 4.887468338012695\n",
      "(64, 33)\n",
      "step 14246, loss is 4.8802971839904785\n",
      "(64, 33)\n",
      "step 14247, loss is 4.957486152648926\n",
      "(64, 33)\n",
      "step 14248, loss is 4.819714069366455\n",
      "(64, 33)\n",
      "step 14249, loss is 4.762276649475098\n",
      "(64, 33)\n",
      "step 14250, loss is 4.774118900299072\n",
      "(64, 33)\n",
      "step 14251, loss is 4.785025119781494\n",
      "(64, 33)\n",
      "step 14252, loss is 4.89300537109375\n",
      "(64, 33)\n",
      "step 14253, loss is 4.820175647735596\n",
      "(64, 33)\n",
      "step 14254, loss is 4.885686874389648\n",
      "(64, 33)\n",
      "step 14255, loss is 4.816772937774658\n",
      "(64, 33)\n",
      "step 14256, loss is 4.815040111541748\n",
      "(64, 33)\n",
      "step 14257, loss is 4.872837066650391\n",
      "(64, 33)\n",
      "step 14258, loss is 4.92502498626709\n",
      "(64, 33)\n",
      "step 14259, loss is 4.675303936004639\n",
      "(64, 33)\n",
      "step 14260, loss is 4.85548734664917\n",
      "(64, 33)\n",
      "step 14261, loss is 4.73975944519043\n",
      "(64, 33)\n",
      "step 14262, loss is 4.900771141052246\n",
      "(64, 33)\n",
      "step 14263, loss is 4.730452537536621\n",
      "(64, 33)\n",
      "step 14264, loss is 4.797122001647949\n",
      "(64, 33)\n",
      "step 14265, loss is 4.692053318023682\n",
      "(64, 33)\n",
      "step 14266, loss is 4.818765640258789\n",
      "(64, 33)\n",
      "step 14267, loss is 4.648530006408691\n",
      "(64, 33)\n",
      "step 14268, loss is 4.924448013305664\n",
      "(64, 33)\n",
      "step 14269, loss is 4.722654819488525\n",
      "(64, 33)\n",
      "step 14270, loss is 4.644105434417725\n",
      "(64, 33)\n",
      "step 14271, loss is 4.89384126663208\n",
      "(64, 33)\n",
      "step 14272, loss is 4.5609636306762695\n",
      "(64, 33)\n",
      "step 14273, loss is 4.647150039672852\n",
      "(64, 33)\n",
      "step 14274, loss is 4.716838359832764\n",
      "(64, 33)\n",
      "step 14275, loss is 4.862303256988525\n",
      "(64, 33)\n",
      "step 14276, loss is 4.775577545166016\n",
      "(64, 33)\n",
      "step 14277, loss is 4.755896091461182\n",
      "(64, 33)\n",
      "step 14278, loss is 4.583321571350098\n",
      "(64, 33)\n",
      "step 14279, loss is 4.705886363983154\n",
      "(64, 33)\n",
      "step 14280, loss is 4.861848831176758\n",
      "(64, 33)\n",
      "step 14281, loss is 4.726283550262451\n",
      "(64, 33)\n",
      "step 14282, loss is 4.804176330566406\n",
      "(64, 33)\n",
      "step 14283, loss is 4.686466217041016\n",
      "(64, 33)\n",
      "step 14284, loss is 4.671725749969482\n",
      "(64, 33)\n",
      "step 14285, loss is 4.989748954772949\n",
      "(64, 33)\n",
      "step 14286, loss is 4.830899715423584\n",
      "(64, 33)\n",
      "step 14287, loss is 4.850734710693359\n",
      "(64, 33)\n",
      "step 14288, loss is 4.762833118438721\n",
      "(64, 33)\n",
      "step 14289, loss is 4.743264675140381\n",
      "(64, 33)\n",
      "step 14290, loss is 4.863974571228027\n",
      "(64, 33)\n",
      "step 14291, loss is 4.864583969116211\n",
      "(64, 33)\n",
      "step 14292, loss is 4.86277961730957\n",
      "(64, 33)\n",
      "step 14293, loss is 4.772620677947998\n",
      "(64, 33)\n",
      "step 14294, loss is 4.776585578918457\n",
      "(64, 33)\n",
      "step 14295, loss is 4.785353183746338\n",
      "(64, 33)\n",
      "step 14296, loss is 4.858211994171143\n",
      "(64, 33)\n",
      "step 14297, loss is 4.9144086837768555\n",
      "(64, 33)\n",
      "step 14298, loss is 4.838294506072998\n",
      "(64, 33)\n",
      "step 14299, loss is 4.755543231964111\n",
      "(64, 33)\n",
      "step 14300, loss is 4.860228538513184\n",
      "(64, 33)\n",
      "step 14301, loss is 4.9561591148376465\n",
      "(64, 33)\n",
      "step 14302, loss is 5.00211763381958\n",
      "(64, 33)\n",
      "step 14303, loss is 4.848786354064941\n",
      "(64, 33)\n",
      "step 14304, loss is 4.879640579223633\n",
      "(64, 33)\n",
      "step 14305, loss is 4.753434658050537\n",
      "(64, 33)\n",
      "step 14306, loss is 4.754042625427246\n",
      "(64, 33)\n",
      "step 14307, loss is 4.812632083892822\n",
      "(64, 33)\n",
      "step 14308, loss is 4.808894157409668\n",
      "(64, 33)\n",
      "step 14309, loss is 4.962181568145752\n",
      "(64, 33)\n",
      "step 14310, loss is 4.698103427886963\n",
      "(64, 33)\n",
      "step 14311, loss is 4.9743266105651855\n",
      "(64, 33)\n",
      "step 14312, loss is 4.69435977935791\n",
      "(64, 33)\n",
      "step 14313, loss is 4.885685920715332\n",
      "(64, 33)\n",
      "step 14314, loss is 4.645504474639893\n",
      "(64, 33)\n",
      "step 14315, loss is 4.700782775878906\n",
      "(64, 33)\n",
      "step 14316, loss is 4.798711776733398\n",
      "(64, 33)\n",
      "step 14317, loss is 4.796463966369629\n",
      "(64, 33)\n",
      "step 14318, loss is 4.952536106109619\n",
      "(64, 33)\n",
      "step 14319, loss is 4.876267910003662\n",
      "(64, 33)\n",
      "step 14320, loss is 4.9461565017700195\n",
      "(64, 33)\n",
      "step 14321, loss is 4.738016605377197\n",
      "(64, 33)\n",
      "step 14322, loss is 4.905588150024414\n",
      "(64, 33)\n",
      "step 14323, loss is 4.733067512512207\n",
      "(64, 33)\n",
      "step 14324, loss is 4.747701644897461\n",
      "(64, 33)\n",
      "step 14325, loss is 4.811215400695801\n",
      "(64, 33)\n",
      "step 14326, loss is 4.810993671417236\n",
      "(64, 33)\n",
      "step 14327, loss is 4.931304931640625\n",
      "(64, 33)\n",
      "step 14328, loss is 4.9269185066223145\n",
      "(64, 33)\n",
      "step 14329, loss is 4.678211688995361\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14330, loss is 4.933806896209717\n",
      "(64, 33)\n",
      "step 14331, loss is 4.84836483001709\n",
      "(64, 33)\n",
      "step 14332, loss is 4.800871849060059\n",
      "(64, 33)\n",
      "step 14333, loss is 4.730476379394531\n",
      "(64, 33)\n",
      "step 14334, loss is 4.944204807281494\n",
      "(64, 33)\n",
      "step 14335, loss is 4.93373441696167\n",
      "(64, 33)\n",
      "step 14336, loss is 4.962209224700928\n",
      "(64, 33)\n",
      "step 14337, loss is 4.780068397521973\n",
      "(64, 33)\n",
      "step 14338, loss is 4.693909168243408\n",
      "(64, 33)\n",
      "step 14339, loss is 4.742142677307129\n",
      "(64, 33)\n",
      "step 14340, loss is 4.842012882232666\n",
      "(64, 33)\n",
      "step 14341, loss is 5.011964797973633\n",
      "(64, 33)\n",
      "step 14342, loss is 4.638340950012207\n",
      "(64, 33)\n",
      "step 14343, loss is 4.843723773956299\n",
      "(64, 33)\n",
      "step 14344, loss is 4.885281085968018\n",
      "(64, 33)\n",
      "step 14345, loss is 4.880477428436279\n",
      "(64, 33)\n",
      "step 14346, loss is 4.663559436798096\n",
      "(64, 33)\n",
      "step 14347, loss is 4.962418079376221\n",
      "(64, 33)\n",
      "step 14348, loss is 5.0348381996154785\n",
      "(64, 33)\n",
      "step 14349, loss is 4.958865165710449\n",
      "(64, 33)\n",
      "step 14350, loss is 4.953421592712402\n",
      "(64, 33)\n",
      "step 14351, loss is 4.809069633483887\n",
      "(64, 33)\n",
      "step 14352, loss is 4.921420097351074\n",
      "(64, 33)\n",
      "step 14353, loss is 4.895413398742676\n",
      "(64, 33)\n",
      "step 14354, loss is 4.767570495605469\n",
      "(64, 33)\n",
      "step 14355, loss is 4.45356559753418\n",
      "(64, 33)\n",
      "step 14356, loss is 4.803580284118652\n",
      "(64, 33)\n",
      "step 14357, loss is 4.811286449432373\n",
      "(64, 33)\n",
      "step 14358, loss is 4.837072849273682\n",
      "(64, 33)\n",
      "step 14359, loss is 4.617853164672852\n",
      "(64, 33)\n",
      "step 14360, loss is 4.7368927001953125\n",
      "(64, 33)\n",
      "step 14361, loss is 4.808453559875488\n",
      "(64, 33)\n",
      "step 14362, loss is 4.988332271575928\n",
      "(64, 33)\n",
      "step 14363, loss is 4.797373294830322\n",
      "(64, 33)\n",
      "step 14364, loss is 4.831095218658447\n",
      "(64, 33)\n",
      "step 14365, loss is 4.732537746429443\n",
      "(64, 33)\n",
      "step 14366, loss is 4.736233711242676\n",
      "(64, 33)\n",
      "step 14367, loss is 4.801863193511963\n",
      "(64, 33)\n",
      "step 14368, loss is 4.764673233032227\n",
      "(64, 33)\n",
      "step 14369, loss is 4.693112373352051\n",
      "(64, 33)\n",
      "step 14370, loss is 4.842336654663086\n",
      "(64, 33)\n",
      "step 14371, loss is 4.710944175720215\n",
      "(64, 33)\n",
      "step 14372, loss is 4.806146621704102\n",
      "(64, 33)\n",
      "step 14373, loss is 4.670408248901367\n",
      "(64, 33)\n",
      "step 14374, loss is 4.8256120681762695\n",
      "(64, 33)\n",
      "step 14375, loss is 4.582156181335449\n",
      "(64, 33)\n",
      "step 14376, loss is 4.83953857421875\n",
      "(64, 33)\n",
      "step 14377, loss is 4.74031925201416\n",
      "(64, 33)\n",
      "step 14378, loss is 4.9697418212890625\n",
      "(64, 33)\n",
      "step 14379, loss is 4.673879623413086\n",
      "(64, 33)\n",
      "step 14380, loss is 4.700939178466797\n",
      "(64, 33)\n",
      "step 14381, loss is 4.882650852203369\n",
      "(64, 33)\n",
      "step 14382, loss is 4.916888236999512\n",
      "(64, 33)\n",
      "step 14383, loss is 4.663743019104004\n",
      "(64, 33)\n",
      "step 14384, loss is 4.700450897216797\n",
      "(64, 33)\n",
      "step 14385, loss is 4.858334541320801\n",
      "(64, 33)\n",
      "step 14386, loss is 4.864584445953369\n",
      "(64, 33)\n",
      "step 14387, loss is 4.773172378540039\n",
      "(64, 33)\n",
      "step 14388, loss is 4.875367641448975\n",
      "(64, 33)\n",
      "step 14389, loss is 4.877286911010742\n",
      "(64, 33)\n",
      "step 14390, loss is 4.85402774810791\n",
      "(64, 33)\n",
      "step 14391, loss is 4.72221040725708\n",
      "(64, 33)\n",
      "step 14392, loss is 4.851872444152832\n",
      "(64, 33)\n",
      "step 14393, loss is 4.87548828125\n",
      "(64, 33)\n",
      "step 14394, loss is 4.786647319793701\n",
      "(64, 33)\n",
      "step 14395, loss is 4.8065185546875\n",
      "(64, 33)\n",
      "step 14396, loss is 4.774196624755859\n",
      "(64, 33)\n",
      "step 14397, loss is 4.898215293884277\n",
      "(64, 33)\n",
      "step 14398, loss is 4.861188888549805\n",
      "(64, 33)\n",
      "step 14399, loss is 4.640424728393555\n",
      "(64, 33)\n",
      "step 14400, loss is 4.776669502258301\n",
      "(64, 33)\n",
      "step 14401, loss is 5.080837249755859\n",
      "(64, 33)\n",
      "step 14402, loss is 4.814558029174805\n",
      "(64, 33)\n",
      "step 14403, loss is 4.812895774841309\n",
      "(64, 33)\n",
      "step 14404, loss is 4.790773868560791\n",
      "(64, 33)\n",
      "step 14405, loss is 4.713722229003906\n",
      "(64, 33)\n",
      "step 14406, loss is 4.853985786437988\n",
      "(64, 33)\n",
      "step 14407, loss is 4.806373596191406\n",
      "(64, 33)\n",
      "step 14408, loss is 4.721897602081299\n",
      "(64, 33)\n",
      "step 14409, loss is 4.684237480163574\n",
      "(64, 33)\n",
      "step 14410, loss is 4.836485385894775\n",
      "(64, 33)\n",
      "step 14411, loss is 4.738701820373535\n",
      "(64, 33)\n",
      "step 14412, loss is 4.7978434562683105\n",
      "(64, 33)\n",
      "step 14413, loss is 4.7652153968811035\n",
      "(64, 33)\n",
      "step 14414, loss is 4.682780742645264\n",
      "(64, 33)\n",
      "step 14415, loss is 4.759184837341309\n",
      "(64, 33)\n",
      "step 14416, loss is 4.835935592651367\n",
      "(64, 33)\n",
      "step 14417, loss is 4.880743026733398\n",
      "(64, 33)\n",
      "step 14418, loss is 4.831700325012207\n",
      "(64, 33)\n",
      "step 14419, loss is 4.7031426429748535\n",
      "(64, 33)\n",
      "step 14420, loss is 4.807765483856201\n",
      "(64, 33)\n",
      "step 14421, loss is 4.8217339515686035\n",
      "(64, 33)\n",
      "step 14422, loss is 4.846959590911865\n",
      "(64, 33)\n",
      "step 14423, loss is 4.6860175132751465\n",
      "(64, 33)\n",
      "step 14424, loss is 4.8776655197143555\n",
      "(64, 33)\n",
      "step 14425, loss is 4.737785816192627\n",
      "(64, 33)\n",
      "step 14426, loss is 4.842840194702148\n",
      "(64, 33)\n",
      "step 14427, loss is 4.970330715179443\n",
      "(64, 33)\n",
      "step 14428, loss is 4.709554672241211\n",
      "(64, 33)\n",
      "step 14429, loss is 4.905963897705078\n",
      "(64, 33)\n",
      "step 14430, loss is 4.796224117279053\n",
      "(64, 33)\n",
      "step 14431, loss is 4.564757347106934\n",
      "(64, 33)\n",
      "step 14432, loss is 4.775591850280762\n",
      "(64, 33)\n",
      "step 14433, loss is 4.919035911560059\n",
      "(64, 33)\n",
      "step 14434, loss is 4.73870849609375\n",
      "(64, 33)\n",
      "step 14435, loss is 4.713991165161133\n",
      "(64, 33)\n",
      "step 14436, loss is 4.8562235832214355\n",
      "(64, 33)\n",
      "step 14437, loss is 4.830676555633545\n",
      "(64, 33)\n",
      "step 14438, loss is 4.921938896179199\n",
      "(64, 33)\n",
      "step 14439, loss is 4.641880989074707\n",
      "(64, 33)\n",
      "step 14440, loss is 4.886989593505859\n",
      "(64, 33)\n",
      "step 14441, loss is 4.725734710693359\n",
      "(64, 33)\n",
      "step 14442, loss is 4.793907642364502\n",
      "(64, 33)\n",
      "step 14443, loss is 4.872228622436523\n",
      "(64, 33)\n",
      "step 14444, loss is 4.796557426452637\n",
      "(64, 33)\n",
      "step 14445, loss is 4.818134784698486\n",
      "(64, 33)\n",
      "step 14446, loss is 4.785316467285156\n",
      "(64, 33)\n",
      "step 14447, loss is 4.575078964233398\n",
      "(64, 33)\n",
      "step 14448, loss is 4.725694179534912\n",
      "(64, 33)\n",
      "step 14449, loss is 4.868517875671387\n",
      "(64, 33)\n",
      "step 14450, loss is 4.882484436035156\n",
      "(64, 33)\n",
      "step 14451, loss is 4.743557453155518\n",
      "(64, 33)\n",
      "step 14452, loss is 4.7482523918151855\n",
      "(64, 33)\n",
      "step 14453, loss is 4.821750640869141\n",
      "(64, 33)\n",
      "step 14454, loss is 4.878448486328125\n",
      "(64, 33)\n",
      "step 14455, loss is 4.801942825317383\n",
      "(64, 33)\n",
      "step 14456, loss is 4.677536487579346\n",
      "(64, 33)\n",
      "step 14457, loss is 4.784248352050781\n",
      "(64, 33)\n",
      "step 14458, loss is 4.870635509490967\n",
      "(64, 33)\n",
      "step 14459, loss is 4.729129791259766\n",
      "(64, 33)\n",
      "step 14460, loss is 4.846992492675781\n",
      "(64, 33)\n",
      "step 14461, loss is 4.635087966918945\n",
      "(64, 33)\n",
      "step 14462, loss is 4.779995441436768\n",
      "(64, 33)\n",
      "step 14463, loss is 4.79884672164917\n",
      "(64, 33)\n",
      "step 14464, loss is 4.907981872558594\n",
      "(64, 33)\n",
      "step 14465, loss is 4.75510311126709\n",
      "(64, 33)\n",
      "step 14466, loss is 4.846977710723877\n",
      "(64, 33)\n",
      "step 14467, loss is 4.948124408721924\n",
      "(64, 33)\n",
      "step 14468, loss is 5.002178192138672\n",
      "(64, 33)\n",
      "step 14469, loss is 4.89047908782959\n",
      "(64, 33)\n",
      "step 14470, loss is 4.8032355308532715\n",
      "(64, 33)\n",
      "step 14471, loss is 4.767118453979492\n",
      "(64, 33)\n",
      "step 14472, loss is 4.9245219230651855\n",
      "(64, 33)\n",
      "step 14473, loss is 4.809067249298096\n",
      "(64, 33)\n",
      "step 14474, loss is 4.62912654876709\n",
      "(64, 33)\n",
      "step 14475, loss is 4.886806488037109\n",
      "(64, 33)\n",
      "step 14476, loss is 4.919820785522461\n",
      "(64, 33)\n",
      "step 14477, loss is 4.583353042602539\n",
      "(64, 33)\n",
      "step 14478, loss is 4.981997013092041\n",
      "(64, 33)\n",
      "step 14479, loss is 4.806105613708496\n",
      "(64, 33)\n",
      "step 14480, loss is 5.066815376281738\n",
      "(64, 33)\n",
      "step 14481, loss is 4.6371355056762695\n",
      "(64, 33)\n",
      "step 14482, loss is 4.792383193969727\n",
      "(64, 33)\n",
      "step 14483, loss is 4.848803997039795\n",
      "(64, 33)\n",
      "step 14484, loss is 4.815042495727539\n",
      "(64, 33)\n",
      "step 14485, loss is 4.815969467163086\n",
      "(64, 33)\n",
      "step 14486, loss is 4.904067039489746\n",
      "(64, 33)\n",
      "step 14487, loss is 4.787859916687012\n",
      "(64, 33)\n",
      "step 14488, loss is 4.997330188751221\n",
      "(64, 33)\n",
      "step 14489, loss is 4.769546985626221\n",
      "(64, 33)\n",
      "step 14490, loss is 4.799880027770996\n",
      "(64, 33)\n",
      "step 14491, loss is 4.714380741119385\n",
      "(64, 33)\n",
      "step 14492, loss is 4.747236728668213\n",
      "(64, 33)\n",
      "step 14493, loss is 4.966836452484131\n",
      "(64, 33)\n",
      "step 14494, loss is 4.823217391967773\n",
      "(64, 33)\n",
      "step 14495, loss is 4.942349433898926\n",
      "(64, 33)\n",
      "step 14496, loss is 4.660697937011719\n",
      "(64, 33)\n",
      "step 14497, loss is 4.837618350982666\n",
      "(64, 33)\n",
      "step 14498, loss is 4.805252552032471\n",
      "(64, 33)\n",
      "step 14499, loss is 4.740647315979004\n",
      "(64, 33)\n",
      "step 14500, loss is 4.903746128082275\n",
      "(64, 33)\n",
      "step 14501, loss is 4.85813570022583\n",
      "(64, 33)\n",
      "step 14502, loss is 5.068140983581543\n",
      "(64, 33)\n",
      "step 14503, loss is 4.904873847961426\n",
      "(64, 33)\n",
      "step 14504, loss is 4.964326858520508\n",
      "(64, 33)\n",
      "step 14505, loss is 4.74188756942749\n",
      "(64, 33)\n",
      "step 14506, loss is 4.72015380859375\n",
      "(64, 33)\n",
      "step 14507, loss is 4.780554294586182\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14508, loss is 4.852108955383301\n",
      "(64, 33)\n",
      "step 14509, loss is 4.798117160797119\n",
      "(64, 33)\n",
      "step 14510, loss is 4.81484317779541\n",
      "(64, 33)\n",
      "step 14511, loss is 4.874939918518066\n",
      "(64, 33)\n",
      "step 14512, loss is 4.597016334533691\n",
      "(64, 33)\n",
      "step 14513, loss is 4.774458408355713\n",
      "(64, 33)\n",
      "step 14514, loss is 4.941164970397949\n",
      "(64, 33)\n",
      "step 14515, loss is 4.722617149353027\n",
      "(64, 33)\n",
      "step 14516, loss is 4.818179130554199\n",
      "(64, 33)\n",
      "step 14517, loss is 4.769935607910156\n",
      "(64, 33)\n",
      "step 14518, loss is 4.9753313064575195\n",
      "(64, 33)\n",
      "step 14519, loss is 4.797419548034668\n",
      "(64, 33)\n",
      "step 14520, loss is 4.747443675994873\n",
      "(64, 33)\n",
      "step 14521, loss is 4.931538105010986\n",
      "(64, 33)\n",
      "step 14522, loss is 4.830506324768066\n",
      "(64, 33)\n",
      "step 14523, loss is 4.780782222747803\n",
      "(64, 33)\n",
      "step 14524, loss is 4.852104663848877\n",
      "(64, 33)\n",
      "step 14525, loss is 4.879234313964844\n",
      "(64, 33)\n",
      "step 14526, loss is 4.881552696228027\n",
      "(64, 33)\n",
      "step 14527, loss is 4.921586990356445\n",
      "(64, 33)\n",
      "step 14528, loss is 4.891819953918457\n",
      "(64, 33)\n",
      "step 14529, loss is 4.887956619262695\n",
      "(64, 33)\n",
      "step 14530, loss is 4.785114288330078\n",
      "(64, 33)\n",
      "step 14531, loss is 4.738317489624023\n",
      "(64, 33)\n",
      "step 14532, loss is 4.785147666931152\n",
      "(64, 33)\n",
      "step 14533, loss is 4.710365295410156\n",
      "(64, 33)\n",
      "step 14534, loss is 4.700552463531494\n",
      "(64, 33)\n",
      "step 14535, loss is 4.720498085021973\n",
      "(64, 33)\n",
      "step 14536, loss is 4.9476423263549805\n",
      "(64, 33)\n",
      "step 14537, loss is 4.714261054992676\n",
      "(64, 33)\n",
      "step 14538, loss is 4.788580417633057\n",
      "(64, 33)\n",
      "step 14539, loss is 4.971372604370117\n",
      "(64, 33)\n",
      "step 14540, loss is 4.769087314605713\n",
      "(64, 33)\n",
      "step 14541, loss is 4.883706092834473\n",
      "(64, 33)\n",
      "step 14542, loss is 4.829680919647217\n",
      "(64, 33)\n",
      "step 14543, loss is 5.053809642791748\n",
      "(64, 33)\n",
      "step 14544, loss is 4.7417426109313965\n",
      "(64, 33)\n",
      "step 14545, loss is 4.750467777252197\n",
      "(64, 33)\n",
      "step 14546, loss is 4.67626953125\n",
      "(64, 33)\n",
      "step 14547, loss is 4.6894097328186035\n",
      "(64, 33)\n",
      "step 14548, loss is 4.795999050140381\n",
      "(64, 33)\n",
      "step 14549, loss is 4.765834808349609\n",
      "(64, 33)\n",
      "step 14550, loss is 4.723765850067139\n",
      "(64, 33)\n",
      "step 14551, loss is 4.767458915710449\n",
      "(64, 33)\n",
      "step 14552, loss is 4.821188926696777\n",
      "(64, 33)\n",
      "step 14553, loss is 4.818117618560791\n",
      "(64, 33)\n",
      "step 14554, loss is 4.88870906829834\n",
      "(64, 33)\n",
      "step 14555, loss is 4.834887981414795\n",
      "(64, 33)\n",
      "step 14556, loss is 4.68403959274292\n",
      "(64, 33)\n",
      "step 14557, loss is 4.785341739654541\n",
      "(64, 33)\n",
      "step 14558, loss is 4.792416095733643\n",
      "(64, 33)\n",
      "step 14559, loss is 4.711633205413818\n",
      "(64, 33)\n",
      "step 14560, loss is 4.947521686553955\n",
      "(64, 33)\n",
      "step 14561, loss is 4.938436985015869\n",
      "(64, 33)\n",
      "step 14562, loss is 4.736274242401123\n",
      "(64, 33)\n",
      "step 14563, loss is 4.63319730758667\n",
      "(64, 33)\n",
      "step 14564, loss is 4.961004257202148\n",
      "(64, 33)\n",
      "step 14565, loss is 4.827024459838867\n",
      "(64, 33)\n",
      "step 14566, loss is 4.918981552124023\n",
      "(64, 33)\n",
      "step 14567, loss is 4.711732864379883\n",
      "(64, 33)\n",
      "step 14568, loss is 4.886547565460205\n",
      "(64, 33)\n",
      "step 14569, loss is 4.810471057891846\n",
      "(64, 33)\n",
      "step 14570, loss is 4.708062171936035\n",
      "(64, 33)\n",
      "step 14571, loss is 4.588298320770264\n",
      "(64, 33)\n",
      "step 14572, loss is 4.916543006896973\n",
      "(64, 33)\n",
      "step 14573, loss is 4.719515800476074\n",
      "(64, 33)\n",
      "step 14574, loss is 5.020593643188477\n",
      "(64, 33)\n",
      "step 14575, loss is 4.845383644104004\n",
      "(64, 33)\n",
      "step 14576, loss is 4.847243309020996\n",
      "(64, 33)\n",
      "step 14577, loss is 4.763784885406494\n",
      "(64, 33)\n",
      "step 14578, loss is 4.89470100402832\n",
      "(64, 33)\n",
      "step 14579, loss is 4.785745143890381\n",
      "(64, 33)\n",
      "step 14580, loss is 4.770129680633545\n",
      "(64, 33)\n",
      "step 14581, loss is 5.028215408325195\n",
      "(64, 33)\n",
      "step 14582, loss is 4.682322978973389\n",
      "(64, 33)\n",
      "step 14583, loss is 4.817800045013428\n",
      "(64, 33)\n",
      "step 14584, loss is 4.762795925140381\n",
      "(64, 33)\n",
      "step 14585, loss is 4.756356716156006\n",
      "(64, 33)\n",
      "step 14586, loss is 4.620636940002441\n",
      "(64, 33)\n",
      "step 14587, loss is 4.714217662811279\n",
      "(64, 33)\n",
      "step 14588, loss is 4.945175647735596\n",
      "(64, 33)\n",
      "step 14589, loss is 4.728325843811035\n",
      "(64, 33)\n",
      "step 14590, loss is 4.7473578453063965\n",
      "(64, 33)\n",
      "step 14591, loss is 4.791783332824707\n",
      "(64, 33)\n",
      "step 14592, loss is 4.340059280395508\n",
      "(64, 33)\n",
      "step 14593, loss is 4.6373138427734375\n",
      "(64, 33)\n",
      "step 14594, loss is 4.6624321937561035\n",
      "(64, 33)\n",
      "step 14595, loss is 4.917099475860596\n",
      "(64, 33)\n",
      "step 14596, loss is 4.6154866218566895\n",
      "(64, 33)\n",
      "step 14597, loss is 4.834806442260742\n",
      "(64, 33)\n",
      "step 14598, loss is 4.749490737915039\n",
      "(64, 33)\n",
      "step 14599, loss is 4.697895050048828\n",
      "(64, 33)\n",
      "step 14600, loss is 4.843621253967285\n",
      "(64, 33)\n",
      "step 14601, loss is 4.809592247009277\n",
      "(64, 33)\n",
      "step 14602, loss is 4.8168182373046875\n",
      "(64, 33)\n",
      "step 14603, loss is 4.869449615478516\n",
      "(64, 33)\n",
      "step 14604, loss is 4.960891246795654\n",
      "(64, 33)\n",
      "step 14605, loss is 4.828682899475098\n",
      "(64, 33)\n",
      "step 14606, loss is 4.899072647094727\n",
      "(64, 33)\n",
      "step 14607, loss is 4.653666973114014\n",
      "(64, 33)\n",
      "step 14608, loss is 4.762857437133789\n",
      "(64, 33)\n",
      "step 14609, loss is 4.866333961486816\n",
      "(64, 33)\n",
      "step 14610, loss is 4.995898246765137\n",
      "(64, 33)\n",
      "step 14611, loss is 4.9065141677856445\n",
      "(64, 33)\n",
      "step 14612, loss is 4.691398620605469\n",
      "(64, 33)\n",
      "step 14613, loss is 4.759860992431641\n",
      "(64, 33)\n",
      "step 14614, loss is 4.824792861938477\n",
      "(64, 33)\n",
      "step 14615, loss is 4.874841690063477\n",
      "(64, 33)\n",
      "step 14616, loss is 4.884659767150879\n",
      "(64, 33)\n",
      "step 14617, loss is 4.541271209716797\n",
      "(64, 33)\n",
      "step 14618, loss is 4.825865268707275\n",
      "(64, 33)\n",
      "step 14619, loss is 4.685859680175781\n",
      "(64, 33)\n",
      "step 14620, loss is 4.682886600494385\n",
      "(64, 33)\n",
      "step 14621, loss is 4.932118892669678\n",
      "(64, 33)\n",
      "step 14622, loss is 4.91587495803833\n",
      "(64, 33)\n",
      "step 14623, loss is 4.889956474304199\n",
      "(64, 33)\n",
      "step 14624, loss is 4.838068962097168\n",
      "(64, 33)\n",
      "step 14625, loss is 4.704719543457031\n",
      "(64, 33)\n",
      "step 14626, loss is 4.660235404968262\n",
      "(64, 33)\n",
      "step 14627, loss is 4.822952747344971\n",
      "(64, 33)\n",
      "step 14628, loss is 4.9103851318359375\n",
      "(64, 33)\n",
      "step 14629, loss is 4.88789701461792\n",
      "(64, 33)\n",
      "step 14630, loss is 4.896066665649414\n",
      "(64, 33)\n",
      "step 14631, loss is 4.958563327789307\n",
      "(64, 33)\n",
      "step 14632, loss is 4.928640842437744\n",
      "(64, 33)\n",
      "step 14633, loss is 4.789185523986816\n",
      "(64, 33)\n",
      "step 14634, loss is 4.77442741394043\n",
      "(64, 33)\n",
      "step 14635, loss is 4.676645278930664\n",
      "(64, 33)\n",
      "step 14636, loss is 4.8161940574646\n",
      "(64, 33)\n",
      "step 14637, loss is 4.592103958129883\n",
      "(64, 33)\n",
      "step 14638, loss is 4.854983806610107\n",
      "(64, 33)\n",
      "step 14639, loss is 4.882130146026611\n",
      "(64, 33)\n",
      "step 14640, loss is 4.774466514587402\n",
      "(64, 33)\n",
      "step 14641, loss is 4.911141395568848\n",
      "(64, 33)\n",
      "step 14642, loss is 4.71585750579834\n",
      "(64, 33)\n",
      "step 14643, loss is 4.71262788772583\n",
      "(64, 33)\n",
      "step 14644, loss is 4.821199893951416\n",
      "(64, 33)\n",
      "step 14645, loss is 4.720558166503906\n",
      "(64, 33)\n",
      "step 14646, loss is 4.696341037750244\n",
      "(64, 33)\n",
      "step 14647, loss is 4.778733253479004\n",
      "(64, 33)\n",
      "step 14648, loss is 4.663827419281006\n",
      "(64, 33)\n",
      "step 14649, loss is 4.718828201293945\n",
      "(64, 33)\n",
      "step 14650, loss is 4.665768623352051\n",
      "(64, 33)\n",
      "step 14651, loss is 4.800097942352295\n",
      "(64, 33)\n",
      "step 14652, loss is 5.107948303222656\n",
      "(64, 33)\n",
      "step 14653, loss is 5.082653999328613\n",
      "(64, 33)\n",
      "step 14654, loss is 4.663447856903076\n",
      "(64, 33)\n",
      "step 14655, loss is 4.604987144470215\n",
      "(64, 33)\n",
      "step 14656, loss is 4.89791202545166\n",
      "(64, 33)\n",
      "step 14657, loss is 4.7526164054870605\n",
      "(64, 33)\n",
      "step 14658, loss is 5.035312175750732\n",
      "(64, 33)\n",
      "step 14659, loss is 4.625068664550781\n",
      "(64, 33)\n",
      "step 14660, loss is 4.819619178771973\n",
      "(64, 33)\n",
      "step 14661, loss is 4.7826619148254395\n",
      "(64, 33)\n",
      "step 14662, loss is 4.818519592285156\n",
      "(64, 33)\n",
      "step 14663, loss is 4.831833839416504\n",
      "(64, 33)\n",
      "step 14664, loss is 4.957769393920898\n",
      "(64, 33)\n",
      "step 14665, loss is 4.7089643478393555\n",
      "(64, 33)\n",
      "step 14666, loss is 4.577746868133545\n",
      "(64, 33)\n",
      "step 14667, loss is 4.7440972328186035\n",
      "(64, 33)\n",
      "step 14668, loss is 4.795185565948486\n",
      "(64, 33)\n",
      "step 14669, loss is 4.920112609863281\n",
      "(64, 33)\n",
      "step 14670, loss is 4.479976177215576\n",
      "(64, 33)\n",
      "step 14671, loss is 4.68873929977417\n",
      "(64, 33)\n",
      "step 14672, loss is 4.894724369049072\n",
      "(64, 33)\n",
      "step 14673, loss is 4.7835235595703125\n",
      "(64, 33)\n",
      "step 14674, loss is 4.7188401222229\n",
      "(64, 33)\n",
      "step 14675, loss is 4.803595066070557\n",
      "(64, 33)\n",
      "step 14676, loss is 4.7971415519714355\n",
      "(64, 33)\n",
      "step 14677, loss is 4.532291412353516\n",
      "(64, 33)\n",
      "step 14678, loss is 4.795570373535156\n",
      "(64, 33)\n",
      "step 14679, loss is 4.91752290725708\n",
      "(64, 33)\n",
      "step 14680, loss is 4.827049732208252\n",
      "(64, 33)\n",
      "step 14681, loss is 4.863358497619629\n",
      "(64, 33)\n",
      "step 14682, loss is 4.716114521026611\n",
      "(64, 33)\n",
      "step 14683, loss is 4.732517242431641\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14684, loss is 4.721726417541504\n",
      "(64, 33)\n",
      "step 14685, loss is 4.7915873527526855\n",
      "(64, 33)\n",
      "step 14686, loss is 4.760275363922119\n",
      "(64, 33)\n",
      "step 14687, loss is 4.900991916656494\n",
      "(64, 33)\n",
      "step 14688, loss is 4.789300918579102\n",
      "(64, 33)\n",
      "step 14689, loss is 4.742780685424805\n",
      "(64, 33)\n",
      "step 14690, loss is 4.853560924530029\n",
      "(64, 33)\n",
      "step 14691, loss is 4.812946319580078\n",
      "(64, 33)\n",
      "step 14692, loss is 4.6764302253723145\n",
      "(64, 33)\n",
      "step 14693, loss is 4.684791088104248\n",
      "(64, 33)\n",
      "step 14694, loss is 4.872961044311523\n",
      "(64, 33)\n",
      "step 14695, loss is 4.724595069885254\n",
      "(64, 33)\n",
      "step 14696, loss is 4.744222640991211\n",
      "(64, 33)\n",
      "step 14697, loss is 4.6406755447387695\n",
      "(64, 33)\n",
      "step 14698, loss is 4.769416809082031\n",
      "(64, 33)\n",
      "step 14699, loss is 4.772209644317627\n",
      "(64, 33)\n",
      "step 14700, loss is 4.660691738128662\n",
      "(64, 33)\n",
      "step 14701, loss is 4.844545364379883\n",
      "(64, 33)\n",
      "step 14702, loss is 4.776026248931885\n",
      "(64, 33)\n",
      "step 14703, loss is 4.778462886810303\n",
      "(64, 33)\n",
      "step 14704, loss is 4.853266716003418\n",
      "(64, 33)\n",
      "step 14705, loss is 4.777688026428223\n",
      "(64, 33)\n",
      "step 14706, loss is 4.661447525024414\n",
      "(64, 33)\n",
      "step 14707, loss is 4.843111038208008\n",
      "(64, 33)\n",
      "step 14708, loss is 4.896381378173828\n",
      "(64, 33)\n",
      "step 14709, loss is 4.804837226867676\n",
      "(64, 33)\n",
      "step 14710, loss is 4.657392978668213\n",
      "(64, 33)\n",
      "step 14711, loss is 4.749582290649414\n",
      "(64, 33)\n",
      "step 14712, loss is 4.7079973220825195\n",
      "(64, 33)\n",
      "step 14713, loss is 4.658060073852539\n",
      "(64, 33)\n",
      "step 14714, loss is 4.819939613342285\n",
      "(64, 33)\n",
      "step 14715, loss is 4.738441467285156\n",
      "(64, 33)\n",
      "step 14716, loss is 4.842264652252197\n",
      "(64, 33)\n",
      "step 14717, loss is 4.683468341827393\n",
      "(64, 33)\n",
      "step 14718, loss is 4.750000953674316\n",
      "(64, 33)\n",
      "step 14719, loss is 4.967320919036865\n",
      "(64, 33)\n",
      "step 14720, loss is 4.772036552429199\n",
      "(64, 33)\n",
      "step 14721, loss is 4.632471084594727\n",
      "(64, 33)\n",
      "step 14722, loss is 4.764770030975342\n",
      "(64, 33)\n",
      "step 14723, loss is 4.936788082122803\n",
      "(64, 33)\n",
      "step 14724, loss is 4.630191326141357\n",
      "(64, 33)\n",
      "step 14725, loss is 4.845350742340088\n",
      "(64, 33)\n",
      "step 14726, loss is 4.687155246734619\n",
      "(64, 33)\n",
      "step 14727, loss is 4.995798110961914\n",
      "(64, 33)\n",
      "step 14728, loss is 4.861837387084961\n",
      "(64, 33)\n",
      "step 14729, loss is 4.60844087600708\n",
      "(64, 33)\n",
      "step 14730, loss is 4.887031078338623\n",
      "(64, 33)\n",
      "step 14731, loss is 4.671689987182617\n",
      "(64, 33)\n",
      "step 14732, loss is 4.655555248260498\n",
      "(64, 33)\n",
      "step 14733, loss is 4.985429286956787\n",
      "(64, 33)\n",
      "step 14734, loss is 4.807196617126465\n",
      "(64, 33)\n",
      "step 14735, loss is 4.726912021636963\n",
      "(64, 33)\n",
      "step 14736, loss is 4.796811580657959\n",
      "(64, 33)\n",
      "step 14737, loss is 5.053142070770264\n",
      "(64, 33)\n",
      "step 14738, loss is 4.676522254943848\n",
      "(64, 33)\n",
      "step 14739, loss is 4.56118106842041\n",
      "(64, 33)\n",
      "step 14740, loss is 4.948641777038574\n",
      "(64, 33)\n",
      "step 14741, loss is 4.815315246582031\n",
      "(64, 33)\n",
      "step 14742, loss is 4.666900157928467\n",
      "(64, 33)\n",
      "step 14743, loss is 4.835578918457031\n",
      "(64, 33)\n",
      "step 14744, loss is 4.755546569824219\n",
      "(64, 33)\n",
      "step 14745, loss is 4.6063690185546875\n",
      "(64, 33)\n",
      "step 14746, loss is 4.704540729522705\n",
      "(64, 33)\n",
      "step 14747, loss is 4.658120632171631\n",
      "(64, 33)\n",
      "step 14748, loss is 4.8922119140625\n",
      "(64, 33)\n",
      "step 14749, loss is 4.908228397369385\n",
      "(64, 33)\n",
      "step 14750, loss is 4.682156085968018\n",
      "(64, 33)\n",
      "step 14751, loss is 4.92702579498291\n",
      "(64, 33)\n",
      "step 14752, loss is 4.8260908126831055\n",
      "(64, 33)\n",
      "step 14753, loss is 4.780965805053711\n",
      "(64, 33)\n",
      "step 14754, loss is 4.846869468688965\n",
      "(64, 33)\n",
      "step 14755, loss is 4.866429328918457\n",
      "(64, 33)\n",
      "step 14756, loss is 4.905385494232178\n",
      "(64, 33)\n",
      "step 14757, loss is 4.849612712860107\n",
      "(64, 33)\n",
      "step 14758, loss is 4.990341663360596\n",
      "(64, 33)\n",
      "step 14759, loss is 4.805388450622559\n",
      "(64, 33)\n",
      "step 14760, loss is 4.8371734619140625\n",
      "(64, 33)\n",
      "step 14761, loss is 4.745578289031982\n",
      "(64, 33)\n",
      "step 14762, loss is 4.848099231719971\n",
      "(64, 33)\n",
      "step 14763, loss is 4.662377834320068\n",
      "(64, 33)\n",
      "step 14764, loss is 4.652736186981201\n",
      "(64, 33)\n",
      "step 14765, loss is 4.884976863861084\n",
      "(64, 33)\n",
      "step 14766, loss is 4.708523273468018\n",
      "(64, 33)\n",
      "step 14767, loss is 5.120665073394775\n",
      "(64, 33)\n",
      "step 14768, loss is 4.703187465667725\n",
      "(64, 33)\n",
      "step 14769, loss is 4.7037272453308105\n",
      "(64, 33)\n",
      "step 14770, loss is 4.817870616912842\n",
      "(64, 33)\n",
      "step 14771, loss is 4.617945194244385\n",
      "(64, 33)\n",
      "step 14772, loss is 4.621664047241211\n",
      "(64, 33)\n",
      "step 14773, loss is 4.824670314788818\n",
      "(64, 33)\n",
      "step 14774, loss is 4.868149280548096\n",
      "(64, 33)\n",
      "step 14775, loss is 4.737132549285889\n",
      "(64, 33)\n",
      "step 14776, loss is 4.980721473693848\n",
      "(64, 33)\n",
      "step 14777, loss is 4.710992813110352\n",
      "(64, 33)\n",
      "step 14778, loss is 4.8687872886657715\n",
      "(64, 33)\n",
      "step 14779, loss is 4.928475856781006\n",
      "(64, 33)\n",
      "step 14780, loss is 4.794673919677734\n",
      "(64, 33)\n",
      "step 14781, loss is 4.866035461425781\n",
      "(64, 33)\n",
      "step 14782, loss is 4.693004608154297\n",
      "(64, 33)\n",
      "step 14783, loss is 4.834731101989746\n",
      "(64, 33)\n",
      "step 14784, loss is 4.82259464263916\n",
      "(64, 33)\n",
      "step 14785, loss is 4.820197582244873\n",
      "(64, 33)\n",
      "step 14786, loss is 4.785212516784668\n",
      "(64, 33)\n",
      "step 14787, loss is 4.912283897399902\n",
      "(64, 33)\n",
      "step 14788, loss is 4.558887481689453\n",
      "(64, 33)\n",
      "step 14789, loss is 4.75895881652832\n",
      "(64, 33)\n",
      "step 14790, loss is 4.7923407554626465\n",
      "(64, 33)\n",
      "step 14791, loss is 4.902290344238281\n",
      "(64, 33)\n",
      "step 14792, loss is 4.753878593444824\n",
      "(64, 33)\n",
      "step 14793, loss is 4.576311111450195\n",
      "(64, 33)\n",
      "step 14794, loss is 4.760181903839111\n",
      "(64, 33)\n",
      "step 14795, loss is 4.789193630218506\n",
      "(64, 33)\n",
      "step 14796, loss is 4.923890113830566\n",
      "(64, 33)\n",
      "step 14797, loss is 4.546668529510498\n",
      "(64, 33)\n",
      "step 14798, loss is 4.825815677642822\n",
      "(64, 33)\n",
      "step 14799, loss is 4.802303314208984\n",
      "(64, 33)\n",
      "step 14800, loss is 4.944013595581055\n",
      "(64, 33)\n",
      "step 14801, loss is 4.8279523849487305\n",
      "(64, 33)\n",
      "step 14802, loss is 4.716324329376221\n",
      "(64, 33)\n",
      "step 14803, loss is 4.708497047424316\n",
      "(64, 33)\n",
      "step 14804, loss is 4.697739601135254\n",
      "(64, 33)\n",
      "step 14805, loss is 4.576507568359375\n",
      "(64, 33)\n",
      "step 14806, loss is 4.908055782318115\n",
      "(64, 33)\n",
      "step 14807, loss is 4.9074201583862305\n",
      "(64, 33)\n",
      "step 14808, loss is 4.728734970092773\n",
      "(64, 33)\n",
      "step 14809, loss is 4.757050514221191\n",
      "(64, 33)\n",
      "step 14810, loss is 4.654950141906738\n",
      "(64, 33)\n",
      "step 14811, loss is 4.848663330078125\n",
      "(64, 33)\n",
      "step 14812, loss is 4.948949813842773\n",
      "(64, 33)\n",
      "step 14813, loss is 4.890334129333496\n",
      "(64, 33)\n",
      "step 14814, loss is 4.759106636047363\n",
      "(64, 33)\n",
      "step 14815, loss is 4.915254592895508\n",
      "(64, 33)\n",
      "step 14816, loss is 4.696740627288818\n",
      "(64, 33)\n",
      "step 14817, loss is 4.929632663726807\n",
      "(64, 33)\n",
      "step 14818, loss is 4.980448246002197\n",
      "(64, 33)\n",
      "step 14819, loss is 4.834195137023926\n",
      "(64, 33)\n",
      "step 14820, loss is 4.990817070007324\n",
      "(64, 33)\n",
      "step 14821, loss is 4.860300064086914\n",
      "(64, 33)\n",
      "step 14822, loss is 4.972701072692871\n",
      "(64, 33)\n",
      "step 14823, loss is 4.7875285148620605\n",
      "(64, 33)\n",
      "step 14824, loss is 4.87003231048584\n",
      "(64, 33)\n",
      "step 14825, loss is 4.8783278465271\n",
      "(64, 33)\n",
      "step 14826, loss is 4.655788898468018\n",
      "(64, 33)\n",
      "step 14827, loss is 4.903878211975098\n",
      "(64, 33)\n",
      "step 14828, loss is 4.90724515914917\n",
      "(64, 33)\n",
      "step 14829, loss is 4.875865459442139\n",
      "(64, 33)\n",
      "step 14830, loss is 4.791969299316406\n",
      "(64, 33)\n",
      "step 14831, loss is 4.667651653289795\n",
      "(64, 33)\n",
      "step 14832, loss is 4.710746765136719\n",
      "(64, 33)\n",
      "step 14833, loss is 4.687039852142334\n",
      "(64, 33)\n",
      "step 14834, loss is 4.807897567749023\n",
      "(64, 33)\n",
      "step 14835, loss is 4.663204193115234\n",
      "(64, 33)\n",
      "step 14836, loss is 4.790092945098877\n",
      "(64, 33)\n",
      "step 14837, loss is 4.9024658203125\n",
      "(64, 33)\n",
      "step 14838, loss is 4.976985454559326\n",
      "(64, 33)\n",
      "step 14839, loss is 4.975623607635498\n",
      "(64, 33)\n",
      "step 14840, loss is 4.831374168395996\n",
      "(64, 33)\n",
      "step 14841, loss is 4.728481292724609\n",
      "(64, 33)\n",
      "step 14842, loss is 4.700462341308594\n",
      "(64, 33)\n",
      "step 14843, loss is 4.775365829467773\n",
      "(64, 33)\n",
      "step 14844, loss is 4.7556023597717285\n",
      "(64, 33)\n",
      "step 14845, loss is 5.031940937042236\n",
      "(64, 33)\n",
      "step 14846, loss is 4.718766689300537\n",
      "(64, 33)\n",
      "step 14847, loss is 4.994744777679443\n",
      "(64, 33)\n",
      "step 14848, loss is 4.815588474273682\n",
      "(64, 33)\n",
      "step 14849, loss is 4.734245300292969\n",
      "(64, 33)\n",
      "step 14850, loss is 4.798924922943115\n",
      "(64, 33)\n",
      "step 14851, loss is 4.769689083099365\n",
      "(64, 33)\n",
      "step 14852, loss is 4.864243030548096\n",
      "(64, 33)\n",
      "step 14853, loss is 4.7333807945251465\n",
      "(64, 33)\n",
      "step 14854, loss is 4.738511562347412\n",
      "(64, 33)\n",
      "step 14855, loss is 4.687803268432617\n",
      "(64, 33)\n",
      "step 14856, loss is 4.912656307220459\n",
      "(64, 33)\n",
      "step 14857, loss is 4.937858581542969\n",
      "(64, 33)\n",
      "step 14858, loss is 4.730757713317871\n",
      "(64, 33)\n",
      "step 14859, loss is 4.874939441680908\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14860, loss is 4.801639080047607\n",
      "(64, 33)\n",
      "step 14861, loss is 4.729671478271484\n",
      "(64, 33)\n",
      "step 14862, loss is 4.85163688659668\n",
      "(64, 33)\n",
      "step 14863, loss is 4.912925720214844\n",
      "(64, 33)\n",
      "step 14864, loss is 4.970580101013184\n",
      "(64, 33)\n",
      "step 14865, loss is 4.675058841705322\n",
      "(64, 33)\n",
      "step 14866, loss is 4.776515483856201\n",
      "(64, 33)\n",
      "step 14867, loss is 4.718066215515137\n",
      "(64, 33)\n",
      "step 14868, loss is 4.525155544281006\n",
      "(64, 33)\n",
      "step 14869, loss is 4.7986555099487305\n",
      "(64, 33)\n",
      "step 14870, loss is 4.802795886993408\n",
      "(64, 33)\n",
      "step 14871, loss is 4.889460563659668\n",
      "(64, 33)\n",
      "step 14872, loss is 4.770792484283447\n",
      "(64, 33)\n",
      "step 14873, loss is 4.837776184082031\n",
      "(64, 33)\n",
      "step 14874, loss is 5.086047172546387\n",
      "(64, 33)\n",
      "step 14875, loss is 5.0129218101501465\n",
      "(64, 33)\n",
      "step 14876, loss is 4.708067893981934\n",
      "(64, 33)\n",
      "step 14877, loss is 4.883201599121094\n",
      "(64, 33)\n",
      "step 14878, loss is 4.691946029663086\n",
      "(64, 33)\n",
      "step 14879, loss is 4.828334331512451\n",
      "(64, 33)\n",
      "step 14880, loss is 4.836256504058838\n",
      "(64, 33)\n",
      "step 14881, loss is 4.862613677978516\n",
      "(64, 33)\n",
      "step 14882, loss is 4.745629787445068\n",
      "(64, 33)\n",
      "step 14883, loss is 4.898867130279541\n",
      "(64, 33)\n",
      "step 14884, loss is 4.758946895599365\n",
      "(64, 33)\n",
      "step 14885, loss is 4.757774829864502\n",
      "(64, 33)\n",
      "step 14886, loss is 4.859535217285156\n",
      "(64, 33)\n",
      "step 14887, loss is 4.763255596160889\n",
      "(64, 33)\n",
      "step 14888, loss is 4.92161750793457\n",
      "(64, 33)\n",
      "step 14889, loss is 4.898838520050049\n",
      "(64, 33)\n",
      "step 14890, loss is 4.7799530029296875\n",
      "(64, 33)\n",
      "step 14891, loss is 4.788007736206055\n",
      "(64, 33)\n",
      "step 14892, loss is 4.765416145324707\n",
      "(64, 33)\n",
      "step 14893, loss is 4.764793395996094\n",
      "(64, 33)\n",
      "step 14894, loss is 4.8906402587890625\n",
      "(64, 33)\n",
      "step 14895, loss is 4.762096881866455\n",
      "(64, 33)\n",
      "step 14896, loss is 4.866971015930176\n",
      "(64, 33)\n",
      "step 14897, loss is 4.802947044372559\n",
      "(64, 33)\n",
      "step 14898, loss is 4.6923956871032715\n",
      "(64, 33)\n",
      "step 14899, loss is 4.568255424499512\n",
      "(64, 33)\n",
      "step 14900, loss is 4.799137592315674\n",
      "(64, 33)\n",
      "step 14901, loss is 4.823121070861816\n",
      "(64, 33)\n",
      "step 14902, loss is 4.889915466308594\n",
      "(64, 33)\n",
      "step 14903, loss is 4.784504413604736\n",
      "(64, 33)\n",
      "step 14904, loss is 4.621211528778076\n",
      "(64, 33)\n",
      "step 14905, loss is 4.786165237426758\n",
      "(64, 33)\n",
      "step 14906, loss is 4.826278209686279\n",
      "(64, 33)\n",
      "step 14907, loss is 4.771365165710449\n",
      "(64, 33)\n",
      "step 14908, loss is 4.912621021270752\n",
      "(64, 33)\n",
      "step 14909, loss is 4.818500995635986\n",
      "(64, 33)\n",
      "step 14910, loss is 4.808689594268799\n",
      "(64, 33)\n",
      "step 14911, loss is 4.876730918884277\n",
      "(64, 33)\n",
      "step 14912, loss is 5.020051002502441\n",
      "(64, 33)\n",
      "step 14913, loss is 4.883989334106445\n",
      "(64, 33)\n",
      "step 14914, loss is 5.019323348999023\n",
      "(64, 33)\n",
      "step 14915, loss is 4.759849548339844\n",
      "(64, 33)\n",
      "step 14916, loss is 4.849125862121582\n",
      "(64, 33)\n",
      "step 14917, loss is 4.788204669952393\n",
      "(64, 33)\n",
      "step 14918, loss is 4.7017598152160645\n",
      "(64, 33)\n",
      "step 14919, loss is 4.816778182983398\n",
      "(64, 33)\n",
      "step 14920, loss is 4.780258655548096\n",
      "(64, 33)\n",
      "step 14921, loss is 4.96688985824585\n",
      "(64, 33)\n",
      "step 14922, loss is 4.665672302246094\n",
      "(64, 33)\n",
      "step 14923, loss is 4.672517776489258\n",
      "(64, 33)\n",
      "step 14924, loss is 4.868139266967773\n",
      "(64, 33)\n",
      "step 14925, loss is 4.868429183959961\n",
      "(64, 33)\n",
      "step 14926, loss is 4.727457046508789\n",
      "(64, 33)\n",
      "step 14927, loss is 4.916825771331787\n",
      "(64, 33)\n",
      "step 14928, loss is 4.795315265655518\n",
      "(64, 33)\n",
      "step 14929, loss is 4.894189357757568\n",
      "(64, 33)\n",
      "step 14930, loss is 4.962218284606934\n",
      "(64, 33)\n",
      "step 14931, loss is 4.741570949554443\n",
      "(64, 33)\n",
      "step 14932, loss is 4.9934401512146\n",
      "(64, 33)\n",
      "step 14933, loss is 4.724027633666992\n",
      "(64, 33)\n",
      "step 14934, loss is 4.868793964385986\n",
      "(64, 33)\n",
      "step 14935, loss is 4.754796981811523\n",
      "(64, 33)\n",
      "step 14936, loss is 4.737085342407227\n",
      "(64, 33)\n",
      "step 14937, loss is 4.855827331542969\n",
      "(64, 33)\n",
      "step 14938, loss is 4.9076457023620605\n",
      "(64, 33)\n",
      "step 14939, loss is 4.855214595794678\n",
      "(64, 33)\n",
      "step 14940, loss is 4.816467761993408\n",
      "(64, 33)\n",
      "step 14941, loss is 4.721107006072998\n",
      "(64, 33)\n",
      "step 14942, loss is 4.745830059051514\n",
      "(64, 33)\n",
      "step 14943, loss is 5.02366828918457\n",
      "(64, 33)\n",
      "step 14944, loss is 4.816566467285156\n",
      "(64, 33)\n",
      "step 14945, loss is 4.901527404785156\n",
      "(64, 33)\n",
      "step 14946, loss is 4.781073570251465\n",
      "(64, 33)\n",
      "step 14947, loss is 4.905426502227783\n",
      "(64, 33)\n",
      "step 14948, loss is 4.8057074546813965\n",
      "(64, 33)\n",
      "step 14949, loss is 4.938003063201904\n",
      "(64, 33)\n",
      "step 14950, loss is 5.002379417419434\n",
      "(64, 33)\n",
      "step 14951, loss is 4.935865879058838\n",
      "(64, 33)\n",
      "step 14952, loss is 4.776034355163574\n",
      "(64, 33)\n",
      "step 14953, loss is 5.050694465637207\n",
      "(64, 33)\n",
      "step 14954, loss is 4.66436767578125\n",
      "(64, 33)\n",
      "step 14955, loss is 4.7136712074279785\n",
      "(64, 33)\n",
      "step 14956, loss is 4.612713813781738\n",
      "(64, 33)\n",
      "step 14957, loss is 5.004476070404053\n",
      "(64, 33)\n",
      "step 14958, loss is 4.866224765777588\n",
      "(64, 33)\n",
      "step 14959, loss is 4.750629425048828\n",
      "(64, 33)\n",
      "step 14960, loss is 4.902800559997559\n",
      "(64, 33)\n",
      "step 14961, loss is 4.8680315017700195\n",
      "(64, 33)\n",
      "step 14962, loss is 4.980095386505127\n",
      "(64, 33)\n",
      "step 14963, loss is 4.684279441833496\n",
      "(64, 33)\n",
      "step 14964, loss is 4.96539831161499\n",
      "(64, 33)\n",
      "step 14965, loss is 4.883072853088379\n",
      "(64, 33)\n",
      "step 14966, loss is 5.003834247589111\n",
      "(64, 33)\n",
      "step 14967, loss is 4.832764148712158\n",
      "(64, 33)\n",
      "step 14968, loss is 4.663384437561035\n",
      "(64, 33)\n",
      "step 14969, loss is 4.950911045074463\n",
      "(64, 33)\n",
      "step 14970, loss is 4.893325328826904\n",
      "(64, 33)\n",
      "step 14971, loss is 4.776122570037842\n",
      "(64, 33)\n",
      "step 14972, loss is 4.816483497619629\n",
      "(64, 33)\n",
      "step 14973, loss is 4.809131145477295\n",
      "(64, 33)\n",
      "step 14974, loss is 4.631202220916748\n",
      "(64, 33)\n",
      "step 14975, loss is 4.979515075683594\n",
      "(64, 33)\n",
      "step 14976, loss is 4.744147300720215\n",
      "(64, 33)\n",
      "step 14977, loss is 4.69441032409668\n",
      "(64, 33)\n",
      "step 14978, loss is 4.855481147766113\n",
      "(64, 33)\n",
      "step 14979, loss is 4.6823410987854\n",
      "(64, 33)\n",
      "step 14980, loss is 4.720268726348877\n",
      "(64, 33)\n",
      "step 14981, loss is 4.903944969177246\n",
      "(64, 33)\n",
      "step 14982, loss is 4.73309326171875\n",
      "(64, 33)\n",
      "step 14983, loss is 4.799367427825928\n",
      "(64, 33)\n",
      "step 14984, loss is 5.055659294128418\n",
      "(64, 33)\n",
      "step 14985, loss is 4.724924087524414\n",
      "(64, 33)\n",
      "step 14986, loss is 4.819243907928467\n",
      "(64, 33)\n",
      "step 14987, loss is 4.763820648193359\n",
      "(64, 33)\n",
      "step 14988, loss is 4.640313625335693\n",
      "(64, 33)\n",
      "step 14989, loss is 4.840144634246826\n",
      "(64, 33)\n",
      "step 14990, loss is 4.802793502807617\n",
      "(64, 33)\n",
      "step 14991, loss is 4.943990230560303\n",
      "(64, 33)\n",
      "step 14992, loss is 4.816624641418457\n",
      "(64, 33)\n",
      "step 14993, loss is 4.656569480895996\n",
      "(64, 33)\n",
      "step 14994, loss is 4.8183488845825195\n",
      "(64, 33)\n",
      "step 14995, loss is 4.646057605743408\n",
      "(64, 33)\n",
      "step 14996, loss is 4.83269739151001\n",
      "(64, 33)\n",
      "step 14997, loss is 4.838681221008301\n",
      "(64, 33)\n",
      "step 14998, loss is 4.738481521606445\n",
      "(64, 33)\n",
      "step 14999, loss is 5.022860050201416\n",
      "(64, 33)\n",
      "step 15000, loss is 4.793224334716797\n",
      "(64, 33)\n",
      "step 15001, loss is 4.7607421875\n",
      "(64, 33)\n",
      "step 15002, loss is 4.725788593292236\n",
      "(64, 33)\n",
      "step 15003, loss is 4.628432750701904\n",
      "(64, 33)\n",
      "step 15004, loss is 4.651952743530273\n",
      "(64, 33)\n",
      "step 15005, loss is 4.823761940002441\n",
      "(64, 33)\n",
      "step 15006, loss is 4.950677394866943\n",
      "(64, 33)\n",
      "step 15007, loss is 4.9209794998168945\n",
      "(64, 33)\n",
      "step 15008, loss is 4.677610397338867\n",
      "(64, 33)\n",
      "step 15009, loss is 4.989356517791748\n",
      "(64, 33)\n",
      "step 15010, loss is 5.052708625793457\n",
      "(64, 33)\n",
      "step 15011, loss is 4.8959269523620605\n",
      "(64, 33)\n",
      "step 15012, loss is 4.829189777374268\n",
      "(64, 33)\n",
      "step 15013, loss is 4.855327129364014\n",
      "(64, 33)\n",
      "step 15014, loss is 4.7685418128967285\n",
      "(64, 33)\n",
      "step 15015, loss is 4.570962429046631\n",
      "(64, 33)\n",
      "step 15016, loss is 4.793365478515625\n",
      "(64, 33)\n",
      "step 15017, loss is 4.780069351196289\n",
      "(64, 33)\n",
      "step 15018, loss is 4.83388090133667\n",
      "(64, 33)\n",
      "step 15019, loss is 4.929452419281006\n",
      "(64, 33)\n",
      "step 15020, loss is 4.883110046386719\n",
      "(64, 33)\n",
      "step 15021, loss is 4.8741302490234375\n",
      "(64, 33)\n",
      "step 15022, loss is 4.7104315757751465\n",
      "(64, 33)\n",
      "step 15023, loss is 4.741145133972168\n",
      "(64, 33)\n",
      "step 15024, loss is 4.695791721343994\n",
      "(64, 33)\n",
      "step 15025, loss is 4.83358907699585\n",
      "(64, 33)\n",
      "step 15026, loss is 4.792308330535889\n",
      "(64, 33)\n",
      "step 15027, loss is 4.591907978057861\n",
      "(64, 33)\n",
      "step 15028, loss is 4.87794828414917\n",
      "(64, 33)\n",
      "step 15029, loss is 4.87884521484375\n",
      "(64, 33)\n",
      "step 15030, loss is 4.767096996307373\n",
      "(64, 33)\n",
      "step 15031, loss is 4.905284881591797\n",
      "(64, 33)\n",
      "step 15032, loss is 4.761091232299805\n",
      "(64, 33)\n",
      "step 15033, loss is 4.883638381958008\n",
      "(64, 33)\n",
      "step 15034, loss is 4.959156513214111\n",
      "(64, 33)\n",
      "step 15035, loss is 4.735818862915039\n",
      "(64, 33)\n",
      "step 15036, loss is 4.965152263641357\n",
      "(64, 33)\n",
      "step 15037, loss is 4.629218578338623\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15038, loss is 4.882742881774902\n",
      "(64, 33)\n",
      "step 15039, loss is 4.677640914916992\n",
      "(64, 33)\n",
      "step 15040, loss is 4.827307224273682\n",
      "(64, 33)\n",
      "step 15041, loss is 4.812363147735596\n",
      "(64, 33)\n",
      "step 15042, loss is 4.701672077178955\n",
      "(64, 33)\n",
      "step 15043, loss is 4.8085036277771\n",
      "(64, 33)\n",
      "step 15044, loss is 4.943007946014404\n",
      "(64, 33)\n",
      "step 15045, loss is 4.725844383239746\n",
      "(64, 33)\n",
      "step 15046, loss is 4.6735687255859375\n",
      "(64, 33)\n",
      "step 15047, loss is 4.59861421585083\n",
      "(64, 33)\n",
      "step 15048, loss is 4.843520641326904\n",
      "(64, 33)\n",
      "step 15049, loss is 4.841104030609131\n",
      "(64, 33)\n",
      "step 15050, loss is 4.893474578857422\n",
      "(64, 33)\n",
      "step 15051, loss is 4.753773212432861\n",
      "(64, 33)\n",
      "step 15052, loss is 4.806292533874512\n",
      "(64, 33)\n",
      "step 15053, loss is 4.741186618804932\n",
      "(64, 33)\n",
      "step 15054, loss is 4.575143337249756\n",
      "(64, 33)\n",
      "step 15055, loss is 4.712740421295166\n",
      "(64, 33)\n",
      "step 15056, loss is 4.815053939819336\n",
      "(64, 33)\n",
      "step 15057, loss is 4.945736885070801\n",
      "(64, 33)\n",
      "step 15058, loss is 4.758434295654297\n",
      "(64, 33)\n",
      "step 15059, loss is 4.7352495193481445\n",
      "(64, 33)\n",
      "step 15060, loss is 4.897050380706787\n",
      "(64, 33)\n",
      "step 15061, loss is 4.702824592590332\n",
      "(64, 33)\n",
      "step 15062, loss is 4.712294578552246\n",
      "(64, 33)\n",
      "step 15063, loss is 4.773216247558594\n",
      "(64, 33)\n",
      "step 15064, loss is 4.746346950531006\n",
      "(64, 33)\n",
      "step 15065, loss is 4.735724449157715\n",
      "(64, 33)\n",
      "step 15066, loss is 4.904430866241455\n",
      "(64, 33)\n",
      "step 15067, loss is 4.918157577514648\n",
      "(64, 33)\n",
      "step 15068, loss is 4.827550411224365\n",
      "(64, 33)\n",
      "step 15069, loss is 4.891201496124268\n",
      "(64, 33)\n",
      "step 15070, loss is 4.943288326263428\n",
      "(64, 33)\n",
      "step 15071, loss is 4.867953300476074\n",
      "(64, 33)\n",
      "step 15072, loss is 4.872341632843018\n",
      "(64, 33)\n",
      "step 15073, loss is 5.036813735961914\n",
      "(64, 33)\n",
      "step 15074, loss is 4.8456525802612305\n",
      "(64, 33)\n",
      "step 15075, loss is 4.82266902923584\n",
      "(64, 33)\n",
      "step 15076, loss is 4.821975231170654\n",
      "(64, 33)\n",
      "step 15077, loss is 4.788362503051758\n",
      "(64, 33)\n",
      "step 15078, loss is 4.993941783905029\n",
      "(64, 33)\n",
      "step 15079, loss is 4.98883056640625\n",
      "(64, 33)\n",
      "step 15080, loss is 5.0284929275512695\n",
      "(64, 33)\n",
      "step 15081, loss is 4.945994853973389\n",
      "(64, 33)\n",
      "step 15082, loss is 4.926722526550293\n",
      "(64, 33)\n",
      "step 15083, loss is 4.869799613952637\n",
      "(64, 33)\n",
      "step 15084, loss is 4.919919967651367\n",
      "(64, 33)\n",
      "step 15085, loss is 4.798460960388184\n",
      "(64, 33)\n",
      "step 15086, loss is 4.909146308898926\n",
      "(64, 33)\n",
      "step 15087, loss is 4.85083532333374\n",
      "(64, 33)\n",
      "step 15088, loss is 4.873046875\n",
      "(64, 33)\n",
      "step 15089, loss is 4.880918979644775\n",
      "(64, 33)\n",
      "step 15090, loss is 4.818465709686279\n",
      "(64, 33)\n",
      "step 15091, loss is 4.606093406677246\n",
      "(64, 33)\n",
      "step 15092, loss is 4.829536437988281\n",
      "(64, 33)\n",
      "step 15093, loss is 4.775064945220947\n",
      "(64, 33)\n",
      "step 15094, loss is 4.7164201736450195\n",
      "(64, 33)\n",
      "step 15095, loss is 4.843019962310791\n",
      "(64, 33)\n",
      "step 15096, loss is 4.752604007720947\n",
      "(64, 33)\n",
      "step 15097, loss is 4.906816005706787\n",
      "(64, 33)\n",
      "step 15098, loss is 4.663764953613281\n",
      "(64, 33)\n",
      "step 15099, loss is 4.7094550132751465\n",
      "(64, 33)\n",
      "step 15100, loss is 4.662350654602051\n",
      "(64, 33)\n",
      "step 15101, loss is 4.8902153968811035\n",
      "(64, 33)\n",
      "step 15102, loss is 4.6530561447143555\n",
      "(64, 33)\n",
      "step 15103, loss is 4.788122653961182\n",
      "(64, 33)\n",
      "step 15104, loss is 4.97075080871582\n",
      "(64, 33)\n",
      "step 15105, loss is 4.833629608154297\n",
      "(64, 33)\n",
      "step 15106, loss is 4.867885589599609\n",
      "(64, 33)\n",
      "step 15107, loss is 4.681263446807861\n",
      "(64, 33)\n",
      "step 15108, loss is 4.96496057510376\n",
      "(64, 33)\n",
      "step 15109, loss is 4.823498249053955\n",
      "(64, 33)\n",
      "step 15110, loss is 4.8303985595703125\n",
      "(64, 33)\n",
      "step 15111, loss is 4.840588092803955\n",
      "(64, 33)\n",
      "step 15112, loss is 4.919966697692871\n",
      "(64, 33)\n",
      "step 15113, loss is 4.881541728973389\n",
      "(64, 33)\n",
      "step 15114, loss is 4.794680118560791\n",
      "(64, 33)\n",
      "step 15115, loss is 4.971532821655273\n",
      "(64, 33)\n",
      "step 15116, loss is 4.667020320892334\n",
      "(64, 33)\n",
      "step 15117, loss is 4.834975719451904\n",
      "(64, 33)\n",
      "step 15118, loss is 4.7677459716796875\n",
      "(64, 33)\n",
      "step 15119, loss is 4.75396203994751\n",
      "(64, 33)\n",
      "step 15120, loss is 4.784482002258301\n",
      "(64, 33)\n",
      "step 15121, loss is 4.832359790802002\n",
      "(64, 33)\n",
      "step 15122, loss is 4.935496807098389\n",
      "(64, 33)\n",
      "step 15123, loss is 4.903067111968994\n",
      "(64, 33)\n",
      "step 15124, loss is 4.947840690612793\n",
      "(64, 33)\n",
      "step 15125, loss is 4.815175533294678\n",
      "(64, 33)\n",
      "step 15126, loss is 4.824108123779297\n",
      "(64, 33)\n",
      "step 15127, loss is 4.967246055603027\n",
      "(64, 33)\n",
      "step 15128, loss is 4.935555934906006\n",
      "(64, 33)\n",
      "step 15129, loss is 4.626495361328125\n",
      "(64, 33)\n",
      "step 15130, loss is 4.8611578941345215\n",
      "(64, 33)\n",
      "step 15131, loss is 4.76131010055542\n",
      "(64, 33)\n",
      "step 15132, loss is 4.930839538574219\n",
      "(64, 33)\n",
      "step 15133, loss is 4.596963882446289\n",
      "(64, 33)\n",
      "step 15134, loss is 4.940394401550293\n",
      "(64, 33)\n",
      "step 15135, loss is 4.698883056640625\n",
      "(64, 33)\n",
      "step 15136, loss is 4.928971767425537\n",
      "(64, 33)\n",
      "step 15137, loss is 4.863753795623779\n",
      "(64, 33)\n",
      "step 15138, loss is 4.66285514831543\n",
      "(64, 33)\n",
      "step 15139, loss is 4.695518970489502\n",
      "(64, 33)\n",
      "step 15140, loss is 4.823545455932617\n",
      "(64, 33)\n",
      "step 15141, loss is 4.8916754722595215\n",
      "(64, 33)\n",
      "step 15142, loss is 4.749881267547607\n",
      "(64, 33)\n",
      "step 15143, loss is 4.845644950866699\n",
      "(64, 33)\n",
      "step 15144, loss is 4.757755279541016\n",
      "(64, 33)\n",
      "step 15145, loss is 4.53835391998291\n",
      "(64, 33)\n",
      "step 15146, loss is 4.868514060974121\n",
      "(64, 33)\n",
      "step 15147, loss is 4.926307678222656\n",
      "(64, 33)\n",
      "step 15148, loss is 4.673951148986816\n",
      "(64, 33)\n",
      "step 15149, loss is 4.803515911102295\n",
      "(64, 33)\n",
      "step 15150, loss is 4.822567939758301\n",
      "(64, 33)\n",
      "step 15151, loss is 4.84579610824585\n",
      "(64, 33)\n",
      "step 15152, loss is 4.744688510894775\n",
      "(64, 33)\n",
      "step 15153, loss is 4.977149963378906\n",
      "(64, 33)\n",
      "step 15154, loss is 4.6440253257751465\n",
      "(64, 33)\n",
      "step 15155, loss is 4.8923516273498535\n",
      "(64, 33)\n",
      "step 15156, loss is 4.757104873657227\n",
      "(64, 33)\n",
      "step 15157, loss is 4.874173164367676\n",
      "(64, 33)\n",
      "step 15158, loss is 4.795858860015869\n",
      "(64, 33)\n",
      "step 15159, loss is 4.8629326820373535\n",
      "(64, 33)\n",
      "step 15160, loss is 4.7396416664123535\n",
      "(64, 33)\n",
      "step 15161, loss is 4.778254508972168\n",
      "(64, 33)\n",
      "step 15162, loss is 4.963279724121094\n",
      "(64, 33)\n",
      "step 15163, loss is 4.984358310699463\n",
      "(64, 33)\n",
      "step 15164, loss is 4.755037307739258\n",
      "(64, 33)\n",
      "step 15165, loss is 4.95263671875\n",
      "(64, 33)\n",
      "step 15166, loss is 4.715925693511963\n",
      "(64, 33)\n",
      "step 15167, loss is 4.7977495193481445\n",
      "(64, 33)\n",
      "step 15168, loss is 4.892046928405762\n",
      "(64, 33)\n",
      "step 15169, loss is 4.829416751861572\n",
      "(64, 33)\n",
      "step 15170, loss is 4.89676570892334\n",
      "(64, 33)\n",
      "step 15171, loss is 4.870424270629883\n",
      "(64, 33)\n",
      "step 15172, loss is 4.878729820251465\n",
      "(64, 33)\n",
      "step 15173, loss is 4.8140177726745605\n",
      "(64, 33)\n",
      "step 15174, loss is 4.864827632904053\n",
      "(64, 33)\n",
      "step 15175, loss is 4.661809921264648\n",
      "(64, 33)\n",
      "step 15176, loss is 4.939621448516846\n",
      "(64, 33)\n",
      "step 15177, loss is 4.901864051818848\n",
      "(64, 33)\n",
      "step 15178, loss is 4.82590913772583\n",
      "(64, 33)\n",
      "step 15179, loss is 4.757818222045898\n",
      "(64, 33)\n",
      "step 15180, loss is 4.771093845367432\n",
      "(64, 33)\n",
      "step 15181, loss is 4.86696720123291\n",
      "(64, 33)\n",
      "step 15182, loss is 4.7301506996154785\n",
      "(64, 33)\n",
      "step 15183, loss is 4.919983863830566\n",
      "(64, 33)\n",
      "step 15184, loss is 4.756560325622559\n",
      "(64, 33)\n",
      "step 15185, loss is 4.951500415802002\n",
      "(64, 33)\n",
      "step 15186, loss is 4.900827884674072\n",
      "(64, 33)\n",
      "step 15187, loss is 4.714351654052734\n",
      "(64, 33)\n",
      "step 15188, loss is 4.828381061553955\n",
      "(64, 33)\n",
      "step 15189, loss is 4.785912036895752\n",
      "(64, 33)\n",
      "step 15190, loss is 4.808051586151123\n",
      "(64, 33)\n",
      "step 15191, loss is 4.565803050994873\n",
      "(64, 33)\n",
      "step 15192, loss is 4.72003173828125\n",
      "(64, 33)\n",
      "step 15193, loss is 4.882266521453857\n",
      "(64, 33)\n",
      "step 15194, loss is 4.801397800445557\n",
      "(64, 33)\n",
      "step 15195, loss is 4.728826999664307\n",
      "(64, 33)\n",
      "step 15196, loss is 4.707905292510986\n",
      "(64, 33)\n",
      "step 15197, loss is 4.707262992858887\n",
      "(64, 33)\n",
      "step 15198, loss is 4.750380039215088\n",
      "(64, 33)\n",
      "step 15199, loss is 4.9066033363342285\n",
      "(64, 33)\n",
      "step 15200, loss is 4.943074703216553\n",
      "(64, 33)\n",
      "step 15201, loss is 4.793791770935059\n",
      "(64, 33)\n",
      "step 15202, loss is 4.810669898986816\n",
      "(64, 33)\n",
      "step 15203, loss is 4.633971691131592\n",
      "(64, 33)\n",
      "step 15204, loss is 4.7610015869140625\n",
      "(64, 33)\n",
      "step 15205, loss is 4.8245768547058105\n",
      "(64, 33)\n",
      "step 15206, loss is 4.831472396850586\n",
      "(64, 33)\n",
      "step 15207, loss is 4.756274223327637\n",
      "(64, 33)\n",
      "step 15208, loss is 4.939490795135498\n",
      "(64, 33)\n",
      "step 15209, loss is 4.872082233428955\n",
      "(64, 33)\n",
      "step 15210, loss is 4.845983505249023\n",
      "(64, 33)\n",
      "step 15211, loss is 4.883838176727295\n",
      "(64, 33)\n",
      "step 15212, loss is 4.624709606170654\n",
      "(64, 33)\n",
      "step 15213, loss is 4.749590873718262\n",
      "(64, 33)\n",
      "step 15214, loss is 4.6519246101379395\n",
      "(64, 33)\n",
      "step 15215, loss is 4.908447742462158\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15216, loss is 4.9090070724487305\n",
      "(64, 33)\n",
      "step 15217, loss is 4.8774638175964355\n",
      "(64, 33)\n",
      "step 15218, loss is 4.768433094024658\n",
      "(64, 33)\n",
      "step 15219, loss is 4.825172424316406\n",
      "(64, 33)\n",
      "step 15220, loss is 4.837158679962158\n",
      "(64, 33)\n",
      "step 15221, loss is 4.822013854980469\n",
      "(64, 33)\n",
      "step 15222, loss is 4.778956890106201\n",
      "(64, 33)\n",
      "step 15223, loss is 4.684959411621094\n",
      "(64, 33)\n",
      "step 15224, loss is 4.775052547454834\n",
      "(64, 33)\n",
      "step 15225, loss is 4.7566680908203125\n",
      "(64, 33)\n",
      "step 15226, loss is 4.818190574645996\n",
      "(64, 33)\n",
      "step 15227, loss is 4.7742180824279785\n",
      "(64, 33)\n",
      "step 15228, loss is 4.645292282104492\n",
      "(64, 33)\n",
      "step 15229, loss is 4.768354415893555\n",
      "(64, 33)\n",
      "step 15230, loss is 4.714683532714844\n",
      "(64, 33)\n",
      "step 15231, loss is 4.898469924926758\n",
      "(64, 33)\n",
      "step 15232, loss is 4.845479965209961\n",
      "(64, 33)\n",
      "step 15233, loss is 4.839527606964111\n",
      "(64, 33)\n",
      "step 15234, loss is 4.72857141494751\n",
      "(64, 33)\n",
      "step 15235, loss is 4.749938011169434\n",
      "(64, 33)\n",
      "step 15236, loss is 4.673602104187012\n",
      "(64, 33)\n",
      "step 15237, loss is 4.7798075675964355\n",
      "(64, 33)\n",
      "step 15238, loss is 4.85337495803833\n",
      "(64, 33)\n",
      "step 15239, loss is 4.694234848022461\n",
      "(64, 33)\n",
      "step 15240, loss is 4.781172752380371\n",
      "(64, 33)\n",
      "step 15241, loss is 4.775600433349609\n",
      "(64, 33)\n",
      "step 15242, loss is 4.747231960296631\n",
      "(64, 33)\n",
      "step 15243, loss is 4.68742036819458\n",
      "(64, 33)\n",
      "step 15244, loss is 4.650272846221924\n",
      "(64, 33)\n",
      "step 15245, loss is 5.006608963012695\n",
      "(64, 33)\n",
      "step 15246, loss is 4.797276496887207\n",
      "(64, 33)\n",
      "step 15247, loss is 4.682995796203613\n",
      "(64, 33)\n",
      "step 15248, loss is 4.661023139953613\n",
      "(64, 33)\n",
      "step 15249, loss is 4.7097930908203125\n",
      "(64, 33)\n",
      "step 15250, loss is 4.72211217880249\n",
      "(64, 33)\n",
      "step 15251, loss is 4.768380165100098\n",
      "(64, 33)\n",
      "step 15252, loss is 4.812211036682129\n",
      "(64, 33)\n",
      "step 15253, loss is 4.840973854064941\n",
      "(64, 33)\n",
      "step 15254, loss is 4.831247329711914\n",
      "(64, 33)\n",
      "step 15255, loss is 4.732983589172363\n",
      "(64, 33)\n",
      "step 15256, loss is 4.711000919342041\n",
      "(64, 33)\n",
      "step 15257, loss is 4.954660415649414\n",
      "(64, 33)\n",
      "step 15258, loss is 4.777000427246094\n",
      "(64, 33)\n",
      "step 15259, loss is 4.873567581176758\n",
      "(64, 33)\n",
      "step 15260, loss is 4.765817642211914\n",
      "(64, 33)\n",
      "step 15261, loss is 4.769351005554199\n",
      "(64, 33)\n",
      "step 15262, loss is 4.671779632568359\n",
      "(64, 33)\n",
      "step 15263, loss is 4.737126350402832\n",
      "(64, 33)\n",
      "step 15264, loss is 4.797688007354736\n",
      "(64, 33)\n",
      "step 15265, loss is 4.766876220703125\n",
      "(64, 33)\n",
      "step 15266, loss is 4.915979862213135\n",
      "(64, 33)\n",
      "step 15267, loss is 4.695296764373779\n",
      "(64, 33)\n",
      "step 15268, loss is 4.686427116394043\n",
      "(64, 33)\n",
      "step 15269, loss is 4.875892639160156\n",
      "(64, 33)\n",
      "step 15270, loss is 4.822998523712158\n",
      "(64, 33)\n",
      "step 15271, loss is 4.672549247741699\n",
      "(64, 33)\n",
      "step 15272, loss is 4.763237476348877\n",
      "(64, 33)\n",
      "step 15273, loss is 4.715933322906494\n",
      "(64, 33)\n",
      "step 15274, loss is 4.7657952308654785\n",
      "(64, 33)\n",
      "step 15275, loss is 4.826228618621826\n",
      "(64, 33)\n",
      "step 15276, loss is 4.68789529800415\n",
      "(64, 33)\n",
      "step 15277, loss is 4.7390217781066895\n",
      "(64, 33)\n",
      "step 15278, loss is 4.7627787590026855\n",
      "(64, 33)\n",
      "step 15279, loss is 4.798800945281982\n",
      "(64, 33)\n",
      "step 15280, loss is 4.989627838134766\n",
      "(64, 33)\n",
      "step 15281, loss is 4.758664608001709\n",
      "(64, 33)\n",
      "step 15282, loss is 4.894909381866455\n",
      "(64, 33)\n",
      "step 15283, loss is 4.72801399230957\n",
      "(64, 33)\n",
      "step 15284, loss is 5.001075744628906\n",
      "(64, 33)\n",
      "step 15285, loss is 4.840667724609375\n",
      "(64, 33)\n",
      "step 15286, loss is 4.663234233856201\n",
      "(64, 33)\n",
      "step 15287, loss is 4.86768102645874\n",
      "(64, 33)\n",
      "step 15288, loss is 4.7859930992126465\n",
      "(64, 33)\n",
      "step 15289, loss is 4.910353183746338\n",
      "(64, 33)\n",
      "step 15290, loss is 4.803919315338135\n",
      "(64, 33)\n",
      "step 15291, loss is 4.5279083251953125\n",
      "(64, 33)\n",
      "step 15292, loss is 4.721942901611328\n",
      "(64, 33)\n",
      "step 15293, loss is 4.744208335876465\n",
      "(64, 33)\n",
      "step 15294, loss is 4.773187637329102\n",
      "(64, 33)\n",
      "step 15295, loss is 4.658205986022949\n",
      "(64, 33)\n",
      "step 15296, loss is 4.827753067016602\n",
      "(64, 33)\n",
      "step 15297, loss is 4.538834095001221\n",
      "(64, 33)\n",
      "step 15298, loss is 4.793989658355713\n",
      "(64, 33)\n",
      "step 15299, loss is 4.765909194946289\n",
      "(64, 33)\n",
      "step 15300, loss is 4.729769229888916\n",
      "(64, 33)\n",
      "step 15301, loss is 4.744872570037842\n",
      "(64, 33)\n",
      "step 15302, loss is 4.747027397155762\n",
      "(64, 33)\n",
      "step 15303, loss is 4.91475248336792\n",
      "(64, 33)\n",
      "step 15304, loss is 4.775165557861328\n",
      "(64, 33)\n",
      "step 15305, loss is 4.644923210144043\n",
      "(64, 33)\n",
      "step 15306, loss is 4.772069454193115\n",
      "(64, 33)\n",
      "step 15307, loss is 4.921100616455078\n",
      "(64, 33)\n",
      "step 15308, loss is 4.782844066619873\n",
      "(64, 33)\n",
      "step 15309, loss is 4.839475631713867\n",
      "(64, 33)\n",
      "step 15310, loss is 4.763206958770752\n",
      "(64, 33)\n",
      "step 15311, loss is 4.820926666259766\n",
      "(64, 33)\n",
      "step 15312, loss is 4.805258274078369\n",
      "(64, 33)\n",
      "step 15313, loss is 4.7615437507629395\n",
      "(64, 33)\n",
      "step 15314, loss is 4.867048740386963\n",
      "(64, 33)\n",
      "step 15315, loss is 4.8863844871521\n",
      "(64, 33)\n",
      "step 15316, loss is 4.997334003448486\n",
      "(64, 33)\n",
      "step 15317, loss is 4.849078178405762\n",
      "(64, 33)\n",
      "step 15318, loss is 4.716130256652832\n",
      "(64, 33)\n",
      "step 15319, loss is 4.798942565917969\n",
      "(64, 33)\n",
      "step 15320, loss is 4.798498630523682\n",
      "(64, 33)\n",
      "step 15321, loss is 4.750304698944092\n",
      "(64, 33)\n",
      "step 15322, loss is 4.850433826446533\n",
      "(64, 33)\n",
      "step 15323, loss is 4.717706203460693\n",
      "(64, 33)\n",
      "step 15324, loss is 4.695842266082764\n",
      "(64, 33)\n",
      "step 15325, loss is 4.847135543823242\n",
      "(64, 33)\n",
      "step 15326, loss is 4.938037395477295\n",
      "(64, 33)\n",
      "step 15327, loss is 4.823169231414795\n",
      "(64, 33)\n",
      "step 15328, loss is 4.904428482055664\n",
      "(64, 33)\n",
      "step 15329, loss is 4.828632831573486\n",
      "(64, 33)\n",
      "step 15330, loss is 4.6912455558776855\n",
      "(64, 33)\n",
      "step 15331, loss is 4.865396499633789\n",
      "(64, 33)\n",
      "step 15332, loss is 4.754542350769043\n",
      "(64, 33)\n",
      "step 15333, loss is 4.959173202514648\n",
      "(64, 33)\n",
      "step 15334, loss is 4.67807674407959\n",
      "(64, 33)\n",
      "step 15335, loss is 4.833600997924805\n",
      "(64, 33)\n",
      "step 15336, loss is 4.804591655731201\n",
      "(64, 33)\n",
      "step 15337, loss is 4.993026256561279\n",
      "(64, 33)\n",
      "step 15338, loss is 4.867822647094727\n",
      "(64, 33)\n",
      "step 15339, loss is 4.616147041320801\n",
      "(64, 33)\n",
      "step 15340, loss is 4.8466668128967285\n",
      "(64, 33)\n",
      "step 15341, loss is 4.748718738555908\n",
      "(64, 33)\n",
      "step 15342, loss is 4.923612594604492\n",
      "(64, 33)\n",
      "step 15343, loss is 4.754614353179932\n",
      "(64, 33)\n",
      "step 15344, loss is 4.939687252044678\n",
      "(64, 33)\n",
      "step 15345, loss is 4.8651533126831055\n",
      "(64, 33)\n",
      "step 15346, loss is 4.9193549156188965\n",
      "(64, 33)\n",
      "step 15347, loss is 4.817320823669434\n",
      "(64, 33)\n",
      "step 15348, loss is 4.7758660316467285\n",
      "(64, 33)\n",
      "step 15349, loss is 4.827494144439697\n",
      "(64, 33)\n",
      "step 15350, loss is 4.886782646179199\n",
      "(64, 33)\n",
      "step 15351, loss is 4.923329830169678\n",
      "(64, 33)\n",
      "step 15352, loss is 4.786745548248291\n",
      "(64, 33)\n",
      "step 15353, loss is 4.666136741638184\n",
      "(64, 33)\n",
      "step 15354, loss is 4.839482307434082\n",
      "(64, 33)\n",
      "step 15355, loss is 4.858338832855225\n",
      "(64, 33)\n",
      "step 15356, loss is 4.91966438293457\n",
      "(64, 33)\n",
      "step 15357, loss is 4.7232465744018555\n",
      "(64, 33)\n",
      "step 15358, loss is 4.663505554199219\n",
      "(64, 33)\n",
      "step 15359, loss is 4.884047985076904\n",
      "(64, 33)\n",
      "step 15360, loss is 4.73582649230957\n",
      "(64, 33)\n",
      "step 15361, loss is 4.934695720672607\n",
      "(64, 33)\n",
      "step 15362, loss is 4.856603145599365\n",
      "(64, 33)\n",
      "step 15363, loss is 4.798014163970947\n",
      "(64, 33)\n",
      "step 15364, loss is 4.764255523681641\n",
      "(64, 33)\n",
      "step 15365, loss is 4.892378330230713\n",
      "(64, 33)\n",
      "step 15366, loss is 4.646087646484375\n",
      "(64, 33)\n",
      "step 15367, loss is 4.674919128417969\n",
      "(64, 33)\n",
      "step 15368, loss is 4.896303653717041\n",
      "(64, 33)\n",
      "step 15369, loss is 4.645019054412842\n",
      "(64, 33)\n",
      "step 15370, loss is 4.862985134124756\n",
      "(64, 33)\n",
      "step 15371, loss is 4.951016426086426\n",
      "(64, 33)\n",
      "step 15372, loss is 4.749462604522705\n",
      "(64, 33)\n",
      "step 15373, loss is 4.723132133483887\n",
      "(64, 33)\n",
      "step 15374, loss is 4.930294036865234\n",
      "(64, 33)\n",
      "step 15375, loss is 4.83638858795166\n",
      "(64, 33)\n",
      "step 15376, loss is 4.9822468757629395\n",
      "(64, 33)\n",
      "step 15377, loss is 4.7083916664123535\n",
      "(64, 33)\n",
      "step 15378, loss is 4.822946071624756\n",
      "(64, 33)\n",
      "step 15379, loss is 4.775290012359619\n",
      "(64, 33)\n",
      "step 15380, loss is 4.719729423522949\n",
      "(64, 33)\n",
      "step 15381, loss is 4.873347282409668\n",
      "(64, 33)\n",
      "step 15382, loss is 4.80912971496582\n",
      "(64, 33)\n",
      "step 15383, loss is 4.869248867034912\n",
      "(64, 33)\n",
      "step 15384, loss is 4.920878887176514\n",
      "(64, 33)\n",
      "step 15385, loss is 4.674560546875\n",
      "(64, 33)\n",
      "step 15386, loss is 4.8299336433410645\n",
      "(64, 33)\n",
      "step 15387, loss is 4.78804874420166\n",
      "(64, 33)\n",
      "step 15388, loss is 4.9301910400390625\n",
      "(64, 33)\n",
      "step 15389, loss is 4.859886646270752\n",
      "(64, 33)\n",
      "step 15390, loss is 4.830496311187744\n",
      "(64, 33)\n",
      "step 15391, loss is 4.8347907066345215\n",
      "(64, 33)\n",
      "step 15392, loss is 4.861328125\n",
      "(64, 33)\n",
      "step 15393, loss is 4.868644714355469\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15394, loss is 4.727906703948975\n",
      "(64, 33)\n",
      "step 15395, loss is 4.870428562164307\n",
      "(64, 33)\n",
      "step 15396, loss is 4.836113452911377\n",
      "(64, 33)\n",
      "step 15397, loss is 4.833305358886719\n",
      "(64, 33)\n",
      "step 15398, loss is 4.90371036529541\n",
      "(64, 33)\n",
      "step 15399, loss is 4.872570991516113\n",
      "(64, 33)\n",
      "step 15400, loss is 4.692194938659668\n",
      "(64, 33)\n",
      "step 15401, loss is 4.985346794128418\n",
      "(64, 33)\n",
      "step 15402, loss is 5.002170085906982\n",
      "(64, 33)\n",
      "step 15403, loss is 4.651738166809082\n",
      "(64, 33)\n",
      "step 15404, loss is 4.798896789550781\n",
      "(64, 33)\n",
      "step 15405, loss is 4.947441101074219\n",
      "(64, 33)\n",
      "step 15406, loss is 4.670328617095947\n",
      "(64, 33)\n",
      "step 15407, loss is 4.9343976974487305\n",
      "(64, 33)\n",
      "step 15408, loss is 4.963135242462158\n",
      "(64, 33)\n",
      "step 15409, loss is 4.823992729187012\n",
      "(64, 33)\n",
      "step 15410, loss is 4.928887844085693\n",
      "(64, 33)\n",
      "step 15411, loss is 4.763224124908447\n",
      "(64, 33)\n",
      "step 15412, loss is 4.732243061065674\n",
      "(64, 33)\n",
      "step 15413, loss is 4.621765613555908\n",
      "(64, 33)\n",
      "step 15414, loss is 4.637566566467285\n",
      "(64, 33)\n",
      "step 15415, loss is 4.962339878082275\n",
      "(64, 33)\n",
      "step 15416, loss is 4.820038318634033\n",
      "(64, 33)\n",
      "step 15417, loss is 4.775051116943359\n",
      "(64, 33)\n",
      "step 15418, loss is 4.799534320831299\n",
      "(64, 33)\n",
      "step 15419, loss is 4.965150833129883\n",
      "(64, 33)\n",
      "step 15420, loss is 4.95866584777832\n",
      "(64, 33)\n",
      "step 15421, loss is 4.906636714935303\n",
      "(64, 33)\n",
      "step 15422, loss is 4.795702934265137\n",
      "(64, 33)\n",
      "step 15423, loss is 4.829778671264648\n",
      "(64, 33)\n",
      "step 15424, loss is 4.812158107757568\n",
      "(64, 33)\n",
      "step 15425, loss is 4.847845554351807\n",
      "(64, 33)\n",
      "step 15426, loss is 4.813618183135986\n",
      "(64, 33)\n",
      "step 15427, loss is 4.876240253448486\n",
      "(64, 33)\n",
      "step 15428, loss is 4.609067440032959\n",
      "(64, 33)\n",
      "step 15429, loss is 4.748016834259033\n",
      "(64, 33)\n",
      "step 15430, loss is 4.785569667816162\n",
      "(64, 33)\n",
      "step 15431, loss is 4.8667144775390625\n",
      "(64, 33)\n",
      "step 15432, loss is 4.648997783660889\n",
      "(64, 33)\n",
      "step 15433, loss is 4.90869140625\n",
      "(64, 33)\n",
      "step 15434, loss is 4.759565830230713\n",
      "(64, 33)\n",
      "step 15435, loss is 4.91008996963501\n",
      "(64, 33)\n",
      "step 15436, loss is 4.864099025726318\n",
      "(64, 33)\n",
      "step 15437, loss is 4.6683173179626465\n",
      "(64, 33)\n",
      "step 15438, loss is 4.6537556648254395\n",
      "(64, 33)\n",
      "step 15439, loss is 4.782058238983154\n",
      "(64, 33)\n",
      "step 15440, loss is 4.677929878234863\n",
      "(64, 33)\n",
      "step 15441, loss is 4.845175266265869\n",
      "(64, 33)\n",
      "step 15442, loss is 4.758266448974609\n",
      "(64, 33)\n",
      "step 15443, loss is 4.77620267868042\n",
      "(64, 33)\n",
      "step 15444, loss is 4.888616561889648\n",
      "(64, 33)\n",
      "step 15445, loss is 4.911910533905029\n",
      "(64, 33)\n",
      "step 15446, loss is 4.974456787109375\n",
      "(64, 33)\n",
      "step 15447, loss is 4.849664688110352\n",
      "(64, 33)\n",
      "step 15448, loss is 5.034366607666016\n",
      "(64, 33)\n",
      "step 15449, loss is 4.827150821685791\n",
      "(64, 33)\n",
      "step 15450, loss is 4.929465293884277\n",
      "(64, 33)\n",
      "step 15451, loss is 5.0507612228393555\n",
      "(64, 33)\n",
      "step 15452, loss is 4.900927543640137\n",
      "(64, 33)\n",
      "step 15453, loss is 4.727586269378662\n",
      "(64, 33)\n",
      "step 15454, loss is 4.712020397186279\n",
      "(64, 33)\n",
      "step 15455, loss is 4.595612049102783\n",
      "(64, 33)\n",
      "step 15456, loss is 4.8479084968566895\n",
      "(64, 33)\n",
      "step 15457, loss is 4.743808269500732\n",
      "(64, 33)\n",
      "step 15458, loss is 5.011017799377441\n",
      "(64, 33)\n",
      "step 15459, loss is 4.813030242919922\n",
      "(64, 33)\n",
      "step 15460, loss is 4.508057117462158\n",
      "(64, 33)\n",
      "step 15461, loss is 4.766260623931885\n",
      "(64, 33)\n",
      "step 15462, loss is 4.55751371383667\n",
      "(64, 33)\n",
      "step 15463, loss is 4.726968288421631\n",
      "(64, 33)\n",
      "step 15464, loss is 4.669215679168701\n",
      "(64, 33)\n",
      "step 15465, loss is 4.8416032791137695\n",
      "(64, 33)\n",
      "step 15466, loss is 4.833165168762207\n",
      "(64, 33)\n",
      "step 15467, loss is 4.657296180725098\n",
      "(64, 33)\n",
      "step 15468, loss is 4.934029579162598\n",
      "(64, 33)\n",
      "step 15469, loss is 4.867255210876465\n",
      "(64, 33)\n",
      "step 15470, loss is 4.7100958824157715\n",
      "(64, 33)\n",
      "step 15471, loss is 4.827547073364258\n",
      "(64, 33)\n",
      "step 15472, loss is 4.8748860359191895\n",
      "(64, 33)\n",
      "step 15473, loss is 4.9070024490356445\n",
      "(64, 33)\n",
      "step 15474, loss is 4.795063018798828\n",
      "(64, 33)\n",
      "step 15475, loss is 4.753481864929199\n",
      "(64, 33)\n",
      "step 15476, loss is 4.951649188995361\n",
      "(64, 33)\n",
      "step 15477, loss is 4.7899627685546875\n",
      "(64, 33)\n",
      "step 15478, loss is 4.873420715332031\n",
      "(64, 33)\n",
      "step 15479, loss is 4.855955600738525\n",
      "(64, 33)\n",
      "step 15480, loss is 4.778207778930664\n",
      "(64, 33)\n",
      "step 15481, loss is 4.945831775665283\n",
      "(64, 33)\n",
      "step 15482, loss is 5.048906326293945\n",
      "(64, 33)\n",
      "step 15483, loss is 4.741166114807129\n",
      "(64, 33)\n",
      "step 15484, loss is 4.677906513214111\n",
      "(64, 33)\n",
      "step 15485, loss is 4.8233866691589355\n",
      "(64, 33)\n",
      "step 15486, loss is 4.743330478668213\n",
      "(64, 33)\n",
      "step 15487, loss is 4.695465087890625\n",
      "(64, 33)\n",
      "step 15488, loss is 4.656088352203369\n",
      "(64, 33)\n",
      "step 15489, loss is 4.578019618988037\n",
      "(64, 33)\n",
      "step 15490, loss is 4.6596808433532715\n",
      "(64, 33)\n",
      "step 15491, loss is 4.784148216247559\n",
      "(64, 33)\n",
      "step 15492, loss is 4.629777908325195\n",
      "(64, 33)\n",
      "step 15493, loss is 4.723367691040039\n",
      "(64, 33)\n",
      "step 15494, loss is 4.692142009735107\n",
      "(64, 33)\n",
      "step 15495, loss is 4.829216480255127\n",
      "(64, 33)\n",
      "step 15496, loss is 4.796694755554199\n",
      "(64, 33)\n",
      "step 15497, loss is 4.689645767211914\n",
      "(64, 33)\n",
      "step 15498, loss is 4.934584140777588\n",
      "(64, 33)\n",
      "step 15499, loss is 4.823439598083496\n",
      "(64, 33)\n",
      "step 15500, loss is 4.975738525390625\n",
      "(64, 33)\n",
      "step 15501, loss is 4.875817775726318\n",
      "(64, 33)\n",
      "step 15502, loss is 4.900699615478516\n",
      "(64, 33)\n",
      "step 15503, loss is 4.814202308654785\n",
      "(64, 33)\n",
      "step 15504, loss is 4.8463053703308105\n",
      "(64, 33)\n",
      "step 15505, loss is 4.7814154624938965\n",
      "(64, 33)\n",
      "step 15506, loss is 4.743537425994873\n",
      "(64, 33)\n",
      "step 15507, loss is 4.72356653213501\n",
      "(64, 33)\n",
      "step 15508, loss is 4.6093058586120605\n",
      "(64, 33)\n",
      "step 15509, loss is 4.502043724060059\n",
      "(64, 33)\n",
      "step 15510, loss is 4.814351558685303\n",
      "(64, 33)\n",
      "step 15511, loss is 4.683110237121582\n",
      "(64, 33)\n",
      "step 15512, loss is 4.833688735961914\n",
      "(64, 33)\n",
      "step 15513, loss is 4.8816094398498535\n",
      "(64, 33)\n",
      "step 15514, loss is 4.77454948425293\n",
      "(64, 33)\n",
      "step 15515, loss is 4.566438674926758\n",
      "(64, 33)\n",
      "step 15516, loss is 4.726500511169434\n",
      "(64, 33)\n",
      "step 15517, loss is 4.843503952026367\n",
      "(64, 33)\n",
      "step 15518, loss is 4.658754825592041\n",
      "(64, 33)\n",
      "step 15519, loss is 4.834700584411621\n",
      "(64, 33)\n",
      "step 15520, loss is 4.669178009033203\n",
      "(64, 33)\n",
      "step 15521, loss is 4.845760345458984\n",
      "(64, 33)\n",
      "step 15522, loss is 4.928108215332031\n",
      "(64, 33)\n",
      "step 15523, loss is 4.852243900299072\n",
      "(64, 33)\n",
      "step 15524, loss is 4.863348960876465\n",
      "(64, 33)\n",
      "step 15525, loss is 4.7316131591796875\n",
      "(64, 33)\n",
      "step 15526, loss is 4.668518543243408\n",
      "(64, 33)\n",
      "step 15527, loss is 4.817433834075928\n",
      "(64, 33)\n",
      "step 15528, loss is 4.66115140914917\n",
      "(64, 33)\n",
      "step 15529, loss is 4.829304218292236\n",
      "(64, 33)\n",
      "step 15530, loss is 4.64671516418457\n",
      "(64, 33)\n",
      "step 15531, loss is 4.866686820983887\n",
      "(64, 33)\n",
      "step 15532, loss is 4.699751853942871\n",
      "(64, 33)\n",
      "step 15533, loss is 4.833272457122803\n",
      "(64, 33)\n",
      "step 15534, loss is 4.912303924560547\n",
      "(64, 33)\n",
      "step 15535, loss is 4.772176742553711\n",
      "(64, 33)\n",
      "step 15536, loss is 4.737069606781006\n",
      "(64, 33)\n",
      "step 15537, loss is 4.759796619415283\n",
      "(64, 33)\n",
      "step 15538, loss is 4.863201141357422\n",
      "(64, 33)\n",
      "step 15539, loss is 4.870353698730469\n",
      "(64, 33)\n",
      "step 15540, loss is 4.794337272644043\n",
      "(64, 33)\n",
      "step 15541, loss is 4.69964075088501\n",
      "(64, 33)\n",
      "step 15542, loss is 4.508749485015869\n",
      "(64, 33)\n",
      "step 15543, loss is 5.024369716644287\n",
      "(64, 33)\n",
      "step 15544, loss is 4.895961284637451\n",
      "(64, 33)\n",
      "step 15545, loss is 4.6481828689575195\n",
      "(64, 33)\n",
      "step 15546, loss is 4.876944541931152\n",
      "(64, 33)\n",
      "step 15547, loss is 4.66383695602417\n",
      "(64, 33)\n",
      "step 15548, loss is 4.868321418762207\n",
      "(64, 33)\n",
      "step 15549, loss is 4.742274761199951\n",
      "(64, 33)\n",
      "step 15550, loss is 4.795087814331055\n",
      "(64, 33)\n",
      "step 15551, loss is 4.706116199493408\n",
      "(64, 33)\n",
      "step 15552, loss is 4.896998882293701\n",
      "(64, 33)\n",
      "step 15553, loss is 4.819716930389404\n",
      "(64, 33)\n",
      "step 15554, loss is 4.904044151306152\n",
      "(64, 33)\n",
      "step 15555, loss is 4.753876686096191\n",
      "(64, 33)\n",
      "step 15556, loss is 4.822298526763916\n",
      "(64, 33)\n",
      "step 15557, loss is 4.7145094871521\n",
      "(64, 33)\n",
      "step 15558, loss is 4.742636203765869\n",
      "(64, 33)\n",
      "step 15559, loss is 4.850959300994873\n",
      "(64, 33)\n",
      "step 15560, loss is 4.820146560668945\n",
      "(64, 33)\n",
      "step 15561, loss is 4.712630271911621\n",
      "(64, 33)\n",
      "step 15562, loss is 4.857344627380371\n",
      "(64, 33)\n",
      "step 15563, loss is 4.63075065612793\n",
      "(64, 33)\n",
      "step 15564, loss is 4.563539505004883\n",
      "(64, 33)\n",
      "step 15565, loss is 4.813471794128418\n",
      "(64, 33)\n",
      "step 15566, loss is 4.79849910736084\n",
      "(64, 33)\n",
      "step 15567, loss is 4.818779945373535\n",
      "(64, 33)\n",
      "step 15568, loss is 4.626046657562256\n",
      "(64, 33)\n",
      "step 15569, loss is 4.8081536293029785\n",
      "(64, 33)\n",
      "step 15570, loss is 4.7159929275512695\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15571, loss is 4.587611198425293\n",
      "(64, 33)\n",
      "step 15572, loss is 4.690124988555908\n",
      "(64, 33)\n",
      "step 15573, loss is 4.829098701477051\n",
      "(64, 33)\n",
      "step 15574, loss is 4.802137851715088\n",
      "(64, 33)\n",
      "step 15575, loss is 4.769870281219482\n",
      "(64, 33)\n",
      "step 15576, loss is 4.684711456298828\n",
      "(64, 33)\n",
      "step 15577, loss is 4.725904941558838\n",
      "(64, 33)\n",
      "step 15578, loss is 4.871499061584473\n",
      "(64, 33)\n",
      "step 15579, loss is 4.779818058013916\n",
      "(64, 33)\n",
      "step 15580, loss is 4.769450664520264\n",
      "(64, 33)\n",
      "step 15581, loss is 4.708691120147705\n",
      "(64, 33)\n",
      "step 15582, loss is 4.8365349769592285\n",
      "(64, 33)\n",
      "step 15583, loss is 4.737679481506348\n",
      "(64, 33)\n",
      "step 15584, loss is 4.698916435241699\n",
      "(64, 33)\n",
      "step 15585, loss is 4.739720344543457\n",
      "(64, 33)\n",
      "step 15586, loss is 4.700353622436523\n",
      "(64, 33)\n",
      "step 15587, loss is 4.90912389755249\n",
      "(64, 33)\n",
      "step 15588, loss is 4.665830612182617\n",
      "(64, 33)\n",
      "step 15589, loss is 4.871491432189941\n",
      "(64, 33)\n",
      "step 15590, loss is 4.826774597167969\n",
      "(64, 33)\n",
      "step 15591, loss is 4.7971343994140625\n",
      "(64, 33)\n",
      "step 15592, loss is 4.8131818771362305\n",
      "(64, 33)\n",
      "step 15593, loss is 4.697373867034912\n",
      "(64, 33)\n",
      "step 15594, loss is 4.8643269538879395\n",
      "(64, 33)\n",
      "step 15595, loss is 4.839750289916992\n",
      "(64, 33)\n",
      "step 15596, loss is 4.824945449829102\n",
      "(64, 33)\n",
      "step 15597, loss is 4.794534683227539\n",
      "(64, 33)\n",
      "step 15598, loss is 4.980970859527588\n",
      "(64, 33)\n",
      "step 15599, loss is 4.853923320770264\n",
      "(64, 33)\n",
      "step 15600, loss is 4.795907974243164\n",
      "(64, 33)\n",
      "step 15601, loss is 4.7543439865112305\n",
      "(64, 33)\n",
      "step 15602, loss is 4.793595314025879\n",
      "(64, 33)\n",
      "step 15603, loss is 4.644659042358398\n",
      "(64, 33)\n",
      "step 15604, loss is 4.913825988769531\n",
      "(64, 33)\n",
      "step 15605, loss is 4.6472086906433105\n",
      "(64, 33)\n",
      "step 15606, loss is 4.839736461639404\n",
      "(64, 33)\n",
      "step 15607, loss is 4.865934371948242\n",
      "(64, 33)\n",
      "step 15608, loss is 4.972508430480957\n",
      "(64, 33)\n",
      "step 15609, loss is 4.762110710144043\n",
      "(64, 33)\n",
      "step 15610, loss is 4.7527337074279785\n",
      "(64, 33)\n",
      "step 15611, loss is 4.742392539978027\n",
      "(64, 33)\n",
      "step 15612, loss is 4.950557231903076\n",
      "(64, 33)\n",
      "step 15613, loss is 4.788976669311523\n",
      "(64, 33)\n",
      "step 15614, loss is 4.875528812408447\n",
      "(64, 33)\n",
      "step 15615, loss is 4.827062129974365\n",
      "(64, 33)\n",
      "step 15616, loss is 4.926731109619141\n",
      "(64, 33)\n",
      "step 15617, loss is 4.8232550621032715\n",
      "(64, 33)\n",
      "step 15618, loss is 4.816749572753906\n",
      "(64, 33)\n",
      "step 15619, loss is 4.744736194610596\n",
      "(64, 33)\n",
      "step 15620, loss is 4.834506034851074\n",
      "(64, 33)\n",
      "step 15621, loss is 4.825700283050537\n",
      "(64, 33)\n",
      "step 15622, loss is 4.85389518737793\n",
      "(64, 33)\n",
      "step 15623, loss is 4.886781215667725\n",
      "(64, 33)\n",
      "step 15624, loss is 4.774471282958984\n",
      "(64, 33)\n",
      "step 15625, loss is 4.664558410644531\n",
      "(64, 33)\n",
      "step 15626, loss is 4.857820510864258\n",
      "(64, 33)\n",
      "step 15627, loss is 4.656285285949707\n",
      "(64, 33)\n",
      "step 15628, loss is 4.91355562210083\n",
      "(64, 33)\n",
      "step 15629, loss is 4.806602954864502\n",
      "(64, 33)\n",
      "step 15630, loss is 4.995115280151367\n",
      "(64, 33)\n",
      "step 15631, loss is 4.918261528015137\n",
      "(64, 33)\n",
      "step 15632, loss is 4.937018394470215\n",
      "(64, 33)\n",
      "step 15633, loss is 4.8672637939453125\n",
      "(64, 33)\n",
      "step 15634, loss is 4.692359924316406\n",
      "(64, 33)\n",
      "step 15635, loss is 4.727149963378906\n",
      "(64, 33)\n",
      "step 15636, loss is 4.922848701477051\n",
      "(64, 33)\n",
      "step 15637, loss is 4.812595844268799\n",
      "(64, 33)\n",
      "step 15638, loss is 4.776656627655029\n",
      "(64, 33)\n",
      "step 15639, loss is 4.578703880310059\n",
      "(64, 33)\n",
      "step 15640, loss is 4.890201568603516\n",
      "(64, 33)\n",
      "step 15641, loss is 4.6574835777282715\n",
      "(64, 33)\n",
      "step 15642, loss is 4.516030311584473\n",
      "(64, 33)\n",
      "step 15643, loss is 4.7942728996276855\n",
      "(64, 33)\n",
      "step 15644, loss is 4.786744594573975\n",
      "(64, 33)\n",
      "step 15645, loss is 4.813697814941406\n",
      "(64, 33)\n",
      "step 15646, loss is 4.809398174285889\n",
      "(64, 33)\n",
      "step 15647, loss is 4.936824798583984\n",
      "(64, 33)\n",
      "step 15648, loss is 4.998802185058594\n",
      "(64, 33)\n",
      "step 15649, loss is 4.864876747131348\n",
      "(64, 33)\n",
      "step 15650, loss is 4.780911445617676\n",
      "(64, 33)\n",
      "step 15651, loss is 4.810447692871094\n",
      "(64, 33)\n",
      "step 15652, loss is 4.714093208312988\n",
      "(64, 33)\n",
      "step 15653, loss is 4.7350592613220215\n",
      "(64, 33)\n",
      "step 15654, loss is 4.910484790802002\n",
      "(64, 33)\n",
      "step 15655, loss is 4.952638149261475\n",
      "(64, 33)\n",
      "step 15656, loss is 4.661159515380859\n",
      "(64, 33)\n",
      "step 15657, loss is 4.645716190338135\n",
      "(64, 33)\n",
      "step 15658, loss is 4.836420059204102\n",
      "(64, 33)\n",
      "step 15659, loss is 4.816189765930176\n",
      "(64, 33)\n",
      "step 15660, loss is 4.943036079406738\n",
      "(64, 33)\n",
      "step 15661, loss is 4.763105392456055\n",
      "(64, 33)\n",
      "step 15662, loss is 4.785372257232666\n",
      "(64, 33)\n",
      "step 15663, loss is 4.737251281738281\n",
      "(64, 33)\n",
      "step 15664, loss is 4.620076656341553\n",
      "(64, 33)\n",
      "step 15665, loss is 4.921886444091797\n",
      "(64, 33)\n",
      "step 15666, loss is 4.644680976867676\n",
      "(64, 33)\n",
      "step 15667, loss is 4.842341423034668\n",
      "(64, 33)\n",
      "step 15668, loss is 4.822761058807373\n",
      "(64, 33)\n",
      "step 15669, loss is 4.842066764831543\n",
      "(64, 33)\n",
      "step 15670, loss is 4.665651321411133\n",
      "(64, 33)\n",
      "step 15671, loss is 4.783409595489502\n",
      "(64, 33)\n",
      "step 15672, loss is 4.725622653961182\n",
      "(64, 33)\n",
      "step 15673, loss is 4.70219087600708\n",
      "(64, 33)\n",
      "step 15674, loss is 4.796191215515137\n",
      "(64, 33)\n",
      "step 15675, loss is 4.814451694488525\n",
      "(64, 33)\n",
      "step 15676, loss is 4.920119762420654\n",
      "(64, 33)\n",
      "step 15677, loss is 4.82179594039917\n",
      "(64, 33)\n",
      "step 15678, loss is 4.916747093200684\n",
      "(64, 33)\n",
      "step 15679, loss is 4.474129676818848\n",
      "(64, 33)\n",
      "step 15680, loss is 4.811158657073975\n",
      "(64, 33)\n",
      "step 15681, loss is 4.7379584312438965\n",
      "(64, 33)\n",
      "step 15682, loss is 4.760245323181152\n",
      "(64, 33)\n",
      "step 15683, loss is 4.715635299682617\n",
      "(64, 33)\n",
      "step 15684, loss is 4.714443206787109\n",
      "(64, 33)\n",
      "step 15685, loss is 4.71962308883667\n",
      "(64, 33)\n",
      "step 15686, loss is 4.840472221374512\n",
      "(64, 33)\n",
      "step 15687, loss is 4.872988700866699\n",
      "(64, 33)\n",
      "step 15688, loss is 4.778352737426758\n",
      "(64, 33)\n",
      "step 15689, loss is 4.801198959350586\n",
      "(64, 33)\n",
      "step 15690, loss is 4.69663667678833\n",
      "(64, 33)\n",
      "step 15691, loss is 4.777215480804443\n",
      "(64, 33)\n",
      "step 15692, loss is 5.055102825164795\n",
      "(64, 33)\n",
      "step 15693, loss is 4.733216285705566\n",
      "(64, 33)\n",
      "step 15694, loss is 4.875752925872803\n",
      "(64, 33)\n",
      "step 15695, loss is 4.898278713226318\n",
      "(64, 33)\n",
      "step 15696, loss is 4.687392711639404\n",
      "(64, 33)\n",
      "step 15697, loss is 4.916971206665039\n",
      "(64, 33)\n",
      "step 15698, loss is 4.797847747802734\n",
      "(64, 33)\n",
      "step 15699, loss is 4.9467949867248535\n",
      "(64, 33)\n",
      "step 15700, loss is 4.849135398864746\n",
      "(64, 33)\n",
      "step 15701, loss is 4.787932395935059\n",
      "(64, 33)\n",
      "step 15702, loss is 4.805572509765625\n",
      "(64, 33)\n",
      "step 15703, loss is 4.762698173522949\n",
      "(64, 33)\n",
      "step 15704, loss is 4.781875133514404\n",
      "(64, 33)\n",
      "step 15705, loss is 4.734559535980225\n",
      "(64, 33)\n",
      "step 15706, loss is 4.77651309967041\n",
      "(64, 33)\n",
      "step 15707, loss is 4.803737163543701\n",
      "(64, 33)\n",
      "step 15708, loss is 4.920267581939697\n",
      "(64, 33)\n",
      "step 15709, loss is 4.833359718322754\n",
      "(64, 33)\n",
      "step 15710, loss is 4.636855602264404\n",
      "(64, 33)\n",
      "step 15711, loss is 4.820941925048828\n",
      "(64, 33)\n",
      "step 15712, loss is 4.860016345977783\n",
      "(64, 33)\n",
      "step 15713, loss is 4.719788074493408\n",
      "(64, 33)\n",
      "step 15714, loss is 4.719138145446777\n",
      "(64, 33)\n",
      "step 15715, loss is 4.66147518157959\n",
      "(64, 33)\n",
      "step 15716, loss is 4.745875358581543\n",
      "(64, 33)\n",
      "step 15717, loss is 4.726568222045898\n",
      "(64, 33)\n",
      "step 15718, loss is 4.879725933074951\n",
      "(64, 33)\n",
      "step 15719, loss is 4.682650566101074\n",
      "(64, 33)\n",
      "step 15720, loss is 4.616960048675537\n",
      "(64, 33)\n",
      "step 15721, loss is 4.801150798797607\n",
      "(64, 33)\n",
      "step 15722, loss is 4.872462272644043\n",
      "(64, 33)\n",
      "step 15723, loss is 4.8401384353637695\n",
      "(64, 33)\n",
      "step 15724, loss is 4.714156627655029\n",
      "(64, 33)\n",
      "step 15725, loss is 4.879486083984375\n",
      "(64, 33)\n",
      "step 15726, loss is 4.810995578765869\n",
      "(64, 33)\n",
      "step 15727, loss is 4.587757587432861\n",
      "(64, 33)\n",
      "step 15728, loss is 4.713500022888184\n",
      "(64, 33)\n",
      "step 15729, loss is 4.920888900756836\n",
      "(64, 33)\n",
      "step 15730, loss is 4.936314582824707\n",
      "(64, 33)\n",
      "step 15731, loss is 4.872514247894287\n",
      "(64, 33)\n",
      "step 15732, loss is 4.720026969909668\n",
      "(64, 33)\n",
      "step 15733, loss is 4.965416431427002\n",
      "(64, 33)\n",
      "step 15734, loss is 4.767770767211914\n",
      "(64, 33)\n",
      "step 15735, loss is 4.932845592498779\n",
      "(64, 33)\n",
      "step 15736, loss is 4.851253509521484\n",
      "(64, 33)\n",
      "step 15737, loss is 4.740277290344238\n",
      "(64, 33)\n",
      "step 15738, loss is 4.834159851074219\n",
      "(64, 33)\n",
      "step 15739, loss is 4.749008655548096\n",
      "(64, 33)\n",
      "step 15740, loss is 4.835762023925781\n",
      "(64, 33)\n",
      "step 15741, loss is 4.920459747314453\n",
      "(64, 33)\n",
      "step 15742, loss is 4.758657932281494\n",
      "(64, 33)\n",
      "step 15743, loss is 4.795662879943848\n",
      "(64, 33)\n",
      "step 15744, loss is 4.8368730545043945\n",
      "(64, 33)\n",
      "step 15745, loss is 4.849245071411133\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15746, loss is 4.911388397216797\n",
      "(64, 33)\n",
      "step 15747, loss is 4.843124866485596\n",
      "(64, 33)\n",
      "step 15748, loss is 4.811609268188477\n",
      "(64, 33)\n",
      "step 15749, loss is 4.797701835632324\n",
      "(64, 33)\n",
      "step 15750, loss is 4.941555976867676\n",
      "(64, 33)\n",
      "step 15751, loss is 4.82230281829834\n",
      "(64, 33)\n",
      "step 15752, loss is 4.740257263183594\n",
      "(64, 33)\n",
      "step 15753, loss is 4.871458530426025\n",
      "(64, 33)\n",
      "step 15754, loss is 4.87136697769165\n",
      "(64, 33)\n",
      "step 15755, loss is 4.718998908996582\n",
      "(64, 33)\n",
      "step 15756, loss is 4.797610759735107\n",
      "(64, 33)\n",
      "step 15757, loss is 4.741073131561279\n",
      "(64, 33)\n",
      "step 15758, loss is 4.9360270500183105\n",
      "(64, 33)\n",
      "step 15759, loss is 4.7054595947265625\n",
      "(64, 33)\n",
      "step 15760, loss is 4.798191070556641\n",
      "(64, 33)\n",
      "step 15761, loss is 4.696095943450928\n",
      "(64, 33)\n",
      "step 15762, loss is 4.57914924621582\n",
      "(64, 33)\n",
      "step 15763, loss is 4.875082969665527\n",
      "(64, 33)\n",
      "step 15764, loss is 4.647177219390869\n",
      "(64, 33)\n",
      "step 15765, loss is 4.79139518737793\n",
      "(64, 33)\n",
      "step 15766, loss is 4.635026931762695\n",
      "(64, 33)\n",
      "step 15767, loss is 4.988647937774658\n",
      "(64, 33)\n",
      "step 15768, loss is 4.7435688972473145\n",
      "(64, 33)\n",
      "step 15769, loss is 4.9192609786987305\n",
      "(64, 33)\n",
      "step 15770, loss is 4.625059127807617\n",
      "(64, 33)\n",
      "step 15771, loss is 4.783345699310303\n",
      "(64, 33)\n",
      "step 15772, loss is 4.765373229980469\n",
      "(64, 33)\n",
      "step 15773, loss is 4.762617588043213\n",
      "(64, 33)\n",
      "step 15774, loss is 4.713733673095703\n",
      "(64, 33)\n",
      "step 15775, loss is 4.813081741333008\n",
      "(64, 33)\n",
      "step 15776, loss is 4.885672569274902\n",
      "(64, 33)\n",
      "step 15777, loss is 4.638443946838379\n",
      "(64, 33)\n",
      "step 15778, loss is 4.937571048736572\n",
      "(64, 33)\n",
      "step 15779, loss is 4.728327751159668\n",
      "(64, 33)\n",
      "step 15780, loss is 4.740441799163818\n",
      "(64, 33)\n",
      "step 15781, loss is 4.95509147644043\n",
      "(64, 33)\n",
      "step 15782, loss is 4.84102201461792\n",
      "(64, 33)\n",
      "step 15783, loss is 4.789981842041016\n",
      "(64, 33)\n",
      "step 15784, loss is 4.629560470581055\n",
      "(64, 33)\n",
      "step 15785, loss is 4.869826793670654\n",
      "(64, 33)\n",
      "step 15786, loss is 4.855278015136719\n",
      "(64, 33)\n",
      "step 15787, loss is 4.855560302734375\n",
      "(64, 33)\n",
      "step 15788, loss is 4.74712610244751\n",
      "(64, 33)\n",
      "step 15789, loss is 4.7491607666015625\n",
      "(64, 33)\n",
      "step 15790, loss is 4.636091709136963\n",
      "(64, 33)\n",
      "step 15791, loss is 4.776444911956787\n",
      "(64, 33)\n",
      "step 15792, loss is 4.7899861335754395\n",
      "(64, 33)\n",
      "step 15793, loss is 4.952466011047363\n",
      "(64, 33)\n",
      "step 15794, loss is 4.771944046020508\n",
      "(64, 33)\n",
      "step 15795, loss is 4.855985164642334\n",
      "(64, 33)\n",
      "step 15796, loss is 4.692131042480469\n",
      "(64, 33)\n",
      "step 15797, loss is 4.864848613739014\n",
      "(64, 33)\n",
      "step 15798, loss is 4.782301425933838\n",
      "(64, 33)\n",
      "step 15799, loss is 4.897480010986328\n",
      "(64, 33)\n",
      "step 15800, loss is 4.8450846672058105\n",
      "(64, 33)\n",
      "step 15801, loss is 4.769217014312744\n",
      "(64, 33)\n",
      "step 15802, loss is 4.819456100463867\n",
      "(64, 33)\n",
      "step 15803, loss is 4.519836902618408\n",
      "(64, 33)\n",
      "step 15804, loss is 4.885265827178955\n",
      "(64, 33)\n",
      "step 15805, loss is 4.753647327423096\n",
      "(64, 33)\n",
      "step 15806, loss is 4.896795272827148\n",
      "(64, 33)\n",
      "step 15807, loss is 4.633101463317871\n",
      "(64, 33)\n",
      "step 15808, loss is 4.910314559936523\n",
      "(64, 33)\n",
      "step 15809, loss is 4.723672866821289\n",
      "(64, 33)\n",
      "step 15810, loss is 4.935220718383789\n",
      "(64, 33)\n",
      "step 15811, loss is 4.671665191650391\n",
      "(64, 33)\n",
      "step 15812, loss is 4.846621036529541\n",
      "(64, 33)\n",
      "step 15813, loss is 4.583958148956299\n",
      "(64, 33)\n",
      "step 15814, loss is 4.7444748878479\n",
      "(64, 33)\n",
      "step 15815, loss is 4.8226447105407715\n",
      "(64, 33)\n",
      "step 15816, loss is 4.862259864807129\n",
      "(64, 33)\n",
      "step 15817, loss is 4.80353307723999\n",
      "(64, 33)\n",
      "step 15818, loss is 4.832462787628174\n",
      "(64, 33)\n",
      "step 15819, loss is 4.820831775665283\n",
      "(64, 33)\n",
      "step 15820, loss is 4.782557487487793\n",
      "(64, 33)\n",
      "step 15821, loss is 4.816068649291992\n",
      "(64, 33)\n",
      "step 15822, loss is 4.58579683303833\n",
      "(64, 33)\n",
      "step 15823, loss is 4.796004772186279\n",
      "(64, 33)\n",
      "step 15824, loss is 4.830254077911377\n",
      "(64, 33)\n",
      "step 15825, loss is 4.756459712982178\n",
      "(64, 33)\n",
      "step 15826, loss is 4.863256454467773\n",
      "(64, 33)\n",
      "step 15827, loss is 4.691627502441406\n",
      "(64, 33)\n",
      "step 15828, loss is 4.624926567077637\n",
      "(64, 33)\n",
      "step 15829, loss is 4.7448625564575195\n",
      "(64, 33)\n",
      "step 15830, loss is 5.084043502807617\n",
      "(64, 33)\n",
      "step 15831, loss is 4.826383113861084\n",
      "(64, 33)\n",
      "step 15832, loss is 4.79813814163208\n",
      "(64, 33)\n",
      "step 15833, loss is 4.541658401489258\n",
      "(64, 33)\n",
      "step 15834, loss is 5.057360649108887\n",
      "(64, 33)\n",
      "step 15835, loss is 4.715845108032227\n",
      "(64, 33)\n",
      "step 15836, loss is 4.952520370483398\n",
      "(64, 33)\n",
      "step 15837, loss is 4.799559116363525\n",
      "(64, 33)\n",
      "step 15838, loss is 4.98911190032959\n",
      "(64, 33)\n",
      "step 15839, loss is 4.844419002532959\n",
      "(64, 33)\n",
      "step 15840, loss is 4.778944492340088\n",
      "(64, 33)\n",
      "step 15841, loss is 4.862040996551514\n",
      "(64, 33)\n",
      "step 15842, loss is 4.53601598739624\n",
      "(64, 33)\n",
      "step 15843, loss is 4.646550178527832\n",
      "(64, 33)\n",
      "step 15844, loss is 4.776252746582031\n",
      "(64, 33)\n",
      "step 15845, loss is 4.797236442565918\n",
      "(64, 33)\n",
      "step 15846, loss is 4.700423717498779\n",
      "(64, 33)\n",
      "step 15847, loss is 4.740599632263184\n",
      "(64, 33)\n",
      "step 15848, loss is 4.843481063842773\n",
      "(64, 33)\n",
      "step 15849, loss is 4.761500358581543\n",
      "(64, 33)\n",
      "step 15850, loss is 4.762241363525391\n",
      "(64, 33)\n",
      "step 15851, loss is 4.684771537780762\n",
      "(64, 33)\n",
      "step 15852, loss is 4.800089359283447\n",
      "(64, 33)\n",
      "step 15853, loss is 4.771642684936523\n",
      "(64, 33)\n",
      "step 15854, loss is 4.697414875030518\n",
      "(64, 33)\n",
      "step 15855, loss is 4.604292392730713\n",
      "(64, 33)\n",
      "step 15856, loss is 4.857773303985596\n",
      "(64, 33)\n",
      "step 15857, loss is 4.907681465148926\n",
      "(64, 33)\n",
      "step 15858, loss is 4.7872490882873535\n",
      "(64, 33)\n",
      "step 15859, loss is 4.826559066772461\n",
      "(64, 33)\n",
      "step 15860, loss is 4.774914264678955\n",
      "(64, 33)\n",
      "step 15861, loss is 4.797513008117676\n",
      "(64, 33)\n",
      "step 15862, loss is 4.809948921203613\n",
      "(64, 33)\n",
      "step 15863, loss is 4.864178657531738\n",
      "(64, 33)\n",
      "step 15864, loss is 4.887869358062744\n",
      "(64, 33)\n",
      "step 15865, loss is 4.7839484214782715\n",
      "(64, 33)\n",
      "step 15866, loss is 4.96552038192749\n",
      "(64, 33)\n",
      "step 15867, loss is 4.831052780151367\n",
      "(64, 33)\n",
      "step 15868, loss is 4.589446544647217\n",
      "(64, 33)\n",
      "step 15869, loss is 4.838059902191162\n",
      "(64, 33)\n",
      "step 15870, loss is 4.9523234367370605\n",
      "(64, 33)\n",
      "step 15871, loss is 4.709868431091309\n",
      "(64, 33)\n",
      "step 15872, loss is 4.83462381362915\n",
      "(64, 33)\n",
      "step 15873, loss is 4.795701503753662\n",
      "(64, 33)\n",
      "step 15874, loss is 4.80106258392334\n",
      "(64, 33)\n",
      "step 15875, loss is 4.596324443817139\n",
      "(64, 33)\n",
      "step 15876, loss is 4.889633655548096\n",
      "(64, 33)\n",
      "step 15877, loss is 4.8210272789001465\n",
      "(64, 33)\n",
      "step 15878, loss is 4.804306507110596\n",
      "(64, 33)\n",
      "step 15879, loss is 4.708158493041992\n",
      "(64, 33)\n",
      "step 15880, loss is 4.754985809326172\n",
      "(64, 33)\n",
      "step 15881, loss is 4.736055374145508\n",
      "(64, 33)\n",
      "step 15882, loss is 4.835293292999268\n",
      "(64, 33)\n",
      "step 15883, loss is 4.644046306610107\n",
      "(64, 33)\n",
      "step 15884, loss is 4.982166767120361\n",
      "(64, 33)\n",
      "step 15885, loss is 4.740604877471924\n",
      "(64, 33)\n",
      "step 15886, loss is 4.955074787139893\n",
      "(64, 33)\n",
      "step 15887, loss is 4.868368148803711\n",
      "(64, 33)\n",
      "step 15888, loss is 4.792396068572998\n",
      "(64, 33)\n",
      "step 15889, loss is 4.797127723693848\n",
      "(64, 33)\n",
      "step 15890, loss is 4.88094425201416\n",
      "(64, 33)\n",
      "step 15891, loss is 4.644458770751953\n",
      "(64, 33)\n",
      "step 15892, loss is 4.895020484924316\n",
      "(64, 33)\n",
      "step 15893, loss is 4.823886394500732\n",
      "(64, 33)\n",
      "step 15894, loss is 5.003274917602539\n",
      "(64, 33)\n",
      "step 15895, loss is 4.713719367980957\n",
      "(64, 33)\n",
      "step 15896, loss is 4.807629585266113\n",
      "(64, 33)\n",
      "step 15897, loss is 4.823630332946777\n",
      "(64, 33)\n",
      "step 15898, loss is 4.937840461730957\n",
      "(64, 33)\n",
      "step 15899, loss is 4.860739707946777\n",
      "(64, 33)\n",
      "step 15900, loss is 4.741799831390381\n",
      "(64, 33)\n",
      "step 15901, loss is 4.771895885467529\n",
      "(64, 33)\n",
      "step 15902, loss is 4.835627555847168\n",
      "(64, 33)\n",
      "step 15903, loss is 4.724891662597656\n",
      "(64, 33)\n",
      "step 15904, loss is 4.804236888885498\n",
      "(64, 33)\n",
      "step 15905, loss is 4.862746715545654\n",
      "(64, 33)\n",
      "step 15906, loss is 4.778601169586182\n",
      "(64, 33)\n",
      "step 15907, loss is 4.680855751037598\n",
      "(64, 33)\n",
      "step 15908, loss is 4.95214319229126\n",
      "(64, 33)\n",
      "step 15909, loss is 5.030160427093506\n",
      "(64, 33)\n",
      "step 15910, loss is 4.6556172370910645\n",
      "(64, 33)\n",
      "step 15911, loss is 4.8269243240356445\n",
      "(64, 33)\n",
      "step 15912, loss is 4.699272632598877\n",
      "(64, 33)\n",
      "step 15913, loss is 4.853528022766113\n",
      "(64, 33)\n",
      "step 15914, loss is 4.9410881996154785\n",
      "(64, 33)\n",
      "step 15915, loss is 4.818686485290527\n",
      "(64, 33)\n",
      "step 15916, loss is 4.732899188995361\n",
      "(64, 33)\n",
      "step 15917, loss is 4.6132917404174805\n",
      "(64, 33)\n",
      "step 15918, loss is 4.687306880950928\n",
      "(64, 33)\n",
      "step 15919, loss is 4.785539150238037\n",
      "(64, 33)\n",
      "step 15920, loss is 4.672362327575684\n",
      "(64, 33)\n",
      "step 15921, loss is 4.92684268951416\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15922, loss is 4.769255638122559\n",
      "(64, 33)\n",
      "step 15923, loss is 4.82043981552124\n",
      "(64, 33)\n",
      "step 15924, loss is 4.792009353637695\n",
      "(64, 33)\n",
      "step 15925, loss is 4.816319465637207\n",
      "(64, 33)\n",
      "step 15926, loss is 4.85109281539917\n",
      "(64, 33)\n",
      "step 15927, loss is 4.699507236480713\n",
      "(64, 33)\n",
      "step 15928, loss is 4.782498359680176\n",
      "(64, 33)\n",
      "step 15929, loss is 4.902439117431641\n",
      "(64, 33)\n",
      "step 15930, loss is 4.842373371124268\n",
      "(64, 33)\n",
      "step 15931, loss is 4.696503639221191\n",
      "(64, 33)\n",
      "step 15932, loss is 4.72108268737793\n",
      "(64, 33)\n",
      "step 15933, loss is 4.772841930389404\n",
      "(64, 33)\n",
      "step 15934, loss is 4.76422643661499\n",
      "(64, 33)\n",
      "step 15935, loss is 4.682927131652832\n",
      "(64, 33)\n",
      "step 15936, loss is 4.8207478523254395\n",
      "(64, 33)\n",
      "step 15937, loss is 4.874114990234375\n",
      "(64, 33)\n",
      "step 15938, loss is 4.783161163330078\n",
      "(64, 33)\n",
      "step 15939, loss is 4.88826322555542\n",
      "(64, 33)\n",
      "step 15940, loss is 4.728131294250488\n",
      "(64, 33)\n",
      "step 15941, loss is 4.497786998748779\n",
      "(64, 33)\n",
      "step 15942, loss is 4.812433242797852\n",
      "(64, 33)\n",
      "step 15943, loss is 4.83900785446167\n",
      "(64, 33)\n",
      "step 15944, loss is 4.785955905914307\n",
      "(64, 33)\n",
      "step 15945, loss is 4.728954792022705\n",
      "(64, 33)\n",
      "step 15946, loss is 4.7900071144104\n",
      "(64, 33)\n",
      "step 15947, loss is 4.696257591247559\n",
      "(64, 33)\n",
      "step 15948, loss is 4.710122585296631\n",
      "(64, 33)\n",
      "step 15949, loss is 4.91188383102417\n",
      "(64, 33)\n",
      "step 15950, loss is 4.788492679595947\n",
      "(64, 33)\n",
      "step 15951, loss is 4.801213264465332\n",
      "(64, 33)\n",
      "step 15952, loss is 4.729387283325195\n",
      "(64, 33)\n",
      "step 15953, loss is 4.818007946014404\n",
      "(64, 33)\n",
      "step 15954, loss is 4.880563259124756\n",
      "(64, 33)\n",
      "step 15955, loss is 4.7934489250183105\n",
      "(64, 33)\n",
      "step 15956, loss is 4.610496520996094\n",
      "(64, 33)\n",
      "step 15957, loss is 4.8202948570251465\n",
      "(64, 33)\n",
      "step 15958, loss is 4.876627445220947\n",
      "(64, 33)\n",
      "step 15959, loss is 4.939770221710205\n",
      "(64, 33)\n",
      "step 15960, loss is 4.8456621170043945\n",
      "(64, 33)\n",
      "step 15961, loss is 4.830114841461182\n",
      "(64, 33)\n",
      "step 15962, loss is 4.723154544830322\n",
      "(64, 33)\n",
      "step 15963, loss is 4.887620449066162\n",
      "(64, 33)\n",
      "step 15964, loss is 4.815515995025635\n",
      "(64, 33)\n",
      "step 15965, loss is 4.909853458404541\n",
      "(64, 33)\n",
      "step 15966, loss is 4.803915023803711\n",
      "(64, 33)\n",
      "step 15967, loss is 4.742507457733154\n",
      "(64, 33)\n",
      "step 15968, loss is 4.733320236206055\n",
      "(64, 33)\n",
      "step 15969, loss is 4.795234203338623\n",
      "(64, 33)\n",
      "step 15970, loss is 4.836224555969238\n",
      "(64, 33)\n",
      "step 15971, loss is 4.746593952178955\n",
      "(64, 33)\n",
      "step 15972, loss is 4.641176700592041\n",
      "(64, 33)\n",
      "step 15973, loss is 4.65537166595459\n",
      "(64, 33)\n",
      "step 15974, loss is 4.702053546905518\n",
      "(64, 33)\n",
      "step 15975, loss is 4.781011581420898\n",
      "(64, 33)\n",
      "step 15976, loss is 4.600197792053223\n",
      "(64, 33)\n",
      "step 15977, loss is 4.869919776916504\n",
      "(64, 33)\n",
      "step 15978, loss is 4.874222755432129\n",
      "(64, 33)\n",
      "step 15979, loss is 5.009693622589111\n",
      "(64, 33)\n",
      "step 15980, loss is 4.810637950897217\n",
      "(64, 33)\n",
      "step 15981, loss is 4.798359394073486\n",
      "(64, 33)\n",
      "step 15982, loss is 4.831705570220947\n",
      "(64, 33)\n",
      "step 15983, loss is 4.891415596008301\n",
      "(64, 33)\n",
      "step 15984, loss is 4.9969162940979\n",
      "(64, 33)\n",
      "step 15985, loss is 4.686947822570801\n",
      "(64, 33)\n",
      "step 15986, loss is 4.804842948913574\n",
      "(64, 33)\n",
      "step 15987, loss is 4.809997081756592\n",
      "(64, 33)\n",
      "step 15988, loss is 4.875524997711182\n",
      "(64, 33)\n",
      "step 15989, loss is 4.794775485992432\n",
      "(64, 33)\n",
      "step 15990, loss is 4.724360466003418\n",
      "(64, 33)\n",
      "step 15991, loss is 4.737508773803711\n",
      "(64, 33)\n",
      "step 15992, loss is 5.069982051849365\n",
      "(64, 33)\n",
      "step 15993, loss is 4.735342502593994\n",
      "(64, 33)\n",
      "step 15994, loss is 4.793194770812988\n",
      "(64, 33)\n",
      "step 15995, loss is 4.810021877288818\n",
      "(64, 33)\n",
      "step 15996, loss is 4.930355072021484\n",
      "(64, 33)\n",
      "step 15997, loss is 4.7636237144470215\n",
      "(64, 33)\n",
      "step 15998, loss is 4.6214728355407715\n",
      "(64, 33)\n",
      "step 15999, loss is 4.87129545211792\n",
      "(64, 33)\n",
      "step 16000, loss is 4.76204776763916\n",
      "(64, 33)\n",
      "step 16001, loss is 4.800761699676514\n",
      "(64, 33)\n",
      "step 16002, loss is 5.104618072509766\n",
      "(64, 33)\n",
      "step 16003, loss is 4.708737373352051\n",
      "(64, 33)\n",
      "step 16004, loss is 4.732743263244629\n",
      "(64, 33)\n",
      "step 16005, loss is 4.7800822257995605\n",
      "(64, 33)\n",
      "step 16006, loss is 4.791827201843262\n",
      "(64, 33)\n",
      "step 16007, loss is 4.990324020385742\n",
      "(64, 33)\n",
      "step 16008, loss is 4.761167049407959\n",
      "(64, 33)\n",
      "step 16009, loss is 4.752824306488037\n",
      "(64, 33)\n",
      "step 16010, loss is 4.710683345794678\n",
      "(64, 33)\n",
      "step 16011, loss is 4.710536003112793\n",
      "(64, 33)\n",
      "step 16012, loss is 4.591015815734863\n",
      "(64, 33)\n",
      "step 16013, loss is 4.880813121795654\n",
      "(64, 33)\n",
      "step 16014, loss is 4.720728397369385\n",
      "(64, 33)\n",
      "step 16015, loss is 4.826673984527588\n",
      "(64, 33)\n",
      "step 16016, loss is 4.829093933105469\n",
      "(64, 33)\n",
      "step 16017, loss is 4.809216499328613\n",
      "(64, 33)\n",
      "step 16018, loss is 4.681110858917236\n",
      "(64, 33)\n",
      "step 16019, loss is 4.812028408050537\n",
      "(64, 33)\n",
      "step 16020, loss is 4.753371238708496\n",
      "(64, 33)\n",
      "step 16021, loss is 4.827486991882324\n",
      "(64, 33)\n",
      "step 16022, loss is 4.895150661468506\n",
      "(64, 33)\n",
      "step 16023, loss is 5.0142598152160645\n",
      "(64, 33)\n",
      "step 16024, loss is 4.896612167358398\n",
      "(64, 33)\n",
      "step 16025, loss is 4.877979278564453\n",
      "(64, 33)\n",
      "step 16026, loss is 4.802436351776123\n",
      "(64, 33)\n",
      "step 16027, loss is 4.761740684509277\n",
      "(64, 33)\n",
      "step 16028, loss is 4.571589946746826\n",
      "(64, 33)\n",
      "step 16029, loss is 4.8055644035339355\n",
      "(64, 33)\n",
      "step 16030, loss is 4.80397891998291\n",
      "(64, 33)\n",
      "step 16031, loss is 4.6453728675842285\n",
      "(64, 33)\n",
      "step 16032, loss is 4.74178409576416\n",
      "(64, 33)\n",
      "step 16033, loss is 4.922694206237793\n",
      "(64, 33)\n",
      "step 16034, loss is 4.886207103729248\n",
      "(64, 33)\n",
      "step 16035, loss is 4.718952178955078\n",
      "(64, 33)\n",
      "step 16036, loss is 4.640832424163818\n",
      "(64, 33)\n",
      "step 16037, loss is 4.749545097351074\n",
      "(64, 33)\n",
      "step 16038, loss is 4.696924686431885\n",
      "(64, 33)\n",
      "step 16039, loss is 4.784883499145508\n",
      "(64, 33)\n",
      "step 16040, loss is 4.79223108291626\n",
      "(64, 33)\n",
      "step 16041, loss is 4.8847126960754395\n",
      "(64, 33)\n",
      "step 16042, loss is 4.727939605712891\n",
      "(64, 33)\n",
      "step 16043, loss is 4.865286827087402\n",
      "(64, 33)\n",
      "step 16044, loss is 4.773227214813232\n",
      "(64, 33)\n",
      "step 16045, loss is 4.889163017272949\n",
      "(64, 33)\n",
      "step 16046, loss is 4.838947296142578\n",
      "(64, 33)\n",
      "step 16047, loss is 4.753208160400391\n",
      "(64, 33)\n",
      "step 16048, loss is 4.861423969268799\n",
      "(64, 33)\n",
      "step 16049, loss is 4.9304046630859375\n",
      "(64, 33)\n",
      "step 16050, loss is 4.877446174621582\n",
      "(64, 33)\n",
      "step 16051, loss is 4.743363857269287\n",
      "(64, 33)\n",
      "step 16052, loss is 4.860891342163086\n",
      "(64, 33)\n",
      "step 16053, loss is 4.882934093475342\n",
      "(64, 33)\n",
      "step 16054, loss is 4.703377723693848\n",
      "(64, 33)\n",
      "step 16055, loss is 4.93317174911499\n",
      "(64, 33)\n",
      "step 16056, loss is 4.952037811279297\n",
      "(64, 33)\n",
      "step 16057, loss is 4.833235263824463\n",
      "(64, 33)\n",
      "step 16058, loss is 4.779099941253662\n",
      "(64, 33)\n",
      "step 16059, loss is 4.809007167816162\n",
      "(64, 33)\n",
      "step 16060, loss is 4.643001079559326\n",
      "(64, 33)\n",
      "step 16061, loss is 4.891444683074951\n",
      "(64, 33)\n",
      "step 16062, loss is 4.747649192810059\n",
      "(64, 33)\n",
      "step 16063, loss is 4.734308242797852\n",
      "(64, 33)\n",
      "step 16064, loss is 4.980599403381348\n",
      "(64, 33)\n",
      "step 16065, loss is 4.680169582366943\n",
      "(64, 33)\n",
      "step 16066, loss is 4.7992024421691895\n",
      "(64, 33)\n",
      "step 16067, loss is 4.783341407775879\n",
      "(64, 33)\n",
      "step 16068, loss is 4.705552101135254\n",
      "(64, 33)\n",
      "step 16069, loss is 4.758117198944092\n",
      "(64, 33)\n",
      "step 16070, loss is 5.013872146606445\n",
      "(64, 33)\n",
      "step 16071, loss is 4.827186584472656\n",
      "(64, 33)\n",
      "step 16072, loss is 4.750605583190918\n",
      "(64, 33)\n",
      "step 16073, loss is 4.748313903808594\n",
      "(64, 33)\n",
      "step 16074, loss is 4.839055061340332\n",
      "(64, 33)\n",
      "step 16075, loss is 4.529735565185547\n",
      "(64, 33)\n",
      "step 16076, loss is 4.580131530761719\n",
      "(64, 33)\n",
      "step 16077, loss is 5.00540828704834\n",
      "(64, 33)\n",
      "step 16078, loss is 4.745650768280029\n",
      "(64, 33)\n",
      "step 16079, loss is 4.832760810852051\n",
      "(64, 33)\n",
      "step 16080, loss is 4.762988567352295\n",
      "(64, 33)\n",
      "step 16081, loss is 4.884158134460449\n",
      "(64, 33)\n",
      "step 16082, loss is 4.51626443862915\n",
      "(64, 33)\n",
      "step 16083, loss is 4.822256565093994\n",
      "(64, 33)\n",
      "step 16084, loss is 5.0480194091796875\n",
      "(64, 33)\n",
      "step 16085, loss is 4.730578899383545\n",
      "(64, 33)\n",
      "step 16086, loss is 5.054670810699463\n",
      "(64, 33)\n",
      "step 16087, loss is 4.802015781402588\n",
      "(64, 33)\n",
      "step 16088, loss is 4.827788829803467\n",
      "(64, 33)\n",
      "step 16089, loss is 4.670029163360596\n",
      "(64, 33)\n",
      "step 16090, loss is 4.628419876098633\n",
      "(64, 33)\n",
      "step 16091, loss is 4.933709621429443\n",
      "(64, 33)\n",
      "step 16092, loss is 4.8784613609313965\n",
      "(64, 33)\n",
      "step 16093, loss is 4.7542901039123535\n",
      "(64, 33)\n",
      "step 16094, loss is 4.871017932891846\n",
      "(64, 33)\n",
      "step 16095, loss is 4.757944107055664\n",
      "(64, 33)\n",
      "step 16096, loss is 4.807706356048584\n",
      "(64, 33)\n",
      "step 16097, loss is 4.559048175811768\n",
      "(64, 33)\n",
      "step 16098, loss is 4.949586868286133\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16099, loss is 4.901332855224609\n",
      "(64, 33)\n",
      "step 16100, loss is 4.71083927154541\n",
      "(64, 33)\n",
      "step 16101, loss is 4.612913131713867\n",
      "(64, 33)\n",
      "step 16102, loss is 5.076975345611572\n",
      "(64, 33)\n",
      "step 16103, loss is 4.587735652923584\n",
      "(64, 33)\n",
      "step 16104, loss is 4.822879791259766\n",
      "(64, 33)\n",
      "step 16105, loss is 4.71390962600708\n",
      "(64, 33)\n",
      "step 16106, loss is 4.852663993835449\n",
      "(64, 33)\n",
      "step 16107, loss is 4.866702079772949\n",
      "(64, 33)\n",
      "step 16108, loss is 4.899317741394043\n",
      "(64, 33)\n",
      "step 16109, loss is 4.758490562438965\n",
      "(64, 33)\n",
      "step 16110, loss is 4.744557857513428\n",
      "(64, 33)\n",
      "step 16111, loss is 4.729548454284668\n",
      "(64, 33)\n",
      "step 16112, loss is 4.855613708496094\n",
      "(64, 33)\n",
      "step 16113, loss is 4.746921062469482\n",
      "(64, 33)\n",
      "step 16114, loss is 4.925492763519287\n",
      "(64, 33)\n",
      "step 16115, loss is 4.84505558013916\n",
      "(64, 33)\n",
      "step 16116, loss is 4.948624134063721\n",
      "(64, 33)\n",
      "step 16117, loss is 4.8763041496276855\n",
      "(64, 33)\n",
      "step 16118, loss is 4.676286697387695\n",
      "(64, 33)\n",
      "step 16119, loss is 4.91740083694458\n",
      "(64, 33)\n",
      "step 16120, loss is 4.944974899291992\n",
      "(64, 33)\n",
      "step 16121, loss is 4.7794928550720215\n",
      "(64, 33)\n",
      "step 16122, loss is 4.671550273895264\n",
      "(64, 33)\n",
      "step 16123, loss is 4.723446369171143\n",
      "(64, 33)\n",
      "step 16124, loss is 4.740303993225098\n",
      "(64, 33)\n",
      "step 16125, loss is 4.7118659019470215\n",
      "(64, 33)\n",
      "step 16126, loss is 4.790544033050537\n",
      "(64, 33)\n",
      "step 16127, loss is 4.701746940612793\n",
      "(64, 33)\n",
      "step 16128, loss is 4.733745574951172\n",
      "(64, 33)\n",
      "step 16129, loss is 4.718778610229492\n",
      "(64, 33)\n",
      "step 16130, loss is 4.696564674377441\n",
      "(64, 33)\n",
      "step 16131, loss is 4.756713390350342\n",
      "(64, 33)\n",
      "step 16132, loss is 4.800807476043701\n",
      "(64, 33)\n",
      "step 16133, loss is 4.913763999938965\n",
      "(64, 33)\n",
      "step 16134, loss is 4.63820743560791\n",
      "(64, 33)\n",
      "step 16135, loss is 4.672344207763672\n",
      "(64, 33)\n",
      "step 16136, loss is 4.978269577026367\n",
      "(64, 33)\n",
      "step 16137, loss is 4.9651923179626465\n",
      "(64, 33)\n",
      "step 16138, loss is 4.856092929840088\n",
      "(64, 33)\n",
      "step 16139, loss is 4.85782527923584\n",
      "(64, 33)\n",
      "step 16140, loss is 4.714038372039795\n",
      "(64, 33)\n",
      "step 16141, loss is 4.822876930236816\n",
      "(64, 33)\n",
      "step 16142, loss is 4.724658966064453\n",
      "(64, 33)\n",
      "step 16143, loss is 4.847322940826416\n",
      "(64, 33)\n",
      "step 16144, loss is 4.769707679748535\n",
      "(64, 33)\n",
      "step 16145, loss is 4.940068244934082\n",
      "(64, 33)\n",
      "step 16146, loss is 4.63239860534668\n",
      "(64, 33)\n",
      "step 16147, loss is 4.796324729919434\n",
      "(64, 33)\n",
      "step 16148, loss is 4.713378429412842\n",
      "(64, 33)\n",
      "step 16149, loss is 4.902755260467529\n",
      "(64, 33)\n",
      "step 16150, loss is 4.705725193023682\n",
      "(64, 33)\n",
      "step 16151, loss is 4.717095375061035\n",
      "(64, 33)\n",
      "step 16152, loss is 4.818840980529785\n",
      "(64, 33)\n",
      "step 16153, loss is 4.646586894989014\n",
      "(64, 33)\n",
      "step 16154, loss is 4.938853740692139\n",
      "(64, 33)\n",
      "step 16155, loss is 4.849521160125732\n",
      "(64, 33)\n",
      "step 16156, loss is 4.9278154373168945\n",
      "(64, 33)\n",
      "step 16157, loss is 4.715699672698975\n",
      "(64, 33)\n",
      "step 16158, loss is 4.928394794464111\n",
      "(64, 33)\n",
      "step 16159, loss is 4.789466857910156\n",
      "(64, 33)\n",
      "step 16160, loss is 4.774195194244385\n",
      "(64, 33)\n",
      "step 16161, loss is 4.78016996383667\n",
      "(64, 33)\n",
      "step 16162, loss is 4.9326395988464355\n",
      "(64, 33)\n",
      "step 16163, loss is 4.753304958343506\n",
      "(64, 33)\n",
      "step 16164, loss is 4.739109992980957\n",
      "(64, 33)\n",
      "step 16165, loss is 4.926782608032227\n",
      "(64, 33)\n",
      "step 16166, loss is 4.80379056930542\n",
      "(64, 33)\n",
      "step 16167, loss is 4.8204216957092285\n",
      "(64, 33)\n",
      "step 16168, loss is 4.835212230682373\n",
      "(64, 33)\n",
      "step 16169, loss is 4.7470011711120605\n",
      "(64, 33)\n",
      "step 16170, loss is 4.871401786804199\n",
      "(64, 33)\n",
      "step 16171, loss is 5.010953903198242\n",
      "(64, 33)\n",
      "step 16172, loss is 4.764495849609375\n",
      "(64, 33)\n",
      "step 16173, loss is 4.835381984710693\n",
      "(64, 33)\n",
      "step 16174, loss is 4.702121734619141\n",
      "(64, 33)\n",
      "step 16175, loss is 4.8542256355285645\n",
      "(64, 33)\n",
      "step 16176, loss is 5.012643337249756\n",
      "(64, 33)\n",
      "step 16177, loss is 4.667464733123779\n",
      "(64, 33)\n",
      "step 16178, loss is 4.800969123840332\n",
      "(64, 33)\n",
      "step 16179, loss is 4.7318501472473145\n",
      "(64, 33)\n",
      "step 16180, loss is 4.869001865386963\n",
      "(64, 33)\n",
      "step 16181, loss is 5.0734992027282715\n",
      "(64, 33)\n",
      "step 16182, loss is 4.656160354614258\n",
      "(64, 33)\n",
      "step 16183, loss is 4.952639102935791\n",
      "(64, 33)\n",
      "step 16184, loss is 4.917181015014648\n",
      "(64, 33)\n",
      "step 16185, loss is 4.865219593048096\n",
      "(64, 33)\n",
      "step 16186, loss is 4.852363109588623\n",
      "(64, 33)\n",
      "step 16187, loss is 4.708651542663574\n",
      "(64, 33)\n",
      "step 16188, loss is 4.7511749267578125\n",
      "(64, 33)\n",
      "step 16189, loss is 4.621194839477539\n",
      "(64, 33)\n",
      "step 16190, loss is 4.6036601066589355\n",
      "(64, 33)\n",
      "step 16191, loss is 5.0541181564331055\n",
      "(64, 33)\n",
      "step 16192, loss is 4.950875759124756\n",
      "(64, 33)\n",
      "step 16193, loss is 4.783055782318115\n",
      "(64, 33)\n",
      "step 16194, loss is 4.752849578857422\n",
      "(64, 33)\n",
      "step 16195, loss is 4.894706726074219\n",
      "(64, 33)\n",
      "step 16196, loss is 4.777867794036865\n",
      "(64, 33)\n",
      "step 16197, loss is 4.810231685638428\n",
      "(64, 33)\n",
      "step 16198, loss is 4.813141822814941\n",
      "(64, 33)\n",
      "step 16199, loss is 4.858578205108643\n",
      "(64, 33)\n",
      "step 16200, loss is 4.756083965301514\n",
      "(64, 33)\n",
      "step 16201, loss is 4.619620323181152\n",
      "(64, 33)\n",
      "step 16202, loss is 4.920090198516846\n",
      "(64, 33)\n",
      "step 16203, loss is 4.690524101257324\n",
      "(64, 33)\n",
      "step 16204, loss is 4.743649959564209\n",
      "(64, 33)\n",
      "step 16205, loss is 4.830790996551514\n",
      "(64, 33)\n",
      "step 16206, loss is 4.757265567779541\n",
      "(64, 33)\n",
      "step 16207, loss is 4.78821325302124\n",
      "(64, 33)\n",
      "step 16208, loss is 4.779090881347656\n",
      "(64, 33)\n",
      "step 16209, loss is 5.017475605010986\n",
      "(64, 33)\n",
      "step 16210, loss is 4.743762969970703\n",
      "(64, 33)\n",
      "step 16211, loss is 4.768583297729492\n",
      "(64, 33)\n",
      "step 16212, loss is 4.6828837394714355\n",
      "(64, 33)\n",
      "step 16213, loss is 4.757308006286621\n",
      "(64, 33)\n",
      "step 16214, loss is 4.796127796173096\n",
      "(64, 33)\n",
      "step 16215, loss is 4.715294361114502\n",
      "(64, 33)\n",
      "step 16216, loss is 4.730765342712402\n",
      "(64, 33)\n",
      "step 16217, loss is 4.847283840179443\n",
      "(64, 33)\n",
      "step 16218, loss is 4.868581771850586\n",
      "(64, 33)\n",
      "step 16219, loss is 4.672713279724121\n",
      "(64, 33)\n",
      "step 16220, loss is 4.865074157714844\n",
      "(64, 33)\n",
      "step 16221, loss is 4.592452526092529\n",
      "(64, 33)\n",
      "step 16222, loss is 4.80794095993042\n",
      "(64, 33)\n",
      "step 16223, loss is 4.865231513977051\n",
      "(64, 33)\n",
      "step 16224, loss is 4.8613691329956055\n",
      "(64, 33)\n",
      "step 16225, loss is 4.723597049713135\n",
      "(64, 33)\n",
      "step 16226, loss is 4.803787708282471\n",
      "(64, 33)\n",
      "step 16227, loss is 4.846093654632568\n",
      "(64, 33)\n",
      "step 16228, loss is 4.852180480957031\n",
      "(64, 33)\n",
      "step 16229, loss is 4.844069957733154\n",
      "(64, 33)\n",
      "step 16230, loss is 4.887290954589844\n",
      "(64, 33)\n",
      "step 16231, loss is 4.796806335449219\n",
      "(64, 33)\n",
      "step 16232, loss is 4.978737831115723\n",
      "(64, 33)\n",
      "step 16233, loss is 4.897181034088135\n",
      "(64, 33)\n",
      "step 16234, loss is 4.941107273101807\n",
      "(64, 33)\n",
      "step 16235, loss is 4.756526470184326\n",
      "(64, 33)\n",
      "step 16236, loss is 4.672763347625732\n",
      "(64, 33)\n",
      "step 16237, loss is 4.8611884117126465\n",
      "(64, 33)\n",
      "step 16238, loss is 4.887681484222412\n",
      "(64, 33)\n",
      "step 16239, loss is 4.6693620681762695\n",
      "(64, 33)\n",
      "step 16240, loss is 4.71300745010376\n",
      "(64, 33)\n",
      "step 16241, loss is 4.7354841232299805\n",
      "(64, 33)\n",
      "step 16242, loss is 4.91876220703125\n",
      "(64, 33)\n",
      "step 16243, loss is 4.976009845733643\n",
      "(64, 33)\n",
      "step 16244, loss is 4.8092217445373535\n",
      "(64, 33)\n",
      "step 16245, loss is 4.755826950073242\n",
      "(64, 33)\n",
      "step 16246, loss is 4.824045181274414\n",
      "(64, 33)\n",
      "step 16247, loss is 4.774583339691162\n",
      "(64, 33)\n",
      "step 16248, loss is 4.851225852966309\n",
      "(64, 33)\n",
      "step 16249, loss is 4.745800495147705\n",
      "(64, 33)\n",
      "step 16250, loss is 4.910786151885986\n",
      "(64, 33)\n",
      "step 16251, loss is 4.86633825302124\n",
      "(64, 33)\n",
      "step 16252, loss is 4.600185871124268\n",
      "(64, 33)\n",
      "step 16253, loss is 4.913332939147949\n",
      "(64, 33)\n",
      "step 16254, loss is 4.652886390686035\n",
      "(64, 33)\n",
      "step 16255, loss is 4.888406753540039\n",
      "(64, 33)\n",
      "step 16256, loss is 4.910508155822754\n",
      "(64, 33)\n",
      "step 16257, loss is 4.860525131225586\n",
      "(64, 33)\n",
      "step 16258, loss is 4.78244686126709\n",
      "(64, 33)\n",
      "step 16259, loss is 4.77377462387085\n",
      "(64, 33)\n",
      "step 16260, loss is 4.8013691902160645\n",
      "(64, 33)\n",
      "step 16261, loss is 4.894110679626465\n",
      "(64, 33)\n",
      "step 16262, loss is 4.619134902954102\n",
      "(64, 33)\n",
      "step 16263, loss is 4.778777122497559\n",
      "(64, 33)\n",
      "step 16264, loss is 5.012027740478516\n",
      "(64, 33)\n",
      "step 16265, loss is 4.86060905456543\n",
      "(64, 33)\n",
      "step 16266, loss is 4.880490303039551\n",
      "(64, 33)\n",
      "step 16267, loss is 4.765819072723389\n",
      "(64, 33)\n",
      "step 16268, loss is 4.651356220245361\n",
      "(64, 33)\n",
      "step 16269, loss is 4.654134750366211\n",
      "(64, 33)\n",
      "step 16270, loss is 5.0580573081970215\n",
      "(64, 33)\n",
      "step 16271, loss is 4.799952030181885\n",
      "(64, 33)\n",
      "step 16272, loss is 4.781280517578125\n",
      "(64, 33)\n",
      "step 16273, loss is 4.783295631408691\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16274, loss is 4.892972946166992\n",
      "(64, 33)\n",
      "step 16275, loss is 4.907534599304199\n",
      "(64, 33)\n",
      "step 16276, loss is 4.85988712310791\n",
      "(64, 33)\n",
      "step 16277, loss is 4.916687488555908\n",
      "(64, 33)\n",
      "step 16278, loss is 4.759118556976318\n",
      "(64, 33)\n",
      "step 16279, loss is 4.501878261566162\n",
      "(64, 33)\n",
      "step 16280, loss is 4.837758541107178\n",
      "(64, 33)\n",
      "step 16281, loss is 4.849368095397949\n",
      "(64, 33)\n",
      "step 16282, loss is 4.940906047821045\n",
      "(64, 33)\n",
      "step 16283, loss is 4.8676838874816895\n",
      "(64, 33)\n",
      "step 16284, loss is 4.631250381469727\n",
      "(64, 33)\n",
      "step 16285, loss is 4.921056747436523\n",
      "(64, 33)\n",
      "step 16286, loss is 4.730748653411865\n",
      "(64, 33)\n",
      "step 16287, loss is 4.742471218109131\n",
      "(64, 33)\n",
      "step 16288, loss is 4.83600378036499\n",
      "(64, 33)\n",
      "step 16289, loss is 4.764819622039795\n",
      "(64, 33)\n",
      "step 16290, loss is 4.7067108154296875\n",
      "(64, 33)\n",
      "step 16291, loss is 4.707947731018066\n",
      "(64, 33)\n",
      "step 16292, loss is 5.0130720138549805\n",
      "(64, 33)\n",
      "step 16293, loss is 4.826578617095947\n",
      "(64, 33)\n",
      "step 16294, loss is 4.8374247550964355\n",
      "(64, 33)\n",
      "step 16295, loss is 4.90089225769043\n",
      "(64, 33)\n",
      "step 16296, loss is 4.7725725173950195\n",
      "(64, 33)\n",
      "step 16297, loss is 4.801440715789795\n",
      "(64, 33)\n",
      "step 16298, loss is 4.942506790161133\n",
      "(64, 33)\n",
      "step 16299, loss is 4.827020168304443\n",
      "(64, 33)\n",
      "step 16300, loss is 4.88269567489624\n",
      "(64, 33)\n",
      "step 16301, loss is 4.79257345199585\n",
      "(64, 33)\n",
      "step 16302, loss is 4.792951583862305\n",
      "(64, 33)\n",
      "step 16303, loss is 4.705382347106934\n",
      "(64, 33)\n",
      "step 16304, loss is 4.76685905456543\n",
      "(64, 33)\n",
      "step 16305, loss is 4.748157024383545\n",
      "(64, 33)\n",
      "step 16306, loss is 4.765440464019775\n",
      "(64, 33)\n",
      "step 16307, loss is 4.8113932609558105\n",
      "(64, 33)\n",
      "step 16308, loss is 4.827592849731445\n",
      "(64, 33)\n",
      "step 16309, loss is 4.916718482971191\n",
      "(64, 33)\n",
      "step 16310, loss is 4.519140243530273\n",
      "(64, 33)\n",
      "step 16311, loss is 4.670642375946045\n",
      "(64, 33)\n",
      "step 16312, loss is 4.809656620025635\n",
      "(64, 33)\n",
      "step 16313, loss is 4.79157018661499\n",
      "(64, 33)\n",
      "step 16314, loss is 4.923572540283203\n",
      "(64, 33)\n",
      "step 16315, loss is 4.703377723693848\n",
      "(64, 33)\n",
      "step 16316, loss is 4.977123737335205\n",
      "(64, 33)\n",
      "step 16317, loss is 4.684360504150391\n",
      "(64, 33)\n",
      "step 16318, loss is 4.5988593101501465\n",
      "(64, 33)\n",
      "step 16319, loss is 4.770641326904297\n",
      "(64, 33)\n",
      "step 16320, loss is 4.788161754608154\n",
      "(64, 33)\n",
      "step 16321, loss is 4.748854160308838\n",
      "(64, 33)\n",
      "step 16322, loss is 4.856054306030273\n",
      "(64, 33)\n",
      "step 16323, loss is 4.839022636413574\n",
      "(64, 33)\n",
      "step 16324, loss is 4.995328903198242\n",
      "(64, 33)\n",
      "step 16325, loss is 4.808202743530273\n",
      "(64, 33)\n",
      "step 16326, loss is 4.783949375152588\n",
      "(64, 33)\n",
      "step 16327, loss is 4.7680792808532715\n",
      "(64, 33)\n",
      "step 16328, loss is 4.734710693359375\n",
      "(64, 33)\n",
      "step 16329, loss is 4.625289440155029\n",
      "(64, 33)\n",
      "step 16330, loss is 4.790998935699463\n",
      "(64, 33)\n",
      "step 16331, loss is 4.649600028991699\n",
      "(64, 33)\n",
      "step 16332, loss is 4.811838626861572\n",
      "(64, 33)\n",
      "step 16333, loss is 4.8234076499938965\n",
      "(64, 33)\n",
      "step 16334, loss is 4.824448585510254\n",
      "(64, 33)\n",
      "step 16335, loss is 4.918598175048828\n",
      "(64, 33)\n",
      "step 16336, loss is 4.882779598236084\n",
      "(64, 33)\n",
      "step 16337, loss is 4.791915416717529\n",
      "(64, 33)\n",
      "step 16338, loss is 4.876046657562256\n",
      "(64, 33)\n",
      "step 16339, loss is 4.7336249351501465\n",
      "(64, 33)\n",
      "step 16340, loss is 4.93511962890625\n",
      "(64, 33)\n",
      "step 16341, loss is 4.8267741203308105\n",
      "(64, 33)\n",
      "step 16342, loss is 4.718244552612305\n",
      "(64, 33)\n",
      "step 16343, loss is 4.834803104400635\n",
      "(64, 33)\n",
      "step 16344, loss is 4.953579902648926\n",
      "(64, 33)\n",
      "step 16345, loss is 4.931921482086182\n",
      "(64, 33)\n",
      "step 16346, loss is 4.790887832641602\n",
      "(64, 33)\n",
      "step 16347, loss is 4.708396911621094\n",
      "(64, 33)\n",
      "step 16348, loss is 4.744844436645508\n",
      "(64, 33)\n",
      "step 16349, loss is 4.907403945922852\n",
      "(64, 33)\n",
      "step 16350, loss is 4.7628703117370605\n",
      "(64, 33)\n",
      "step 16351, loss is 4.709702968597412\n",
      "(64, 33)\n",
      "step 16352, loss is 4.7305498123168945\n",
      "(64, 33)\n",
      "step 16353, loss is 4.831480979919434\n",
      "(64, 33)\n",
      "step 16354, loss is 5.112027168273926\n",
      "(64, 33)\n",
      "step 16355, loss is 4.761908531188965\n",
      "(64, 33)\n",
      "step 16356, loss is 4.869009017944336\n",
      "(64, 33)\n",
      "step 16357, loss is 4.680078506469727\n",
      "(64, 33)\n",
      "step 16358, loss is 4.773191452026367\n",
      "(64, 33)\n",
      "step 16359, loss is 4.735193252563477\n",
      "(64, 33)\n",
      "step 16360, loss is 4.683823108673096\n",
      "(64, 33)\n",
      "step 16361, loss is 4.718943119049072\n",
      "(64, 33)\n",
      "step 16362, loss is 4.80493688583374\n",
      "(64, 33)\n",
      "step 16363, loss is 4.819471836090088\n",
      "(64, 33)\n",
      "step 16364, loss is 4.901818752288818\n",
      "(64, 33)\n",
      "step 16365, loss is 4.756388187408447\n",
      "(64, 33)\n",
      "step 16366, loss is 4.779378414154053\n",
      "(64, 33)\n",
      "step 16367, loss is 4.768099784851074\n",
      "(64, 33)\n",
      "step 16368, loss is 4.710381507873535\n",
      "(64, 33)\n",
      "step 16369, loss is 4.721527576446533\n",
      "(64, 33)\n",
      "step 16370, loss is 4.6926350593566895\n",
      "(64, 33)\n",
      "step 16371, loss is 4.69386625289917\n",
      "(64, 33)\n",
      "step 16372, loss is 4.896930694580078\n",
      "(64, 33)\n",
      "step 16373, loss is 4.855160236358643\n",
      "(64, 33)\n",
      "step 16374, loss is 4.658819198608398\n",
      "(64, 33)\n",
      "step 16375, loss is 4.907316207885742\n",
      "(64, 33)\n",
      "step 16376, loss is 4.702730178833008\n",
      "(64, 33)\n",
      "step 16377, loss is 4.695411205291748\n",
      "(64, 33)\n",
      "step 16378, loss is 4.828007221221924\n",
      "(64, 33)\n",
      "step 16379, loss is 4.747230529785156\n",
      "(64, 33)\n",
      "step 16380, loss is 4.76165771484375\n",
      "(64, 33)\n",
      "step 16381, loss is 4.869910717010498\n",
      "(64, 33)\n",
      "step 16382, loss is 4.854816436767578\n",
      "(64, 33)\n",
      "step 16383, loss is 4.890921115875244\n",
      "(64, 33)\n",
      "step 16384, loss is 4.771488666534424\n",
      "(64, 33)\n",
      "step 16385, loss is 4.6528449058532715\n",
      "(64, 33)\n",
      "step 16386, loss is 4.904592990875244\n",
      "(64, 33)\n",
      "step 16387, loss is 4.763914108276367\n",
      "(64, 33)\n",
      "step 16388, loss is 4.8043599128723145\n",
      "(64, 33)\n",
      "step 16389, loss is 4.687975883483887\n",
      "(64, 33)\n",
      "step 16390, loss is 4.894479751586914\n",
      "(64, 33)\n",
      "step 16391, loss is 4.770085334777832\n",
      "(64, 33)\n",
      "step 16392, loss is 4.840854644775391\n",
      "(64, 33)\n",
      "step 16393, loss is 4.837600231170654\n",
      "(64, 33)\n",
      "step 16394, loss is 4.948417663574219\n",
      "(64, 33)\n",
      "step 16395, loss is 4.6678643226623535\n",
      "(64, 33)\n",
      "step 16396, loss is 4.832462787628174\n",
      "(64, 33)\n",
      "step 16397, loss is 4.82057523727417\n",
      "(64, 33)\n",
      "step 16398, loss is 4.6880927085876465\n",
      "(64, 33)\n",
      "step 16399, loss is 4.826410293579102\n",
      "(64, 33)\n",
      "step 16400, loss is 5.022319316864014\n",
      "(64, 33)\n",
      "step 16401, loss is 4.814126491546631\n",
      "(64, 33)\n",
      "step 16402, loss is 4.687838077545166\n",
      "(64, 33)\n",
      "step 16403, loss is 4.757634162902832\n",
      "(64, 33)\n",
      "step 16404, loss is 4.8015594482421875\n",
      "(64, 33)\n",
      "step 16405, loss is 4.735273838043213\n",
      "(64, 33)\n",
      "step 16406, loss is 4.654080867767334\n",
      "(64, 33)\n",
      "step 16407, loss is 4.880309581756592\n",
      "(64, 33)\n",
      "step 16408, loss is 4.640575885772705\n",
      "(64, 33)\n",
      "step 16409, loss is 4.901241302490234\n",
      "(64, 33)\n",
      "step 16410, loss is 4.8461408615112305\n",
      "(64, 33)\n",
      "step 16411, loss is 4.759039878845215\n",
      "(64, 33)\n",
      "step 16412, loss is 4.794196605682373\n",
      "(64, 33)\n",
      "step 16413, loss is 4.812063694000244\n",
      "(64, 33)\n",
      "step 16414, loss is 4.739382743835449\n",
      "(64, 33)\n",
      "step 16415, loss is 4.7378668785095215\n",
      "(64, 33)\n",
      "step 16416, loss is 4.790855407714844\n",
      "(64, 33)\n",
      "step 16417, loss is 4.973826885223389\n",
      "(64, 33)\n",
      "step 16418, loss is 4.935057640075684\n",
      "(64, 33)\n",
      "step 16419, loss is 4.591569900512695\n",
      "(64, 33)\n",
      "step 16420, loss is 4.866416931152344\n",
      "(64, 33)\n",
      "step 16421, loss is 4.946656227111816\n",
      "(64, 33)\n",
      "step 16422, loss is 4.70343017578125\n",
      "(64, 33)\n",
      "step 16423, loss is 4.758651256561279\n",
      "(64, 33)\n",
      "step 16424, loss is 4.880331993103027\n",
      "(64, 33)\n",
      "step 16425, loss is 4.874524116516113\n",
      "(64, 33)\n",
      "step 16426, loss is 4.893962860107422\n",
      "(64, 33)\n",
      "step 16427, loss is 5.019345760345459\n",
      "(64, 33)\n",
      "step 16428, loss is 4.955657958984375\n",
      "(64, 33)\n",
      "step 16429, loss is 4.844882488250732\n",
      "(64, 33)\n",
      "step 16430, loss is 4.784063339233398\n",
      "(64, 33)\n",
      "step 16431, loss is 4.793877601623535\n",
      "(64, 33)\n",
      "step 16432, loss is 4.804126262664795\n",
      "(64, 33)\n",
      "step 16433, loss is 4.773187637329102\n",
      "(64, 33)\n",
      "step 16434, loss is 4.928009510040283\n",
      "(64, 33)\n",
      "step 16435, loss is 4.8042311668396\n",
      "(64, 33)\n",
      "step 16436, loss is 4.817768096923828\n",
      "(64, 33)\n",
      "step 16437, loss is 4.907853603363037\n",
      "(64, 33)\n",
      "step 16438, loss is 4.644251823425293\n",
      "(64, 33)\n",
      "step 16439, loss is 4.850076198577881\n",
      "(64, 33)\n",
      "step 16440, loss is 4.773595809936523\n",
      "(64, 33)\n",
      "step 16441, loss is 4.788418769836426\n",
      "(64, 33)\n",
      "step 16442, loss is 4.7631402015686035\n",
      "(64, 33)\n",
      "step 16443, loss is 4.679339408874512\n",
      "(64, 33)\n",
      "step 16444, loss is 4.781266689300537\n",
      "(64, 33)\n",
      "step 16445, loss is 4.7550506591796875\n",
      "(64, 33)\n",
      "step 16446, loss is 4.897511005401611\n",
      "(64, 33)\n",
      "step 16447, loss is 4.935067653656006\n",
      "(64, 33)\n",
      "step 16448, loss is 4.726310729980469\n",
      "(64, 33)\n",
      "step 16449, loss is 4.784945487976074\n",
      "(64, 33)\n",
      "step 16450, loss is 5.000583648681641\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16451, loss is 4.78192138671875\n",
      "(64, 33)\n",
      "step 16452, loss is 4.818552494049072\n",
      "(64, 33)\n",
      "step 16453, loss is 4.653385639190674\n",
      "(64, 33)\n",
      "step 16454, loss is 4.6741108894348145\n",
      "(64, 33)\n",
      "step 16455, loss is 4.71314811706543\n",
      "(64, 33)\n",
      "step 16456, loss is 4.741893768310547\n",
      "(64, 33)\n",
      "step 16457, loss is 4.9014811515808105\n",
      "(64, 33)\n",
      "step 16458, loss is 4.761281967163086\n",
      "(64, 33)\n",
      "step 16459, loss is 4.908144950866699\n",
      "(64, 33)\n",
      "step 16460, loss is 4.640908241271973\n",
      "(64, 33)\n",
      "step 16461, loss is 4.961437702178955\n",
      "(64, 33)\n",
      "step 16462, loss is 4.9272260665893555\n",
      "(64, 33)\n",
      "step 16463, loss is 4.9626851081848145\n",
      "(64, 33)\n",
      "step 16464, loss is 4.726078510284424\n",
      "(64, 33)\n",
      "step 16465, loss is 4.882508754730225\n",
      "(64, 33)\n",
      "step 16466, loss is 4.852763652801514\n",
      "(64, 33)\n",
      "step 16467, loss is 4.869078636169434\n",
      "(64, 33)\n",
      "step 16468, loss is 4.796032905578613\n",
      "(64, 33)\n",
      "step 16469, loss is 4.759718418121338\n",
      "(64, 33)\n",
      "step 16470, loss is 4.800711154937744\n",
      "(64, 33)\n",
      "step 16471, loss is 4.759166240692139\n",
      "(64, 33)\n",
      "step 16472, loss is 4.837426662445068\n",
      "(64, 33)\n",
      "step 16473, loss is 4.7745137214660645\n",
      "(64, 33)\n",
      "step 16474, loss is 4.803842544555664\n",
      "(64, 33)\n",
      "step 16475, loss is 4.821571350097656\n",
      "(64, 33)\n",
      "step 16476, loss is 4.763546943664551\n",
      "(64, 33)\n",
      "step 16477, loss is 4.740087509155273\n",
      "(64, 33)\n",
      "step 16478, loss is 4.6336774826049805\n",
      "(64, 33)\n",
      "step 16479, loss is 4.818324089050293\n",
      "(64, 33)\n",
      "step 16480, loss is 4.818796157836914\n",
      "(64, 33)\n",
      "step 16481, loss is 4.782691955566406\n",
      "(64, 33)\n",
      "step 16482, loss is 4.6358208656311035\n",
      "(64, 33)\n",
      "step 16483, loss is 4.668299674987793\n",
      "(64, 33)\n",
      "step 16484, loss is 4.747359752655029\n",
      "(64, 33)\n",
      "step 16485, loss is 4.867516040802002\n",
      "(64, 33)\n",
      "step 16486, loss is 4.841069221496582\n",
      "(64, 33)\n",
      "step 16487, loss is 4.606907367706299\n",
      "(64, 33)\n",
      "step 16488, loss is 4.791531562805176\n",
      "(64, 33)\n",
      "step 16489, loss is 4.942830562591553\n",
      "(64, 33)\n",
      "step 16490, loss is 4.694507122039795\n",
      "(64, 33)\n",
      "step 16491, loss is 5.116805553436279\n",
      "(64, 33)\n",
      "step 16492, loss is 4.7151923179626465\n",
      "(64, 33)\n",
      "step 16493, loss is 4.665589332580566\n",
      "(64, 33)\n",
      "step 16494, loss is 4.864882946014404\n",
      "(64, 33)\n",
      "step 16495, loss is 4.616680145263672\n",
      "(64, 33)\n",
      "step 16496, loss is 5.10224723815918\n",
      "(64, 33)\n",
      "step 16497, loss is 4.783073425292969\n",
      "(64, 33)\n",
      "step 16498, loss is 4.816755771636963\n",
      "(64, 33)\n",
      "step 16499, loss is 4.988245010375977\n",
      "(64, 33)\n",
      "step 16500, loss is 4.991917610168457\n",
      "(64, 33)\n",
      "step 16501, loss is 4.726113319396973\n",
      "(64, 33)\n",
      "step 16502, loss is 4.795718193054199\n",
      "(64, 33)\n",
      "step 16503, loss is 4.820016384124756\n",
      "(64, 33)\n",
      "step 16504, loss is 4.5187225341796875\n",
      "(64, 33)\n",
      "step 16505, loss is 4.980714321136475\n",
      "(64, 33)\n",
      "step 16506, loss is 4.8285298347473145\n",
      "(64, 33)\n",
      "step 16507, loss is 5.040301322937012\n",
      "(64, 33)\n",
      "step 16508, loss is 4.859373569488525\n",
      "(64, 33)\n",
      "step 16509, loss is 4.696527481079102\n",
      "(64, 33)\n",
      "step 16510, loss is 4.766266822814941\n",
      "(64, 33)\n",
      "step 16511, loss is 4.768461227416992\n",
      "(64, 33)\n",
      "step 16512, loss is 4.832526683807373\n",
      "(64, 33)\n",
      "step 16513, loss is 4.741060733795166\n",
      "(64, 33)\n",
      "step 16514, loss is 4.982314586639404\n",
      "(64, 33)\n",
      "step 16515, loss is 4.581573486328125\n",
      "(64, 33)\n",
      "step 16516, loss is 4.866966247558594\n",
      "(64, 33)\n",
      "step 16517, loss is 4.891880035400391\n",
      "(64, 33)\n",
      "step 16518, loss is 4.74098539352417\n",
      "(64, 33)\n",
      "step 16519, loss is 4.613229751586914\n",
      "(64, 33)\n",
      "step 16520, loss is 4.939949989318848\n",
      "(64, 33)\n",
      "step 16521, loss is 4.77466344833374\n",
      "(64, 33)\n",
      "step 16522, loss is 4.54338264465332\n",
      "(64, 33)\n",
      "step 16523, loss is 4.623054027557373\n",
      "(64, 33)\n",
      "step 16524, loss is 4.680970191955566\n",
      "(64, 33)\n",
      "step 16525, loss is 4.758969306945801\n",
      "(64, 33)\n",
      "step 16526, loss is 4.788207530975342\n",
      "(64, 33)\n",
      "step 16527, loss is 4.7124457359313965\n",
      "(64, 33)\n",
      "step 16528, loss is 4.879753589630127\n",
      "(64, 33)\n",
      "step 16529, loss is 4.787353515625\n",
      "(64, 33)\n",
      "step 16530, loss is 4.873515605926514\n",
      "(64, 33)\n",
      "step 16531, loss is 4.820399284362793\n",
      "(64, 33)\n",
      "step 16532, loss is 4.9281325340271\n",
      "(64, 33)\n",
      "step 16533, loss is 4.864291667938232\n",
      "(64, 33)\n",
      "step 16534, loss is 4.7189154624938965\n",
      "(64, 33)\n",
      "step 16535, loss is 4.700999736785889\n",
      "(64, 33)\n",
      "step 16536, loss is 4.600235939025879\n",
      "(64, 33)\n",
      "step 16537, loss is 4.810159206390381\n",
      "(64, 33)\n",
      "step 16538, loss is 4.708970546722412\n",
      "(64, 33)\n",
      "step 16539, loss is 4.719480037689209\n",
      "(64, 33)\n",
      "step 16540, loss is 4.730795860290527\n",
      "(64, 33)\n",
      "step 16541, loss is 5.061305046081543\n",
      "(64, 33)\n",
      "step 16542, loss is 4.80884313583374\n",
      "(64, 33)\n",
      "step 16543, loss is 4.8713459968566895\n",
      "(64, 33)\n",
      "step 16544, loss is 4.787662029266357\n",
      "(64, 33)\n",
      "step 16545, loss is 4.612607955932617\n",
      "(64, 33)\n",
      "step 16546, loss is 4.843212127685547\n",
      "(64, 33)\n",
      "step 16547, loss is 4.862907409667969\n",
      "(64, 33)\n",
      "step 16548, loss is 4.631443023681641\n",
      "(64, 33)\n",
      "step 16549, loss is 4.831685543060303\n",
      "(64, 33)\n",
      "step 16550, loss is 4.817986011505127\n",
      "(64, 33)\n",
      "step 16551, loss is 4.828532695770264\n",
      "(64, 33)\n",
      "step 16552, loss is 4.780684471130371\n",
      "(64, 33)\n",
      "step 16553, loss is 4.852307319641113\n",
      "(64, 33)\n",
      "step 16554, loss is 4.778109550476074\n",
      "(64, 33)\n",
      "step 16555, loss is 4.797667980194092\n",
      "(64, 33)\n",
      "step 16556, loss is 5.055217266082764\n",
      "(64, 33)\n",
      "step 16557, loss is 4.746199131011963\n",
      "(64, 33)\n",
      "step 16558, loss is 4.826439380645752\n",
      "(64, 33)\n",
      "step 16559, loss is 4.749515056610107\n",
      "(64, 33)\n",
      "step 16560, loss is 4.6097412109375\n",
      "(64, 33)\n",
      "step 16561, loss is 4.838465690612793\n",
      "(64, 33)\n",
      "step 16562, loss is 4.787332057952881\n",
      "(64, 33)\n",
      "step 16563, loss is 4.663552761077881\n",
      "(64, 33)\n",
      "step 16564, loss is 4.864657402038574\n",
      "(64, 33)\n",
      "step 16565, loss is 4.614482402801514\n",
      "(64, 33)\n",
      "step 16566, loss is 4.844150543212891\n",
      "(64, 33)\n",
      "step 16567, loss is 4.63283109664917\n",
      "(64, 33)\n",
      "step 16568, loss is 4.954814434051514\n",
      "(64, 33)\n",
      "step 16569, loss is 4.778926372528076\n",
      "(64, 33)\n",
      "step 16570, loss is 4.881409645080566\n",
      "(64, 33)\n",
      "step 16571, loss is 4.668766975402832\n",
      "(64, 33)\n",
      "step 16572, loss is 4.862697601318359\n",
      "(64, 33)\n",
      "step 16573, loss is 4.912240028381348\n",
      "(64, 33)\n",
      "step 16574, loss is 4.934234142303467\n",
      "(64, 33)\n",
      "step 16575, loss is 4.8059492111206055\n",
      "(64, 33)\n",
      "step 16576, loss is 4.8491997718811035\n",
      "(64, 33)\n",
      "step 16577, loss is 4.722293853759766\n",
      "(64, 33)\n",
      "step 16578, loss is 4.826704978942871\n",
      "(64, 33)\n",
      "step 16579, loss is 4.90348482131958\n",
      "(64, 33)\n",
      "step 16580, loss is 4.843616485595703\n",
      "(64, 33)\n",
      "step 16581, loss is 4.8961181640625\n",
      "(64, 33)\n",
      "step 16582, loss is 4.685487270355225\n",
      "(64, 33)\n",
      "step 16583, loss is 4.902044773101807\n",
      "(64, 33)\n",
      "step 16584, loss is 4.78322172164917\n",
      "(64, 33)\n",
      "step 16585, loss is 4.709630489349365\n",
      "(64, 33)\n",
      "step 16586, loss is 4.787021636962891\n",
      "(64, 33)\n",
      "step 16587, loss is 4.607377052307129\n",
      "(64, 33)\n",
      "step 16588, loss is 4.679408550262451\n",
      "(64, 33)\n",
      "step 16589, loss is 4.75062894821167\n",
      "(64, 33)\n",
      "step 16590, loss is 4.736805438995361\n",
      "(64, 33)\n",
      "step 16591, loss is 4.933784008026123\n",
      "(64, 33)\n",
      "step 16592, loss is 4.754417896270752\n",
      "(64, 33)\n",
      "step 16593, loss is 4.74609375\n",
      "(64, 33)\n",
      "step 16594, loss is 4.672298431396484\n",
      "(64, 33)\n",
      "step 16595, loss is 4.745558261871338\n",
      "(64, 33)\n",
      "step 16596, loss is 4.919718265533447\n",
      "(64, 33)\n",
      "step 16597, loss is 4.7071614265441895\n",
      "(64, 33)\n",
      "step 16598, loss is 4.715081691741943\n",
      "(64, 33)\n",
      "step 16599, loss is 4.712149143218994\n",
      "(64, 33)\n",
      "step 16600, loss is 4.619019985198975\n",
      "(64, 33)\n",
      "step 16601, loss is 4.901775360107422\n",
      "(64, 33)\n",
      "step 16602, loss is 4.789227485656738\n",
      "(64, 33)\n",
      "step 16603, loss is 5.065349578857422\n",
      "(64, 33)\n",
      "step 16604, loss is 4.77025032043457\n",
      "(64, 33)\n",
      "step 16605, loss is 4.917165756225586\n",
      "(64, 33)\n",
      "step 16606, loss is 4.725485801696777\n",
      "(64, 33)\n",
      "step 16607, loss is 4.728836536407471\n",
      "(64, 33)\n",
      "step 16608, loss is 4.670276165008545\n",
      "(64, 33)\n",
      "step 16609, loss is 4.736135005950928\n",
      "(64, 33)\n",
      "step 16610, loss is 4.930588722229004\n",
      "(64, 33)\n",
      "step 16611, loss is 4.845390319824219\n",
      "(64, 33)\n",
      "step 16612, loss is 4.63729190826416\n",
      "(64, 33)\n",
      "step 16613, loss is 4.76146936416626\n",
      "(64, 33)\n",
      "step 16614, loss is 4.685841083526611\n",
      "(64, 33)\n",
      "step 16615, loss is 4.8120012283325195\n",
      "(64, 33)\n",
      "step 16616, loss is 4.873677730560303\n",
      "(64, 33)\n",
      "step 16617, loss is 4.796446800231934\n",
      "(64, 33)\n",
      "step 16618, loss is 4.807109832763672\n",
      "(64, 33)\n",
      "step 16619, loss is 4.881978988647461\n",
      "(64, 33)\n",
      "step 16620, loss is 4.73311185836792\n",
      "(64, 33)\n",
      "step 16621, loss is 4.894341468811035\n",
      "(64, 33)\n",
      "step 16622, loss is 4.7051239013671875\n",
      "(64, 33)\n",
      "step 16623, loss is 4.769216060638428\n",
      "(64, 33)\n",
      "step 16624, loss is 4.99594259262085\n",
      "(64, 33)\n",
      "step 16625, loss is 4.665866851806641\n",
      "(64, 33)\n",
      "step 16626, loss is 4.793275833129883\n",
      "(64, 33)\n",
      "step 16627, loss is 4.608670234680176\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16628, loss is 4.783577919006348\n",
      "(64, 33)\n",
      "step 16629, loss is 5.00197696685791\n",
      "(64, 33)\n",
      "step 16630, loss is 4.928413391113281\n",
      "(64, 33)\n",
      "step 16631, loss is 4.636496067047119\n",
      "(64, 33)\n",
      "step 16632, loss is 4.709565162658691\n",
      "(64, 33)\n",
      "step 16633, loss is 4.867519855499268\n",
      "(64, 33)\n",
      "step 16634, loss is 4.841362476348877\n",
      "(64, 33)\n",
      "step 16635, loss is 4.487686634063721\n",
      "(64, 33)\n",
      "step 16636, loss is 4.854894638061523\n",
      "(64, 33)\n",
      "step 16637, loss is 4.753936767578125\n",
      "(64, 33)\n",
      "step 16638, loss is 4.657716274261475\n",
      "(64, 33)\n",
      "step 16639, loss is 4.745964050292969\n",
      "(64, 33)\n",
      "step 16640, loss is 4.7172627449035645\n",
      "(64, 33)\n",
      "step 16641, loss is 4.848824501037598\n",
      "(64, 33)\n",
      "step 16642, loss is 4.955124855041504\n",
      "(64, 33)\n",
      "step 16643, loss is 4.740551471710205\n",
      "(64, 33)\n",
      "step 16644, loss is 4.653112888336182\n",
      "(64, 33)\n",
      "step 16645, loss is 4.774379253387451\n",
      "(64, 33)\n",
      "step 16646, loss is 4.815428256988525\n",
      "(64, 33)\n",
      "step 16647, loss is 4.91951322555542\n",
      "(64, 33)\n",
      "step 16648, loss is 4.750190734863281\n",
      "(64, 33)\n",
      "step 16649, loss is 4.776154518127441\n",
      "(64, 33)\n",
      "step 16650, loss is 4.698517799377441\n",
      "(64, 33)\n",
      "step 16651, loss is 4.88916540145874\n",
      "(64, 33)\n",
      "step 16652, loss is 4.691229820251465\n",
      "(64, 33)\n",
      "step 16653, loss is 4.7746758460998535\n",
      "(64, 33)\n",
      "step 16654, loss is 4.862562656402588\n",
      "(64, 33)\n",
      "step 16655, loss is 4.766353130340576\n",
      "(64, 33)\n",
      "step 16656, loss is 4.7650885581970215\n",
      "(64, 33)\n",
      "step 16657, loss is 4.798257827758789\n",
      "(64, 33)\n",
      "step 16658, loss is 4.859573841094971\n",
      "(64, 33)\n",
      "step 16659, loss is 4.651777267456055\n",
      "(64, 33)\n",
      "step 16660, loss is 4.862918853759766\n",
      "(64, 33)\n",
      "step 16661, loss is 4.800304889678955\n",
      "(64, 33)\n",
      "step 16662, loss is 4.968249797821045\n",
      "(64, 33)\n",
      "step 16663, loss is 4.67647123336792\n",
      "(64, 33)\n",
      "step 16664, loss is 4.941432952880859\n",
      "(64, 33)\n",
      "step 16665, loss is 4.724183559417725\n",
      "(64, 33)\n",
      "step 16666, loss is 4.899444103240967\n",
      "(64, 33)\n",
      "step 16667, loss is 4.747147083282471\n",
      "(64, 33)\n",
      "step 16668, loss is 4.748962879180908\n",
      "(64, 33)\n",
      "step 16669, loss is 4.693486213684082\n",
      "(64, 33)\n",
      "step 16670, loss is 4.744537353515625\n",
      "(64, 33)\n",
      "step 16671, loss is 4.624849319458008\n",
      "(64, 33)\n",
      "step 16672, loss is 4.629251956939697\n",
      "(64, 33)\n",
      "step 16673, loss is 4.975174903869629\n",
      "(64, 33)\n",
      "step 16674, loss is 4.8143134117126465\n",
      "(64, 33)\n",
      "step 16675, loss is 4.7323689460754395\n",
      "(64, 33)\n",
      "step 16676, loss is 4.921172142028809\n",
      "(64, 33)\n",
      "step 16677, loss is 4.796291351318359\n",
      "(64, 33)\n",
      "step 16678, loss is 4.767516136169434\n",
      "(64, 33)\n",
      "step 16679, loss is 4.655519485473633\n",
      "(64, 33)\n",
      "step 16680, loss is 4.738663196563721\n",
      "(64, 33)\n",
      "step 16681, loss is 4.719796657562256\n",
      "(64, 33)\n",
      "step 16682, loss is 4.817715644836426\n",
      "(64, 33)\n",
      "step 16683, loss is 4.797108173370361\n",
      "(64, 33)\n",
      "step 16684, loss is 4.905066013336182\n",
      "(64, 33)\n",
      "step 16685, loss is 4.653765678405762\n",
      "(64, 33)\n",
      "step 16686, loss is 4.720510482788086\n",
      "(64, 33)\n",
      "step 16687, loss is 4.773220062255859\n",
      "(64, 33)\n",
      "step 16688, loss is 4.66520357131958\n",
      "(64, 33)\n",
      "step 16689, loss is 4.885465621948242\n",
      "(64, 33)\n",
      "step 16690, loss is 4.835067272186279\n",
      "(64, 33)\n",
      "step 16691, loss is 4.752511978149414\n",
      "(64, 33)\n",
      "step 16692, loss is 4.880986213684082\n",
      "(64, 33)\n",
      "step 16693, loss is 4.736037731170654\n",
      "(64, 33)\n",
      "step 16694, loss is 4.751451015472412\n",
      "(64, 33)\n",
      "step 16695, loss is 4.692907333374023\n",
      "(64, 33)\n",
      "step 16696, loss is 4.765805721282959\n",
      "(64, 33)\n",
      "step 16697, loss is 4.805253982543945\n",
      "(64, 33)\n",
      "step 16698, loss is 4.649782180786133\n",
      "(64, 33)\n",
      "step 16699, loss is 4.773276329040527\n",
      "(64, 33)\n",
      "step 16700, loss is 4.715797424316406\n",
      "(64, 33)\n",
      "step 16701, loss is 4.719975471496582\n",
      "(64, 33)\n",
      "step 16702, loss is 4.7599568367004395\n",
      "(64, 33)\n",
      "step 16703, loss is 4.796328067779541\n",
      "(64, 33)\n",
      "step 16704, loss is 4.813486576080322\n",
      "(64, 33)\n",
      "step 16705, loss is 4.8084611892700195\n",
      "(64, 33)\n",
      "step 16706, loss is 4.732347011566162\n",
      "(64, 33)\n",
      "step 16707, loss is 4.875249862670898\n",
      "(64, 33)\n",
      "step 16708, loss is 4.9405035972595215\n",
      "(64, 33)\n",
      "step 16709, loss is 4.658575057983398\n",
      "(64, 33)\n",
      "step 16710, loss is 4.850495338439941\n",
      "(64, 33)\n",
      "step 16711, loss is 4.5590362548828125\n",
      "(64, 33)\n",
      "step 16712, loss is 4.984940052032471\n",
      "(64, 33)\n",
      "step 16713, loss is 4.737502574920654\n",
      "(64, 33)\n",
      "step 16714, loss is 4.691411972045898\n",
      "(64, 33)\n",
      "step 16715, loss is 4.800632476806641\n",
      "(64, 33)\n",
      "step 16716, loss is 4.768258094787598\n",
      "(64, 33)\n",
      "step 16717, loss is 4.78215217590332\n",
      "(64, 33)\n",
      "step 16718, loss is 4.964607238769531\n",
      "(64, 33)\n",
      "step 16719, loss is 4.741650581359863\n",
      "(64, 33)\n",
      "step 16720, loss is 4.841109752655029\n",
      "(64, 33)\n",
      "step 16721, loss is 4.876815319061279\n",
      "(64, 33)\n",
      "step 16722, loss is 4.729273319244385\n",
      "(64, 33)\n",
      "step 16723, loss is 4.762443542480469\n",
      "(64, 33)\n",
      "step 16724, loss is 4.744200706481934\n",
      "(64, 33)\n",
      "step 16725, loss is 4.889810085296631\n",
      "(64, 33)\n",
      "step 16726, loss is 4.720165252685547\n",
      "(64, 33)\n",
      "step 16727, loss is 4.517397880554199\n",
      "(64, 33)\n",
      "step 16728, loss is 4.825590133666992\n",
      "(64, 33)\n",
      "step 16729, loss is 4.792843818664551\n",
      "(64, 33)\n",
      "step 16730, loss is 4.960935592651367\n",
      "(64, 33)\n",
      "step 16731, loss is 4.8712568283081055\n",
      "(64, 33)\n",
      "step 16732, loss is 4.824080467224121\n",
      "(64, 33)\n",
      "step 16733, loss is 4.764656066894531\n",
      "(64, 33)\n",
      "step 16734, loss is 4.768028259277344\n",
      "(64, 33)\n",
      "step 16735, loss is 4.710030555725098\n",
      "(64, 33)\n",
      "step 16736, loss is 4.80902624130249\n",
      "(64, 33)\n",
      "step 16737, loss is 4.770138740539551\n",
      "(64, 33)\n",
      "step 16738, loss is 4.920854568481445\n",
      "(64, 33)\n",
      "step 16739, loss is 4.759341716766357\n",
      "(64, 33)\n",
      "step 16740, loss is 4.776605129241943\n",
      "(64, 33)\n",
      "step 16741, loss is 4.680488586425781\n",
      "(64, 33)\n",
      "step 16742, loss is 4.838169097900391\n",
      "(64, 33)\n",
      "step 16743, loss is 4.621265888214111\n",
      "(64, 33)\n",
      "step 16744, loss is 4.883296489715576\n",
      "(64, 33)\n",
      "step 16745, loss is 4.760509967803955\n",
      "(64, 33)\n",
      "step 16746, loss is 4.710165500640869\n",
      "(64, 33)\n",
      "step 16747, loss is 5.021781921386719\n",
      "(64, 33)\n",
      "step 16748, loss is 4.77608060836792\n",
      "(64, 33)\n",
      "step 16749, loss is 4.970749855041504\n",
      "(64, 33)\n",
      "step 16750, loss is 4.846357345581055\n",
      "(64, 33)\n",
      "step 16751, loss is 4.956342697143555\n",
      "(64, 33)\n",
      "step 16752, loss is 4.639927864074707\n",
      "(64, 33)\n",
      "step 16753, loss is 4.932923316955566\n",
      "(64, 33)\n",
      "step 16754, loss is 4.645305156707764\n",
      "(64, 33)\n",
      "step 16755, loss is 4.698704242706299\n",
      "(64, 33)\n",
      "step 16756, loss is 4.763420104980469\n",
      "(64, 33)\n",
      "step 16757, loss is 4.712852478027344\n",
      "(64, 33)\n",
      "step 16758, loss is 5.064946174621582\n",
      "(64, 33)\n",
      "step 16759, loss is 4.981656551361084\n",
      "(64, 33)\n",
      "step 16760, loss is 4.9184250831604\n",
      "(64, 33)\n",
      "step 16761, loss is 4.77846622467041\n",
      "(64, 33)\n",
      "step 16762, loss is 4.844603538513184\n",
      "(64, 33)\n",
      "step 16763, loss is 4.84819221496582\n",
      "(64, 33)\n",
      "step 16764, loss is 4.631148815155029\n",
      "(64, 33)\n",
      "step 16765, loss is 4.631213665008545\n",
      "(64, 33)\n",
      "step 16766, loss is 4.479272365570068\n",
      "(64, 33)\n",
      "step 16767, loss is 4.910157203674316\n",
      "(64, 33)\n",
      "step 16768, loss is 4.8122358322143555\n",
      "(64, 33)\n",
      "step 16769, loss is 4.784843444824219\n",
      "(64, 33)\n",
      "step 16770, loss is 4.89491605758667\n",
      "(64, 33)\n",
      "step 16771, loss is 4.849034309387207\n",
      "(64, 33)\n",
      "step 16772, loss is 4.734482288360596\n",
      "(64, 33)\n",
      "step 16773, loss is 4.640199661254883\n",
      "(64, 33)\n",
      "step 16774, loss is 4.741192817687988\n",
      "(64, 33)\n",
      "step 16775, loss is 4.658318996429443\n",
      "(64, 33)\n",
      "step 16776, loss is 4.876531600952148\n",
      "(64, 33)\n",
      "step 16777, loss is 4.906683921813965\n",
      "(64, 33)\n",
      "step 16778, loss is 4.663029670715332\n",
      "(64, 33)\n",
      "step 16779, loss is 4.775289058685303\n",
      "(64, 33)\n",
      "step 16780, loss is 4.778298854827881\n",
      "(64, 33)\n",
      "step 16781, loss is 4.7571187019348145\n",
      "(64, 33)\n",
      "step 16782, loss is 4.783555030822754\n",
      "(64, 33)\n",
      "step 16783, loss is 4.951839447021484\n",
      "(64, 33)\n",
      "step 16784, loss is 4.7753987312316895\n",
      "(64, 33)\n",
      "step 16785, loss is 4.822688579559326\n",
      "(64, 33)\n",
      "step 16786, loss is 4.789276123046875\n",
      "(64, 33)\n",
      "step 16787, loss is 4.702634811401367\n",
      "(64, 33)\n",
      "step 16788, loss is 4.89481782913208\n",
      "(64, 33)\n",
      "step 16789, loss is 4.8620381355285645\n",
      "(64, 33)\n",
      "step 16790, loss is 4.712898254394531\n",
      "(64, 33)\n",
      "step 16791, loss is 4.733770370483398\n",
      "(64, 33)\n",
      "step 16792, loss is 4.903683662414551\n",
      "(64, 33)\n",
      "step 16793, loss is 4.704062461853027\n",
      "(64, 33)\n",
      "step 16794, loss is 4.860903739929199\n",
      "(64, 33)\n",
      "step 16795, loss is 4.801215648651123\n",
      "(64, 33)\n",
      "step 16796, loss is 4.774303436279297\n",
      "(64, 33)\n",
      "step 16797, loss is 4.7641682624816895\n",
      "(64, 33)\n",
      "step 16798, loss is 4.692314147949219\n",
      "(64, 33)\n",
      "step 16799, loss is 4.711395740509033\n",
      "(64, 33)\n",
      "step 16800, loss is 4.9682841300964355\n",
      "(64, 33)\n",
      "step 16801, loss is 4.8642401695251465\n",
      "(64, 33)\n",
      "step 16802, loss is 4.812365531921387\n",
      "(64, 33)\n",
      "step 16803, loss is 4.92402458190918\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16804, loss is 4.930887699127197\n",
      "(64, 33)\n",
      "step 16805, loss is 4.78488302230835\n",
      "(64, 33)\n",
      "step 16806, loss is 4.610520839691162\n",
      "(64, 33)\n",
      "step 16807, loss is 4.629036903381348\n",
      "(64, 33)\n",
      "step 16808, loss is 4.819908142089844\n",
      "(64, 33)\n",
      "step 16809, loss is 4.853280544281006\n",
      "(64, 33)\n",
      "step 16810, loss is 4.841674327850342\n",
      "(64, 33)\n",
      "step 16811, loss is 4.532683372497559\n",
      "(64, 33)\n",
      "step 16812, loss is 4.960437297821045\n",
      "(64, 33)\n",
      "step 16813, loss is 4.828959941864014\n",
      "(64, 33)\n",
      "step 16814, loss is 4.7724385261535645\n",
      "(64, 33)\n",
      "step 16815, loss is 4.986547946929932\n",
      "(64, 33)\n",
      "step 16816, loss is 4.7656636238098145\n",
      "(64, 33)\n",
      "step 16817, loss is 4.993898391723633\n",
      "(64, 33)\n",
      "step 16818, loss is 4.839431285858154\n",
      "(64, 33)\n",
      "step 16819, loss is 4.806207180023193\n",
      "(64, 33)\n",
      "step 16820, loss is 4.79629373550415\n",
      "(64, 33)\n",
      "step 16821, loss is 4.801906108856201\n",
      "(64, 33)\n",
      "step 16822, loss is 4.8496994972229\n",
      "(64, 33)\n",
      "step 16823, loss is 4.839634895324707\n",
      "(64, 33)\n",
      "step 16824, loss is 5.034914970397949\n",
      "(64, 33)\n",
      "step 16825, loss is 4.798273086547852\n",
      "(64, 33)\n",
      "step 16826, loss is 4.712020397186279\n",
      "(64, 33)\n",
      "step 16827, loss is 4.807878017425537\n",
      "(64, 33)\n",
      "step 16828, loss is 4.858153343200684\n",
      "(64, 33)\n",
      "step 16829, loss is 4.792586803436279\n",
      "(64, 33)\n",
      "step 16830, loss is 4.80914306640625\n",
      "(64, 33)\n",
      "step 16831, loss is 4.957863807678223\n",
      "(64, 33)\n",
      "step 16832, loss is 4.723310947418213\n",
      "(64, 33)\n",
      "step 16833, loss is 4.691829681396484\n",
      "(64, 33)\n",
      "step 16834, loss is 4.82852840423584\n",
      "(64, 33)\n",
      "step 16835, loss is 4.674168586730957\n",
      "(64, 33)\n",
      "step 16836, loss is 4.9205193519592285\n",
      "(64, 33)\n",
      "step 16837, loss is 4.779088497161865\n",
      "(64, 33)\n",
      "step 16838, loss is 4.754916667938232\n",
      "(64, 33)\n",
      "step 16839, loss is 4.752986431121826\n",
      "(64, 33)\n",
      "step 16840, loss is 4.684832572937012\n",
      "(64, 33)\n",
      "step 16841, loss is 4.886557102203369\n",
      "(64, 33)\n",
      "step 16842, loss is 4.842986583709717\n",
      "(64, 33)\n",
      "step 16843, loss is 4.942878246307373\n",
      "(64, 33)\n",
      "step 16844, loss is 4.6069254875183105\n",
      "(64, 33)\n",
      "step 16845, loss is 4.846052169799805\n",
      "(64, 33)\n",
      "step 16846, loss is 4.823819160461426\n",
      "(64, 33)\n",
      "step 16847, loss is 4.638327598571777\n",
      "(64, 33)\n",
      "step 16848, loss is 4.619690895080566\n",
      "(64, 33)\n",
      "step 16849, loss is 4.751951217651367\n",
      "(64, 33)\n",
      "step 16850, loss is 4.729442596435547\n",
      "(64, 33)\n",
      "step 16851, loss is 4.71167516708374\n",
      "(64, 33)\n",
      "step 16852, loss is 4.796607971191406\n",
      "(64, 33)\n",
      "step 16853, loss is 4.758212566375732\n",
      "(64, 33)\n",
      "step 16854, loss is 4.728106498718262\n",
      "(64, 33)\n",
      "step 16855, loss is 4.836572647094727\n",
      "(64, 33)\n",
      "step 16856, loss is 4.844712734222412\n",
      "(64, 33)\n",
      "step 16857, loss is 5.06295108795166\n",
      "(64, 33)\n",
      "step 16858, loss is 4.89133882522583\n",
      "(64, 33)\n",
      "step 16859, loss is 4.8552045822143555\n",
      "(64, 33)\n",
      "step 16860, loss is 4.721110820770264\n",
      "(64, 33)\n",
      "step 16861, loss is 4.7606377601623535\n",
      "(64, 33)\n",
      "step 16862, loss is 4.833192825317383\n",
      "(64, 33)\n",
      "step 16863, loss is 4.930089950561523\n",
      "(64, 33)\n",
      "step 16864, loss is 4.925461292266846\n",
      "(64, 33)\n",
      "step 16865, loss is 4.874773979187012\n",
      "(64, 33)\n",
      "step 16866, loss is 4.862873554229736\n",
      "(64, 33)\n",
      "step 16867, loss is 4.830289840698242\n",
      "(64, 33)\n",
      "step 16868, loss is 4.7706074714660645\n",
      "(64, 33)\n",
      "step 16869, loss is 4.88031005859375\n",
      "(64, 33)\n",
      "step 16870, loss is 4.737152099609375\n",
      "(64, 33)\n",
      "step 16871, loss is 4.777098178863525\n",
      "(64, 33)\n",
      "step 16872, loss is 4.701831340789795\n",
      "(64, 33)\n",
      "step 16873, loss is 4.935944080352783\n",
      "(64, 33)\n",
      "step 16874, loss is 4.779646396636963\n",
      "(64, 33)\n",
      "step 16875, loss is 4.886122703552246\n",
      "(64, 33)\n",
      "step 16876, loss is 4.7132487297058105\n",
      "(64, 33)\n",
      "step 16877, loss is 4.666386127471924\n",
      "(64, 33)\n",
      "step 16878, loss is 4.8233642578125\n",
      "(64, 33)\n",
      "step 16879, loss is 4.686679363250732\n",
      "(64, 33)\n",
      "step 16880, loss is 4.8870439529418945\n",
      "(64, 33)\n",
      "step 16881, loss is 4.716992378234863\n",
      "(64, 33)\n",
      "step 16882, loss is 4.754578113555908\n",
      "(64, 33)\n",
      "step 16883, loss is 4.830533027648926\n",
      "(64, 33)\n",
      "step 16884, loss is 4.8263654708862305\n",
      "(64, 33)\n",
      "step 16885, loss is 4.694828033447266\n",
      "(64, 33)\n",
      "step 16886, loss is 4.942426681518555\n",
      "(64, 33)\n",
      "step 16887, loss is 4.785447120666504\n",
      "(64, 33)\n",
      "step 16888, loss is 4.928154468536377\n",
      "(64, 33)\n",
      "step 16889, loss is 4.67933464050293\n",
      "(64, 33)\n",
      "step 16890, loss is 4.843912601470947\n",
      "(64, 33)\n",
      "step 16891, loss is 4.788341045379639\n",
      "(64, 33)\n",
      "step 16892, loss is 4.7239789962768555\n",
      "(64, 33)\n",
      "step 16893, loss is 5.117607116699219\n",
      "(64, 33)\n",
      "step 16894, loss is 4.501221656799316\n",
      "(64, 33)\n",
      "step 16895, loss is 4.757946968078613\n",
      "(64, 33)\n",
      "step 16896, loss is 4.670738220214844\n",
      "(64, 33)\n",
      "step 16897, loss is 4.869899749755859\n",
      "(64, 33)\n",
      "step 16898, loss is 4.703358173370361\n",
      "(64, 33)\n",
      "step 16899, loss is 4.867095947265625\n",
      "(64, 33)\n",
      "step 16900, loss is 4.690122127532959\n",
      "(64, 33)\n",
      "step 16901, loss is 4.782703399658203\n",
      "(64, 33)\n",
      "step 16902, loss is 4.74211311340332\n",
      "(64, 33)\n",
      "step 16903, loss is 4.7605133056640625\n",
      "(64, 33)\n",
      "step 16904, loss is 4.700965404510498\n",
      "(64, 33)\n",
      "step 16905, loss is 4.697253704071045\n",
      "(64, 33)\n",
      "step 16906, loss is 4.9556145668029785\n",
      "(64, 33)\n",
      "step 16907, loss is 4.853116035461426\n",
      "(64, 33)\n",
      "step 16908, loss is 4.929527282714844\n",
      "(64, 33)\n",
      "step 16909, loss is 4.736260414123535\n",
      "(64, 33)\n",
      "step 16910, loss is 4.875117301940918\n",
      "(64, 33)\n",
      "step 16911, loss is 4.918752193450928\n",
      "(64, 33)\n",
      "step 16912, loss is 4.844933032989502\n",
      "(64, 33)\n",
      "step 16913, loss is 4.928300857543945\n",
      "(64, 33)\n",
      "step 16914, loss is 4.7618408203125\n",
      "(64, 33)\n",
      "step 16915, loss is 4.9886250495910645\n",
      "(64, 33)\n",
      "step 16916, loss is 4.764512062072754\n",
      "(64, 33)\n",
      "step 16917, loss is 4.929152965545654\n",
      "(64, 33)\n",
      "step 16918, loss is 4.848037242889404\n",
      "(64, 33)\n",
      "step 16919, loss is 4.731226444244385\n",
      "(64, 33)\n",
      "step 16920, loss is 5.007312297821045\n",
      "(64, 33)\n",
      "step 16921, loss is 4.807376861572266\n",
      "(64, 33)\n",
      "step 16922, loss is 4.883410453796387\n",
      "(64, 33)\n",
      "step 16923, loss is 4.772135257720947\n",
      "(64, 33)\n",
      "step 16924, loss is 4.818192005157471\n",
      "(64, 33)\n",
      "step 16925, loss is 4.857547283172607\n",
      "(64, 33)\n",
      "step 16926, loss is 4.843676567077637\n",
      "(64, 33)\n",
      "step 16927, loss is 4.847320079803467\n",
      "(64, 33)\n",
      "step 16928, loss is 4.788834095001221\n",
      "(64, 33)\n",
      "step 16929, loss is 4.693243980407715\n",
      "(64, 33)\n",
      "step 16930, loss is 4.620780944824219\n",
      "(64, 33)\n",
      "step 16931, loss is 4.814358711242676\n",
      "(64, 33)\n",
      "step 16932, loss is 5.055180072784424\n",
      "(64, 33)\n",
      "step 16933, loss is 4.718255519866943\n",
      "(64, 33)\n",
      "step 16934, loss is 4.761171817779541\n",
      "(64, 33)\n",
      "step 16935, loss is 4.816860675811768\n",
      "(64, 33)\n",
      "step 16936, loss is 4.694810390472412\n",
      "(64, 33)\n",
      "step 16937, loss is 4.737504482269287\n",
      "(64, 33)\n",
      "step 16938, loss is 4.886138916015625\n",
      "(64, 33)\n",
      "step 16939, loss is 4.869840145111084\n",
      "(64, 33)\n",
      "step 16940, loss is 4.700157165527344\n",
      "(64, 33)\n",
      "step 16941, loss is 4.6368279457092285\n",
      "(64, 33)\n",
      "step 16942, loss is 4.748859405517578\n",
      "(64, 33)\n",
      "step 16943, loss is 4.90792989730835\n",
      "(64, 33)\n",
      "step 16944, loss is 4.695806503295898\n",
      "(64, 33)\n",
      "step 16945, loss is 4.8353657722473145\n",
      "(64, 33)\n",
      "step 16946, loss is 4.8269853591918945\n",
      "(64, 33)\n",
      "step 16947, loss is 4.710554599761963\n",
      "(64, 33)\n",
      "step 16948, loss is 4.724071025848389\n",
      "(64, 33)\n",
      "step 16949, loss is 4.899185657501221\n",
      "(64, 33)\n",
      "step 16950, loss is 4.742267608642578\n",
      "(64, 33)\n",
      "step 16951, loss is 4.703676223754883\n",
      "(64, 33)\n",
      "step 16952, loss is 4.869933605194092\n",
      "(64, 33)\n",
      "step 16953, loss is 4.881717681884766\n",
      "(64, 33)\n",
      "step 16954, loss is 4.761713981628418\n",
      "(64, 33)\n",
      "step 16955, loss is 4.833786964416504\n",
      "(64, 33)\n",
      "step 16956, loss is 4.780091762542725\n",
      "(64, 33)\n",
      "step 16957, loss is 4.833317756652832\n",
      "(64, 33)\n",
      "step 16958, loss is 4.74639368057251\n",
      "(64, 33)\n",
      "step 16959, loss is 5.032425403594971\n",
      "(64, 33)\n",
      "step 16960, loss is 4.773689270019531\n",
      "(64, 33)\n",
      "step 16961, loss is 4.81857442855835\n",
      "(64, 33)\n",
      "step 16962, loss is 4.649847507476807\n",
      "(64, 33)\n",
      "step 16963, loss is 4.961341381072998\n",
      "(64, 33)\n",
      "step 16964, loss is 4.857278823852539\n",
      "(64, 33)\n",
      "step 16965, loss is 4.829207420349121\n",
      "(64, 33)\n",
      "step 16966, loss is 4.719512939453125\n",
      "(64, 33)\n",
      "step 16967, loss is 4.84957218170166\n",
      "(64, 33)\n",
      "step 16968, loss is 4.977158546447754\n",
      "(64, 33)\n",
      "step 16969, loss is 4.845241546630859\n",
      "(64, 33)\n",
      "step 16970, loss is 4.842084884643555\n",
      "(64, 33)\n",
      "step 16971, loss is 4.7911906242370605\n",
      "(64, 33)\n",
      "step 16972, loss is 4.858086585998535\n",
      "(64, 33)\n",
      "step 16973, loss is 4.818878173828125\n",
      "(64, 33)\n",
      "step 16974, loss is 4.808653354644775\n",
      "(64, 33)\n",
      "step 16975, loss is 4.671022415161133\n",
      "(64, 33)\n",
      "step 16976, loss is 4.675455093383789\n",
      "(64, 33)\n",
      "step 16977, loss is 4.744636535644531\n",
      "(64, 33)\n",
      "step 16978, loss is 4.832962512969971\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16979, loss is 5.013523578643799\n",
      "(64, 33)\n",
      "step 16980, loss is 4.839625835418701\n",
      "(64, 33)\n",
      "step 16981, loss is 5.012291431427002\n",
      "(64, 33)\n",
      "step 16982, loss is 4.790677070617676\n",
      "(64, 33)\n",
      "step 16983, loss is 4.878707408905029\n",
      "(64, 33)\n",
      "step 16984, loss is 4.723255634307861\n",
      "(64, 33)\n",
      "step 16985, loss is 4.830131530761719\n",
      "(64, 33)\n",
      "step 16986, loss is 4.704795837402344\n",
      "(64, 33)\n",
      "step 16987, loss is 4.647128105163574\n",
      "(64, 33)\n",
      "step 16988, loss is 4.767146110534668\n",
      "(64, 33)\n",
      "step 16989, loss is 4.79388427734375\n",
      "(64, 33)\n",
      "step 16990, loss is 4.703718662261963\n",
      "(64, 33)\n",
      "step 16991, loss is 4.745596408843994\n",
      "(64, 33)\n",
      "step 16992, loss is 4.781350612640381\n",
      "(64, 33)\n",
      "step 16993, loss is 4.751423358917236\n",
      "(64, 33)\n",
      "step 16994, loss is 4.8932647705078125\n",
      "(64, 33)\n",
      "step 16995, loss is 4.829309463500977\n",
      "(64, 33)\n",
      "step 16996, loss is 4.872645854949951\n",
      "(64, 33)\n",
      "step 16997, loss is 4.802134990692139\n",
      "(64, 33)\n",
      "step 16998, loss is 4.766455173492432\n",
      "(64, 33)\n",
      "step 16999, loss is 4.7611212730407715\n",
      "(64, 33)\n",
      "step 17000, loss is 4.953481197357178\n",
      "(64, 33)\n",
      "step 17001, loss is 4.922684192657471\n",
      "(64, 33)\n",
      "step 17002, loss is 4.708559989929199\n",
      "(64, 33)\n",
      "step 17003, loss is 4.924734592437744\n",
      "(64, 33)\n",
      "step 17004, loss is 4.777625560760498\n",
      "(64, 33)\n",
      "step 17005, loss is 4.712109088897705\n",
      "(64, 33)\n",
      "step 17006, loss is 4.7024078369140625\n",
      "(64, 33)\n",
      "step 17007, loss is 4.655704021453857\n",
      "(64, 33)\n",
      "step 17008, loss is 4.843604564666748\n",
      "(64, 33)\n",
      "step 17009, loss is 4.9290337562561035\n",
      "(64, 33)\n",
      "step 17010, loss is 4.577159881591797\n",
      "(64, 33)\n",
      "step 17011, loss is 4.701858997344971\n",
      "(64, 33)\n",
      "step 17012, loss is 4.901292324066162\n",
      "(64, 33)\n",
      "step 17013, loss is 4.802053928375244\n",
      "(64, 33)\n",
      "step 17014, loss is 4.922563552856445\n",
      "(64, 33)\n",
      "step 17015, loss is 4.84091329574585\n",
      "(64, 33)\n",
      "step 17016, loss is 4.710378170013428\n",
      "(64, 33)\n",
      "step 17017, loss is 5.0222625732421875\n",
      "(64, 33)\n",
      "step 17018, loss is 4.878921985626221\n",
      "(64, 33)\n",
      "step 17019, loss is 4.758480072021484\n",
      "(64, 33)\n",
      "step 17020, loss is 4.6786274909973145\n",
      "(64, 33)\n",
      "step 17021, loss is 4.773348331451416\n",
      "(64, 33)\n",
      "step 17022, loss is 4.666889667510986\n",
      "(64, 33)\n",
      "step 17023, loss is 4.874200344085693\n",
      "(64, 33)\n",
      "step 17024, loss is 4.634298324584961\n",
      "(64, 33)\n",
      "step 17025, loss is 4.801220417022705\n",
      "(64, 33)\n",
      "step 17026, loss is 4.844238758087158\n",
      "(64, 33)\n",
      "step 17027, loss is 4.849025726318359\n",
      "(64, 33)\n",
      "step 17028, loss is 4.773618221282959\n",
      "(64, 33)\n",
      "step 17029, loss is 4.718338966369629\n",
      "(64, 33)\n",
      "step 17030, loss is 4.783256530761719\n",
      "(64, 33)\n",
      "step 17031, loss is 4.819083213806152\n",
      "(64, 33)\n",
      "step 17032, loss is 4.800453186035156\n",
      "(64, 33)\n",
      "step 17033, loss is 4.740148544311523\n",
      "(64, 33)\n",
      "step 17034, loss is 4.909150123596191\n",
      "(64, 33)\n",
      "step 17035, loss is 4.713348865509033\n",
      "(64, 33)\n",
      "step 17036, loss is 4.828337669372559\n",
      "(64, 33)\n",
      "step 17037, loss is 4.784183502197266\n",
      "(64, 33)\n",
      "step 17038, loss is 4.894303798675537\n",
      "(64, 33)\n",
      "step 17039, loss is 4.808043479919434\n",
      "(64, 33)\n",
      "step 17040, loss is 4.8195319175720215\n",
      "(64, 33)\n",
      "step 17041, loss is 4.951944351196289\n",
      "(64, 33)\n",
      "step 17042, loss is 4.770888328552246\n",
      "(64, 33)\n",
      "step 17043, loss is 4.806262493133545\n",
      "(64, 33)\n",
      "step 17044, loss is 4.656311511993408\n",
      "(64, 33)\n",
      "step 17045, loss is 4.8325090408325195\n",
      "(64, 33)\n",
      "step 17046, loss is 4.667696475982666\n",
      "(64, 33)\n",
      "step 17047, loss is 4.622340679168701\n",
      "(64, 33)\n",
      "step 17048, loss is 4.717779159545898\n",
      "(64, 33)\n",
      "step 17049, loss is 4.837253570556641\n",
      "(64, 33)\n",
      "step 17050, loss is 4.73067569732666\n",
      "(64, 33)\n",
      "step 17051, loss is 4.87518310546875\n",
      "(64, 33)\n",
      "step 17052, loss is 4.899994373321533\n",
      "(64, 33)\n",
      "step 17053, loss is 4.774698257446289\n",
      "(64, 33)\n",
      "step 17054, loss is 4.7126240730285645\n",
      "(64, 33)\n",
      "step 17055, loss is 4.843344211578369\n",
      "(64, 33)\n",
      "step 17056, loss is 4.7833733558654785\n",
      "(64, 33)\n",
      "step 17057, loss is 4.770164966583252\n",
      "(64, 33)\n",
      "step 17058, loss is 4.721126556396484\n",
      "(64, 33)\n",
      "step 17059, loss is 4.779866695404053\n",
      "(64, 33)\n",
      "step 17060, loss is 4.7571563720703125\n",
      "(64, 33)\n",
      "step 17061, loss is 4.863280296325684\n",
      "(64, 33)\n",
      "step 17062, loss is 4.823605537414551\n",
      "(64, 33)\n",
      "step 17063, loss is 4.884519577026367\n",
      "(64, 33)\n",
      "step 17064, loss is 4.8576459884643555\n",
      "(64, 33)\n",
      "step 17065, loss is 4.894118309020996\n",
      "(64, 33)\n",
      "step 17066, loss is 4.876490592956543\n",
      "(64, 33)\n",
      "step 17067, loss is 4.84514045715332\n",
      "(64, 33)\n",
      "step 17068, loss is 4.657309532165527\n",
      "(64, 33)\n",
      "step 17069, loss is 4.855901718139648\n",
      "(64, 33)\n",
      "step 17070, loss is 4.719862937927246\n",
      "(64, 33)\n",
      "step 17071, loss is 4.972768306732178\n",
      "(64, 33)\n",
      "step 17072, loss is 4.7679524421691895\n",
      "(64, 33)\n",
      "step 17073, loss is 4.798473834991455\n",
      "(64, 33)\n",
      "step 17074, loss is 4.718870162963867\n",
      "(64, 33)\n",
      "step 17075, loss is 4.71697998046875\n",
      "(64, 33)\n",
      "step 17076, loss is 4.912914752960205\n",
      "(64, 33)\n",
      "step 17077, loss is 4.754983901977539\n",
      "(64, 33)\n",
      "step 17078, loss is 4.739399433135986\n",
      "(64, 33)\n",
      "step 17079, loss is 4.697384357452393\n",
      "(64, 33)\n",
      "step 17080, loss is 4.88148832321167\n",
      "(64, 33)\n",
      "step 17081, loss is 4.72377347946167\n",
      "(64, 33)\n",
      "step 17082, loss is 4.864477157592773\n",
      "(64, 33)\n",
      "step 17083, loss is 4.83641242980957\n",
      "(64, 33)\n",
      "step 17084, loss is 4.7808051109313965\n",
      "(64, 33)\n",
      "step 17085, loss is 4.763318061828613\n",
      "(64, 33)\n",
      "step 17086, loss is 4.76613712310791\n",
      "(64, 33)\n",
      "step 17087, loss is 4.956055164337158\n",
      "(64, 33)\n",
      "step 17088, loss is 4.943175315856934\n",
      "(64, 33)\n",
      "step 17089, loss is 4.887428283691406\n",
      "(64, 33)\n",
      "step 17090, loss is 4.949584484100342\n",
      "(64, 33)\n",
      "step 17091, loss is 4.77479887008667\n",
      "(64, 33)\n",
      "step 17092, loss is 4.804896354675293\n",
      "(64, 33)\n",
      "step 17093, loss is 4.770514965057373\n",
      "(64, 33)\n",
      "step 17094, loss is 4.925151824951172\n",
      "(64, 33)\n",
      "step 17095, loss is 4.844176769256592\n",
      "(64, 33)\n",
      "step 17096, loss is 4.684732437133789\n",
      "(64, 33)\n",
      "step 17097, loss is 4.706126689910889\n",
      "(64, 33)\n",
      "step 17098, loss is 4.710036754608154\n",
      "(64, 33)\n",
      "step 17099, loss is 4.767037868499756\n",
      "(64, 33)\n",
      "step 17100, loss is 4.7951765060424805\n",
      "(64, 33)\n",
      "step 17101, loss is 4.770924091339111\n",
      "(64, 33)\n",
      "step 17102, loss is 4.91129732131958\n",
      "(64, 33)\n",
      "step 17103, loss is 4.647141933441162\n",
      "(64, 33)\n",
      "step 17104, loss is 4.760076999664307\n",
      "(64, 33)\n",
      "step 17105, loss is 4.857083797454834\n",
      "(64, 33)\n",
      "step 17106, loss is 4.92592716217041\n",
      "(64, 33)\n",
      "step 17107, loss is 4.723342418670654\n",
      "(64, 33)\n",
      "step 17108, loss is 4.6537628173828125\n",
      "(64, 33)\n",
      "step 17109, loss is 4.787078380584717\n",
      "(64, 33)\n",
      "step 17110, loss is 4.909329891204834\n",
      "(64, 33)\n",
      "step 17111, loss is 4.899929046630859\n",
      "(64, 33)\n",
      "step 17112, loss is 4.7287468910217285\n",
      "(64, 33)\n",
      "step 17113, loss is 4.773767948150635\n",
      "(64, 33)\n",
      "step 17114, loss is 4.75859260559082\n",
      "(64, 33)\n",
      "step 17115, loss is 4.7998456954956055\n",
      "(64, 33)\n",
      "step 17116, loss is 4.773576259613037\n",
      "(64, 33)\n",
      "step 17117, loss is 4.823611259460449\n",
      "(64, 33)\n",
      "step 17118, loss is 4.591643810272217\n",
      "(64, 33)\n",
      "step 17119, loss is 4.625057220458984\n",
      "(64, 33)\n",
      "step 17120, loss is 4.627363204956055\n",
      "(64, 33)\n",
      "step 17121, loss is 4.849361896514893\n",
      "(64, 33)\n",
      "step 17122, loss is 4.835121154785156\n",
      "(64, 33)\n",
      "step 17123, loss is 4.946765422821045\n",
      "(64, 33)\n",
      "step 17124, loss is 4.725381851196289\n",
      "(64, 33)\n",
      "step 17125, loss is 4.878468036651611\n",
      "(64, 33)\n",
      "step 17126, loss is 4.957320690155029\n",
      "(64, 33)\n",
      "step 17127, loss is 4.787712097167969\n",
      "(64, 33)\n",
      "step 17128, loss is 4.618113994598389\n",
      "(64, 33)\n",
      "step 17129, loss is 4.884549617767334\n",
      "(64, 33)\n",
      "step 17130, loss is 4.7201762199401855\n",
      "(64, 33)\n",
      "step 17131, loss is 4.81009578704834\n",
      "(64, 33)\n",
      "step 17132, loss is 4.852802276611328\n",
      "(64, 33)\n",
      "step 17133, loss is 4.651706695556641\n",
      "(64, 33)\n",
      "step 17134, loss is 4.87009334564209\n",
      "(64, 33)\n",
      "step 17135, loss is 4.813601016998291\n",
      "(64, 33)\n",
      "step 17136, loss is 4.902962684631348\n",
      "(64, 33)\n",
      "step 17137, loss is 4.5359206199646\n",
      "(64, 33)\n",
      "step 17138, loss is 4.806568622589111\n",
      "(64, 33)\n",
      "step 17139, loss is 4.980676651000977\n",
      "(64, 33)\n",
      "step 17140, loss is 5.042911529541016\n",
      "(64, 33)\n",
      "step 17141, loss is 4.690678119659424\n",
      "(64, 33)\n",
      "step 17142, loss is 4.634851455688477\n",
      "(64, 33)\n",
      "step 17143, loss is 4.5426025390625\n",
      "(64, 33)\n",
      "step 17144, loss is 4.7855916023254395\n",
      "(64, 33)\n",
      "step 17145, loss is 4.731719970703125\n",
      "(64, 33)\n",
      "step 17146, loss is 4.870606899261475\n",
      "(64, 33)\n",
      "step 17147, loss is 4.8616743087768555\n",
      "(64, 33)\n",
      "step 17148, loss is 4.72167444229126\n",
      "(64, 33)\n",
      "step 17149, loss is 4.753471851348877\n",
      "(64, 33)\n",
      "step 17150, loss is 4.6147661209106445\n",
      "(64, 33)\n",
      "step 17151, loss is 4.833414077758789\n",
      "(64, 33)\n",
      "step 17152, loss is 4.920074462890625\n",
      "(64, 33)\n",
      "step 17153, loss is 4.874456405639648\n",
      "(64, 33)\n",
      "step 17154, loss is 4.955817222595215\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17155, loss is 4.635875701904297\n",
      "(64, 33)\n",
      "step 17156, loss is 4.701871871948242\n",
      "(64, 33)\n",
      "step 17157, loss is 4.823968887329102\n",
      "(64, 33)\n",
      "step 17158, loss is 4.7213592529296875\n",
      "(64, 33)\n",
      "step 17159, loss is 4.802188873291016\n",
      "(64, 33)\n",
      "step 17160, loss is 4.827872276306152\n",
      "(64, 33)\n",
      "step 17161, loss is 4.787630558013916\n",
      "(64, 33)\n",
      "step 17162, loss is 4.916709899902344\n",
      "(64, 33)\n",
      "step 17163, loss is 4.7432475090026855\n",
      "(64, 33)\n",
      "step 17164, loss is 4.938522815704346\n",
      "(64, 33)\n",
      "step 17165, loss is 4.76816987991333\n",
      "(64, 33)\n",
      "step 17166, loss is 4.872561454772949\n",
      "(64, 33)\n",
      "step 17167, loss is 4.775507926940918\n",
      "(64, 33)\n",
      "step 17168, loss is 5.032654285430908\n",
      "(64, 33)\n",
      "step 17169, loss is 4.726086616516113\n",
      "(64, 33)\n",
      "step 17170, loss is 5.018365859985352\n",
      "(64, 33)\n",
      "step 17171, loss is 4.7297892570495605\n",
      "(64, 33)\n",
      "step 17172, loss is 4.712316513061523\n",
      "(64, 33)\n",
      "step 17173, loss is 4.804470062255859\n",
      "(64, 33)\n",
      "step 17174, loss is 4.636589050292969\n",
      "(64, 33)\n",
      "step 17175, loss is 5.030089378356934\n",
      "(64, 33)\n",
      "step 17176, loss is 4.878990173339844\n",
      "(64, 33)\n",
      "step 17177, loss is 4.737251281738281\n",
      "(64, 33)\n",
      "step 17178, loss is 4.823121070861816\n",
      "(64, 33)\n",
      "step 17179, loss is 4.808892726898193\n",
      "(64, 33)\n",
      "step 17180, loss is 4.754443645477295\n",
      "(64, 33)\n",
      "step 17181, loss is 4.800982475280762\n",
      "(64, 33)\n",
      "step 17182, loss is 4.763489246368408\n",
      "(64, 33)\n",
      "step 17183, loss is 4.6966633796691895\n",
      "(64, 33)\n",
      "step 17184, loss is 4.823915481567383\n",
      "(64, 33)\n",
      "step 17185, loss is 4.895545959472656\n",
      "(64, 33)\n",
      "step 17186, loss is 4.758607864379883\n",
      "(64, 33)\n",
      "step 17187, loss is 4.9769697189331055\n",
      "(64, 33)\n",
      "step 17188, loss is 4.721691608428955\n",
      "(64, 33)\n",
      "step 17189, loss is 4.876023769378662\n",
      "(64, 33)\n",
      "step 17190, loss is 4.751619338989258\n",
      "(64, 33)\n",
      "step 17191, loss is 4.515330791473389\n",
      "(64, 33)\n",
      "step 17192, loss is 4.785384178161621\n",
      "(64, 33)\n",
      "step 17193, loss is 4.773671627044678\n",
      "(64, 33)\n",
      "step 17194, loss is 4.740750789642334\n",
      "(64, 33)\n",
      "step 17195, loss is 4.751343250274658\n",
      "(64, 33)\n",
      "step 17196, loss is 4.893518924713135\n",
      "(64, 33)\n",
      "step 17197, loss is 4.881687641143799\n",
      "(64, 33)\n",
      "step 17198, loss is 4.697493076324463\n",
      "(64, 33)\n",
      "step 17199, loss is 4.832063674926758\n",
      "(64, 33)\n",
      "step 17200, loss is 4.7620086669921875\n",
      "(64, 33)\n",
      "step 17201, loss is 4.941422462463379\n",
      "(64, 33)\n",
      "step 17202, loss is 4.79238748550415\n",
      "(64, 33)\n",
      "step 17203, loss is 4.886693477630615\n",
      "(64, 33)\n",
      "step 17204, loss is 4.695903778076172\n",
      "(64, 33)\n",
      "step 17205, loss is 4.706839561462402\n",
      "(64, 33)\n",
      "step 17206, loss is 4.704833507537842\n",
      "(64, 33)\n",
      "step 17207, loss is 4.8975653648376465\n",
      "(64, 33)\n",
      "step 17208, loss is 4.764495849609375\n",
      "(64, 33)\n",
      "step 17209, loss is 4.8647332191467285\n",
      "(64, 33)\n",
      "step 17210, loss is 4.664732933044434\n",
      "(64, 33)\n",
      "step 17211, loss is 4.773102760314941\n",
      "(64, 33)\n",
      "step 17212, loss is 4.868147373199463\n",
      "(64, 33)\n",
      "step 17213, loss is 4.726531982421875\n",
      "(64, 33)\n",
      "step 17214, loss is 4.780158042907715\n",
      "(64, 33)\n",
      "step 17215, loss is 4.7570295333862305\n",
      "(64, 33)\n",
      "step 17216, loss is 4.897366523742676\n",
      "(64, 33)\n",
      "step 17217, loss is 4.722470283508301\n",
      "(64, 33)\n",
      "step 17218, loss is 4.774771213531494\n",
      "(64, 33)\n",
      "step 17219, loss is 4.948256969451904\n",
      "(64, 33)\n",
      "step 17220, loss is 4.870661735534668\n",
      "(64, 33)\n",
      "step 17221, loss is 4.723074913024902\n",
      "(64, 33)\n",
      "step 17222, loss is 4.8355255126953125\n",
      "(64, 33)\n",
      "step 17223, loss is 4.890495300292969\n",
      "(64, 33)\n",
      "step 17224, loss is 4.7371625900268555\n",
      "(64, 33)\n",
      "step 17225, loss is 4.731915473937988\n",
      "(64, 33)\n",
      "step 17226, loss is 4.985632419586182\n",
      "(64, 33)\n",
      "step 17227, loss is 4.892115116119385\n",
      "(64, 33)\n",
      "step 17228, loss is 4.8467631340026855\n",
      "(64, 33)\n",
      "step 17229, loss is 4.951992511749268\n",
      "(64, 33)\n",
      "step 17230, loss is 4.975205421447754\n",
      "(64, 33)\n",
      "step 17231, loss is 4.780308246612549\n",
      "(64, 33)\n",
      "step 17232, loss is 4.602843761444092\n",
      "(64, 33)\n",
      "step 17233, loss is 4.844142436981201\n",
      "(64, 33)\n",
      "step 17234, loss is 4.684344291687012\n",
      "(64, 33)\n",
      "step 17235, loss is 4.938316345214844\n",
      "(64, 33)\n",
      "step 17236, loss is 4.916870594024658\n",
      "(64, 33)\n",
      "step 17237, loss is 4.960384368896484\n",
      "(64, 33)\n",
      "step 17238, loss is 4.869869232177734\n",
      "(64, 33)\n",
      "step 17239, loss is 4.743178367614746\n",
      "(64, 33)\n",
      "step 17240, loss is 4.746694564819336\n",
      "(64, 33)\n",
      "step 17241, loss is 4.898243427276611\n",
      "(64, 33)\n",
      "step 17242, loss is 4.809417724609375\n",
      "(64, 33)\n",
      "step 17243, loss is 4.798015594482422\n",
      "(64, 33)\n",
      "step 17244, loss is 4.858062267303467\n",
      "(64, 33)\n",
      "step 17245, loss is 4.661728382110596\n",
      "(64, 33)\n",
      "step 17246, loss is 4.817892074584961\n",
      "(64, 33)\n",
      "step 17247, loss is 4.822890281677246\n",
      "(64, 33)\n",
      "step 17248, loss is 4.639191150665283\n",
      "(64, 33)\n",
      "step 17249, loss is 4.818105220794678\n",
      "(64, 33)\n",
      "step 17250, loss is 4.656928062438965\n",
      "(64, 33)\n",
      "step 17251, loss is 4.741694450378418\n",
      "(64, 33)\n",
      "step 17252, loss is 4.673480987548828\n",
      "(64, 33)\n",
      "step 17253, loss is 4.784117221832275\n",
      "(64, 33)\n",
      "step 17254, loss is 4.922413349151611\n",
      "(64, 33)\n",
      "step 17255, loss is 4.786905765533447\n",
      "(64, 33)\n",
      "step 17256, loss is 4.814571857452393\n",
      "(64, 33)\n",
      "step 17257, loss is 4.7292022705078125\n",
      "(64, 33)\n",
      "step 17258, loss is 4.625689506530762\n",
      "(64, 33)\n",
      "step 17259, loss is 4.745968818664551\n",
      "(64, 33)\n",
      "step 17260, loss is 4.7928466796875\n",
      "(64, 33)\n",
      "step 17261, loss is 4.89188814163208\n",
      "(64, 33)\n",
      "step 17262, loss is 4.74176549911499\n",
      "(64, 33)\n",
      "step 17263, loss is 4.751396179199219\n",
      "(64, 33)\n",
      "step 17264, loss is 4.885870456695557\n",
      "(64, 33)\n",
      "step 17265, loss is 4.726254463195801\n",
      "(64, 33)\n",
      "step 17266, loss is 4.8219828605651855\n",
      "(64, 33)\n",
      "step 17267, loss is 4.7473554611206055\n",
      "(64, 33)\n",
      "step 17268, loss is 4.7536845207214355\n",
      "(64, 33)\n",
      "step 17269, loss is 4.921653747558594\n",
      "(64, 33)\n",
      "step 17270, loss is 4.6503167152404785\n",
      "(64, 33)\n",
      "step 17271, loss is 4.690010070800781\n",
      "(64, 33)\n",
      "step 17272, loss is 4.776987552642822\n",
      "(64, 33)\n",
      "step 17273, loss is 4.833801746368408\n",
      "(64, 33)\n",
      "step 17274, loss is 4.87437105178833\n",
      "(64, 33)\n",
      "step 17275, loss is 4.827704906463623\n",
      "(64, 33)\n",
      "step 17276, loss is 4.850923538208008\n",
      "(64, 33)\n",
      "step 17277, loss is 4.626667499542236\n",
      "(64, 33)\n",
      "step 17278, loss is 4.804062366485596\n",
      "(64, 33)\n",
      "step 17279, loss is 4.918398380279541\n",
      "(64, 33)\n",
      "step 17280, loss is 4.954320907592773\n",
      "(64, 33)\n",
      "step 17281, loss is 4.803515434265137\n",
      "(64, 33)\n",
      "step 17282, loss is 4.771548748016357\n",
      "(64, 33)\n",
      "step 17283, loss is 4.559118270874023\n",
      "(64, 33)\n",
      "step 17284, loss is 4.70009708404541\n",
      "(64, 33)\n",
      "step 17285, loss is 4.712926864624023\n",
      "(64, 33)\n",
      "step 17286, loss is 4.855583190917969\n",
      "(64, 33)\n",
      "step 17287, loss is 4.912286758422852\n",
      "(64, 33)\n",
      "step 17288, loss is 4.7049560546875\n",
      "(64, 33)\n",
      "step 17289, loss is 4.83092737197876\n",
      "(64, 33)\n",
      "step 17290, loss is 4.7114949226379395\n",
      "(64, 33)\n",
      "step 17291, loss is 4.733939170837402\n",
      "(64, 33)\n",
      "step 17292, loss is 4.8657612800598145\n",
      "(64, 33)\n",
      "step 17293, loss is 4.996091842651367\n",
      "(64, 33)\n",
      "step 17294, loss is 4.6880388259887695\n",
      "(64, 33)\n",
      "step 17295, loss is 4.760969638824463\n",
      "(64, 33)\n",
      "step 17296, loss is 4.8045854568481445\n",
      "(64, 33)\n",
      "step 17297, loss is 4.965106010437012\n",
      "(64, 33)\n",
      "step 17298, loss is 4.735440254211426\n",
      "(64, 33)\n",
      "step 17299, loss is 4.71427583694458\n",
      "(64, 33)\n",
      "step 17300, loss is 4.812119007110596\n",
      "(64, 33)\n",
      "step 17301, loss is 4.985527038574219\n",
      "(64, 33)\n",
      "step 17302, loss is 4.788967132568359\n",
      "(64, 33)\n",
      "step 17303, loss is 4.893012046813965\n",
      "(64, 33)\n",
      "step 17304, loss is 4.9261860847473145\n",
      "(64, 33)\n",
      "step 17305, loss is 4.917368412017822\n",
      "(64, 33)\n",
      "step 17306, loss is 4.883595943450928\n",
      "(64, 33)\n",
      "step 17307, loss is 4.7599568367004395\n",
      "(64, 33)\n",
      "step 17308, loss is 4.596000671386719\n",
      "(64, 33)\n",
      "step 17309, loss is 4.704071998596191\n",
      "(64, 33)\n",
      "step 17310, loss is 4.723846435546875\n",
      "(64, 33)\n",
      "step 17311, loss is 4.8903422355651855\n",
      "(64, 33)\n",
      "step 17312, loss is 4.672988414764404\n",
      "(64, 33)\n",
      "step 17313, loss is 4.7272257804870605\n",
      "(64, 33)\n",
      "step 17314, loss is 4.8206963539123535\n",
      "(64, 33)\n",
      "step 17315, loss is 5.043752670288086\n",
      "(64, 33)\n",
      "step 17316, loss is 4.922972679138184\n",
      "(64, 33)\n",
      "step 17317, loss is 4.885103225708008\n",
      "(64, 33)\n",
      "step 17318, loss is 4.794633388519287\n",
      "(64, 33)\n",
      "step 17319, loss is 4.786581516265869\n",
      "(64, 33)\n",
      "step 17320, loss is 4.82758903503418\n",
      "(64, 33)\n",
      "step 17321, loss is 5.00704288482666\n",
      "(64, 33)\n",
      "step 17322, loss is 4.862309455871582\n",
      "(64, 33)\n",
      "step 17323, loss is 4.845061302185059\n",
      "(64, 33)\n",
      "step 17324, loss is 4.981193542480469\n",
      "(64, 33)\n",
      "step 17325, loss is 4.8060832023620605\n",
      "(64, 33)\n",
      "step 17326, loss is 4.825807094573975\n",
      "(64, 33)\n",
      "step 17327, loss is 4.846832275390625\n",
      "(64, 33)\n",
      "step 17328, loss is 4.921087741851807\n",
      "(64, 33)\n",
      "step 17329, loss is 4.945953845977783\n",
      "(64, 33)\n",
      "step 17330, loss is 4.737380027770996\n",
      "(64, 33)\n",
      "step 17331, loss is 4.833010196685791\n",
      "(64, 33)\n",
      "step 17332, loss is 4.766313552856445\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17333, loss is 4.840606212615967\n",
      "(64, 33)\n",
      "step 17334, loss is 4.909409523010254\n",
      "(64, 33)\n",
      "step 17335, loss is 4.788402080535889\n",
      "(64, 33)\n",
      "step 17336, loss is 4.906108379364014\n",
      "(64, 33)\n",
      "step 17337, loss is 4.823354244232178\n",
      "(64, 33)\n",
      "step 17338, loss is 4.84702205657959\n",
      "(64, 33)\n",
      "step 17339, loss is 4.934563159942627\n",
      "(64, 33)\n",
      "step 17340, loss is 4.863268852233887\n",
      "(64, 33)\n",
      "step 17341, loss is 4.920511722564697\n",
      "(64, 33)\n",
      "step 17342, loss is 4.8355326652526855\n",
      "(64, 33)\n",
      "step 17343, loss is 4.893922328948975\n",
      "(64, 33)\n",
      "step 17344, loss is 4.842380046844482\n",
      "(64, 33)\n",
      "step 17345, loss is 4.806127548217773\n",
      "(64, 33)\n",
      "step 17346, loss is 4.628510475158691\n",
      "(64, 33)\n",
      "step 17347, loss is 4.957523822784424\n",
      "(64, 33)\n",
      "step 17348, loss is 4.929508209228516\n",
      "(64, 33)\n",
      "step 17349, loss is 4.791990280151367\n",
      "(64, 33)\n",
      "step 17350, loss is 4.852782726287842\n",
      "(64, 33)\n",
      "step 17351, loss is 4.800217151641846\n",
      "(64, 33)\n",
      "step 17352, loss is 4.856166839599609\n",
      "(64, 33)\n",
      "step 17353, loss is 4.737553119659424\n",
      "(64, 33)\n",
      "step 17354, loss is 5.066615104675293\n",
      "(64, 33)\n",
      "step 17355, loss is 4.8428263664245605\n",
      "(64, 33)\n",
      "step 17356, loss is 4.801560878753662\n",
      "(64, 33)\n",
      "step 17357, loss is 4.848191738128662\n",
      "(64, 33)\n",
      "step 17358, loss is 4.974019527435303\n",
      "(64, 33)\n",
      "step 17359, loss is 4.661757469177246\n",
      "(64, 33)\n",
      "step 17360, loss is 4.865503311157227\n",
      "(64, 33)\n",
      "step 17361, loss is 4.751694202423096\n",
      "(64, 33)\n",
      "step 17362, loss is 4.813338756561279\n",
      "(64, 33)\n",
      "step 17363, loss is 5.017925262451172\n",
      "(64, 33)\n",
      "step 17364, loss is 4.838632106781006\n",
      "(64, 33)\n",
      "step 17365, loss is 4.626955986022949\n",
      "(64, 33)\n",
      "step 17366, loss is 4.781755447387695\n",
      "(64, 33)\n",
      "step 17367, loss is 4.999534606933594\n",
      "(64, 33)\n",
      "step 17368, loss is 4.763983726501465\n",
      "(64, 33)\n",
      "step 17369, loss is 4.761436462402344\n",
      "(64, 33)\n",
      "step 17370, loss is 4.727085113525391\n",
      "(64, 33)\n",
      "step 17371, loss is 4.810545444488525\n",
      "(64, 33)\n",
      "step 17372, loss is 4.902222633361816\n",
      "(64, 33)\n",
      "step 17373, loss is 4.833510875701904\n",
      "(64, 33)\n",
      "step 17374, loss is 4.672426223754883\n",
      "(64, 33)\n",
      "step 17375, loss is 4.804214954376221\n",
      "(64, 33)\n",
      "step 17376, loss is 4.9768805503845215\n",
      "(64, 33)\n",
      "step 17377, loss is 4.769642353057861\n",
      "(64, 33)\n",
      "step 17378, loss is 4.859884738922119\n",
      "(64, 33)\n",
      "step 17379, loss is 4.730352401733398\n",
      "(64, 33)\n",
      "step 17380, loss is 4.800056457519531\n",
      "(64, 33)\n",
      "step 17381, loss is 4.759742736816406\n",
      "(64, 33)\n",
      "step 17382, loss is 4.7361321449279785\n",
      "(64, 33)\n",
      "step 17383, loss is 4.796896457672119\n",
      "(64, 33)\n",
      "step 17384, loss is 4.4724812507629395\n",
      "(64, 33)\n",
      "step 17385, loss is 4.828061580657959\n",
      "(64, 33)\n",
      "step 17386, loss is 4.614325523376465\n",
      "(64, 33)\n",
      "step 17387, loss is 4.921439170837402\n",
      "(64, 33)\n",
      "step 17388, loss is 4.794155120849609\n",
      "(64, 33)\n",
      "step 17389, loss is 5.132667541503906\n",
      "(64, 33)\n",
      "step 17390, loss is 4.790107727050781\n",
      "(64, 33)\n",
      "step 17391, loss is 4.914722919464111\n",
      "(64, 33)\n",
      "step 17392, loss is 4.818480968475342\n",
      "(64, 33)\n",
      "step 17393, loss is 4.775144100189209\n",
      "(64, 33)\n",
      "step 17394, loss is 4.882388114929199\n",
      "(64, 33)\n",
      "step 17395, loss is 4.85156774520874\n",
      "(64, 33)\n",
      "step 17396, loss is 4.8837890625\n",
      "(64, 33)\n",
      "step 17397, loss is 4.522987365722656\n",
      "(64, 33)\n",
      "step 17398, loss is 4.809604644775391\n",
      "(64, 33)\n",
      "step 17399, loss is 4.888274669647217\n",
      "(64, 33)\n",
      "step 17400, loss is 4.692586421966553\n",
      "(64, 33)\n",
      "step 17401, loss is 4.8146281242370605\n",
      "(64, 33)\n",
      "step 17402, loss is 4.899599075317383\n",
      "(64, 33)\n",
      "step 17403, loss is 4.774194717407227\n",
      "(64, 33)\n",
      "step 17404, loss is 4.881944179534912\n",
      "(64, 33)\n",
      "step 17405, loss is 4.8237433433532715\n",
      "(64, 33)\n",
      "step 17406, loss is 4.815645217895508\n",
      "(64, 33)\n",
      "step 17407, loss is 4.93634033203125\n",
      "(64, 33)\n",
      "step 17408, loss is 4.90906286239624\n",
      "(64, 33)\n",
      "step 17409, loss is 4.880026817321777\n",
      "(64, 33)\n",
      "step 17410, loss is 4.887639999389648\n",
      "(64, 33)\n",
      "step 17411, loss is 4.7827301025390625\n",
      "(64, 33)\n",
      "step 17412, loss is 4.841846466064453\n",
      "(64, 33)\n",
      "step 17413, loss is 4.912905693054199\n",
      "(64, 33)\n",
      "step 17414, loss is 4.639623641967773\n",
      "(64, 33)\n",
      "step 17415, loss is 4.893001556396484\n",
      "(64, 33)\n",
      "step 17416, loss is 4.902097702026367\n",
      "(64, 33)\n",
      "step 17417, loss is 4.8940510749816895\n",
      "(64, 33)\n",
      "step 17418, loss is 5.026039123535156\n",
      "(64, 33)\n",
      "step 17419, loss is 4.626211166381836\n",
      "(64, 33)\n",
      "step 17420, loss is 4.7590155601501465\n",
      "(64, 33)\n",
      "step 17421, loss is 4.75433874130249\n",
      "(64, 33)\n",
      "step 17422, loss is 4.749139308929443\n",
      "(64, 33)\n",
      "step 17423, loss is 4.807955265045166\n",
      "(64, 33)\n",
      "step 17424, loss is 5.008453369140625\n",
      "(64, 33)\n",
      "step 17425, loss is 4.6592020988464355\n",
      "(64, 33)\n",
      "step 17426, loss is 4.814594268798828\n",
      "(64, 33)\n",
      "step 17427, loss is 4.783163070678711\n",
      "(64, 33)\n",
      "step 17428, loss is 4.69827938079834\n",
      "(64, 33)\n",
      "step 17429, loss is 4.716013431549072\n",
      "(64, 33)\n",
      "step 17430, loss is 4.82957124710083\n",
      "(64, 33)\n",
      "step 17431, loss is 4.972101211547852\n",
      "(64, 33)\n",
      "step 17432, loss is 4.807278156280518\n",
      "(64, 33)\n",
      "step 17433, loss is 4.649748802185059\n",
      "(64, 33)\n",
      "step 17434, loss is 4.869351387023926\n",
      "(64, 33)\n",
      "step 17435, loss is 4.564384460449219\n",
      "(64, 33)\n",
      "step 17436, loss is 4.856173515319824\n",
      "(64, 33)\n",
      "step 17437, loss is 4.747263431549072\n",
      "(64, 33)\n",
      "step 17438, loss is 4.765342712402344\n",
      "(64, 33)\n",
      "step 17439, loss is 4.568253040313721\n",
      "(64, 33)\n",
      "step 17440, loss is 4.739811897277832\n",
      "(64, 33)\n",
      "step 17441, loss is 4.784553527832031\n",
      "(64, 33)\n",
      "step 17442, loss is 4.840303421020508\n",
      "(64, 33)\n",
      "step 17443, loss is 4.631289005279541\n",
      "(64, 33)\n",
      "step 17444, loss is 4.850066184997559\n",
      "(64, 33)\n",
      "step 17445, loss is 4.78180456161499\n",
      "(64, 33)\n",
      "step 17446, loss is 4.725554466247559\n",
      "(64, 33)\n",
      "step 17447, loss is 4.829970359802246\n",
      "(64, 33)\n",
      "step 17448, loss is 4.830533981323242\n",
      "(64, 33)\n",
      "step 17449, loss is 4.649733543395996\n",
      "(64, 33)\n",
      "step 17450, loss is 4.770271301269531\n",
      "(64, 33)\n",
      "step 17451, loss is 4.714669704437256\n",
      "(64, 33)\n",
      "step 17452, loss is 4.897572040557861\n",
      "(64, 33)\n",
      "step 17453, loss is 4.814236164093018\n",
      "(64, 33)\n",
      "step 17454, loss is 4.875375270843506\n",
      "(64, 33)\n",
      "step 17455, loss is 4.747687816619873\n",
      "(64, 33)\n",
      "step 17456, loss is 4.624980449676514\n",
      "(64, 33)\n",
      "step 17457, loss is 4.958138942718506\n",
      "(64, 33)\n",
      "step 17458, loss is 4.886865615844727\n",
      "(64, 33)\n",
      "step 17459, loss is 4.659205436706543\n",
      "(64, 33)\n",
      "step 17460, loss is 4.724311828613281\n",
      "(64, 33)\n",
      "step 17461, loss is 4.893506050109863\n",
      "(64, 33)\n",
      "step 17462, loss is 4.917391300201416\n",
      "(64, 33)\n",
      "step 17463, loss is 4.850326061248779\n",
      "(64, 33)\n",
      "step 17464, loss is 4.802449703216553\n",
      "(64, 33)\n",
      "step 17465, loss is 4.775743007659912\n",
      "(64, 33)\n",
      "step 17466, loss is 4.645979404449463\n",
      "(64, 33)\n",
      "step 17467, loss is 4.892910003662109\n",
      "(64, 33)\n",
      "step 17468, loss is 4.853011608123779\n",
      "(64, 33)\n",
      "step 17469, loss is 4.844249248504639\n",
      "(64, 33)\n",
      "step 17470, loss is 4.986577987670898\n",
      "(64, 33)\n",
      "step 17471, loss is 4.7287211418151855\n",
      "(64, 33)\n",
      "step 17472, loss is 4.809128761291504\n",
      "(64, 33)\n",
      "step 17473, loss is 4.796588897705078\n",
      "(64, 33)\n",
      "step 17474, loss is 4.715451717376709\n",
      "(64, 33)\n",
      "step 17475, loss is 4.789976119995117\n",
      "(64, 33)\n",
      "step 17476, loss is 4.752025604248047\n",
      "(64, 33)\n",
      "step 17477, loss is 4.691598415374756\n",
      "(64, 33)\n",
      "step 17478, loss is 4.745502471923828\n",
      "(64, 33)\n",
      "step 17479, loss is 4.993832588195801\n",
      "(64, 33)\n",
      "step 17480, loss is 4.7868242263793945\n",
      "(64, 33)\n",
      "step 17481, loss is 4.729335784912109\n",
      "(64, 33)\n",
      "step 17482, loss is 4.784091472625732\n",
      "(64, 33)\n",
      "step 17483, loss is 4.8689117431640625\n",
      "(64, 33)\n",
      "step 17484, loss is 4.838984966278076\n",
      "(64, 33)\n",
      "step 17485, loss is 4.920103073120117\n",
      "(64, 33)\n",
      "step 17486, loss is 4.625973224639893\n",
      "(64, 33)\n",
      "step 17487, loss is 4.729286193847656\n",
      "(64, 33)\n",
      "step 17488, loss is 4.96921443939209\n",
      "(64, 33)\n",
      "step 17489, loss is 4.839583873748779\n",
      "(64, 33)\n",
      "step 17490, loss is 4.875608921051025\n",
      "(64, 33)\n",
      "step 17491, loss is 4.960842132568359\n",
      "(64, 33)\n",
      "step 17492, loss is 4.714654445648193\n",
      "(64, 33)\n",
      "step 17493, loss is 4.730570316314697\n",
      "(64, 33)\n",
      "step 17494, loss is 4.980953216552734\n",
      "(64, 33)\n",
      "step 17495, loss is 5.006283760070801\n",
      "(64, 33)\n",
      "step 17496, loss is 4.768401145935059\n",
      "(64, 33)\n",
      "step 17497, loss is 4.730534553527832\n",
      "(64, 33)\n",
      "step 17498, loss is 4.824721336364746\n",
      "(64, 33)\n",
      "step 17499, loss is 4.681508541107178\n",
      "(64, 33)\n",
      "step 17500, loss is 5.063092231750488\n",
      "(64, 33)\n",
      "step 17501, loss is 4.8483171463012695\n",
      "(64, 33)\n",
      "step 17502, loss is 4.946351528167725\n",
      "(64, 33)\n",
      "step 17503, loss is 4.566236972808838\n",
      "(64, 33)\n",
      "step 17504, loss is 4.801456928253174\n",
      "(64, 33)\n",
      "step 17505, loss is 4.707355976104736\n",
      "(64, 33)\n",
      "step 17506, loss is 4.8847737312316895\n",
      "(64, 33)\n",
      "step 17507, loss is 4.712282180786133\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17508, loss is 4.6964030265808105\n",
      "(64, 33)\n",
      "step 17509, loss is 4.6578521728515625\n",
      "(64, 33)\n",
      "step 17510, loss is 4.922303199768066\n",
      "(64, 33)\n",
      "step 17511, loss is 5.008904457092285\n",
      "(64, 33)\n",
      "step 17512, loss is 4.720196723937988\n",
      "(64, 33)\n",
      "step 17513, loss is 4.714376926422119\n",
      "(64, 33)\n",
      "step 17514, loss is 4.76830530166626\n",
      "(64, 33)\n",
      "step 17515, loss is 4.765994548797607\n",
      "(64, 33)\n",
      "step 17516, loss is 4.917459487915039\n",
      "(64, 33)\n",
      "step 17517, loss is 4.736778736114502\n",
      "(64, 33)\n",
      "step 17518, loss is 4.812400817871094\n",
      "(64, 33)\n",
      "step 17519, loss is 4.83772087097168\n",
      "(64, 33)\n",
      "step 17520, loss is 4.766552448272705\n",
      "(64, 33)\n",
      "step 17521, loss is 5.019453525543213\n",
      "(64, 33)\n",
      "step 17522, loss is 4.669309139251709\n",
      "(64, 33)\n",
      "step 17523, loss is 4.876344680786133\n",
      "(64, 33)\n",
      "step 17524, loss is 4.668226718902588\n",
      "(64, 33)\n",
      "step 17525, loss is 4.767673969268799\n",
      "(64, 33)\n",
      "step 17526, loss is 4.725459575653076\n",
      "(64, 33)\n",
      "step 17527, loss is 4.786928176879883\n",
      "(64, 33)\n",
      "step 17528, loss is 4.76668643951416\n",
      "(64, 33)\n",
      "step 17529, loss is 4.758648872375488\n",
      "(64, 33)\n",
      "step 17530, loss is 4.893376350402832\n",
      "(64, 33)\n",
      "step 17531, loss is 4.595550537109375\n",
      "(64, 33)\n",
      "step 17532, loss is 4.896730422973633\n",
      "(64, 33)\n",
      "step 17533, loss is 4.620987892150879\n",
      "(64, 33)\n",
      "step 17534, loss is 4.595476150512695\n",
      "(64, 33)\n",
      "step 17535, loss is 4.8512864112854\n",
      "(64, 33)\n",
      "step 17536, loss is 4.765589714050293\n",
      "(64, 33)\n",
      "step 17537, loss is 4.79649543762207\n",
      "(64, 33)\n",
      "step 17538, loss is 4.704315662384033\n",
      "(64, 33)\n",
      "step 17539, loss is 4.896539211273193\n",
      "(64, 33)\n",
      "step 17540, loss is 4.834085941314697\n",
      "(64, 33)\n",
      "step 17541, loss is 4.891067028045654\n",
      "(64, 33)\n",
      "step 17542, loss is 4.781193256378174\n",
      "(64, 33)\n",
      "step 17543, loss is 4.583209991455078\n",
      "(64, 33)\n",
      "step 17544, loss is 4.583868980407715\n",
      "(64, 33)\n",
      "step 17545, loss is 4.957653522491455\n",
      "(64, 33)\n",
      "step 17546, loss is 4.9139933586120605\n",
      "(64, 33)\n",
      "step 17547, loss is 4.9137864112854\n",
      "(64, 33)\n",
      "step 17548, loss is 4.773100852966309\n",
      "(64, 33)\n",
      "step 17549, loss is 4.666731834411621\n",
      "(64, 33)\n",
      "step 17550, loss is 4.6791157722473145\n",
      "(64, 33)\n",
      "step 17551, loss is 4.75364875793457\n",
      "(64, 33)\n",
      "step 17552, loss is 4.798129081726074\n",
      "(64, 33)\n",
      "step 17553, loss is 4.7642822265625\n",
      "(64, 33)\n",
      "step 17554, loss is 4.894133567810059\n",
      "(64, 33)\n",
      "step 17555, loss is 4.9061198234558105\n",
      "(64, 33)\n",
      "step 17556, loss is 4.588831901550293\n",
      "(64, 33)\n",
      "step 17557, loss is 4.857730865478516\n",
      "(64, 33)\n",
      "step 17558, loss is 4.889951705932617\n",
      "(64, 33)\n",
      "step 17559, loss is 4.627360820770264\n",
      "(64, 33)\n",
      "step 17560, loss is 4.975306987762451\n",
      "(64, 33)\n",
      "step 17561, loss is 4.766118049621582\n",
      "(64, 33)\n",
      "step 17562, loss is 4.705745220184326\n",
      "(64, 33)\n",
      "step 17563, loss is 4.869623184204102\n",
      "(64, 33)\n",
      "step 17564, loss is 5.061868190765381\n",
      "(64, 33)\n",
      "step 17565, loss is 4.745482921600342\n",
      "(64, 33)\n",
      "step 17566, loss is 4.901764869689941\n",
      "(64, 33)\n",
      "step 17567, loss is 4.819432258605957\n",
      "(64, 33)\n",
      "step 17568, loss is 4.843220233917236\n",
      "(64, 33)\n",
      "step 17569, loss is 4.7803850173950195\n",
      "(64, 33)\n",
      "step 17570, loss is 4.837181091308594\n",
      "(64, 33)\n",
      "step 17571, loss is 4.721994876861572\n",
      "(64, 33)\n",
      "step 17572, loss is 4.696796894073486\n",
      "(64, 33)\n",
      "step 17573, loss is 4.843964099884033\n",
      "(64, 33)\n",
      "step 17574, loss is 4.988894462585449\n",
      "(64, 33)\n",
      "step 17575, loss is 4.7040629386901855\n",
      "(64, 33)\n",
      "step 17576, loss is 4.58563756942749\n",
      "(64, 33)\n",
      "step 17577, loss is 4.770064353942871\n",
      "(64, 33)\n",
      "step 17578, loss is 4.696788787841797\n",
      "(64, 33)\n",
      "step 17579, loss is 4.764733791351318\n",
      "(64, 33)\n",
      "step 17580, loss is 4.868654251098633\n",
      "(64, 33)\n",
      "step 17581, loss is 4.874025344848633\n",
      "(64, 33)\n",
      "step 17582, loss is 4.931019306182861\n",
      "(64, 33)\n",
      "step 17583, loss is 4.689345836639404\n",
      "(64, 33)\n",
      "step 17584, loss is 4.867072105407715\n",
      "(64, 33)\n",
      "step 17585, loss is 4.6906538009643555\n",
      "(64, 33)\n",
      "step 17586, loss is 4.743876934051514\n",
      "(64, 33)\n",
      "step 17587, loss is 4.937320709228516\n",
      "(64, 33)\n",
      "step 17588, loss is 4.747584819793701\n",
      "(64, 33)\n",
      "step 17589, loss is 4.877125263214111\n",
      "(64, 33)\n",
      "step 17590, loss is 4.905442237854004\n",
      "(64, 33)\n",
      "step 17591, loss is 4.7567009925842285\n",
      "(64, 33)\n",
      "step 17592, loss is 4.779852390289307\n",
      "(64, 33)\n",
      "step 17593, loss is 4.818454742431641\n",
      "(64, 33)\n",
      "step 17594, loss is 4.701786994934082\n",
      "(64, 33)\n",
      "step 17595, loss is 4.696493625640869\n",
      "(64, 33)\n",
      "step 17596, loss is 4.910359859466553\n",
      "(64, 33)\n",
      "step 17597, loss is 4.734830856323242\n",
      "(64, 33)\n",
      "step 17598, loss is 4.89798641204834\n",
      "(64, 33)\n",
      "step 17599, loss is 4.79192590713501\n",
      "(64, 33)\n",
      "step 17600, loss is 4.691076278686523\n",
      "(64, 33)\n",
      "step 17601, loss is 4.7421064376831055\n",
      "(64, 33)\n",
      "step 17602, loss is 4.852506160736084\n",
      "(64, 33)\n",
      "step 17603, loss is 4.737757682800293\n",
      "(64, 33)\n",
      "step 17604, loss is 4.588093280792236\n",
      "(64, 33)\n",
      "step 17605, loss is 4.744024276733398\n",
      "(64, 33)\n",
      "step 17606, loss is 4.893532752990723\n",
      "(64, 33)\n",
      "step 17607, loss is 4.645323753356934\n",
      "(64, 33)\n",
      "step 17608, loss is 4.781798839569092\n",
      "(64, 33)\n",
      "step 17609, loss is 4.618436813354492\n",
      "(64, 33)\n",
      "step 17610, loss is 4.758747577667236\n",
      "(64, 33)\n",
      "step 17611, loss is 4.791233062744141\n",
      "(64, 33)\n",
      "step 17612, loss is 4.839899063110352\n",
      "(64, 33)\n",
      "step 17613, loss is 5.012590408325195\n",
      "(64, 33)\n",
      "step 17614, loss is 4.864793300628662\n",
      "(64, 33)\n",
      "step 17615, loss is 4.833298206329346\n",
      "(64, 33)\n",
      "step 17616, loss is 4.816347599029541\n",
      "(64, 33)\n",
      "step 17617, loss is 4.683530807495117\n",
      "(64, 33)\n",
      "step 17618, loss is 4.600465297698975\n",
      "(64, 33)\n",
      "step 17619, loss is 4.845464706420898\n",
      "(64, 33)\n",
      "step 17620, loss is 4.637709140777588\n",
      "(64, 33)\n",
      "step 17621, loss is 4.7668375968933105\n",
      "(64, 33)\n",
      "step 17622, loss is 4.769475936889648\n",
      "(64, 33)\n",
      "step 17623, loss is 4.659191608428955\n",
      "(64, 33)\n",
      "step 17624, loss is 4.669900894165039\n",
      "(64, 33)\n",
      "step 17625, loss is 4.77998685836792\n",
      "(64, 33)\n",
      "step 17626, loss is 4.844786643981934\n",
      "(64, 33)\n",
      "step 17627, loss is 4.893795013427734\n",
      "(64, 33)\n",
      "step 17628, loss is 4.84040641784668\n",
      "(64, 33)\n",
      "step 17629, loss is 4.644549369812012\n",
      "(64, 33)\n",
      "step 17630, loss is 4.718046188354492\n",
      "(64, 33)\n",
      "step 17631, loss is 4.655294418334961\n",
      "(64, 33)\n",
      "step 17632, loss is 4.902672290802002\n",
      "(64, 33)\n",
      "step 17633, loss is 4.8134284019470215\n",
      "(64, 33)\n",
      "step 17634, loss is 4.738336563110352\n",
      "(64, 33)\n",
      "step 17635, loss is 4.8146538734436035\n",
      "(64, 33)\n",
      "step 17636, loss is 4.764901638031006\n",
      "(64, 33)\n",
      "step 17637, loss is 4.687084674835205\n",
      "(64, 33)\n",
      "step 17638, loss is 4.797052383422852\n",
      "(64, 33)\n",
      "step 17639, loss is 4.7170939445495605\n",
      "(64, 33)\n",
      "step 17640, loss is 4.648046493530273\n",
      "(64, 33)\n",
      "step 17641, loss is 4.839384078979492\n",
      "(64, 33)\n",
      "step 17642, loss is 4.7879743576049805\n",
      "(64, 33)\n",
      "step 17643, loss is 4.884021282196045\n",
      "(64, 33)\n",
      "step 17644, loss is 4.920126438140869\n",
      "(64, 33)\n",
      "step 17645, loss is 4.69307804107666\n",
      "(64, 33)\n",
      "step 17646, loss is 4.818415641784668\n",
      "(64, 33)\n",
      "step 17647, loss is 4.837331295013428\n",
      "(64, 33)\n",
      "step 17648, loss is 4.918739318847656\n",
      "(64, 33)\n",
      "step 17649, loss is 4.866402626037598\n",
      "(64, 33)\n",
      "step 17650, loss is 4.83547306060791\n",
      "(64, 33)\n",
      "step 17651, loss is 4.729682922363281\n",
      "(64, 33)\n",
      "step 17652, loss is 5.0092315673828125\n",
      "(64, 33)\n",
      "step 17653, loss is 4.703848838806152\n",
      "(64, 33)\n",
      "step 17654, loss is 4.85529088973999\n",
      "(64, 33)\n",
      "step 17655, loss is 4.794747829437256\n",
      "(64, 33)\n",
      "step 17656, loss is 4.881884574890137\n",
      "(64, 33)\n",
      "step 17657, loss is 4.73190975189209\n",
      "(64, 33)\n",
      "step 17658, loss is 4.832320690155029\n",
      "(64, 33)\n",
      "step 17659, loss is 4.7669219970703125\n",
      "(64, 33)\n",
      "step 17660, loss is 4.858288288116455\n",
      "(64, 33)\n",
      "step 17661, loss is 4.786443710327148\n",
      "(64, 33)\n",
      "step 17662, loss is 4.6861066818237305\n",
      "(64, 33)\n",
      "step 17663, loss is 4.624290466308594\n",
      "(64, 33)\n",
      "step 17664, loss is 4.758921146392822\n",
      "(64, 33)\n",
      "step 17665, loss is 4.666183948516846\n",
      "(64, 33)\n",
      "step 17666, loss is 4.765857696533203\n",
      "(64, 33)\n",
      "step 17667, loss is 4.734307765960693\n",
      "(64, 33)\n",
      "step 17668, loss is 4.803244113922119\n",
      "(64, 33)\n",
      "step 17669, loss is 4.930908679962158\n",
      "(64, 33)\n",
      "step 17670, loss is 4.985954284667969\n",
      "(64, 33)\n",
      "step 17671, loss is 4.5895819664001465\n",
      "(64, 33)\n",
      "step 17672, loss is 4.788947105407715\n",
      "(64, 33)\n",
      "step 17673, loss is 4.665485858917236\n",
      "(64, 33)\n",
      "step 17674, loss is 4.695288181304932\n",
      "(64, 33)\n",
      "step 17675, loss is 4.947610378265381\n",
      "(64, 33)\n",
      "step 17676, loss is 4.542703151702881\n",
      "(64, 33)\n",
      "step 17677, loss is 4.627276420593262\n",
      "(64, 33)\n",
      "step 17678, loss is 4.9198222160339355\n",
      "(64, 33)\n",
      "step 17679, loss is 4.618408679962158\n",
      "(64, 33)\n",
      "step 17680, loss is 4.741520881652832\n",
      "(64, 33)\n",
      "step 17681, loss is 4.719154357910156\n",
      "(64, 33)\n",
      "step 17682, loss is 4.678324222564697\n",
      "(64, 33)\n",
      "step 17683, loss is 4.81809663772583\n",
      "(64, 33)\n",
      "step 17684, loss is 4.6987690925598145\n",
      "(64, 33)\n",
      "step 17685, loss is 4.643735408782959\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17686, loss is 4.880177021026611\n",
      "(64, 33)\n",
      "step 17687, loss is 4.802258014678955\n",
      "(64, 33)\n",
      "step 17688, loss is 4.891697883605957\n",
      "(64, 33)\n",
      "step 17689, loss is 4.907521724700928\n",
      "(64, 33)\n",
      "step 17690, loss is 4.817600250244141\n",
      "(64, 33)\n",
      "step 17691, loss is 4.761157512664795\n",
      "(64, 33)\n",
      "step 17692, loss is 4.882418632507324\n",
      "(64, 33)\n",
      "step 17693, loss is 4.948738098144531\n",
      "(64, 33)\n",
      "step 17694, loss is 4.691746711730957\n",
      "(64, 33)\n",
      "step 17695, loss is 4.654141426086426\n",
      "(64, 33)\n",
      "step 17696, loss is 4.775101184844971\n",
      "(64, 33)\n",
      "step 17697, loss is 4.942122459411621\n",
      "(64, 33)\n",
      "step 17698, loss is 4.702969551086426\n",
      "(64, 33)\n",
      "step 17699, loss is 4.7438578605651855\n",
      "(64, 33)\n",
      "step 17700, loss is 4.811337471008301\n",
      "(64, 33)\n",
      "step 17701, loss is 4.815793991088867\n",
      "(64, 33)\n",
      "step 17702, loss is 4.86024808883667\n",
      "(64, 33)\n",
      "step 17703, loss is 4.718266487121582\n",
      "(64, 33)\n",
      "step 17704, loss is 4.771953105926514\n",
      "(64, 33)\n",
      "step 17705, loss is 4.8587870597839355\n",
      "(64, 33)\n",
      "step 17706, loss is 4.880253791809082\n",
      "(64, 33)\n",
      "step 17707, loss is 4.616067886352539\n",
      "(64, 33)\n",
      "step 17708, loss is 4.835546970367432\n",
      "(64, 33)\n",
      "step 17709, loss is 4.935337066650391\n",
      "(64, 33)\n",
      "step 17710, loss is 4.881250381469727\n",
      "(64, 33)\n",
      "step 17711, loss is 4.790795803070068\n",
      "(64, 33)\n",
      "step 17712, loss is 4.790485858917236\n",
      "(64, 33)\n",
      "step 17713, loss is 4.789999961853027\n",
      "(64, 33)\n",
      "step 17714, loss is 4.721001148223877\n",
      "(64, 33)\n",
      "step 17715, loss is 4.75029182434082\n",
      "(64, 33)\n",
      "step 17716, loss is 4.841198921203613\n",
      "(64, 33)\n",
      "step 17717, loss is 4.871351718902588\n",
      "(64, 33)\n",
      "step 17718, loss is 4.578939914703369\n",
      "(64, 33)\n",
      "step 17719, loss is 4.704227447509766\n",
      "(64, 33)\n",
      "step 17720, loss is 4.942524433135986\n",
      "(64, 33)\n",
      "step 17721, loss is 4.851176738739014\n",
      "(64, 33)\n",
      "step 17722, loss is 4.611730575561523\n",
      "(64, 33)\n",
      "step 17723, loss is 4.863499164581299\n",
      "(64, 33)\n",
      "step 17724, loss is 4.841618537902832\n",
      "(64, 33)\n",
      "step 17725, loss is 4.700618743896484\n",
      "(64, 33)\n",
      "step 17726, loss is 4.890481472015381\n",
      "(64, 33)\n",
      "step 17727, loss is 4.692825794219971\n",
      "(64, 33)\n",
      "step 17728, loss is 4.68583345413208\n",
      "(64, 33)\n",
      "step 17729, loss is 4.666688442230225\n",
      "(64, 33)\n",
      "step 17730, loss is 4.828880310058594\n",
      "(64, 33)\n",
      "step 17731, loss is 4.713137149810791\n",
      "(64, 33)\n",
      "step 17732, loss is 4.84322452545166\n",
      "(64, 33)\n",
      "step 17733, loss is 4.756795406341553\n",
      "(64, 33)\n",
      "step 17734, loss is 4.879849433898926\n",
      "(64, 33)\n",
      "step 17735, loss is 4.811783313751221\n",
      "(64, 33)\n",
      "step 17736, loss is 4.817863464355469\n",
      "(64, 33)\n",
      "step 17737, loss is 4.81413459777832\n",
      "(64, 33)\n",
      "step 17738, loss is 4.636624336242676\n",
      "(64, 33)\n",
      "step 17739, loss is 4.744801998138428\n",
      "(64, 33)\n",
      "step 17740, loss is 4.672678470611572\n",
      "(64, 33)\n",
      "step 17741, loss is 4.8064045906066895\n",
      "(64, 33)\n",
      "step 17742, loss is 4.824892520904541\n",
      "(64, 33)\n",
      "step 17743, loss is 4.905885696411133\n",
      "(64, 33)\n",
      "step 17744, loss is 4.8040995597839355\n",
      "(64, 33)\n",
      "step 17745, loss is 4.804888725280762\n",
      "(64, 33)\n",
      "step 17746, loss is 4.732483863830566\n",
      "(64, 33)\n",
      "step 17747, loss is 4.887092113494873\n",
      "(64, 33)\n",
      "step 17748, loss is 4.794467449188232\n",
      "(64, 33)\n",
      "step 17749, loss is 4.699902534484863\n",
      "(64, 33)\n",
      "step 17750, loss is 4.638145923614502\n",
      "(64, 33)\n",
      "step 17751, loss is 4.881629943847656\n",
      "(64, 33)\n",
      "step 17752, loss is 4.87683629989624\n",
      "(64, 33)\n",
      "step 17753, loss is 4.800346374511719\n",
      "(64, 33)\n",
      "step 17754, loss is 4.712356090545654\n",
      "(64, 33)\n",
      "step 17755, loss is 4.804722309112549\n",
      "(64, 33)\n",
      "step 17756, loss is 4.768621921539307\n",
      "(64, 33)\n",
      "step 17757, loss is 4.592774868011475\n",
      "(64, 33)\n",
      "step 17758, loss is 4.894679546356201\n",
      "(64, 33)\n",
      "step 17759, loss is 4.776573181152344\n",
      "(64, 33)\n",
      "step 17760, loss is 4.803409576416016\n",
      "(64, 33)\n",
      "step 17761, loss is 4.776478290557861\n",
      "(64, 33)\n",
      "step 17762, loss is 4.696976661682129\n",
      "(64, 33)\n",
      "step 17763, loss is 4.94608736038208\n",
      "(64, 33)\n",
      "step 17764, loss is 4.900192737579346\n",
      "(64, 33)\n",
      "step 17765, loss is 4.736393451690674\n",
      "(64, 33)\n",
      "step 17766, loss is 4.734551906585693\n",
      "(64, 33)\n",
      "step 17767, loss is 4.8584370613098145\n",
      "(64, 33)\n",
      "step 17768, loss is 4.7236456871032715\n",
      "(64, 33)\n",
      "step 17769, loss is 4.816763877868652\n",
      "(64, 33)\n",
      "step 17770, loss is 4.962014198303223\n",
      "(64, 33)\n",
      "step 17771, loss is 4.80631160736084\n",
      "(64, 33)\n",
      "step 17772, loss is 4.746943950653076\n",
      "(64, 33)\n",
      "step 17773, loss is 4.800089359283447\n",
      "(64, 33)\n",
      "step 17774, loss is 4.792225360870361\n",
      "(64, 33)\n",
      "step 17775, loss is 4.780272960662842\n",
      "(64, 33)\n",
      "step 17776, loss is 4.9172821044921875\n",
      "(64, 33)\n",
      "step 17777, loss is 4.867390155792236\n",
      "(64, 33)\n",
      "step 17778, loss is 4.676731586456299\n",
      "(64, 33)\n",
      "step 17779, loss is 4.593471527099609\n",
      "(64, 33)\n",
      "step 17780, loss is 4.871821880340576\n",
      "(64, 33)\n",
      "step 17781, loss is 4.907303333282471\n",
      "(64, 33)\n",
      "step 17782, loss is 4.826986789703369\n",
      "(64, 33)\n",
      "step 17783, loss is 4.929123401641846\n",
      "(64, 33)\n",
      "step 17784, loss is 4.792444229125977\n",
      "(64, 33)\n",
      "step 17785, loss is 4.658891201019287\n",
      "(64, 33)\n",
      "step 17786, loss is 4.820666790008545\n",
      "(64, 33)\n",
      "step 17787, loss is 4.893281936645508\n",
      "(64, 33)\n",
      "step 17788, loss is 4.8299880027771\n",
      "(64, 33)\n",
      "step 17789, loss is 4.742912769317627\n",
      "(64, 33)\n",
      "step 17790, loss is 4.776689052581787\n",
      "(64, 33)\n",
      "step 17791, loss is 4.773159980773926\n",
      "(64, 33)\n",
      "step 17792, loss is 4.7679009437561035\n",
      "(64, 33)\n",
      "step 17793, loss is 4.8564229011535645\n",
      "(64, 33)\n",
      "step 17794, loss is 4.8865437507629395\n",
      "(64, 33)\n",
      "step 17795, loss is 4.739121437072754\n",
      "(64, 33)\n",
      "step 17796, loss is 4.812375545501709\n",
      "(64, 33)\n",
      "step 17797, loss is 4.716978073120117\n",
      "(64, 33)\n",
      "step 17798, loss is 4.679967880249023\n",
      "(64, 33)\n",
      "step 17799, loss is 4.798210620880127\n",
      "(64, 33)\n",
      "step 17800, loss is 4.697759628295898\n",
      "(64, 33)\n",
      "step 17801, loss is 4.732085704803467\n",
      "(64, 33)\n",
      "step 17802, loss is 4.720155239105225\n",
      "(64, 33)\n",
      "step 17803, loss is 4.8602776527404785\n",
      "(64, 33)\n",
      "step 17804, loss is 5.008488178253174\n",
      "(64, 33)\n",
      "step 17805, loss is 4.866909027099609\n",
      "(64, 33)\n",
      "step 17806, loss is 4.900705814361572\n",
      "(64, 33)\n",
      "step 17807, loss is 4.865163326263428\n",
      "(64, 33)\n",
      "step 17808, loss is 4.637603759765625\n",
      "(64, 33)\n",
      "step 17809, loss is 4.897747039794922\n",
      "(64, 33)\n",
      "step 17810, loss is 4.731916904449463\n",
      "(64, 33)\n",
      "step 17811, loss is 4.711115837097168\n",
      "(64, 33)\n",
      "step 17812, loss is 4.924984931945801\n",
      "(64, 33)\n",
      "step 17813, loss is 4.797698974609375\n",
      "(64, 33)\n",
      "step 17814, loss is 4.792873382568359\n",
      "(64, 33)\n",
      "step 17815, loss is 4.873470783233643\n",
      "(64, 33)\n",
      "step 17816, loss is 4.738157749176025\n",
      "(64, 33)\n",
      "step 17817, loss is 4.719956874847412\n",
      "(64, 33)\n",
      "step 17818, loss is 4.789817810058594\n",
      "(64, 33)\n",
      "step 17819, loss is 4.768104076385498\n",
      "(64, 33)\n",
      "step 17820, loss is 4.746437072753906\n",
      "(64, 33)\n",
      "step 17821, loss is 4.88946008682251\n",
      "(64, 33)\n",
      "step 17822, loss is 4.935626983642578\n",
      "(64, 33)\n",
      "step 17823, loss is 4.801466941833496\n",
      "(64, 33)\n",
      "step 17824, loss is 4.859776496887207\n",
      "(64, 33)\n",
      "step 17825, loss is 4.4963297843933105\n",
      "(64, 33)\n",
      "step 17826, loss is 4.727420330047607\n",
      "(64, 33)\n",
      "step 17827, loss is 4.675624847412109\n",
      "(64, 33)\n",
      "step 17828, loss is 4.744997978210449\n",
      "(64, 33)\n",
      "step 17829, loss is 4.919388294219971\n",
      "(64, 33)\n",
      "step 17830, loss is 4.898405075073242\n",
      "(64, 33)\n",
      "step 17831, loss is 4.943943977355957\n",
      "(64, 33)\n",
      "step 17832, loss is 4.824310779571533\n",
      "(64, 33)\n",
      "step 17833, loss is 4.789041042327881\n",
      "(64, 33)\n",
      "step 17834, loss is 4.80067777633667\n",
      "(64, 33)\n",
      "step 17835, loss is 4.912376880645752\n",
      "(64, 33)\n",
      "step 17836, loss is 4.6940202713012695\n",
      "(64, 33)\n",
      "step 17837, loss is 4.806927680969238\n",
      "(64, 33)\n",
      "step 17838, loss is 4.936787128448486\n",
      "(64, 33)\n",
      "step 17839, loss is 4.850205421447754\n",
      "(64, 33)\n",
      "step 17840, loss is 4.646486759185791\n",
      "(64, 33)\n",
      "step 17841, loss is 4.719537734985352\n",
      "(64, 33)\n",
      "step 17842, loss is 4.8301167488098145\n",
      "(64, 33)\n",
      "step 17843, loss is 4.691926002502441\n",
      "(64, 33)\n",
      "step 17844, loss is 4.771748065948486\n",
      "(64, 33)\n",
      "step 17845, loss is 4.646576881408691\n",
      "(64, 33)\n",
      "step 17846, loss is 4.806955337524414\n",
      "(64, 33)\n",
      "step 17847, loss is 4.685019493103027\n",
      "(64, 33)\n",
      "step 17848, loss is 4.802880764007568\n",
      "(64, 33)\n",
      "step 17849, loss is 4.670307159423828\n",
      "(64, 33)\n",
      "step 17850, loss is 4.724555969238281\n",
      "(64, 33)\n",
      "step 17851, loss is 4.670432090759277\n",
      "(64, 33)\n",
      "step 17852, loss is 4.814348220825195\n",
      "(64, 33)\n",
      "step 17853, loss is 4.909084796905518\n",
      "(64, 33)\n",
      "step 17854, loss is 4.793876647949219\n",
      "(64, 33)\n",
      "step 17855, loss is 4.733832359313965\n",
      "(64, 33)\n",
      "step 17856, loss is 4.673630237579346\n",
      "(64, 33)\n",
      "step 17857, loss is 4.88015079498291\n",
      "(64, 33)\n",
      "step 17858, loss is 4.719696998596191\n",
      "(64, 33)\n",
      "step 17859, loss is 4.76057767868042\n",
      "(64, 33)\n",
      "step 17860, loss is 4.628055095672607\n",
      "(64, 33)\n",
      "step 17861, loss is 4.915316104888916\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17862, loss is 4.934257984161377\n",
      "(64, 33)\n",
      "step 17863, loss is 4.966377258300781\n",
      "(64, 33)\n",
      "step 17864, loss is 4.747873783111572\n",
      "(64, 33)\n",
      "step 17865, loss is 4.934398174285889\n",
      "(64, 33)\n",
      "step 17866, loss is 4.6880106925964355\n",
      "(64, 33)\n",
      "step 17867, loss is 4.819815635681152\n",
      "(64, 33)\n",
      "step 17868, loss is 4.983137130737305\n",
      "(64, 33)\n",
      "step 17869, loss is 4.72677755355835\n",
      "(64, 33)\n",
      "step 17870, loss is 4.746949672698975\n",
      "(64, 33)\n",
      "step 17871, loss is 4.88712215423584\n",
      "(64, 33)\n",
      "step 17872, loss is 4.7132344245910645\n",
      "(64, 33)\n",
      "step 17873, loss is 4.835014343261719\n",
      "(64, 33)\n",
      "step 17874, loss is 4.67047119140625\n",
      "(64, 33)\n",
      "step 17875, loss is 4.7472991943359375\n",
      "(64, 33)\n",
      "step 17876, loss is 4.879732608795166\n",
      "(64, 33)\n",
      "step 17877, loss is 4.975331783294678\n",
      "(64, 33)\n",
      "step 17878, loss is 4.886769771575928\n",
      "(64, 33)\n",
      "step 17879, loss is 4.9303765296936035\n",
      "(64, 33)\n",
      "step 17880, loss is 4.868522644042969\n",
      "(64, 33)\n",
      "step 17881, loss is 4.855384349822998\n",
      "(64, 33)\n",
      "step 17882, loss is 4.957822322845459\n",
      "(64, 33)\n",
      "step 17883, loss is 4.801520824432373\n",
      "(64, 33)\n",
      "step 17884, loss is 4.749190330505371\n",
      "(64, 33)\n",
      "step 17885, loss is 4.764352321624756\n",
      "(64, 33)\n",
      "step 17886, loss is 4.789729595184326\n",
      "(64, 33)\n",
      "step 17887, loss is 4.891390800476074\n",
      "(64, 33)\n",
      "step 17888, loss is 4.80159854888916\n",
      "(64, 33)\n",
      "step 17889, loss is 4.878990173339844\n",
      "(64, 33)\n",
      "step 17890, loss is 4.822719573974609\n",
      "(64, 33)\n",
      "step 17891, loss is 4.804703235626221\n",
      "(64, 33)\n",
      "step 17892, loss is 4.866044044494629\n",
      "(64, 33)\n",
      "step 17893, loss is 4.905818939208984\n",
      "(64, 33)\n",
      "step 17894, loss is 4.6654887199401855\n",
      "(64, 33)\n",
      "step 17895, loss is 4.839123249053955\n",
      "(64, 33)\n",
      "step 17896, loss is 4.734982013702393\n",
      "(64, 33)\n",
      "step 17897, loss is 4.879275798797607\n",
      "(64, 33)\n",
      "step 17898, loss is 4.7215752601623535\n",
      "(64, 33)\n",
      "step 17899, loss is 4.79322624206543\n",
      "(64, 33)\n",
      "step 17900, loss is 4.695464134216309\n",
      "(64, 33)\n",
      "step 17901, loss is 4.806240081787109\n",
      "(64, 33)\n",
      "step 17902, loss is 4.644400596618652\n",
      "(64, 33)\n",
      "step 17903, loss is 4.912247180938721\n",
      "(64, 33)\n",
      "step 17904, loss is 4.7114458084106445\n",
      "(64, 33)\n",
      "step 17905, loss is 4.6269636154174805\n",
      "(64, 33)\n",
      "step 17906, loss is 4.881417751312256\n",
      "(64, 33)\n",
      "step 17907, loss is 4.566603183746338\n",
      "(64, 33)\n",
      "step 17908, loss is 4.633105278015137\n",
      "(64, 33)\n",
      "step 17909, loss is 4.7177653312683105\n",
      "(64, 33)\n",
      "step 17910, loss is 4.843776702880859\n",
      "(64, 33)\n",
      "step 17911, loss is 4.758899211883545\n",
      "(64, 33)\n",
      "step 17912, loss is 4.7359938621521\n",
      "(64, 33)\n",
      "step 17913, loss is 4.558005332946777\n",
      "(64, 33)\n",
      "step 17914, loss is 4.698245525360107\n",
      "(64, 33)\n",
      "step 17915, loss is 4.873208522796631\n",
      "(64, 33)\n",
      "step 17916, loss is 4.716982841491699\n",
      "(64, 33)\n",
      "step 17917, loss is 4.790234088897705\n",
      "(64, 33)\n",
      "step 17918, loss is 4.685505390167236\n",
      "(64, 33)\n",
      "step 17919, loss is 4.655179977416992\n",
      "(64, 33)\n",
      "step 17920, loss is 4.981016635894775\n",
      "(64, 33)\n",
      "step 17921, loss is 4.814045429229736\n",
      "(64, 33)\n",
      "step 17922, loss is 4.865307331085205\n",
      "(64, 33)\n",
      "step 17923, loss is 4.756083965301514\n",
      "(64, 33)\n",
      "step 17924, loss is 4.7297821044921875\n",
      "(64, 33)\n",
      "step 17925, loss is 4.842769622802734\n",
      "(64, 33)\n",
      "step 17926, loss is 4.856805801391602\n",
      "(64, 33)\n",
      "step 17927, loss is 4.850127696990967\n",
      "(64, 33)\n",
      "step 17928, loss is 4.762940406799316\n",
      "(64, 33)\n",
      "step 17929, loss is 4.765568733215332\n",
      "(64, 33)\n",
      "step 17930, loss is 4.771050453186035\n",
      "(64, 33)\n",
      "step 17931, loss is 4.84352445602417\n",
      "(64, 33)\n",
      "step 17932, loss is 4.906339168548584\n",
      "(64, 33)\n",
      "step 17933, loss is 4.840625286102295\n",
      "(64, 33)\n",
      "step 17934, loss is 4.737069129943848\n",
      "(64, 33)\n",
      "step 17935, loss is 4.839700222015381\n",
      "(64, 33)\n",
      "step 17936, loss is 4.943848609924316\n",
      "(64, 33)\n",
      "step 17937, loss is 4.995617389678955\n",
      "(64, 33)\n",
      "step 17938, loss is 4.82674503326416\n",
      "(64, 33)\n",
      "step 17939, loss is 4.839870452880859\n",
      "(64, 33)\n",
      "step 17940, loss is 4.732906341552734\n",
      "(64, 33)\n",
      "step 17941, loss is 4.748524188995361\n",
      "(64, 33)\n",
      "step 17942, loss is 4.792529582977295\n",
      "(64, 33)\n",
      "step 17943, loss is 4.798049449920654\n",
      "(64, 33)\n",
      "step 17944, loss is 4.953179359436035\n",
      "(64, 33)\n",
      "step 17945, loss is 4.691634178161621\n",
      "(64, 33)\n",
      "step 17946, loss is 4.964598178863525\n",
      "(64, 33)\n",
      "step 17947, loss is 4.685142993927002\n",
      "(64, 33)\n",
      "step 17948, loss is 4.871526718139648\n",
      "(64, 33)\n",
      "step 17949, loss is 4.646766662597656\n",
      "(64, 33)\n",
      "step 17950, loss is 4.69618034362793\n",
      "(64, 33)\n",
      "step 17951, loss is 4.776686668395996\n",
      "(64, 33)\n",
      "step 17952, loss is 4.772416114807129\n",
      "(64, 33)\n",
      "step 17953, loss is 4.929525852203369\n",
      "(64, 33)\n",
      "step 17954, loss is 4.86619758605957\n",
      "(64, 33)\n",
      "step 17955, loss is 4.942134857177734\n",
      "(64, 33)\n",
      "step 17956, loss is 4.72407865524292\n",
      "(64, 33)\n",
      "step 17957, loss is 4.8832621574401855\n",
      "(64, 33)\n",
      "step 17958, loss is 4.715386867523193\n",
      "(64, 33)\n",
      "step 17959, loss is 4.71802282333374\n",
      "(64, 33)\n",
      "step 17960, loss is 4.796412467956543\n",
      "(64, 33)\n",
      "step 17961, loss is 4.800814151763916\n",
      "(64, 33)\n",
      "step 17962, loss is 4.923478126525879\n",
      "(64, 33)\n",
      "step 17963, loss is 4.904431343078613\n",
      "(64, 33)\n",
      "step 17964, loss is 4.637335300445557\n",
      "(64, 33)\n",
      "step 17965, loss is 4.918716907501221\n",
      "(64, 33)\n",
      "step 17966, loss is 4.830512046813965\n",
      "(64, 33)\n",
      "step 17967, loss is 4.7930707931518555\n",
      "(64, 33)\n",
      "step 17968, loss is 4.716911792755127\n",
      "(64, 33)\n",
      "step 17969, loss is 4.922935962677002\n",
      "(64, 33)\n",
      "step 17970, loss is 4.929863452911377\n",
      "(64, 33)\n",
      "step 17971, loss is 4.952968597412109\n",
      "(64, 33)\n",
      "step 17972, loss is 4.768279075622559\n",
      "(64, 33)\n",
      "step 17973, loss is 4.685081481933594\n",
      "(64, 33)\n",
      "step 17974, loss is 4.734234809875488\n",
      "(64, 33)\n",
      "step 17975, loss is 4.824622631072998\n",
      "(64, 33)\n",
      "step 17976, loss is 4.995480060577393\n",
      "(64, 33)\n",
      "step 17977, loss is 4.632870674133301\n",
      "(64, 33)\n",
      "step 17978, loss is 4.848831653594971\n",
      "(64, 33)\n",
      "step 17979, loss is 4.863685131072998\n",
      "(64, 33)\n",
      "step 17980, loss is 4.868566989898682\n",
      "(64, 33)\n",
      "step 17981, loss is 4.646045684814453\n",
      "(64, 33)\n",
      "step 17982, loss is 4.952077865600586\n",
      "(64, 33)\n",
      "step 17983, loss is 5.026902198791504\n",
      "(64, 33)\n",
      "step 17984, loss is 4.9369378089904785\n",
      "(64, 33)\n",
      "step 17985, loss is 4.940749645233154\n",
      "(64, 33)\n",
      "step 17986, loss is 4.804933071136475\n",
      "(64, 33)\n",
      "step 17987, loss is 4.899094104766846\n",
      "(64, 33)\n",
      "step 17988, loss is 4.89236307144165\n",
      "(64, 33)\n",
      "step 17989, loss is 4.75573205947876\n",
      "(64, 33)\n",
      "step 17990, loss is 4.4531755447387695\n",
      "(64, 33)\n",
      "step 17991, loss is 4.790327548980713\n",
      "(64, 33)\n",
      "step 17992, loss is 4.790347576141357\n",
      "(64, 33)\n",
      "step 17993, loss is 4.822554588317871\n",
      "(64, 33)\n",
      "step 17994, loss is 4.604218006134033\n",
      "(64, 33)\n",
      "step 17995, loss is 4.73536491394043\n",
      "(64, 33)\n",
      "step 17996, loss is 4.808677673339844\n",
      "(64, 33)\n",
      "step 17997, loss is 4.9732537269592285\n",
      "(64, 33)\n",
      "step 17998, loss is 4.782617092132568\n",
      "(64, 33)\n",
      "step 17999, loss is 4.793856620788574\n",
      "(64, 33)\n",
      "step 18000, loss is 4.728819847106934\n",
      "(64, 33)\n",
      "step 18001, loss is 4.736002445220947\n",
      "(64, 33)\n",
      "step 18002, loss is 4.788921356201172\n",
      "(64, 33)\n",
      "step 18003, loss is 4.748785972595215\n",
      "(64, 33)\n",
      "step 18004, loss is 4.67519474029541\n",
      "(64, 33)\n",
      "step 18005, loss is 4.814550399780273\n",
      "(64, 33)\n",
      "step 18006, loss is 4.701351165771484\n",
      "(64, 33)\n",
      "step 18007, loss is 4.773922920227051\n",
      "(64, 33)\n",
      "step 18008, loss is 4.657660961151123\n",
      "(64, 33)\n",
      "step 18009, loss is 4.805294990539551\n",
      "(64, 33)\n",
      "step 18010, loss is 4.571244239807129\n",
      "(64, 33)\n",
      "step 18011, loss is 4.8213701248168945\n",
      "(64, 33)\n",
      "step 18012, loss is 4.704172134399414\n",
      "(64, 33)\n",
      "step 18013, loss is 4.96461820602417\n",
      "(64, 33)\n",
      "step 18014, loss is 4.654074192047119\n",
      "(64, 33)\n",
      "step 18015, loss is 4.6991753578186035\n",
      "(64, 33)\n",
      "step 18016, loss is 4.858845233917236\n",
      "(64, 33)\n",
      "step 18017, loss is 4.896505832672119\n",
      "(64, 33)\n",
      "step 18018, loss is 4.652282238006592\n",
      "(64, 33)\n",
      "step 18019, loss is 4.689651012420654\n",
      "(64, 33)\n",
      "step 18020, loss is 4.841386318206787\n",
      "(64, 33)\n",
      "step 18021, loss is 4.849766731262207\n",
      "(64, 33)\n",
      "step 18022, loss is 4.753166198730469\n",
      "(64, 33)\n",
      "step 18023, loss is 4.867156982421875\n",
      "(64, 33)\n",
      "step 18024, loss is 4.870882034301758\n",
      "(64, 33)\n",
      "step 18025, loss is 4.839154243469238\n",
      "(64, 33)\n",
      "step 18026, loss is 4.708582401275635\n",
      "(64, 33)\n",
      "step 18027, loss is 4.8206634521484375\n",
      "(64, 33)\n",
      "step 18028, loss is 4.858830451965332\n",
      "(64, 33)\n",
      "step 18029, loss is 4.7671284675598145\n",
      "(64, 33)\n",
      "step 18030, loss is 4.782113552093506\n",
      "(64, 33)\n",
      "step 18031, loss is 4.762397766113281\n",
      "(64, 33)\n",
      "step 18032, loss is 4.901653289794922\n",
      "(64, 33)\n",
      "step 18033, loss is 4.841705322265625\n",
      "(64, 33)\n",
      "step 18034, loss is 4.640512943267822\n",
      "(64, 33)\n",
      "step 18035, loss is 4.764955997467041\n",
      "(64, 33)\n",
      "step 18036, loss is 5.053434371948242\n",
      "(64, 33)\n",
      "step 18037, loss is 4.795854568481445\n",
      "(64, 33)\n",
      "step 18038, loss is 4.787546634674072\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18039, loss is 4.783022880554199\n",
      "(64, 33)\n",
      "step 18040, loss is 4.708784580230713\n",
      "(64, 33)\n",
      "step 18041, loss is 4.844632148742676\n",
      "(64, 33)\n",
      "step 18042, loss is 4.787424087524414\n",
      "(64, 33)\n",
      "step 18043, loss is 4.709835529327393\n",
      "(64, 33)\n",
      "step 18044, loss is 4.686125755310059\n",
      "(64, 33)\n",
      "step 18045, loss is 4.828993797302246\n",
      "(64, 33)\n",
      "step 18046, loss is 4.719096660614014\n",
      "(64, 33)\n",
      "step 18047, loss is 4.75996732711792\n",
      "(64, 33)\n",
      "step 18048, loss is 4.769473075866699\n",
      "(64, 33)\n",
      "step 18049, loss is 4.670798301696777\n",
      "(64, 33)\n",
      "step 18050, loss is 4.757988452911377\n",
      "(64, 33)\n",
      "step 18051, loss is 4.83163595199585\n",
      "(64, 33)\n",
      "step 18052, loss is 4.8788065910339355\n",
      "(64, 33)\n",
      "step 18053, loss is 4.827574729919434\n",
      "(64, 33)\n",
      "step 18054, loss is 4.693276405334473\n",
      "(64, 33)\n",
      "step 18055, loss is 4.781625270843506\n",
      "(64, 33)\n",
      "step 18056, loss is 4.804006576538086\n",
      "(64, 33)\n",
      "step 18057, loss is 4.828952789306641\n",
      "(64, 33)\n",
      "step 18058, loss is 4.684543609619141\n",
      "(64, 33)\n",
      "step 18059, loss is 4.876664638519287\n",
      "(64, 33)\n",
      "step 18060, loss is 4.7394022941589355\n",
      "(64, 33)\n",
      "step 18061, loss is 4.821106433868408\n",
      "(64, 33)\n",
      "step 18062, loss is 4.957031726837158\n",
      "(64, 33)\n",
      "step 18063, loss is 4.692694187164307\n",
      "(64, 33)\n",
      "step 18064, loss is 4.876515865325928\n",
      "(64, 33)\n",
      "step 18065, loss is 4.795050144195557\n",
      "(64, 33)\n",
      "step 18066, loss is 4.558762073516846\n",
      "(64, 33)\n",
      "step 18067, loss is 4.767411231994629\n",
      "(64, 33)\n",
      "step 18068, loss is 4.912783622741699\n",
      "(64, 33)\n",
      "step 18069, loss is 4.715173244476318\n",
      "(64, 33)\n",
      "step 18070, loss is 4.698535442352295\n",
      "(64, 33)\n",
      "step 18071, loss is 4.839316368103027\n",
      "(64, 33)\n",
      "step 18072, loss is 4.827753067016602\n",
      "(64, 33)\n",
      "step 18073, loss is 4.908491611480713\n",
      "(64, 33)\n",
      "step 18074, loss is 4.612555503845215\n",
      "(64, 33)\n",
      "step 18075, loss is 4.869685649871826\n",
      "(64, 33)\n",
      "step 18076, loss is 4.715113162994385\n",
      "(64, 33)\n",
      "step 18077, loss is 4.783134460449219\n",
      "(64, 33)\n",
      "step 18078, loss is 4.86989688873291\n",
      "(64, 33)\n",
      "step 18079, loss is 4.799373626708984\n",
      "(64, 33)\n",
      "step 18080, loss is 4.7969841957092285\n",
      "(64, 33)\n",
      "step 18081, loss is 4.769745349884033\n",
      "(64, 33)\n",
      "step 18082, loss is 4.570400238037109\n",
      "(64, 33)\n",
      "step 18083, loss is 4.711714744567871\n",
      "(64, 33)\n",
      "step 18084, loss is 4.84506368637085\n",
      "(64, 33)\n",
      "step 18085, loss is 4.857611179351807\n",
      "(64, 33)\n",
      "step 18086, loss is 4.732048034667969\n",
      "(64, 33)\n",
      "step 18087, loss is 4.720885753631592\n",
      "(64, 33)\n",
      "step 18088, loss is 4.797111988067627\n",
      "(64, 33)\n",
      "step 18089, loss is 4.875154972076416\n",
      "(64, 33)\n",
      "step 18090, loss is 4.799981594085693\n",
      "(64, 33)\n",
      "step 18091, loss is 4.674075603485107\n",
      "(64, 33)\n",
      "step 18092, loss is 4.763896465301514\n",
      "(64, 33)\n",
      "step 18093, loss is 4.847707748413086\n",
      "(64, 33)\n",
      "step 18094, loss is 4.730415344238281\n",
      "(64, 33)\n",
      "step 18095, loss is 4.8336873054504395\n",
      "(64, 33)\n",
      "step 18096, loss is 4.620027542114258\n",
      "(64, 33)\n",
      "step 18097, loss is 4.767452239990234\n",
      "(64, 33)\n",
      "step 18098, loss is 4.796046257019043\n",
      "(64, 33)\n",
      "step 18099, loss is 4.89962100982666\n",
      "(64, 33)\n",
      "step 18100, loss is 4.746455669403076\n",
      "(64, 33)\n",
      "step 18101, loss is 4.8336663246154785\n",
      "(64, 33)\n",
      "step 18102, loss is 4.934260368347168\n",
      "(64, 33)\n",
      "step 18103, loss is 4.997375011444092\n",
      "(64, 33)\n",
      "step 18104, loss is 4.887543678283691\n",
      "(64, 33)\n",
      "step 18105, loss is 4.783498764038086\n",
      "(64, 33)\n",
      "step 18106, loss is 4.757309436798096\n",
      "(64, 33)\n",
      "step 18107, loss is 4.917634963989258\n",
      "(64, 33)\n",
      "step 18108, loss is 4.802995204925537\n",
      "(64, 33)\n",
      "step 18109, loss is 4.603847503662109\n",
      "(64, 33)\n",
      "step 18110, loss is 4.870966911315918\n",
      "(64, 33)\n",
      "step 18111, loss is 4.8965277671813965\n",
      "(64, 33)\n",
      "step 18112, loss is 4.571638107299805\n",
      "(64, 33)\n",
      "step 18113, loss is 4.963071346282959\n",
      "(64, 33)\n",
      "step 18114, loss is 4.783766269683838\n",
      "(64, 33)\n",
      "step 18115, loss is 5.050703048706055\n",
      "(64, 33)\n",
      "step 18116, loss is 4.617434978485107\n",
      "(64, 33)\n",
      "step 18117, loss is 4.776218891143799\n",
      "(64, 33)\n",
      "step 18118, loss is 4.841231822967529\n",
      "(64, 33)\n",
      "step 18119, loss is 4.786687850952148\n",
      "(64, 33)\n",
      "step 18120, loss is 4.807974338531494\n",
      "(64, 33)\n",
      "step 18121, loss is 4.88852071762085\n",
      "(64, 33)\n",
      "step 18122, loss is 4.777347564697266\n",
      "(64, 33)\n",
      "step 18123, loss is 4.989259719848633\n",
      "(64, 33)\n",
      "step 18124, loss is 4.771462440490723\n",
      "(64, 33)\n",
      "step 18125, loss is 4.770384788513184\n",
      "(64, 33)\n",
      "step 18126, loss is 4.708373069763184\n",
      "(64, 33)\n",
      "step 18127, loss is 4.7362895011901855\n",
      "(64, 33)\n",
      "step 18128, loss is 4.955996513366699\n",
      "(64, 33)\n",
      "step 18129, loss is 4.810850143432617\n",
      "(64, 33)\n",
      "step 18130, loss is 4.934162616729736\n",
      "(64, 33)\n",
      "step 18131, loss is 4.6510009765625\n",
      "(64, 33)\n",
      "step 18132, loss is 4.832907199859619\n",
      "(64, 33)\n",
      "step 18133, loss is 4.783319473266602\n",
      "(64, 33)\n",
      "step 18134, loss is 4.735857963562012\n",
      "(64, 33)\n",
      "step 18135, loss is 4.889473915100098\n",
      "(64, 33)\n",
      "step 18136, loss is 4.84202766418457\n",
      "(64, 33)\n",
      "step 18137, loss is 5.060932636260986\n",
      "(64, 33)\n",
      "step 18138, loss is 4.890422821044922\n",
      "(64, 33)\n",
      "step 18139, loss is 4.949527263641357\n",
      "(64, 33)\n",
      "step 18140, loss is 4.713441848754883\n",
      "(64, 33)\n",
      "step 18141, loss is 4.702915668487549\n",
      "(64, 33)\n",
      "step 18142, loss is 4.7825751304626465\n",
      "(64, 33)\n",
      "step 18143, loss is 4.846096992492676\n",
      "(64, 33)\n",
      "step 18144, loss is 4.78835391998291\n",
      "(64, 33)\n",
      "step 18145, loss is 4.795260906219482\n",
      "(64, 33)\n",
      "step 18146, loss is 4.866591930389404\n",
      "(64, 33)\n",
      "step 18147, loss is 4.578470230102539\n",
      "(64, 33)\n",
      "step 18148, loss is 4.7693400382995605\n",
      "(64, 33)\n",
      "step 18149, loss is 4.920722961425781\n",
      "(64, 33)\n",
      "step 18150, loss is 4.718111038208008\n",
      "(64, 33)\n",
      "step 18151, loss is 4.794467926025391\n",
      "(64, 33)\n",
      "step 18152, loss is 4.759698390960693\n",
      "(64, 33)\n",
      "step 18153, loss is 4.972143650054932\n",
      "(64, 33)\n",
      "step 18154, loss is 4.780084609985352\n",
      "(64, 33)\n",
      "step 18155, loss is 4.72103214263916\n",
      "(64, 33)\n",
      "step 18156, loss is 4.908755779266357\n",
      "(64, 33)\n",
      "step 18157, loss is 4.819077014923096\n",
      "(64, 33)\n",
      "step 18158, loss is 4.764506816864014\n",
      "(64, 33)\n",
      "step 18159, loss is 4.833687782287598\n",
      "(64, 33)\n",
      "step 18160, loss is 4.864841461181641\n",
      "(64, 33)\n",
      "step 18161, loss is 4.876760005950928\n",
      "(64, 33)\n",
      "step 18162, loss is 4.921911716461182\n",
      "(64, 33)\n",
      "step 18163, loss is 4.879938125610352\n",
      "(64, 33)\n",
      "step 18164, loss is 4.883573532104492\n",
      "(64, 33)\n",
      "step 18165, loss is 4.783854007720947\n",
      "(64, 33)\n",
      "step 18166, loss is 4.739967346191406\n",
      "(64, 33)\n",
      "step 18167, loss is 4.763986110687256\n",
      "(64, 33)\n",
      "step 18168, loss is 4.698164463043213\n",
      "(64, 33)\n",
      "step 18169, loss is 4.698629856109619\n",
      "(64, 33)\n",
      "step 18170, loss is 4.716181755065918\n",
      "(64, 33)\n",
      "step 18171, loss is 4.91886568069458\n",
      "(64, 33)\n",
      "step 18172, loss is 4.679715156555176\n",
      "(64, 33)\n",
      "step 18173, loss is 4.751885890960693\n",
      "(64, 33)\n",
      "step 18174, loss is 4.944858551025391\n",
      "(64, 33)\n",
      "step 18175, loss is 4.730648040771484\n",
      "(64, 33)\n",
      "step 18176, loss is 4.871289253234863\n",
      "(64, 33)\n",
      "step 18177, loss is 4.818608283996582\n",
      "(64, 33)\n",
      "step 18178, loss is 5.033781051635742\n",
      "(64, 33)\n",
      "step 18179, loss is 4.7349348068237305\n",
      "(64, 33)\n",
      "step 18180, loss is 4.74242639541626\n",
      "(64, 33)\n",
      "step 18181, loss is 4.66402006149292\n",
      "(64, 33)\n",
      "step 18182, loss is 4.676158905029297\n",
      "(64, 33)\n",
      "step 18183, loss is 4.8063530921936035\n",
      "(64, 33)\n",
      "step 18184, loss is 4.75366735458374\n",
      "(64, 33)\n",
      "step 18185, loss is 4.707937717437744\n",
      "(64, 33)\n",
      "step 18186, loss is 4.752403259277344\n",
      "(64, 33)\n",
      "step 18187, loss is 4.808598041534424\n",
      "(64, 33)\n",
      "step 18188, loss is 4.815606117248535\n",
      "(64, 33)\n",
      "step 18189, loss is 4.882347106933594\n",
      "(64, 33)\n",
      "step 18190, loss is 4.799701690673828\n",
      "(64, 33)\n",
      "step 18191, loss is 4.686983108520508\n",
      "(64, 33)\n",
      "step 18192, loss is 4.772324085235596\n",
      "(64, 33)\n",
      "step 18193, loss is 4.7977705001831055\n",
      "(64, 33)\n",
      "step 18194, loss is 4.703546524047852\n",
      "(64, 33)\n",
      "step 18195, loss is 4.930516242980957\n",
      "(64, 33)\n",
      "step 18196, loss is 4.919215202331543\n",
      "(64, 33)\n",
      "step 18197, loss is 4.713592052459717\n",
      "(64, 33)\n",
      "step 18198, loss is 4.607306480407715\n",
      "(64, 33)\n",
      "step 18199, loss is 4.955887317657471\n",
      "(64, 33)\n",
      "step 18200, loss is 4.7994771003723145\n",
      "(64, 33)\n",
      "step 18201, loss is 4.893888473510742\n",
      "(64, 33)\n",
      "step 18202, loss is 4.691746234893799\n",
      "(64, 33)\n",
      "step 18203, loss is 4.861801624298096\n",
      "(64, 33)\n",
      "step 18204, loss is 4.809453964233398\n",
      "(64, 33)\n",
      "step 18205, loss is 4.711792945861816\n",
      "(64, 33)\n",
      "step 18206, loss is 4.581671237945557\n",
      "(64, 33)\n",
      "step 18207, loss is 4.890288352966309\n",
      "(64, 33)\n",
      "step 18208, loss is 4.696998596191406\n",
      "(64, 33)\n",
      "step 18209, loss is 5.011140823364258\n",
      "(64, 33)\n",
      "step 18210, loss is 4.831813812255859\n",
      "(64, 33)\n",
      "step 18211, loss is 4.841617584228516\n",
      "(64, 33)\n",
      "step 18212, loss is 4.740182876586914\n",
      "(64, 33)\n",
      "step 18213, loss is 4.876006126403809\n",
      "(64, 33)\n",
      "step 18214, loss is 4.783870220184326\n",
      "(64, 33)\n",
      "step 18215, loss is 4.760834693908691\n",
      "(64, 33)\n",
      "step 18216, loss is 5.027894973754883\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18217, loss is 4.658129692077637\n",
      "(64, 33)\n",
      "step 18218, loss is 4.8069891929626465\n",
      "(64, 33)\n",
      "step 18219, loss is 4.747905731201172\n",
      "(64, 33)\n",
      "step 18220, loss is 4.748918056488037\n",
      "(64, 33)\n",
      "step 18221, loss is 4.61742639541626\n",
      "(64, 33)\n",
      "step 18222, loss is 4.703758716583252\n",
      "(64, 33)\n",
      "step 18223, loss is 4.937951564788818\n",
      "(64, 33)\n",
      "step 18224, loss is 4.7013468742370605\n",
      "(64, 33)\n",
      "step 18225, loss is 4.727677345275879\n",
      "(64, 33)\n",
      "step 18226, loss is 4.779460430145264\n",
      "(64, 33)\n",
      "step 18227, loss is 4.334254264831543\n",
      "(64, 33)\n",
      "step 18228, loss is 4.612972736358643\n",
      "(64, 33)\n",
      "step 18229, loss is 4.642328262329102\n",
      "(64, 33)\n",
      "step 18230, loss is 4.911005020141602\n",
      "(64, 33)\n",
      "step 18231, loss is 4.6158952713012695\n",
      "(64, 33)\n",
      "step 18232, loss is 4.807332515716553\n",
      "(64, 33)\n",
      "step 18233, loss is 4.714987277984619\n",
      "(64, 33)\n",
      "step 18234, loss is 4.680948734283447\n",
      "(64, 33)\n",
      "step 18235, loss is 4.832233428955078\n",
      "(64, 33)\n",
      "step 18236, loss is 4.8061628341674805\n",
      "(64, 33)\n",
      "step 18237, loss is 4.796378135681152\n",
      "(64, 33)\n",
      "step 18238, loss is 4.847803115844727\n",
      "(64, 33)\n",
      "step 18239, loss is 4.9458465576171875\n",
      "(64, 33)\n",
      "step 18240, loss is 4.823848724365234\n",
      "(64, 33)\n",
      "step 18241, loss is 4.877946853637695\n",
      "(64, 33)\n",
      "step 18242, loss is 4.639145851135254\n",
      "(64, 33)\n",
      "step 18243, loss is 4.746971607208252\n",
      "(64, 33)\n",
      "step 18244, loss is 4.84859037399292\n",
      "(64, 33)\n",
      "step 18245, loss is 4.995599269866943\n",
      "(64, 33)\n",
      "step 18246, loss is 4.898952960968018\n",
      "(64, 33)\n",
      "step 18247, loss is 4.676634311676025\n",
      "(64, 33)\n",
      "step 18248, loss is 4.750158309936523\n",
      "(64, 33)\n",
      "step 18249, loss is 4.819488525390625\n",
      "(64, 33)\n",
      "step 18250, loss is 4.866844177246094\n",
      "(64, 33)\n",
      "step 18251, loss is 4.872281074523926\n",
      "(64, 33)\n",
      "step 18252, loss is 4.518000602722168\n",
      "(64, 33)\n",
      "step 18253, loss is 4.811517238616943\n",
      "(64, 33)\n",
      "step 18254, loss is 4.674928188323975\n",
      "(64, 33)\n",
      "step 18255, loss is 4.669334411621094\n",
      "(64, 33)\n",
      "step 18256, loss is 4.936885833740234\n",
      "(64, 33)\n",
      "step 18257, loss is 4.910094261169434\n",
      "(64, 33)\n",
      "step 18258, loss is 4.8848724365234375\n",
      "(64, 33)\n",
      "step 18259, loss is 4.815313339233398\n",
      "(64, 33)\n",
      "step 18260, loss is 4.702629089355469\n",
      "(64, 33)\n",
      "step 18261, loss is 4.649192810058594\n",
      "(64, 33)\n",
      "step 18262, loss is 4.816554546356201\n",
      "(64, 33)\n",
      "step 18263, loss is 4.899972438812256\n",
      "(64, 33)\n",
      "step 18264, loss is 4.85699987411499\n",
      "(64, 33)\n",
      "step 18265, loss is 4.874460220336914\n",
      "(64, 33)\n",
      "step 18266, loss is 4.946279525756836\n",
      "(64, 33)\n",
      "step 18267, loss is 4.9235005378723145\n",
      "(64, 33)\n",
      "step 18268, loss is 4.775184631347656\n",
      "(64, 33)\n",
      "step 18269, loss is 4.75371789932251\n",
      "(64, 33)\n",
      "step 18270, loss is 4.668548107147217\n",
      "(64, 33)\n",
      "step 18271, loss is 4.80275821685791\n",
      "(64, 33)\n",
      "step 18272, loss is 4.586418151855469\n",
      "(64, 33)\n",
      "step 18273, loss is 4.841994285583496\n",
      "(64, 33)\n",
      "step 18274, loss is 4.866029739379883\n",
      "(64, 33)\n",
      "step 18275, loss is 4.755167007446289\n",
      "(64, 33)\n",
      "step 18276, loss is 4.88705587387085\n",
      "(64, 33)\n",
      "step 18277, loss is 4.702727794647217\n",
      "(64, 33)\n",
      "step 18278, loss is 4.714081287384033\n",
      "(64, 33)\n",
      "step 18279, loss is 4.789482116699219\n",
      "(64, 33)\n",
      "step 18280, loss is 4.700913429260254\n",
      "(64, 33)\n",
      "step 18281, loss is 4.6813883781433105\n",
      "(64, 33)\n",
      "step 18282, loss is 4.770263671875\n",
      "(64, 33)\n",
      "step 18283, loss is 4.6584038734436035\n",
      "(64, 33)\n",
      "step 18284, loss is 4.685908794403076\n",
      "(64, 33)\n",
      "step 18285, loss is 4.661566734313965\n",
      "(64, 33)\n",
      "step 18286, loss is 4.780030250549316\n",
      "(64, 33)\n",
      "step 18287, loss is 5.100020885467529\n",
      "(64, 33)\n",
      "step 18288, loss is 5.076938152313232\n",
      "(64, 33)\n",
      "step 18289, loss is 4.6668829917907715\n",
      "(64, 33)\n",
      "step 18290, loss is 4.592283248901367\n",
      "(64, 33)\n",
      "step 18291, loss is 4.884981155395508\n",
      "(64, 33)\n",
      "step 18292, loss is 4.727588653564453\n",
      "(64, 33)\n",
      "step 18293, loss is 5.017698764801025\n",
      "(64, 33)\n",
      "step 18294, loss is 4.604646682739258\n",
      "(64, 33)\n",
      "step 18295, loss is 4.815685749053955\n",
      "(64, 33)\n",
      "step 18296, loss is 4.777369499206543\n",
      "(64, 33)\n",
      "step 18297, loss is 4.800975799560547\n",
      "(64, 33)\n",
      "step 18298, loss is 4.814397811889648\n",
      "(64, 33)\n",
      "step 18299, loss is 4.9510884284973145\n",
      "(64, 33)\n",
      "step 18300, loss is 4.694515705108643\n",
      "(64, 33)\n",
      "step 18301, loss is 4.57810115814209\n",
      "(64, 33)\n",
      "step 18302, loss is 4.7270331382751465\n",
      "(64, 33)\n",
      "step 18303, loss is 4.78947114944458\n",
      "(64, 33)\n",
      "step 18304, loss is 4.901126384735107\n",
      "(64, 33)\n",
      "step 18305, loss is 4.458586692810059\n",
      "(64, 33)\n",
      "step 18306, loss is 4.67262601852417\n",
      "(64, 33)\n",
      "step 18307, loss is 4.898209095001221\n",
      "(64, 33)\n",
      "step 18308, loss is 4.773558616638184\n",
      "(64, 33)\n",
      "step 18309, loss is 4.720344066619873\n",
      "(64, 33)\n",
      "step 18310, loss is 4.778151988983154\n",
      "(64, 33)\n",
      "step 18311, loss is 4.787535667419434\n",
      "(64, 33)\n",
      "step 18312, loss is 4.535123825073242\n",
      "(64, 33)\n",
      "step 18313, loss is 4.79918909072876\n",
      "(64, 33)\n",
      "step 18314, loss is 4.901236057281494\n",
      "(64, 33)\n",
      "step 18315, loss is 4.816187858581543\n",
      "(64, 33)\n",
      "step 18316, loss is 4.850892066955566\n",
      "(64, 33)\n",
      "step 18317, loss is 4.707385540008545\n",
      "(64, 33)\n",
      "step 18318, loss is 4.715137004852295\n",
      "(64, 33)\n",
      "step 18319, loss is 4.704311847686768\n",
      "(64, 33)\n",
      "step 18320, loss is 4.77556037902832\n",
      "(64, 33)\n",
      "step 18321, loss is 4.7415547370910645\n",
      "(64, 33)\n",
      "step 18322, loss is 4.901628494262695\n",
      "(64, 33)\n",
      "step 18323, loss is 4.770373344421387\n",
      "(64, 33)\n",
      "step 18324, loss is 4.729616165161133\n",
      "(64, 33)\n",
      "step 18325, loss is 4.836616039276123\n",
      "(64, 33)\n",
      "step 18326, loss is 4.799823760986328\n",
      "(64, 33)\n",
      "step 18327, loss is 4.651849746704102\n",
      "(64, 33)\n",
      "step 18328, loss is 4.682396411895752\n",
      "(64, 33)\n",
      "step 18329, loss is 4.86331844329834\n",
      "(64, 33)\n",
      "step 18330, loss is 4.697972774505615\n",
      "(64, 33)\n",
      "step 18331, loss is 4.738983154296875\n",
      "(64, 33)\n",
      "step 18332, loss is 4.650613784790039\n",
      "(64, 33)\n",
      "step 18333, loss is 4.746125221252441\n",
      "(64, 33)\n",
      "step 18334, loss is 4.77729606628418\n",
      "(64, 33)\n",
      "step 18335, loss is 4.6657867431640625\n",
      "(64, 33)\n",
      "step 18336, loss is 4.841428279876709\n",
      "(64, 33)\n",
      "step 18337, loss is 4.77262020111084\n",
      "(64, 33)\n",
      "step 18338, loss is 4.773983001708984\n",
      "(64, 33)\n",
      "step 18339, loss is 4.832540512084961\n",
      "(64, 33)\n",
      "step 18340, loss is 4.775296211242676\n",
      "(64, 33)\n",
      "step 18341, loss is 4.648068428039551\n",
      "(64, 33)\n",
      "step 18342, loss is 4.8247480392456055\n",
      "(64, 33)\n",
      "step 18343, loss is 4.87040376663208\n",
      "(64, 33)\n",
      "step 18344, loss is 4.803983211517334\n",
      "(64, 33)\n",
      "step 18345, loss is 4.660959720611572\n",
      "(64, 33)\n",
      "step 18346, loss is 4.7213053703308105\n",
      "(64, 33)\n",
      "step 18347, loss is 4.67613410949707\n",
      "(64, 33)\n",
      "step 18348, loss is 4.652513027191162\n",
      "(64, 33)\n",
      "step 18349, loss is 4.797246932983398\n",
      "(64, 33)\n",
      "step 18350, loss is 4.718705654144287\n",
      "(64, 33)\n",
      "step 18351, loss is 4.82803201675415\n",
      "(64, 33)\n",
      "step 18352, loss is 4.675848960876465\n",
      "(64, 33)\n",
      "step 18353, loss is 4.749108791351318\n",
      "(64, 33)\n",
      "step 18354, loss is 4.94741153717041\n",
      "(64, 33)\n",
      "step 18355, loss is 4.752859592437744\n",
      "(64, 33)\n",
      "step 18356, loss is 4.633796215057373\n",
      "(64, 33)\n",
      "step 18357, loss is 4.762722492218018\n",
      "(64, 33)\n",
      "step 18358, loss is 4.921781063079834\n",
      "(64, 33)\n",
      "step 18359, loss is 4.617210865020752\n",
      "(64, 33)\n",
      "step 18360, loss is 4.821287631988525\n",
      "(64, 33)\n",
      "step 18361, loss is 4.676896572113037\n",
      "(64, 33)\n",
      "step 18362, loss is 4.986088752746582\n",
      "(64, 33)\n",
      "step 18363, loss is 4.856133937835693\n",
      "(64, 33)\n",
      "step 18364, loss is 4.5973992347717285\n",
      "(64, 33)\n",
      "step 18365, loss is 4.856893539428711\n",
      "(64, 33)\n",
      "step 18366, loss is 4.650431156158447\n",
      "(64, 33)\n",
      "step 18367, loss is 4.640162944793701\n",
      "(64, 33)\n",
      "step 18368, loss is 4.980658531188965\n",
      "(64, 33)\n",
      "step 18369, loss is 4.77506685256958\n",
      "(64, 33)\n",
      "step 18370, loss is 4.741938591003418\n",
      "(64, 33)\n",
      "step 18371, loss is 4.787508964538574\n",
      "(64, 33)\n",
      "step 18372, loss is 5.0537285804748535\n",
      "(64, 33)\n",
      "step 18373, loss is 4.663930416107178\n",
      "(64, 33)\n",
      "step 18374, loss is 4.530944347381592\n",
      "(64, 33)\n",
      "step 18375, loss is 4.92454195022583\n",
      "(64, 33)\n",
      "step 18376, loss is 4.809138298034668\n",
      "(64, 33)\n",
      "step 18377, loss is 4.6607842445373535\n",
      "(64, 33)\n",
      "step 18378, loss is 4.822908401489258\n",
      "(64, 33)\n",
      "step 18379, loss is 4.730355739593506\n",
      "(64, 33)\n",
      "step 18380, loss is 4.583709239959717\n",
      "(64, 33)\n",
      "step 18381, loss is 4.688730716705322\n",
      "(64, 33)\n",
      "step 18382, loss is 4.643623352050781\n",
      "(64, 33)\n",
      "step 18383, loss is 4.867271423339844\n",
      "(64, 33)\n",
      "step 18384, loss is 4.889270305633545\n",
      "(64, 33)\n",
      "step 18385, loss is 4.6585612297058105\n",
      "(64, 33)\n",
      "step 18386, loss is 4.889089107513428\n",
      "(64, 33)\n",
      "step 18387, loss is 4.802896499633789\n",
      "(64, 33)\n",
      "step 18388, loss is 4.793352127075195\n",
      "(64, 33)\n",
      "step 18389, loss is 4.842874526977539\n",
      "(64, 33)\n",
      "step 18390, loss is 4.860305309295654\n",
      "(64, 33)\n",
      "step 18391, loss is 4.879980087280273\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18392, loss is 4.836799621582031\n",
      "(64, 33)\n",
      "step 18393, loss is 4.984123229980469\n",
      "(64, 33)\n",
      "step 18394, loss is 4.804283618927002\n",
      "(64, 33)\n",
      "step 18395, loss is 4.8417277336120605\n",
      "(64, 33)\n",
      "step 18396, loss is 4.737517356872559\n",
      "(64, 33)\n",
      "step 18397, loss is 4.81829833984375\n",
      "(64, 33)\n",
      "step 18398, loss is 4.642567157745361\n",
      "(64, 33)\n",
      "step 18399, loss is 4.649594783782959\n",
      "(64, 33)\n",
      "step 18400, loss is 4.862510681152344\n",
      "(64, 33)\n",
      "step 18401, loss is 4.7154622077941895\n",
      "(64, 33)\n",
      "step 18402, loss is 5.124105453491211\n",
      "(64, 33)\n",
      "step 18403, loss is 4.704124450683594\n",
      "(64, 33)\n",
      "step 18404, loss is 4.670726776123047\n",
      "(64, 33)\n",
      "step 18405, loss is 4.813425064086914\n",
      "(64, 33)\n",
      "step 18406, loss is 4.598402500152588\n",
      "(64, 33)\n",
      "step 18407, loss is 4.594937801361084\n",
      "(64, 33)\n",
      "step 18408, loss is 4.824053764343262\n",
      "(64, 33)\n",
      "step 18409, loss is 4.857812881469727\n",
      "(64, 33)\n",
      "step 18410, loss is 4.709911823272705\n",
      "(64, 33)\n",
      "step 18411, loss is 4.973354816436768\n",
      "(64, 33)\n",
      "step 18412, loss is 4.718942165374756\n",
      "(64, 33)\n",
      "step 18413, loss is 4.853236198425293\n",
      "(64, 33)\n",
      "step 18414, loss is 4.9129743576049805\n",
      "(64, 33)\n",
      "step 18415, loss is 4.7900519371032715\n",
      "(64, 33)\n",
      "step 18416, loss is 4.865350723266602\n",
      "(64, 33)\n",
      "step 18417, loss is 4.6750407218933105\n",
      "(64, 33)\n",
      "step 18418, loss is 4.811305522918701\n",
      "(64, 33)\n",
      "step 18419, loss is 4.808896541595459\n",
      "(64, 33)\n",
      "step 18420, loss is 4.799343585968018\n",
      "(64, 33)\n",
      "step 18421, loss is 4.777806758880615\n",
      "(64, 33)\n",
      "step 18422, loss is 4.9179792404174805\n",
      "(64, 33)\n",
      "step 18423, loss is 4.549862384796143\n",
      "(64, 33)\n",
      "step 18424, loss is 4.735755443572998\n",
      "(64, 33)\n",
      "step 18425, loss is 4.782294750213623\n",
      "(64, 33)\n",
      "step 18426, loss is 4.877641677856445\n",
      "(64, 33)\n",
      "step 18427, loss is 4.749017715454102\n",
      "(64, 33)\n",
      "step 18428, loss is 4.570956230163574\n",
      "(64, 33)\n",
      "step 18429, loss is 4.748178958892822\n",
      "(64, 33)\n",
      "step 18430, loss is 4.768775939941406\n",
      "(64, 33)\n",
      "step 18431, loss is 4.909642696380615\n",
      "(64, 33)\n",
      "step 18432, loss is 4.54632568359375\n",
      "(64, 33)\n",
      "step 18433, loss is 4.8201069831848145\n",
      "(64, 33)\n",
      "step 18434, loss is 4.8097124099731445\n",
      "(64, 33)\n",
      "step 18435, loss is 4.936495780944824\n",
      "(64, 33)\n",
      "step 18436, loss is 4.825709342956543\n",
      "(64, 33)\n",
      "step 18437, loss is 4.701335906982422\n",
      "(64, 33)\n",
      "step 18438, loss is 4.7031707763671875\n",
      "(64, 33)\n",
      "step 18439, loss is 4.700394153594971\n",
      "(64, 33)\n",
      "step 18440, loss is 4.571378231048584\n",
      "(64, 33)\n",
      "step 18441, loss is 4.897248268127441\n",
      "(64, 33)\n",
      "step 18442, loss is 4.900632858276367\n",
      "(64, 33)\n",
      "step 18443, loss is 4.707806587219238\n",
      "(64, 33)\n",
      "step 18444, loss is 4.746941089630127\n",
      "(64, 33)\n",
      "step 18445, loss is 4.6524457931518555\n",
      "(64, 33)\n",
      "step 18446, loss is 4.841906547546387\n",
      "(64, 33)\n",
      "step 18447, loss is 4.943430423736572\n",
      "(64, 33)\n",
      "step 18448, loss is 4.887401103973389\n",
      "(64, 33)\n",
      "step 18449, loss is 4.752572059631348\n",
      "(64, 33)\n",
      "step 18450, loss is 4.900681972503662\n",
      "(64, 33)\n",
      "step 18451, loss is 4.6823577880859375\n",
      "(64, 33)\n",
      "step 18452, loss is 4.9130353927612305\n",
      "(64, 33)\n",
      "step 18453, loss is 4.954137802124023\n",
      "(64, 33)\n",
      "step 18454, loss is 4.816653251647949\n",
      "(64, 33)\n",
      "step 18455, loss is 4.974570274353027\n",
      "(64, 33)\n",
      "step 18456, loss is 4.839323043823242\n",
      "(64, 33)\n",
      "step 18457, loss is 4.982574462890625\n",
      "(64, 33)\n",
      "step 18458, loss is 4.759626865386963\n",
      "(64, 33)\n",
      "step 18459, loss is 4.8434672355651855\n",
      "(64, 33)\n",
      "step 18460, loss is 4.866060256958008\n",
      "(64, 33)\n",
      "step 18461, loss is 4.62736177444458\n",
      "(64, 33)\n",
      "step 18462, loss is 4.900554180145264\n",
      "(64, 33)\n",
      "step 18463, loss is 4.895320415496826\n",
      "(64, 33)\n",
      "step 18464, loss is 4.8743720054626465\n",
      "(64, 33)\n",
      "step 18465, loss is 4.774632930755615\n",
      "(64, 33)\n",
      "step 18466, loss is 4.674227714538574\n",
      "(64, 33)\n",
      "step 18467, loss is 4.713462829589844\n",
      "(64, 33)\n",
      "step 18468, loss is 4.672112464904785\n",
      "(64, 33)\n",
      "step 18469, loss is 4.784380912780762\n",
      "(64, 33)\n",
      "step 18470, loss is 4.634270668029785\n",
      "(64, 33)\n",
      "step 18471, loss is 4.775055408477783\n",
      "(64, 33)\n",
      "step 18472, loss is 4.894375801086426\n",
      "(64, 33)\n",
      "step 18473, loss is 4.972890377044678\n",
      "(64, 33)\n",
      "step 18474, loss is 4.993629455566406\n",
      "(64, 33)\n",
      "step 18475, loss is 4.820323467254639\n",
      "(64, 33)\n",
      "step 18476, loss is 4.716396331787109\n",
      "(64, 33)\n",
      "step 18477, loss is 4.6932806968688965\n",
      "(64, 33)\n",
      "step 18478, loss is 4.7607269287109375\n",
      "(64, 33)\n",
      "step 18479, loss is 4.728808879852295\n",
      "(64, 33)\n",
      "step 18480, loss is 5.032268047332764\n",
      "(64, 33)\n",
      "step 18481, loss is 4.700297832489014\n",
      "(64, 33)\n",
      "step 18482, loss is 4.975985050201416\n",
      "(64, 33)\n",
      "step 18483, loss is 4.787596702575684\n",
      "(64, 33)\n",
      "step 18484, loss is 4.722067832946777\n",
      "(64, 33)\n",
      "step 18485, loss is 4.777311325073242\n",
      "(64, 33)\n",
      "step 18486, loss is 4.754910945892334\n",
      "(64, 33)\n",
      "step 18487, loss is 4.849767684936523\n",
      "(64, 33)\n",
      "step 18488, loss is 4.71906852722168\n",
      "(64, 33)\n",
      "step 18489, loss is 4.716060161590576\n",
      "(64, 33)\n",
      "step 18490, loss is 4.668591022491455\n",
      "(64, 33)\n",
      "step 18491, loss is 4.906895637512207\n",
      "(64, 33)\n",
      "step 18492, loss is 4.94157600402832\n",
      "(64, 33)\n",
      "step 18493, loss is 4.728759765625\n",
      "(64, 33)\n",
      "step 18494, loss is 4.85231876373291\n",
      "(64, 33)\n",
      "step 18495, loss is 4.784809112548828\n",
      "(64, 33)\n",
      "step 18496, loss is 4.7153096199035645\n",
      "(64, 33)\n",
      "step 18497, loss is 4.818614959716797\n",
      "(64, 33)\n",
      "step 18498, loss is 4.893289566040039\n",
      "(64, 33)\n",
      "step 18499, loss is 4.948383331298828\n",
      "(64, 33)\n",
      "step 18500, loss is 4.651389122009277\n",
      "(64, 33)\n",
      "step 18501, loss is 4.745447635650635\n",
      "(64, 33)\n",
      "step 18502, loss is 4.7068939208984375\n",
      "(64, 33)\n",
      "step 18503, loss is 4.517780780792236\n",
      "(64, 33)\n",
      "step 18504, loss is 4.780571460723877\n",
      "(64, 33)\n",
      "step 18505, loss is 4.785024642944336\n",
      "(64, 33)\n",
      "step 18506, loss is 4.876913547515869\n",
      "(64, 33)\n",
      "step 18507, loss is 4.767953395843506\n",
      "(64, 33)\n",
      "step 18508, loss is 4.828926086425781\n",
      "(64, 33)\n",
      "step 18509, loss is 5.063759803771973\n",
      "(64, 33)\n",
      "step 18510, loss is 5.005173206329346\n",
      "(64, 33)\n",
      "step 18511, loss is 4.683234214782715\n",
      "(64, 33)\n",
      "step 18512, loss is 4.89406681060791\n",
      "(64, 33)\n",
      "step 18513, loss is 4.687141418457031\n",
      "(64, 33)\n",
      "step 18514, loss is 4.803564071655273\n",
      "(64, 33)\n",
      "step 18515, loss is 4.8271660804748535\n",
      "(64, 33)\n",
      "step 18516, loss is 4.851094722747803\n",
      "(64, 33)\n",
      "step 18517, loss is 4.742619514465332\n",
      "(64, 33)\n",
      "step 18518, loss is 4.87540864944458\n",
      "(64, 33)\n",
      "step 18519, loss is 4.754969596862793\n",
      "(64, 33)\n",
      "step 18520, loss is 4.733337879180908\n",
      "(64, 33)\n",
      "step 18521, loss is 4.85908317565918\n",
      "(64, 33)\n",
      "step 18522, loss is 4.747447490692139\n",
      "(64, 33)\n",
      "step 18523, loss is 4.898204326629639\n",
      "(64, 33)\n",
      "step 18524, loss is 4.897937297821045\n",
      "(64, 33)\n",
      "step 18525, loss is 4.775397300720215\n",
      "(64, 33)\n",
      "step 18526, loss is 4.778069019317627\n",
      "(64, 33)\n",
      "step 18527, loss is 4.743217468261719\n",
      "(64, 33)\n",
      "step 18528, loss is 4.752277851104736\n",
      "(64, 33)\n",
      "step 18529, loss is 4.886241436004639\n",
      "(64, 33)\n",
      "step 18530, loss is 4.7488932609558105\n",
      "(64, 33)\n",
      "step 18531, loss is 4.85651969909668\n",
      "(64, 33)\n",
      "step 18532, loss is 4.786184310913086\n",
      "(64, 33)\n",
      "step 18533, loss is 4.676249980926514\n",
      "(64, 33)\n",
      "step 18534, loss is 4.555624485015869\n",
      "(64, 33)\n",
      "step 18535, loss is 4.792316913604736\n",
      "(64, 33)\n",
      "step 18536, loss is 4.822269916534424\n",
      "(64, 33)\n",
      "step 18537, loss is 4.882165431976318\n",
      "(64, 33)\n",
      "step 18538, loss is 4.7682905197143555\n",
      "(64, 33)\n",
      "step 18539, loss is 4.627559185028076\n",
      "(64, 33)\n",
      "step 18540, loss is 4.7778706550598145\n",
      "(64, 33)\n",
      "step 18541, loss is 4.80794620513916\n",
      "(64, 33)\n",
      "step 18542, loss is 4.7717976570129395\n",
      "(64, 33)\n",
      "step 18543, loss is 4.90565299987793\n",
      "(64, 33)\n",
      "step 18544, loss is 4.814023017883301\n",
      "(64, 33)\n",
      "step 18545, loss is 4.800874710083008\n",
      "(64, 33)\n",
      "step 18546, loss is 4.870869159698486\n",
      "(64, 33)\n",
      "step 18547, loss is 4.990034580230713\n",
      "(64, 33)\n",
      "step 18548, loss is 4.875284671783447\n",
      "(64, 33)\n",
      "step 18549, loss is 5.0232157707214355\n",
      "(64, 33)\n",
      "step 18550, loss is 4.7490034103393555\n",
      "(64, 33)\n",
      "step 18551, loss is 4.831653118133545\n",
      "(64, 33)\n",
      "step 18552, loss is 4.7675371170043945\n",
      "(64, 33)\n",
      "step 18553, loss is 4.679123401641846\n",
      "(64, 33)\n",
      "step 18554, loss is 4.8104119300842285\n",
      "(64, 33)\n",
      "step 18555, loss is 4.761034965515137\n",
      "(64, 33)\n",
      "step 18556, loss is 4.952816486358643\n",
      "(64, 33)\n",
      "step 18557, loss is 4.662782192230225\n",
      "(64, 33)\n",
      "step 18558, loss is 4.649766445159912\n",
      "(64, 33)\n",
      "step 18559, loss is 4.8612961769104\n",
      "(64, 33)\n",
      "step 18560, loss is 4.852665901184082\n",
      "(64, 33)\n",
      "step 18561, loss is 4.7168779373168945\n",
      "(64, 33)\n",
      "step 18562, loss is 4.898227214813232\n",
      "(64, 33)\n",
      "step 18563, loss is 4.784679889678955\n",
      "(64, 33)\n",
      "step 18564, loss is 4.8974690437316895\n",
      "(64, 33)\n",
      "step 18565, loss is 4.939700603485107\n",
      "(64, 33)\n",
      "step 18566, loss is 4.736534118652344\n",
      "(64, 33)\n",
      "step 18567, loss is 4.980258941650391\n",
      "(64, 33)\n",
      "step 18568, loss is 4.720009803771973\n",
      "(64, 33)\n",
      "step 18569, loss is 4.852963447570801\n",
      "(64, 33)\n",
      "step 18570, loss is 4.760406970977783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 18571, loss is 4.729618549346924\n",
      "(64, 33)\n",
      "step 18572, loss is 4.852494239807129\n",
      "(64, 33)\n",
      "step 18573, loss is 4.884283542633057\n",
      "(64, 33)\n",
      "step 18574, loss is 4.841896057128906\n",
      "(64, 33)\n",
      "step 18575, loss is 4.801144599914551\n",
      "(64, 33)\n",
      "step 18576, loss is 4.702215194702148\n",
      "(64, 33)\n",
      "step 18577, loss is 4.721255302429199\n",
      "(64, 33)\n",
      "step 18578, loss is 5.007101535797119\n",
      "(64, 33)\n",
      "step 18579, loss is 4.806710720062256\n",
      "(64, 33)\n",
      "step 18580, loss is 4.879400730133057\n",
      "(64, 33)\n",
      "step 18581, loss is 4.75437068939209\n",
      "(64, 33)\n",
      "step 18582, loss is 4.899962425231934\n",
      "(64, 33)\n",
      "step 18583, loss is 4.804808139801025\n",
      "(64, 33)\n",
      "step 18584, loss is 4.929096221923828\n",
      "(64, 33)\n",
      "step 18585, loss is 5.005738735198975\n",
      "(64, 33)\n",
      "step 18586, loss is 4.916806221008301\n",
      "(64, 33)\n",
      "step 18587, loss is 4.7478227615356445\n",
      "(64, 33)\n",
      "step 18588, loss is 5.042856693267822\n",
      "(64, 33)\n",
      "step 18589, loss is 4.641144275665283\n",
      "(64, 33)\n",
      "step 18590, loss is 4.703904151916504\n",
      "(64, 33)\n",
      "step 18591, loss is 4.613391399383545\n",
      "(64, 33)\n",
      "step 18592, loss is 4.987415313720703\n",
      "(64, 33)\n",
      "step 18593, loss is 4.888576507568359\n",
      "(64, 33)\n",
      "step 18594, loss is 4.750027179718018\n",
      "(64, 33)\n",
      "step 18595, loss is 4.891598701477051\n",
      "(64, 33)\n",
      "step 18596, loss is 4.850661754608154\n",
      "(64, 33)\n",
      "step 18597, loss is 4.968018054962158\n",
      "(64, 33)\n",
      "step 18598, loss is 4.664762020111084\n",
      "(64, 33)\n",
      "step 18599, loss is 4.932182312011719\n",
      "(64, 33)\n",
      "step 18600, loss is 4.861198425292969\n",
      "(64, 33)\n",
      "step 18601, loss is 4.994012832641602\n",
      "(64, 33)\n",
      "step 18602, loss is 4.8095383644104\n",
      "(64, 33)\n",
      "step 18603, loss is 4.6492486000061035\n",
      "(64, 33)\n",
      "step 18604, loss is 4.943110942840576\n",
      "(64, 33)\n",
      "step 18605, loss is 4.866324424743652\n",
      "(64, 33)\n",
      "step 18606, loss is 4.752165794372559\n",
      "(64, 33)\n",
      "step 18607, loss is 4.809928894042969\n",
      "(64, 33)\n",
      "step 18608, loss is 4.800903797149658\n",
      "(64, 33)\n",
      "step 18609, loss is 4.620833396911621\n",
      "(64, 33)\n",
      "step 18610, loss is 4.955644130706787\n",
      "(64, 33)\n",
      "step 18611, loss is 4.747816562652588\n",
      "(64, 33)\n",
      "step 18612, loss is 4.690934181213379\n",
      "(64, 33)\n",
      "step 18613, loss is 4.841789245605469\n",
      "(64, 33)\n",
      "step 18614, loss is 4.662050724029541\n",
      "(64, 33)\n",
      "step 18615, loss is 4.707435607910156\n",
      "(64, 33)\n",
      "step 18616, loss is 4.898235321044922\n",
      "(64, 33)\n",
      "step 18617, loss is 4.713381290435791\n",
      "(64, 33)\n",
      "step 18618, loss is 4.791284561157227\n",
      "(64, 33)\n",
      "step 18619, loss is 5.0336737632751465\n",
      "(64, 33)\n",
      "step 18620, loss is 4.712143898010254\n",
      "(64, 33)\n",
      "step 18621, loss is 4.803821563720703\n",
      "(64, 33)\n",
      "step 18622, loss is 4.742983341217041\n",
      "(64, 33)\n",
      "step 18623, loss is 4.625429153442383\n",
      "(64, 33)\n",
      "step 18624, loss is 4.832465648651123\n",
      "(64, 33)\n",
      "step 18625, loss is 4.777158260345459\n",
      "(64, 33)\n",
      "step 18626, loss is 4.947951316833496\n",
      "(64, 33)\n",
      "step 18627, loss is 4.801997184753418\n",
      "(64, 33)\n",
      "step 18628, loss is 4.63322114944458\n",
      "(64, 33)\n",
      "step 18629, loss is 4.805755615234375\n",
      "(64, 33)\n",
      "step 18630, loss is 4.6418609619140625\n",
      "(64, 33)\n",
      "step 18631, loss is 4.808502674102783\n",
      "(64, 33)\n",
      "step 18632, loss is 4.834478855133057\n",
      "(64, 33)\n",
      "step 18633, loss is 4.720052719116211\n",
      "(64, 33)\n",
      "step 18634, loss is 5.000955581665039\n",
      "(64, 33)\n",
      "step 18635, loss is 4.773390769958496\n",
      "(64, 33)\n",
      "step 18636, loss is 4.756287097930908\n",
      "(64, 33)\n",
      "step 18637, loss is 4.735739231109619\n",
      "(64, 33)\n",
      "step 18638, loss is 4.618529319763184\n",
      "(64, 33)\n",
      "step 18639, loss is 4.639444828033447\n",
      "(64, 33)\n",
      "step 18640, loss is 4.814000606536865\n",
      "(64, 33)\n",
      "step 18641, loss is 4.945265293121338\n",
      "(64, 33)\n",
      "step 18642, loss is 4.911951065063477\n",
      "(64, 33)\n",
      "step 18643, loss is 4.67499303817749\n",
      "(64, 33)\n",
      "step 18644, loss is 4.962006568908691\n",
      "(64, 33)\n",
      "step 18645, loss is 5.034866809844971\n",
      "(64, 33)\n",
      "step 18646, loss is 4.888484477996826\n",
      "(64, 33)\n",
      "step 18647, loss is 4.828763961791992\n",
      "(64, 33)\n",
      "step 18648, loss is 4.832643508911133\n",
      "(64, 33)\n",
      "step 18649, loss is 4.771132946014404\n",
      "(64, 33)\n",
      "step 18650, loss is 4.575490474700928\n",
      "(64, 33)\n",
      "step 18651, loss is 4.781955242156982\n",
      "(64, 33)\n",
      "step 18652, loss is 4.772718906402588\n",
      "(64, 33)\n",
      "step 18653, loss is 4.811494827270508\n",
      "(64, 33)\n",
      "step 18654, loss is 4.915344715118408\n",
      "(64, 33)\n",
      "step 18655, loss is 4.8739333152771\n",
      "(64, 33)\n",
      "step 18656, loss is 4.8571672439575195\n",
      "(64, 33)\n",
      "step 18657, loss is 4.682753086090088\n",
      "(64, 33)\n",
      "step 18658, loss is 4.746768474578857\n",
      "(64, 33)\n",
      "step 18659, loss is 4.674498558044434\n",
      "(64, 33)\n",
      "step 18660, loss is 4.825150966644287\n",
      "(64, 33)\n",
      "step 18661, loss is 4.792133808135986\n",
      "(64, 33)\n",
      "step 18662, loss is 4.589567184448242\n",
      "(64, 33)\n",
      "step 18663, loss is 4.873235702514648\n",
      "(64, 33)\n",
      "step 18664, loss is 4.863839626312256\n",
      "(64, 33)\n",
      "step 18665, loss is 4.755364894866943\n",
      "(64, 33)\n",
      "step 18666, loss is 4.889398097991943\n",
      "(64, 33)\n",
      "step 18667, loss is 4.766281604766846\n",
      "(64, 33)\n",
      "step 18668, loss is 4.875473499298096\n",
      "(64, 33)\n",
      "step 18669, loss is 4.938630104064941\n",
      "(64, 33)\n",
      "step 18670, loss is 4.738770484924316\n",
      "(64, 33)\n",
      "step 18671, loss is 4.939916133880615\n",
      "(64, 33)\n",
      "step 18672, loss is 4.627964973449707\n",
      "(64, 33)\n",
      "step 18673, loss is 4.880814075469971\n",
      "(64, 33)\n",
      "step 18674, loss is 4.6668853759765625\n",
      "(64, 33)\n",
      "step 18675, loss is 4.819187164306641\n",
      "(64, 33)\n",
      "step 18676, loss is 4.784441947937012\n",
      "(64, 33)\n",
      "step 18677, loss is 4.685601711273193\n",
      "(64, 33)\n",
      "step 18678, loss is 4.799324035644531\n",
      "(64, 33)\n",
      "step 18679, loss is 4.914778232574463\n",
      "(64, 33)\n",
      "step 18680, loss is 4.712570667266846\n",
      "(64, 33)\n",
      "step 18681, loss is 4.670527935028076\n",
      "(64, 33)\n",
      "step 18682, loss is 4.581050872802734\n",
      "(64, 33)\n",
      "step 18683, loss is 4.823906421661377\n",
      "(64, 33)\n",
      "step 18684, loss is 4.826109409332275\n",
      "(64, 33)\n",
      "step 18685, loss is 4.875431060791016\n",
      "(64, 33)\n",
      "step 18686, loss is 4.753144264221191\n",
      "(64, 33)\n",
      "step 18687, loss is 4.804228782653809\n",
      "(64, 33)\n",
      "step 18688, loss is 4.738652229309082\n",
      "(64, 33)\n",
      "step 18689, loss is 4.572055339813232\n",
      "(64, 33)\n",
      "step 18690, loss is 4.69747257232666\n",
      "(64, 33)\n",
      "step 18691, loss is 4.810091018676758\n",
      "(64, 33)\n",
      "step 18692, loss is 4.930176734924316\n",
      "(64, 33)\n",
      "step 18693, loss is 4.746158123016357\n",
      "(64, 33)\n",
      "step 18694, loss is 4.723644256591797\n",
      "(64, 33)\n",
      "step 18695, loss is 4.888187408447266\n",
      "(64, 33)\n",
      "step 18696, loss is 4.688211917877197\n",
      "(64, 33)\n",
      "step 18697, loss is 4.700965881347656\n",
      "(64, 33)\n",
      "step 18698, loss is 4.754920482635498\n",
      "(64, 33)\n",
      "step 18699, loss is 4.738893508911133\n",
      "(64, 33)\n",
      "step 18700, loss is 4.744678020477295\n",
      "(64, 33)\n",
      "step 18701, loss is 4.906999111175537\n",
      "(64, 33)\n",
      "step 18702, loss is 4.907365322113037\n",
      "(64, 33)\n",
      "step 18703, loss is 4.819408893585205\n",
      "(64, 33)\n",
      "step 18704, loss is 4.887670516967773\n",
      "(64, 33)\n",
      "step 18705, loss is 4.942044258117676\n",
      "(64, 33)\n",
      "step 18706, loss is 4.852793216705322\n",
      "(64, 33)\n",
      "step 18707, loss is 4.864696979522705\n",
      "(64, 33)\n",
      "step 18708, loss is 5.024467945098877\n",
      "(64, 33)\n",
      "step 18709, loss is 4.828972816467285\n",
      "(64, 33)\n",
      "step 18710, loss is 4.797426223754883\n",
      "(64, 33)\n",
      "step 18711, loss is 4.818986415863037\n",
      "(64, 33)\n",
      "step 18712, loss is 4.779906272888184\n",
      "(64, 33)\n",
      "step 18713, loss is 4.977105617523193\n",
      "(64, 33)\n",
      "step 18714, loss is 4.974648952484131\n",
      "(64, 33)\n",
      "step 18715, loss is 5.014900207519531\n",
      "(64, 33)\n",
      "step 18716, loss is 4.9260783195495605\n",
      "(64, 33)\n",
      "step 18717, loss is 4.9076385498046875\n",
      "(64, 33)\n",
      "step 18718, loss is 4.859195232391357\n",
      "(64, 33)\n",
      "step 18719, loss is 4.9110493659973145\n",
      "(64, 33)\n",
      "step 18720, loss is 4.783784866333008\n",
      "(64, 33)\n",
      "step 18721, loss is 4.896045207977295\n",
      "(64, 33)\n",
      "step 18722, loss is 4.821934223175049\n",
      "(64, 33)\n",
      "step 18723, loss is 4.868560314178467\n",
      "(64, 33)\n",
      "step 18724, loss is 4.861629962921143\n",
      "(64, 33)\n",
      "step 18725, loss is 4.790144920349121\n",
      "(64, 33)\n",
      "step 18726, loss is 4.603616714477539\n",
      "(64, 33)\n",
      "step 18727, loss is 4.844177722930908\n",
      "(64, 33)\n",
      "step 18728, loss is 4.774895191192627\n",
      "(64, 33)\n",
      "step 18729, loss is 4.713683128356934\n",
      "(64, 33)\n",
      "step 18730, loss is 4.821958065032959\n",
      "(64, 33)\n",
      "step 18731, loss is 4.733002185821533\n",
      "(64, 33)\n",
      "step 18732, loss is 4.893836498260498\n",
      "(64, 33)\n",
      "step 18733, loss is 4.652894973754883\n",
      "(64, 33)\n",
      "step 18734, loss is 4.693504810333252\n",
      "(64, 33)\n",
      "step 18735, loss is 4.646364212036133\n",
      "(64, 33)\n",
      "step 18736, loss is 4.8757219314575195\n",
      "(64, 33)\n",
      "step 18737, loss is 4.6445441246032715\n",
      "(64, 33)\n",
      "step 18738, loss is 4.7662482261657715\n",
      "(64, 33)\n",
      "step 18739, loss is 4.966062545776367\n",
      "(64, 33)\n",
      "step 18740, loss is 4.834372520446777\n",
      "(64, 33)\n",
      "step 18741, loss is 4.833845615386963\n",
      "(64, 33)\n",
      "step 18742, loss is 4.663272380828857\n",
      "(64, 33)\n",
      "step 18743, loss is 4.938896179199219\n",
      "(64, 33)\n",
      "step 18744, loss is 4.818394660949707\n",
      "(64, 33)\n",
      "step 18745, loss is 4.825084209442139\n",
      "(64, 33)\n",
      "step 18746, loss is 4.835341453552246\n",
      "(64, 33)\n",
      "step 18747, loss is 4.910579204559326\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18748, loss is 4.874222755432129\n",
      "(64, 33)\n",
      "step 18749, loss is 4.780835151672363\n",
      "(64, 33)\n",
      "step 18750, loss is 4.9559855461120605\n",
      "(64, 33)\n",
      "step 18751, loss is 4.652494430541992\n",
      "(64, 33)\n",
      "step 18752, loss is 4.819334030151367\n",
      "(64, 33)\n",
      "step 18753, loss is 4.762763977050781\n",
      "(64, 33)\n",
      "step 18754, loss is 4.744163513183594\n",
      "(64, 33)\n",
      "step 18755, loss is 4.775392532348633\n",
      "(64, 33)\n",
      "step 18756, loss is 4.839531421661377\n",
      "(64, 33)\n",
      "step 18757, loss is 4.929865837097168\n",
      "(64, 33)\n",
      "step 18758, loss is 4.884568691253662\n",
      "(64, 33)\n",
      "step 18759, loss is 4.931035995483398\n",
      "(64, 33)\n",
      "step 18760, loss is 4.790921688079834\n",
      "(64, 33)\n",
      "step 18761, loss is 4.814707279205322\n",
      "(64, 33)\n",
      "step 18762, loss is 4.942800521850586\n",
      "(64, 33)\n",
      "step 18763, loss is 4.913032054901123\n",
      "(64, 33)\n",
      "step 18764, loss is 4.616542816162109\n",
      "(64, 33)\n",
      "step 18765, loss is 4.847631454467773\n",
      "(64, 33)\n",
      "step 18766, loss is 4.751396656036377\n",
      "(64, 33)\n",
      "step 18767, loss is 4.9216766357421875\n",
      "(64, 33)\n",
      "step 18768, loss is 4.59623384475708\n",
      "(64, 33)\n",
      "step 18769, loss is 4.925827980041504\n",
      "(64, 33)\n",
      "step 18770, loss is 4.687378883361816\n",
      "(64, 33)\n",
      "step 18771, loss is 4.923794746398926\n",
      "(64, 33)\n",
      "step 18772, loss is 4.870316505432129\n",
      "(64, 33)\n",
      "step 18773, loss is 4.650389671325684\n",
      "(64, 33)\n",
      "step 18774, loss is 4.685522556304932\n",
      "(64, 33)\n",
      "step 18775, loss is 4.8193359375\n",
      "(64, 33)\n",
      "step 18776, loss is 4.877512454986572\n",
      "(64, 33)\n",
      "step 18777, loss is 4.754865646362305\n",
      "(64, 33)\n",
      "step 18778, loss is 4.821483612060547\n",
      "(64, 33)\n",
      "step 18779, loss is 4.74143123626709\n",
      "(64, 33)\n",
      "step 18780, loss is 4.543449401855469\n",
      "(64, 33)\n",
      "step 18781, loss is 4.866147994995117\n",
      "(64, 33)\n",
      "step 18782, loss is 4.909109592437744\n",
      "(64, 33)\n",
      "step 18783, loss is 4.656656742095947\n",
      "(64, 33)\n",
      "step 18784, loss is 4.794166088104248\n",
      "(64, 33)\n",
      "step 18785, loss is 4.8158698081970215\n",
      "(64, 33)\n",
      "step 18786, loss is 4.832674026489258\n",
      "(64, 33)\n",
      "step 18787, loss is 4.731889247894287\n",
      "(64, 33)\n",
      "step 18788, loss is 4.9652628898620605\n",
      "(64, 33)\n",
      "step 18789, loss is 4.626118183135986\n",
      "(64, 33)\n",
      "step 18790, loss is 4.875369548797607\n",
      "(64, 33)\n",
      "step 18791, loss is 4.755197048187256\n",
      "(64, 33)\n",
      "step 18792, loss is 4.8572492599487305\n",
      "(64, 33)\n",
      "step 18793, loss is 4.78187370300293\n",
      "(64, 33)\n",
      "step 18794, loss is 4.838191986083984\n",
      "(64, 33)\n",
      "step 18795, loss is 4.732626438140869\n",
      "(64, 33)\n",
      "step 18796, loss is 4.755450248718262\n",
      "(64, 33)\n",
      "step 18797, loss is 4.950847625732422\n",
      "(64, 33)\n",
      "step 18798, loss is 4.957146644592285\n",
      "(64, 33)\n",
      "step 18799, loss is 4.748156547546387\n",
      "(64, 33)\n",
      "step 18800, loss is 4.963233947753906\n",
      "(64, 33)\n",
      "step 18801, loss is 4.706047058105469\n",
      "(64, 33)\n",
      "step 18802, loss is 4.772156238555908\n",
      "(64, 33)\n",
      "step 18803, loss is 4.886694431304932\n",
      "(64, 33)\n",
      "step 18804, loss is 4.823613166809082\n",
      "(64, 33)\n",
      "step 18805, loss is 4.887136459350586\n",
      "(64, 33)\n",
      "step 18806, loss is 4.8719048500061035\n",
      "(64, 33)\n",
      "step 18807, loss is 4.863875865936279\n",
      "(64, 33)\n",
      "step 18808, loss is 4.792487621307373\n",
      "(64, 33)\n",
      "step 18809, loss is 4.850422382354736\n",
      "(64, 33)\n",
      "step 18810, loss is 4.654283046722412\n",
      "(64, 33)\n",
      "step 18811, loss is 4.93447208404541\n",
      "(64, 33)\n",
      "step 18812, loss is 4.885414123535156\n",
      "(64, 33)\n",
      "step 18813, loss is 4.80604887008667\n",
      "(64, 33)\n",
      "step 18814, loss is 4.724354267120361\n",
      "(64, 33)\n",
      "step 18815, loss is 4.7631611824035645\n",
      "(64, 33)\n",
      "step 18816, loss is 4.851231575012207\n",
      "(64, 33)\n",
      "step 18817, loss is 4.715297698974609\n",
      "(64, 33)\n",
      "step 18818, loss is 4.909309387207031\n",
      "(64, 33)\n",
      "step 18819, loss is 4.733898162841797\n",
      "(64, 33)\n",
      "step 18820, loss is 4.941822052001953\n",
      "(64, 33)\n",
      "step 18821, loss is 4.88320779800415\n",
      "(64, 33)\n",
      "step 18822, loss is 4.7110915184021\n",
      "(64, 33)\n",
      "step 18823, loss is 4.816877365112305\n",
      "(64, 33)\n",
      "step 18824, loss is 4.774906635284424\n",
      "(64, 33)\n",
      "step 18825, loss is 4.786422252655029\n",
      "(64, 33)\n",
      "step 18826, loss is 4.553659915924072\n",
      "(64, 33)\n",
      "step 18827, loss is 4.691927433013916\n",
      "(64, 33)\n",
      "step 18828, loss is 4.885583400726318\n",
      "(64, 33)\n",
      "step 18829, loss is 4.802685260772705\n",
      "(64, 33)\n",
      "step 18830, loss is 4.718064308166504\n",
      "(64, 33)\n",
      "step 18831, loss is 4.7019429206848145\n",
      "(64, 33)\n",
      "step 18832, loss is 4.705528259277344\n",
      "(64, 33)\n",
      "step 18833, loss is 4.745057106018066\n",
      "(64, 33)\n",
      "step 18834, loss is 4.9048075675964355\n",
      "(64, 33)\n",
      "step 18835, loss is 4.927722930908203\n",
      "(64, 33)\n",
      "step 18836, loss is 4.762019157409668\n",
      "(64, 33)\n",
      "step 18837, loss is 4.7936296463012695\n",
      "(64, 33)\n",
      "step 18838, loss is 4.625605583190918\n",
      "(64, 33)\n",
      "step 18839, loss is 4.753028869628906\n",
      "(64, 33)\n",
      "step 18840, loss is 4.809473037719727\n",
      "(64, 33)\n",
      "step 18841, loss is 4.825079441070557\n",
      "(64, 33)\n",
      "step 18842, loss is 4.742441654205322\n",
      "(64, 33)\n",
      "step 18843, loss is 4.9156317710876465\n",
      "(64, 33)\n",
      "step 18844, loss is 4.86935567855835\n",
      "(64, 33)\n",
      "step 18845, loss is 4.824827671051025\n",
      "(64, 33)\n",
      "step 18846, loss is 4.872104644775391\n",
      "(64, 33)\n",
      "step 18847, loss is 4.61538553237915\n",
      "(64, 33)\n",
      "step 18848, loss is 4.743621349334717\n",
      "(64, 33)\n",
      "step 18849, loss is 4.633671283721924\n",
      "(64, 33)\n",
      "step 18850, loss is 4.9243340492248535\n",
      "(64, 33)\n",
      "step 18851, loss is 4.903990745544434\n",
      "(64, 33)\n",
      "step 18852, loss is 4.866384506225586\n",
      "(64, 33)\n",
      "step 18853, loss is 4.7634172439575195\n",
      "(64, 33)\n",
      "step 18854, loss is 4.817704677581787\n",
      "(64, 33)\n",
      "step 18855, loss is 4.828343391418457\n",
      "(64, 33)\n",
      "step 18856, loss is 4.797854423522949\n",
      "(64, 33)\n",
      "step 18857, loss is 4.772572994232178\n",
      "(64, 33)\n",
      "step 18858, loss is 4.668815612792969\n",
      "(64, 33)\n",
      "step 18859, loss is 4.787614822387695\n",
      "(64, 33)\n",
      "step 18860, loss is 4.731598854064941\n",
      "(64, 33)\n",
      "step 18861, loss is 4.771328449249268\n",
      "(64, 33)\n",
      "step 18862, loss is 4.76351261138916\n",
      "(64, 33)\n",
      "step 18863, loss is 4.637329578399658\n",
      "(64, 33)\n",
      "step 18864, loss is 4.771139621734619\n",
      "(64, 33)\n",
      "step 18865, loss is 4.71677303314209\n",
      "(64, 33)\n",
      "step 18866, loss is 4.891200542449951\n",
      "(64, 33)\n",
      "step 18867, loss is 4.834400177001953\n",
      "(64, 33)\n",
      "step 18868, loss is 4.822981357574463\n",
      "(64, 33)\n",
      "step 18869, loss is 4.717767715454102\n",
      "(64, 33)\n",
      "step 18870, loss is 4.732987880706787\n",
      "(64, 33)\n",
      "step 18871, loss is 4.661680698394775\n",
      "(64, 33)\n",
      "step 18872, loss is 4.759994029998779\n",
      "(64, 33)\n",
      "step 18873, loss is 4.846018314361572\n",
      "(64, 33)\n",
      "step 18874, loss is 4.670268535614014\n",
      "(64, 33)\n",
      "step 18875, loss is 4.7693023681640625\n",
      "(64, 33)\n",
      "step 18876, loss is 4.769440650939941\n",
      "(64, 33)\n",
      "step 18877, loss is 4.74475622177124\n",
      "(64, 33)\n",
      "step 18878, loss is 4.676231861114502\n",
      "(64, 33)\n",
      "step 18879, loss is 4.652365207672119\n",
      "(64, 33)\n",
      "step 18880, loss is 5.000857353210449\n",
      "(64, 33)\n",
      "step 18881, loss is 4.796321392059326\n",
      "(64, 33)\n",
      "step 18882, loss is 4.688880920410156\n",
      "(64, 33)\n",
      "step 18883, loss is 4.644433975219727\n",
      "(64, 33)\n",
      "step 18884, loss is 4.693783760070801\n",
      "(64, 33)\n",
      "step 18885, loss is 4.72095251083374\n",
      "(64, 33)\n",
      "step 18886, loss is 4.740349292755127\n",
      "(64, 33)\n",
      "step 18887, loss is 4.806185245513916\n",
      "(64, 33)\n",
      "step 18888, loss is 4.824810981750488\n",
      "(64, 33)\n",
      "step 18889, loss is 4.815563201904297\n",
      "(64, 33)\n",
      "step 18890, loss is 4.717521667480469\n",
      "(64, 33)\n",
      "step 18891, loss is 4.693608283996582\n",
      "(64, 33)\n",
      "step 18892, loss is 4.928249359130859\n",
      "(64, 33)\n",
      "step 18893, loss is 4.75837516784668\n",
      "(64, 33)\n",
      "step 18894, loss is 4.866346836090088\n",
      "(64, 33)\n",
      "step 18895, loss is 4.756082057952881\n",
      "(64, 33)\n",
      "step 18896, loss is 4.773776531219482\n",
      "(64, 33)\n",
      "step 18897, loss is 4.656205177307129\n",
      "(64, 33)\n",
      "step 18898, loss is 4.738407611846924\n",
      "(64, 33)\n",
      "step 18899, loss is 4.784318923950195\n",
      "(64, 33)\n",
      "step 18900, loss is 4.740061283111572\n",
      "(64, 33)\n",
      "step 18901, loss is 4.909445762634277\n",
      "(64, 33)\n",
      "step 18902, loss is 4.677569389343262\n",
      "(64, 33)\n",
      "step 18903, loss is 4.6706390380859375\n",
      "(64, 33)\n",
      "step 18904, loss is 4.869461536407471\n",
      "(64, 33)\n",
      "step 18905, loss is 4.799320220947266\n",
      "(64, 33)\n",
      "step 18906, loss is 4.658024311065674\n",
      "(64, 33)\n",
      "step 18907, loss is 4.745276927947998\n",
      "(64, 33)\n",
      "step 18908, loss is 4.711705207824707\n",
      "(64, 33)\n",
      "step 18909, loss is 4.75909423828125\n",
      "(64, 33)\n",
      "step 18910, loss is 4.809996604919434\n",
      "(64, 33)\n",
      "step 18911, loss is 4.671975135803223\n",
      "(64, 33)\n",
      "step 18912, loss is 4.727634429931641\n",
      "(64, 33)\n",
      "step 18913, loss is 4.756163120269775\n",
      "(64, 33)\n",
      "step 18914, loss is 4.78680419921875\n",
      "(64, 33)\n",
      "step 18915, loss is 4.983963966369629\n",
      "(64, 33)\n",
      "step 18916, loss is 4.759034156799316\n",
      "(64, 33)\n",
      "step 18917, loss is 4.882516860961914\n",
      "(64, 33)\n",
      "step 18918, loss is 4.723665714263916\n",
      "(64, 33)\n",
      "step 18919, loss is 4.983718395233154\n",
      "(64, 33)\n",
      "step 18920, loss is 4.842250823974609\n",
      "(64, 33)\n",
      "step 18921, loss is 4.632627487182617\n",
      "(64, 33)\n",
      "step 18922, loss is 4.853175640106201\n",
      "(64, 33)\n",
      "step 18923, loss is 4.793532848358154\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18924, loss is 4.8975067138671875\n",
      "(64, 33)\n",
      "step 18925, loss is 4.791947364807129\n",
      "(64, 33)\n",
      "step 18926, loss is 4.4996256828308105\n",
      "(64, 33)\n",
      "step 18927, loss is 4.704376697540283\n",
      "(64, 33)\n",
      "step 18928, loss is 4.74167537689209\n",
      "(64, 33)\n",
      "step 18929, loss is 4.774568557739258\n",
      "(64, 33)\n",
      "step 18930, loss is 4.643807888031006\n",
      "(64, 33)\n",
      "step 18931, loss is 4.821363925933838\n",
      "(64, 33)\n",
      "step 18932, loss is 4.508990287780762\n",
      "(64, 33)\n",
      "step 18933, loss is 4.808017730712891\n",
      "(64, 33)\n",
      "step 18934, loss is 4.763302326202393\n",
      "(64, 33)\n",
      "step 18935, loss is 4.7224016189575195\n",
      "(64, 33)\n",
      "step 18936, loss is 4.742928981781006\n",
      "(64, 33)\n",
      "step 18937, loss is 4.7505717277526855\n",
      "(64, 33)\n",
      "step 18938, loss is 4.910542964935303\n",
      "(64, 33)\n",
      "step 18939, loss is 4.767849922180176\n",
      "(64, 33)\n",
      "step 18940, loss is 4.6304097175598145\n",
      "(64, 33)\n",
      "step 18941, loss is 4.753220081329346\n",
      "(64, 33)\n",
      "step 18942, loss is 4.90648889541626\n",
      "(64, 33)\n",
      "step 18943, loss is 4.779465198516846\n",
      "(64, 33)\n",
      "step 18944, loss is 4.831314563751221\n",
      "(64, 33)\n",
      "step 18945, loss is 4.755283832550049\n",
      "(64, 33)\n",
      "step 18946, loss is 4.812190055847168\n",
      "(64, 33)\n",
      "step 18947, loss is 4.797575950622559\n",
      "(64, 33)\n",
      "step 18948, loss is 4.739366054534912\n",
      "(64, 33)\n",
      "step 18949, loss is 4.842252731323242\n",
      "(64, 33)\n",
      "step 18950, loss is 4.859456539154053\n",
      "(64, 33)\n",
      "step 18951, loss is 4.980487823486328\n",
      "(64, 33)\n",
      "step 18952, loss is 4.846472263336182\n",
      "(64, 33)\n",
      "step 18953, loss is 4.704612731933594\n",
      "(64, 33)\n",
      "step 18954, loss is 4.793002128601074\n",
      "(64, 33)\n",
      "step 18955, loss is 4.786133289337158\n",
      "(64, 33)\n",
      "step 18956, loss is 4.753569602966309\n",
      "(64, 33)\n",
      "step 18957, loss is 4.837159156799316\n",
      "(64, 33)\n",
      "step 18958, loss is 4.712282657623291\n",
      "(64, 33)\n",
      "step 18959, loss is 4.679353713989258\n",
      "(64, 33)\n",
      "step 18960, loss is 4.825093746185303\n",
      "(64, 33)\n",
      "step 18961, loss is 4.91498327255249\n",
      "(64, 33)\n",
      "step 18962, loss is 4.812139511108398\n",
      "(64, 33)\n",
      "step 18963, loss is 4.893043518066406\n",
      "(64, 33)\n",
      "step 18964, loss is 4.805311679840088\n",
      "(64, 33)\n",
      "step 18965, loss is 4.675114631652832\n",
      "(64, 33)\n",
      "step 18966, loss is 4.856939792633057\n",
      "(64, 33)\n",
      "step 18967, loss is 4.745778560638428\n",
      "(64, 33)\n",
      "step 18968, loss is 4.940632343292236\n",
      "(64, 33)\n",
      "step 18969, loss is 4.669731616973877\n",
      "(64, 33)\n",
      "step 18970, loss is 4.840058326721191\n",
      "(64, 33)\n",
      "step 18971, loss is 4.795628547668457\n",
      "(64, 33)\n",
      "step 18972, loss is 4.955187797546387\n",
      "(64, 33)\n",
      "step 18973, loss is 4.862983226776123\n",
      "(64, 33)\n",
      "step 18974, loss is 4.612961292266846\n",
      "(64, 33)\n",
      "step 18975, loss is 4.8402862548828125\n",
      "(64, 33)\n",
      "step 18976, loss is 4.743613243103027\n",
      "(64, 33)\n",
      "step 18977, loss is 4.89723014831543\n",
      "(64, 33)\n",
      "step 18978, loss is 4.726807117462158\n",
      "(64, 33)\n",
      "step 18979, loss is 4.934886932373047\n",
      "(64, 33)\n",
      "step 18980, loss is 4.830934524536133\n",
      "(64, 33)\n",
      "step 18981, loss is 4.909687519073486\n",
      "(64, 33)\n",
      "step 18982, loss is 4.819728851318359\n",
      "(64, 33)\n",
      "step 18983, loss is 4.780999660491943\n",
      "(64, 33)\n",
      "step 18984, loss is 4.8213348388671875\n",
      "(64, 33)\n",
      "step 18985, loss is 4.874207496643066\n",
      "(64, 33)\n",
      "step 18986, loss is 4.936618804931641\n",
      "(64, 33)\n",
      "step 18987, loss is 4.771971702575684\n",
      "(64, 33)\n",
      "step 18988, loss is 4.642699241638184\n",
      "(64, 33)\n",
      "step 18989, loss is 4.829379081726074\n",
      "(64, 33)\n",
      "step 18990, loss is 4.840070724487305\n",
      "(64, 33)\n",
      "step 18991, loss is 4.911056041717529\n",
      "(64, 33)\n",
      "step 18992, loss is 4.719130992889404\n",
      "(64, 33)\n",
      "step 18993, loss is 4.657872676849365\n",
      "(64, 33)\n",
      "step 18994, loss is 4.8822808265686035\n",
      "(64, 33)\n",
      "step 18995, loss is 4.715872764587402\n",
      "(64, 33)\n",
      "step 18996, loss is 4.941459655761719\n",
      "(64, 33)\n",
      "step 18997, loss is 4.848836898803711\n",
      "(64, 33)\n",
      "step 18998, loss is 4.785926818847656\n",
      "(64, 33)\n",
      "step 18999, loss is 4.753799915313721\n",
      "(64, 33)\n",
      "step 19000, loss is 4.89138650894165\n",
      "(64, 33)\n",
      "step 19001, loss is 4.6175127029418945\n",
      "(64, 33)\n",
      "step 19002, loss is 4.661679744720459\n",
      "(64, 33)\n",
      "step 19003, loss is 4.874279499053955\n",
      "(64, 33)\n",
      "step 19004, loss is 4.642879486083984\n",
      "(64, 33)\n",
      "step 19005, loss is 4.861976146697998\n",
      "(64, 33)\n",
      "step 19006, loss is 4.947370529174805\n",
      "(64, 33)\n",
      "step 19007, loss is 4.71785831451416\n",
      "(64, 33)\n",
      "step 19008, loss is 4.711831569671631\n",
      "(64, 33)\n",
      "step 19009, loss is 4.9160685539245605\n",
      "(64, 33)\n",
      "step 19010, loss is 4.821113586425781\n",
      "(64, 33)\n",
      "step 19011, loss is 4.973718643188477\n",
      "(64, 33)\n",
      "step 19012, loss is 4.714964866638184\n",
      "(64, 33)\n",
      "step 19013, loss is 4.789024829864502\n",
      "(64, 33)\n",
      "step 19014, loss is 4.762334823608398\n",
      "(64, 33)\n",
      "step 19015, loss is 4.694253444671631\n",
      "(64, 33)\n",
      "step 19016, loss is 4.853600025177002\n",
      "(64, 33)\n",
      "step 19017, loss is 4.81001615524292\n",
      "(64, 33)\n",
      "step 19018, loss is 4.868766784667969\n",
      "(64, 33)\n",
      "step 19019, loss is 4.892606735229492\n",
      "(64, 33)\n",
      "step 19020, loss is 4.6771955490112305\n",
      "(64, 33)\n",
      "step 19021, loss is 4.821070671081543\n",
      "(64, 33)\n",
      "step 19022, loss is 4.790863513946533\n",
      "(64, 33)\n",
      "step 19023, loss is 4.918244361877441\n",
      "(64, 33)\n",
      "step 19024, loss is 4.865458011627197\n",
      "(64, 33)\n",
      "step 19025, loss is 4.813032627105713\n",
      "(64, 33)\n",
      "step 19026, loss is 4.815348148345947\n",
      "(64, 33)\n",
      "step 19027, loss is 4.852992534637451\n",
      "(64, 33)\n",
      "step 19028, loss is 4.843883991241455\n",
      "(64, 33)\n",
      "step 19029, loss is 4.713071346282959\n",
      "(64, 33)\n",
      "step 19030, loss is 4.863621234893799\n",
      "(64, 33)\n",
      "step 19031, loss is 4.808420658111572\n",
      "(64, 33)\n",
      "step 19032, loss is 4.804938316345215\n",
      "(64, 33)\n",
      "step 19033, loss is 4.889660358428955\n",
      "(64, 33)\n",
      "step 19034, loss is 4.852442741394043\n",
      "(64, 33)\n",
      "step 19035, loss is 4.6606059074401855\n",
      "(64, 33)\n",
      "step 19036, loss is 4.9824700355529785\n",
      "(64, 33)\n",
      "step 19037, loss is 4.979601860046387\n",
      "(64, 33)\n",
      "step 19038, loss is 4.6502275466918945\n",
      "(64, 33)\n",
      "step 19039, loss is 4.791449546813965\n",
      "(64, 33)\n",
      "step 19040, loss is 4.935805797576904\n",
      "(64, 33)\n",
      "step 19041, loss is 4.665369987487793\n",
      "(64, 33)\n",
      "step 19042, loss is 4.919442176818848\n",
      "(64, 33)\n",
      "step 19043, loss is 4.951992034912109\n",
      "(64, 33)\n",
      "step 19044, loss is 4.830060958862305\n",
      "(64, 33)\n",
      "step 19045, loss is 4.900150775909424\n",
      "(64, 33)\n",
      "step 19046, loss is 4.737346172332764\n",
      "(64, 33)\n",
      "step 19047, loss is 4.723769664764404\n",
      "(64, 33)\n",
      "step 19048, loss is 4.604168891906738\n",
      "(64, 33)\n",
      "step 19049, loss is 4.6442484855651855\n",
      "(64, 33)\n",
      "step 19050, loss is 4.965840816497803\n",
      "(64, 33)\n",
      "step 19051, loss is 4.799062252044678\n",
      "(64, 33)\n",
      "step 19052, loss is 4.759388446807861\n",
      "(64, 33)\n",
      "step 19053, loss is 4.773586273193359\n",
      "(64, 33)\n",
      "step 19054, loss is 4.95772123336792\n",
      "(64, 33)\n",
      "step 19055, loss is 4.95662784576416\n",
      "(64, 33)\n",
      "step 19056, loss is 4.888161659240723\n",
      "(64, 33)\n",
      "step 19057, loss is 4.788503646850586\n",
      "(64, 33)\n",
      "step 19058, loss is 4.825549602508545\n",
      "(64, 33)\n",
      "step 19059, loss is 4.793876647949219\n",
      "(64, 33)\n",
      "step 19060, loss is 4.835451126098633\n",
      "(64, 33)\n",
      "step 19061, loss is 4.796274662017822\n",
      "(64, 33)\n",
      "step 19062, loss is 4.866847991943359\n",
      "(64, 33)\n",
      "step 19063, loss is 4.604586601257324\n",
      "(64, 33)\n",
      "step 19064, loss is 4.740507125854492\n",
      "(64, 33)\n",
      "step 19065, loss is 4.775700569152832\n",
      "(64, 33)\n",
      "step 19066, loss is 4.859934329986572\n",
      "(64, 33)\n",
      "step 19067, loss is 4.6359543800354\n",
      "(64, 33)\n",
      "step 19068, loss is 4.8894362449646\n",
      "(64, 33)\n",
      "step 19069, loss is 4.753111362457275\n",
      "(64, 33)\n",
      "step 19070, loss is 4.892085075378418\n",
      "(64, 33)\n",
      "step 19071, loss is 4.8537797927856445\n",
      "(64, 33)\n",
      "step 19072, loss is 4.658964157104492\n",
      "(64, 33)\n",
      "step 19073, loss is 4.643144130706787\n",
      "(64, 33)\n",
      "step 19074, loss is 4.7789835929870605\n",
      "(64, 33)\n",
      "step 19075, loss is 4.662939071655273\n",
      "(64, 33)\n",
      "step 19076, loss is 4.836719036102295\n",
      "(64, 33)\n",
      "step 19077, loss is 4.746427059173584\n",
      "(64, 33)\n",
      "step 19078, loss is 4.752388000488281\n",
      "(64, 33)\n",
      "step 19079, loss is 4.862397193908691\n",
      "(64, 33)\n",
      "step 19080, loss is 4.904332637786865\n",
      "(64, 33)\n",
      "step 19081, loss is 4.9831624031066895\n",
      "(64, 33)\n",
      "step 19082, loss is 4.833280086517334\n",
      "(64, 33)\n",
      "step 19083, loss is 5.021766185760498\n",
      "(64, 33)\n",
      "step 19084, loss is 4.806852340698242\n",
      "(64, 33)\n",
      "step 19085, loss is 4.925611972808838\n",
      "(64, 33)\n",
      "step 19086, loss is 5.04287576675415\n",
      "(64, 33)\n",
      "step 19087, loss is 4.901337146759033\n",
      "(64, 33)\n",
      "step 19088, loss is 4.696176052093506\n",
      "(64, 33)\n",
      "step 19089, loss is 4.704042911529541\n",
      "(64, 33)\n",
      "step 19090, loss is 4.592737197875977\n",
      "(64, 33)\n",
      "step 19091, loss is 4.829152584075928\n",
      "(64, 33)\n",
      "step 19092, loss is 4.730703830718994\n",
      "(64, 33)\n",
      "step 19093, loss is 4.992764472961426\n",
      "(64, 33)\n",
      "step 19094, loss is 4.794738292694092\n",
      "(64, 33)\n",
      "step 19095, loss is 4.488713264465332\n",
      "(64, 33)\n",
      "step 19096, loss is 4.741368293762207\n",
      "(64, 33)\n",
      "step 19097, loss is 4.562383651733398\n",
      "(64, 33)\n",
      "step 19098, loss is 4.7152791023254395\n",
      "(64, 33)\n",
      "step 19099, loss is 4.674731731414795\n",
      "(64, 33)\n",
      "step 19100, loss is 4.8512163162231445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 19101, loss is 4.818859100341797\n",
      "(64, 33)\n",
      "step 19102, loss is 4.643389701843262\n",
      "(64, 33)\n",
      "step 19103, loss is 4.935451984405518\n",
      "(64, 33)\n",
      "step 19104, loss is 4.856441497802734\n",
      "(64, 33)\n",
      "step 19105, loss is 4.69639253616333\n",
      "(64, 33)\n",
      "step 19106, loss is 4.807072639465332\n",
      "(64, 33)\n",
      "step 19107, loss is 4.8611040115356445\n",
      "(64, 33)\n",
      "step 19108, loss is 4.914276123046875\n",
      "(64, 33)\n",
      "step 19109, loss is 4.772191047668457\n",
      "(64, 33)\n",
      "step 19110, loss is 4.7414374351501465\n",
      "(64, 33)\n",
      "step 19111, loss is 4.9311089515686035\n",
      "(64, 33)\n",
      "step 19112, loss is 4.7761406898498535\n",
      "(64, 33)\n",
      "step 19113, loss is 4.868307113647461\n",
      "(64, 33)\n",
      "step 19114, loss is 4.846874713897705\n",
      "(64, 33)\n",
      "step 19115, loss is 4.747588157653809\n",
      "(64, 33)\n",
      "step 19116, loss is 4.917914390563965\n",
      "(64, 33)\n",
      "step 19117, loss is 5.035522937774658\n",
      "(64, 33)\n",
      "step 19118, loss is 4.728235244750977\n",
      "(64, 33)\n",
      "step 19119, loss is 4.680078506469727\n",
      "(64, 33)\n",
      "step 19120, loss is 4.807427883148193\n",
      "(64, 33)\n",
      "step 19121, loss is 4.739632606506348\n",
      "(64, 33)\n",
      "step 19122, loss is 4.684225082397461\n",
      "(64, 33)\n",
      "step 19123, loss is 4.63547945022583\n",
      "(64, 33)\n",
      "step 19124, loss is 4.5624918937683105\n",
      "(64, 33)\n",
      "step 19125, loss is 4.658609867095947\n",
      "(64, 33)\n",
      "step 19126, loss is 4.7825751304626465\n",
      "(64, 33)\n",
      "step 19127, loss is 4.619269371032715\n",
      "(64, 33)\n",
      "step 19128, loss is 4.712465286254883\n",
      "(64, 33)\n",
      "step 19129, loss is 4.675898551940918\n",
      "(64, 33)\n",
      "step 19130, loss is 4.82498836517334\n",
      "(64, 33)\n",
      "step 19131, loss is 4.791518688201904\n",
      "(64, 33)\n",
      "step 19132, loss is 4.6608662605285645\n",
      "(64, 33)\n",
      "step 19133, loss is 4.921241760253906\n",
      "(64, 33)\n",
      "step 19134, loss is 4.811004161834717\n",
      "(64, 33)\n",
      "step 19135, loss is 4.970778942108154\n",
      "(64, 33)\n",
      "step 19136, loss is 4.871650695800781\n",
      "(64, 33)\n",
      "step 19137, loss is 4.87381649017334\n",
      "(64, 33)\n",
      "step 19138, loss is 4.807793140411377\n",
      "(64, 33)\n",
      "step 19139, loss is 4.821182727813721\n",
      "(64, 33)\n",
      "step 19140, loss is 4.785714149475098\n",
      "(64, 33)\n",
      "step 19141, loss is 4.735567092895508\n",
      "(64, 33)\n",
      "step 19142, loss is 4.7164106369018555\n",
      "(64, 33)\n",
      "step 19143, loss is 4.60001802444458\n",
      "(64, 33)\n",
      "step 19144, loss is 4.484831809997559\n",
      "(64, 33)\n",
      "step 19145, loss is 4.806600570678711\n",
      "(64, 33)\n",
      "step 19146, loss is 4.670356273651123\n",
      "(64, 33)\n",
      "step 19147, loss is 4.818643093109131\n",
      "(64, 33)\n",
      "step 19148, loss is 4.866074085235596\n",
      "(64, 33)\n",
      "step 19149, loss is 4.763440132141113\n",
      "(64, 33)\n",
      "step 19150, loss is 4.551712989807129\n",
      "(64, 33)\n",
      "step 19151, loss is 4.715813159942627\n",
      "(64, 33)\n",
      "step 19152, loss is 4.818869113922119\n",
      "(64, 33)\n",
      "step 19153, loss is 4.644357681274414\n",
      "(64, 33)\n",
      "step 19154, loss is 4.833023548126221\n",
      "(64, 33)\n",
      "step 19155, loss is 4.664079189300537\n",
      "(64, 33)\n",
      "step 19156, loss is 4.838837623596191\n",
      "(64, 33)\n",
      "step 19157, loss is 4.914298057556152\n",
      "(64, 33)\n",
      "step 19158, loss is 4.834043502807617\n",
      "(64, 33)\n",
      "step 19159, loss is 4.863322734832764\n",
      "(64, 33)\n",
      "step 19160, loss is 4.7186126708984375\n",
      "(64, 33)\n",
      "step 19161, loss is 4.671751022338867\n",
      "(64, 33)\n",
      "step 19162, loss is 4.815241813659668\n",
      "(64, 33)\n",
      "step 19163, loss is 4.624327659606934\n",
      "(64, 33)\n",
      "step 19164, loss is 4.835188388824463\n",
      "(64, 33)\n",
      "step 19165, loss is 4.6442952156066895\n",
      "(64, 33)\n",
      "step 19166, loss is 4.851247787475586\n",
      "(64, 33)\n",
      "step 19167, loss is 4.70078706741333\n",
      "(64, 33)\n",
      "step 19168, loss is 4.810609817504883\n",
      "(64, 33)\n",
      "step 19169, loss is 4.907111167907715\n",
      "(64, 33)\n",
      "step 19170, loss is 4.769649505615234\n",
      "(64, 33)\n",
      "step 19171, loss is 4.730743408203125\n",
      "(64, 33)\n",
      "step 19172, loss is 4.736116886138916\n",
      "(64, 33)\n",
      "step 19173, loss is 4.838779926300049\n",
      "(64, 33)\n",
      "step 19174, loss is 4.861830711364746\n",
      "(64, 33)\n",
      "step 19175, loss is 4.790234565734863\n",
      "(64, 33)\n",
      "step 19176, loss is 4.673795700073242\n",
      "(64, 33)\n",
      "step 19177, loss is 4.4966206550598145\n",
      "(64, 33)\n",
      "step 19178, loss is 5.007521629333496\n",
      "(64, 33)\n",
      "step 19179, loss is 4.878938674926758\n",
      "(64, 33)\n",
      "step 19180, loss is 4.63067102432251\n",
      "(64, 33)\n",
      "step 19181, loss is 4.887057781219482\n",
      "(64, 33)\n",
      "step 19182, loss is 4.657515048980713\n",
      "(64, 33)\n",
      "step 19183, loss is 4.834316253662109\n",
      "(64, 33)\n",
      "step 19184, loss is 4.740381240844727\n",
      "(64, 33)\n",
      "step 19185, loss is 4.785345554351807\n",
      "(64, 33)\n",
      "step 19186, loss is 4.718597888946533\n",
      "(64, 33)\n",
      "step 19187, loss is 4.871393203735352\n",
      "(64, 33)\n",
      "step 19188, loss is 4.822701930999756\n",
      "(64, 33)\n",
      "step 19189, loss is 4.886054039001465\n",
      "(64, 33)\n",
      "step 19190, loss is 4.734074592590332\n",
      "(64, 33)\n",
      "step 19191, loss is 4.806181907653809\n",
      "(64, 33)\n",
      "step 19192, loss is 4.699423789978027\n",
      "(64, 33)\n",
      "step 19193, loss is 4.737415790557861\n",
      "(64, 33)\n",
      "step 19194, loss is 4.8501105308532715\n",
      "(64, 33)\n",
      "step 19195, loss is 4.810572624206543\n",
      "(64, 33)\n",
      "step 19196, loss is 4.701617240905762\n",
      "(64, 33)\n",
      "step 19197, loss is 4.837106227874756\n",
      "(64, 33)\n",
      "step 19198, loss is 4.619851589202881\n",
      "(64, 33)\n",
      "step 19199, loss is 4.561718463897705\n",
      "(64, 33)\n",
      "step 19200, loss is 4.813026428222656\n",
      "(64, 33)\n",
      "step 19201, loss is 4.787930011749268\n",
      "(64, 33)\n",
      "step 19202, loss is 4.824152946472168\n",
      "(64, 33)\n",
      "step 19203, loss is 4.599928855895996\n",
      "(64, 33)\n",
      "step 19204, loss is 4.779502868652344\n",
      "(64, 33)\n",
      "step 19205, loss is 4.711063861846924\n",
      "(64, 33)\n",
      "step 19206, loss is 4.577305793762207\n",
      "(64, 33)\n",
      "step 19207, loss is 4.665948867797852\n",
      "(64, 33)\n",
      "step 19208, loss is 4.805503845214844\n",
      "(64, 33)\n",
      "step 19209, loss is 4.797476291656494\n",
      "(64, 33)\n",
      "step 19210, loss is 4.772258758544922\n",
      "(64, 33)\n",
      "step 19211, loss is 4.667667388916016\n",
      "(64, 33)\n",
      "step 19212, loss is 4.710846900939941\n",
      "(64, 33)\n",
      "step 19213, loss is 4.863277912139893\n",
      "(64, 33)\n",
      "step 19214, loss is 4.771636009216309\n",
      "(64, 33)\n",
      "step 19215, loss is 4.761451244354248\n",
      "(64, 33)\n",
      "step 19216, loss is 4.686197757720947\n",
      "(64, 33)\n",
      "step 19217, loss is 4.817785263061523\n",
      "(64, 33)\n",
      "step 19218, loss is 4.709550380706787\n",
      "(64, 33)\n",
      "step 19219, loss is 4.696348667144775\n",
      "(64, 33)\n",
      "step 19220, loss is 4.7359161376953125\n",
      "(64, 33)\n",
      "step 19221, loss is 4.698114395141602\n",
      "(64, 33)\n",
      "step 19222, loss is 4.883894443511963\n",
      "(64, 33)\n",
      "step 19223, loss is 4.655274868011475\n",
      "(64, 33)\n",
      "step 19224, loss is 4.855426788330078\n",
      "(64, 33)\n",
      "step 19225, loss is 4.803703308105469\n",
      "(64, 33)\n",
      "step 19226, loss is 4.804790019989014\n",
      "(64, 33)\n",
      "step 19227, loss is 4.788036823272705\n",
      "(64, 33)\n",
      "step 19228, loss is 4.687013626098633\n",
      "(64, 33)\n",
      "step 19229, loss is 4.859137058258057\n",
      "(64, 33)\n",
      "step 19230, loss is 4.846114635467529\n",
      "(64, 33)\n",
      "step 19231, loss is 4.788703441619873\n",
      "(64, 33)\n",
      "step 19232, loss is 4.773360252380371\n",
      "(64, 33)\n",
      "step 19233, loss is 4.972434997558594\n",
      "(64, 33)\n",
      "step 19234, loss is 4.836699485778809\n",
      "(64, 33)\n",
      "step 19235, loss is 4.77155876159668\n",
      "(64, 33)\n",
      "step 19236, loss is 4.7519941329956055\n",
      "(64, 33)\n",
      "step 19237, loss is 4.779178619384766\n",
      "(64, 33)\n",
      "step 19238, loss is 4.654768466949463\n",
      "(64, 33)\n",
      "step 19239, loss is 4.8916544914245605\n",
      "(64, 33)\n",
      "step 19240, loss is 4.651407241821289\n",
      "(64, 33)\n",
      "step 19241, loss is 4.8330488204956055\n",
      "(64, 33)\n",
      "step 19242, loss is 4.861857891082764\n",
      "(64, 33)\n",
      "step 19243, loss is 4.943634986877441\n",
      "(64, 33)\n",
      "step 19244, loss is 4.745798587799072\n",
      "(64, 33)\n",
      "step 19245, loss is 4.751232624053955\n",
      "(64, 33)\n",
      "step 19246, loss is 4.725167751312256\n",
      "(64, 33)\n",
      "step 19247, loss is 4.93147087097168\n",
      "(64, 33)\n",
      "step 19248, loss is 4.762445449829102\n",
      "(64, 33)\n",
      "step 19249, loss is 4.861787796020508\n",
      "(64, 33)\n",
      "step 19250, loss is 4.820448875427246\n",
      "(64, 33)\n",
      "step 19251, loss is 4.905943870544434\n",
      "(64, 33)\n",
      "step 19252, loss is 4.800472736358643\n",
      "(64, 33)\n",
      "step 19253, loss is 4.825547695159912\n",
      "(64, 33)\n",
      "step 19254, loss is 4.738705158233643\n",
      "(64, 33)\n",
      "step 19255, loss is 4.809384822845459\n",
      "(64, 33)\n",
      "step 19256, loss is 4.8107590675354\n",
      "(64, 33)\n",
      "step 19257, loss is 4.838089942932129\n",
      "(64, 33)\n",
      "step 19258, loss is 4.862860679626465\n",
      "(64, 33)\n",
      "step 19259, loss is 4.767634391784668\n",
      "(64, 33)\n",
      "step 19260, loss is 4.65498161315918\n",
      "(64, 33)\n",
      "step 19261, loss is 4.842092990875244\n",
      "(64, 33)\n",
      "step 19262, loss is 4.646269798278809\n",
      "(64, 33)\n",
      "step 19263, loss is 4.907447814941406\n",
      "(64, 33)\n",
      "step 19264, loss is 4.779361724853516\n",
      "(64, 33)\n",
      "step 19265, loss is 4.980625152587891\n",
      "(64, 33)\n",
      "step 19266, loss is 4.909226894378662\n",
      "(64, 33)\n",
      "step 19267, loss is 4.921313762664795\n",
      "(64, 33)\n",
      "step 19268, loss is 4.845808029174805\n",
      "(64, 33)\n",
      "step 19269, loss is 4.686679363250732\n",
      "(64, 33)\n",
      "step 19270, loss is 4.707437038421631\n",
      "(64, 33)\n",
      "step 19271, loss is 4.912126064300537\n",
      "(64, 33)\n",
      "step 19272, loss is 4.798327922821045\n",
      "(64, 33)\n",
      "step 19273, loss is 4.759155750274658\n",
      "(64, 33)\n",
      "step 19274, loss is 4.567299842834473\n",
      "(64, 33)\n",
      "step 19275, loss is 4.8835320472717285\n",
      "(64, 33)\n",
      "step 19276, loss is 4.6219706535339355\n",
      "(64, 33)\n",
      "step 19277, loss is 4.506213665008545\n",
      "(64, 33)\n",
      "step 19278, loss is 4.774155616760254\n",
      "(64, 33)\n",
      "step 19279, loss is 4.773509979248047\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19280, loss is 4.805140018463135\n",
      "(64, 33)\n",
      "step 19281, loss is 4.797563552856445\n",
      "(64, 33)\n",
      "step 19282, loss is 4.927340507507324\n",
      "(64, 33)\n",
      "step 19283, loss is 4.982860088348389\n",
      "(64, 33)\n",
      "step 19284, loss is 4.860823154449463\n",
      "(64, 33)\n",
      "step 19285, loss is 4.760983943939209\n",
      "(64, 33)\n",
      "step 19286, loss is 4.792671203613281\n",
      "(64, 33)\n",
      "step 19287, loss is 4.697898864746094\n",
      "(64, 33)\n",
      "step 19288, loss is 4.721477508544922\n",
      "(64, 33)\n",
      "step 19289, loss is 4.896864891052246\n",
      "(64, 33)\n",
      "step 19290, loss is 4.93315315246582\n",
      "(64, 33)\n",
      "step 19291, loss is 4.641176223754883\n",
      "(64, 33)\n",
      "step 19292, loss is 4.63657283782959\n",
      "(64, 33)\n",
      "step 19293, loss is 4.8371124267578125\n",
      "(64, 33)\n",
      "step 19294, loss is 4.8254828453063965\n",
      "(64, 33)\n",
      "step 19295, loss is 4.917270660400391\n",
      "(64, 33)\n",
      "step 19296, loss is 4.762068748474121\n",
      "(64, 33)\n",
      "step 19297, loss is 4.771797180175781\n",
      "(64, 33)\n",
      "step 19298, loss is 4.747570514678955\n",
      "(64, 33)\n",
      "step 19299, loss is 4.606045722961426\n",
      "(64, 33)\n",
      "step 19300, loss is 4.921555995941162\n",
      "(64, 33)\n",
      "step 19301, loss is 4.655632972717285\n",
      "(64, 33)\n",
      "step 19302, loss is 4.834163188934326\n",
      "(64, 33)\n",
      "step 19303, loss is 4.8065314292907715\n",
      "(64, 33)\n",
      "step 19304, loss is 4.8399338722229\n",
      "(64, 33)\n",
      "step 19305, loss is 4.654209136962891\n",
      "(64, 33)\n",
      "step 19306, loss is 4.768584251403809\n",
      "(64, 33)\n",
      "step 19307, loss is 4.732599258422852\n",
      "(64, 33)\n",
      "step 19308, loss is 4.678009986877441\n",
      "(64, 33)\n",
      "step 19309, loss is 4.790811061859131\n",
      "(64, 33)\n",
      "step 19310, loss is 4.807891845703125\n",
      "(64, 33)\n",
      "step 19311, loss is 4.916103839874268\n",
      "(64, 33)\n",
      "step 19312, loss is 4.802876949310303\n",
      "(64, 33)\n",
      "step 19313, loss is 4.906737804412842\n",
      "(64, 33)\n",
      "step 19314, loss is 4.440712928771973\n",
      "(64, 33)\n",
      "step 19315, loss is 4.798273086547852\n",
      "(64, 33)\n",
      "step 19316, loss is 4.73660945892334\n",
      "(64, 33)\n",
      "step 19317, loss is 4.753322601318359\n",
      "(64, 33)\n",
      "step 19318, loss is 4.696619510650635\n",
      "(64, 33)\n",
      "step 19319, loss is 4.701390743255615\n",
      "(64, 33)\n",
      "step 19320, loss is 4.723519802093506\n",
      "(64, 33)\n",
      "step 19321, loss is 4.819688320159912\n",
      "(64, 33)\n",
      "step 19322, loss is 4.871400356292725\n",
      "(64, 33)\n",
      "step 19323, loss is 4.7744364738464355\n",
      "(64, 33)\n",
      "step 19324, loss is 4.785096168518066\n",
      "(64, 33)\n",
      "step 19325, loss is 4.663485527038574\n",
      "(64, 33)\n",
      "step 19326, loss is 4.77252197265625\n",
      "(64, 33)\n",
      "step 19327, loss is 5.061016082763672\n",
      "(64, 33)\n",
      "step 19328, loss is 4.718912124633789\n",
      "(64, 33)\n",
      "step 19329, loss is 4.872012138366699\n",
      "(64, 33)\n",
      "step 19330, loss is 4.8842363357543945\n",
      "(64, 33)\n",
      "step 19331, loss is 4.678045272827148\n",
      "(64, 33)\n",
      "step 19332, loss is 4.907258987426758\n",
      "(64, 33)\n",
      "step 19333, loss is 4.80404806137085\n",
      "(64, 33)\n",
      "step 19334, loss is 4.932406902313232\n",
      "(64, 33)\n",
      "step 19335, loss is 4.837374210357666\n",
      "(64, 33)\n",
      "step 19336, loss is 4.7876200675964355\n",
      "(64, 33)\n",
      "step 19337, loss is 4.8056769371032715\n",
      "(64, 33)\n",
      "step 19338, loss is 4.750131607055664\n",
      "(64, 33)\n",
      "step 19339, loss is 4.7798309326171875\n",
      "(64, 33)\n",
      "step 19340, loss is 4.717977523803711\n",
      "(64, 33)\n",
      "step 19341, loss is 4.763133525848389\n",
      "(64, 33)\n",
      "step 19342, loss is 4.789746284484863\n",
      "(64, 33)\n",
      "step 19343, loss is 4.909678936004639\n",
      "(64, 33)\n",
      "step 19344, loss is 4.818574905395508\n",
      "(64, 33)\n",
      "step 19345, loss is 4.621167182922363\n",
      "(64, 33)\n",
      "step 19346, loss is 4.810248374938965\n",
      "(64, 33)\n",
      "step 19347, loss is 4.844602584838867\n",
      "(64, 33)\n",
      "step 19348, loss is 4.716599941253662\n",
      "(64, 33)\n",
      "step 19349, loss is 4.7084126472473145\n",
      "(64, 33)\n",
      "step 19350, loss is 4.646888732910156\n",
      "(64, 33)\n",
      "step 19351, loss is 4.733537197113037\n",
      "(64, 33)\n",
      "step 19352, loss is 4.718236446380615\n",
      "(64, 33)\n",
      "step 19353, loss is 4.882823467254639\n",
      "(64, 33)\n",
      "step 19354, loss is 4.676756381988525\n",
      "(64, 33)\n",
      "step 19355, loss is 4.625096321105957\n",
      "(64, 33)\n",
      "step 19356, loss is 4.7721757888793945\n",
      "(64, 33)\n",
      "step 19357, loss is 4.87416934967041\n",
      "(64, 33)\n",
      "step 19358, loss is 4.827391624450684\n",
      "(64, 33)\n",
      "step 19359, loss is 4.7041401863098145\n",
      "(64, 33)\n",
      "step 19360, loss is 4.877250671386719\n",
      "(64, 33)\n",
      "step 19361, loss is 4.801589488983154\n",
      "(64, 33)\n",
      "step 19362, loss is 4.566713333129883\n",
      "(64, 33)\n",
      "step 19363, loss is 4.715948581695557\n",
      "(64, 33)\n",
      "step 19364, loss is 4.917383193969727\n",
      "(64, 33)\n",
      "step 19365, loss is 4.93654727935791\n",
      "(64, 33)\n",
      "step 19366, loss is 4.87595272064209\n",
      "(64, 33)\n",
      "step 19367, loss is 4.715507507324219\n",
      "(64, 33)\n",
      "step 19368, loss is 4.967075824737549\n",
      "(64, 33)\n",
      "step 19369, loss is 4.770251750946045\n",
      "(64, 33)\n",
      "step 19370, loss is 4.923343658447266\n",
      "(64, 33)\n",
      "step 19371, loss is 4.848249912261963\n",
      "(64, 33)\n",
      "step 19372, loss is 4.717947483062744\n",
      "(64, 33)\n",
      "step 19373, loss is 4.819633483886719\n",
      "(64, 33)\n",
      "step 19374, loss is 4.747925758361816\n",
      "(64, 33)\n",
      "step 19375, loss is 4.844740867614746\n",
      "(64, 33)\n",
      "step 19376, loss is 4.915014743804932\n",
      "(64, 33)\n",
      "step 19377, loss is 4.7485198974609375\n",
      "(64, 33)\n",
      "step 19378, loss is 4.778823375701904\n",
      "(64, 33)\n",
      "step 19379, loss is 4.810190677642822\n",
      "(64, 33)\n",
      "step 19380, loss is 4.8268609046936035\n",
      "(64, 33)\n",
      "step 19381, loss is 4.8981852531433105\n",
      "(64, 33)\n",
      "step 19382, loss is 4.817578315734863\n",
      "(64, 33)\n",
      "step 19383, loss is 4.787727355957031\n",
      "(64, 33)\n",
      "step 19384, loss is 4.774806976318359\n",
      "(64, 33)\n",
      "step 19385, loss is 4.944769382476807\n",
      "(64, 33)\n",
      "step 19386, loss is 4.798562049865723\n",
      "(64, 33)\n",
      "step 19387, loss is 4.735682964324951\n",
      "(64, 33)\n",
      "step 19388, loss is 4.861024856567383\n",
      "(64, 33)\n",
      "step 19389, loss is 4.852250099182129\n",
      "(64, 33)\n",
      "step 19390, loss is 4.726213455200195\n",
      "(64, 33)\n",
      "step 19391, loss is 4.788321495056152\n",
      "(64, 33)\n",
      "step 19392, loss is 4.720350742340088\n",
      "(64, 33)\n",
      "step 19393, loss is 4.9201555252075195\n",
      "(64, 33)\n",
      "step 19394, loss is 4.689906120300293\n",
      "(64, 33)\n",
      "step 19395, loss is 4.793964862823486\n",
      "(64, 33)\n",
      "step 19396, loss is 4.6963582038879395\n",
      "(64, 33)\n",
      "step 19397, loss is 4.562990665435791\n",
      "(64, 33)\n",
      "step 19398, loss is 4.851407527923584\n",
      "(64, 33)\n",
      "step 19399, loss is 4.628835678100586\n",
      "(64, 33)\n",
      "step 19400, loss is 4.779322624206543\n",
      "(64, 33)\n",
      "step 19401, loss is 4.62376070022583\n",
      "(64, 33)\n",
      "step 19402, loss is 4.987447738647461\n",
      "(64, 33)\n",
      "step 19403, loss is 4.713916778564453\n",
      "(64, 33)\n",
      "step 19404, loss is 4.914381980895996\n",
      "(64, 33)\n",
      "step 19405, loss is 4.634767532348633\n",
      "(64, 33)\n",
      "step 19406, loss is 4.787300109863281\n",
      "(64, 33)\n",
      "step 19407, loss is 4.75545597076416\n",
      "(64, 33)\n",
      "step 19408, loss is 4.750438213348389\n",
      "(64, 33)\n",
      "step 19409, loss is 4.689357757568359\n",
      "(64, 33)\n",
      "step 19410, loss is 4.8011474609375\n",
      "(64, 33)\n",
      "step 19411, loss is 4.872250080108643\n",
      "(64, 33)\n",
      "step 19412, loss is 4.627519607543945\n",
      "(64, 33)\n",
      "step 19413, loss is 4.935480117797852\n",
      "(64, 33)\n",
      "step 19414, loss is 4.724876880645752\n",
      "(64, 33)\n",
      "step 19415, loss is 4.729790210723877\n",
      "(64, 33)\n",
      "step 19416, loss is 4.938900470733643\n",
      "(64, 33)\n",
      "step 19417, loss is 4.82155179977417\n",
      "(64, 33)\n",
      "step 19418, loss is 4.778409004211426\n",
      "(64, 33)\n",
      "step 19419, loss is 4.617900848388672\n",
      "(64, 33)\n",
      "step 19420, loss is 4.872618675231934\n",
      "(64, 33)\n",
      "step 19421, loss is 4.835122108459473\n",
      "(64, 33)\n",
      "step 19422, loss is 4.836067199707031\n",
      "(64, 33)\n",
      "step 19423, loss is 4.735963821411133\n",
      "(64, 33)\n",
      "step 19424, loss is 4.745964527130127\n",
      "(64, 33)\n",
      "step 19425, loss is 4.629139423370361\n",
      "(64, 33)\n",
      "step 19426, loss is 4.77122163772583\n",
      "(64, 33)\n",
      "step 19427, loss is 4.7816901206970215\n",
      "(64, 33)\n",
      "step 19428, loss is 4.958317756652832\n",
      "(64, 33)\n",
      "step 19429, loss is 4.769323348999023\n",
      "(64, 33)\n",
      "step 19430, loss is 4.835259437561035\n",
      "(64, 33)\n",
      "step 19431, loss is 4.672660827636719\n",
      "(64, 33)\n",
      "step 19432, loss is 4.851426601409912\n",
      "(64, 33)\n",
      "step 19433, loss is 4.771543979644775\n",
      "(64, 33)\n",
      "step 19434, loss is 4.892965316772461\n",
      "(64, 33)\n",
      "step 19435, loss is 4.833237171173096\n",
      "(64, 33)\n",
      "step 19436, loss is 4.771873474121094\n",
      "(64, 33)\n",
      "step 19437, loss is 4.8051438331604\n",
      "(64, 33)\n",
      "step 19438, loss is 4.520975589752197\n",
      "(64, 33)\n",
      "step 19439, loss is 4.870811462402344\n",
      "(64, 33)\n",
      "step 19440, loss is 4.7456440925598145\n",
      "(64, 33)\n",
      "step 19441, loss is 4.882032871246338\n",
      "(64, 33)\n",
      "step 19442, loss is 4.6270880699157715\n",
      "(64, 33)\n",
      "step 19443, loss is 4.900265693664551\n",
      "(64, 33)\n",
      "step 19444, loss is 4.716034889221191\n",
      "(64, 33)\n",
      "step 19445, loss is 4.918271064758301\n",
      "(64, 33)\n",
      "step 19446, loss is 4.6473846435546875\n",
      "(64, 33)\n",
      "step 19447, loss is 4.848027229309082\n",
      "(64, 33)\n",
      "step 19448, loss is 4.564876079559326\n",
      "(64, 33)\n",
      "step 19449, loss is 4.737531661987305\n",
      "(64, 33)\n",
      "step 19450, loss is 4.808760643005371\n",
      "(64, 33)\n",
      "step 19451, loss is 4.852447509765625\n",
      "(64, 33)\n",
      "step 19452, loss is 4.793754577636719\n",
      "(64, 33)\n",
      "step 19453, loss is 4.807950973510742\n",
      "(64, 33)\n",
      "step 19454, loss is 4.801237106323242\n",
      "(64, 33)\n",
      "step 19455, loss is 4.77420711517334\n",
      "(64, 33)\n",
      "step 19456, loss is 4.814561367034912\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19457, loss is 4.5702338218688965\n",
      "(64, 33)\n",
      "step 19458, loss is 4.79221773147583\n",
      "(64, 33)\n",
      "step 19459, loss is 4.816985607147217\n",
      "(64, 33)\n",
      "step 19460, loss is 4.747793674468994\n",
      "(64, 33)\n",
      "step 19461, loss is 4.853829860687256\n",
      "(64, 33)\n",
      "step 19462, loss is 4.697482109069824\n",
      "(64, 33)\n",
      "step 19463, loss is 4.637489318847656\n",
      "(64, 33)\n",
      "step 19464, loss is 4.7371697425842285\n",
      "(64, 33)\n",
      "step 19465, loss is 5.06695556640625\n",
      "(64, 33)\n",
      "step 19466, loss is 4.817830562591553\n",
      "(64, 33)\n",
      "step 19467, loss is 4.811365604400635\n",
      "(64, 33)\n",
      "step 19468, loss is 4.522106647491455\n",
      "(64, 33)\n",
      "step 19469, loss is 5.049096584320068\n",
      "(64, 33)\n",
      "step 19470, loss is 4.713466644287109\n",
      "(64, 33)\n",
      "step 19471, loss is 4.9262776374816895\n",
      "(64, 33)\n",
      "step 19472, loss is 4.775273323059082\n",
      "(64, 33)\n",
      "step 19473, loss is 4.993288993835449\n",
      "(64, 33)\n",
      "step 19474, loss is 4.821284770965576\n",
      "(64, 33)\n",
      "step 19475, loss is 4.754875183105469\n",
      "(64, 33)\n",
      "step 19476, loss is 4.852975368499756\n",
      "(64, 33)\n",
      "step 19477, loss is 4.53091287612915\n",
      "(64, 33)\n",
      "step 19478, loss is 4.6591973304748535\n",
      "(64, 33)\n",
      "step 19479, loss is 4.768998622894287\n",
      "(64, 33)\n",
      "step 19480, loss is 4.770395278930664\n",
      "(64, 33)\n",
      "step 19481, loss is 4.686863899230957\n",
      "(64, 33)\n",
      "step 19482, loss is 4.717398643493652\n",
      "(64, 33)\n",
      "step 19483, loss is 4.832967281341553\n",
      "(64, 33)\n",
      "step 19484, loss is 4.746100425720215\n",
      "(64, 33)\n",
      "step 19485, loss is 4.756980895996094\n",
      "(64, 33)\n",
      "step 19486, loss is 4.6765971183776855\n",
      "(64, 33)\n",
      "step 19487, loss is 4.7782511711120605\n",
      "(64, 33)\n",
      "step 19488, loss is 4.763851165771484\n",
      "(64, 33)\n",
      "step 19489, loss is 4.703658580780029\n",
      "(64, 33)\n",
      "step 19490, loss is 4.590182781219482\n",
      "(64, 33)\n",
      "step 19491, loss is 4.857795715332031\n",
      "(64, 33)\n",
      "step 19492, loss is 4.872302532196045\n",
      "(64, 33)\n",
      "step 19493, loss is 4.7756547927856445\n",
      "(64, 33)\n",
      "step 19494, loss is 4.814573764801025\n",
      "(64, 33)\n",
      "step 19495, loss is 4.77321195602417\n",
      "(64, 33)\n",
      "step 19496, loss is 4.782660961151123\n",
      "(64, 33)\n",
      "step 19497, loss is 4.786718368530273\n",
      "(64, 33)\n",
      "step 19498, loss is 4.831033229827881\n",
      "(64, 33)\n",
      "step 19499, loss is 4.885707855224609\n",
      "(64, 33)\n",
      "step 19500, loss is 4.773296356201172\n",
      "(64, 33)\n",
      "step 19501, loss is 4.96024751663208\n",
      "(64, 33)\n",
      "step 19502, loss is 4.824354648590088\n",
      "(64, 33)\n",
      "step 19503, loss is 4.581543922424316\n",
      "(64, 33)\n",
      "step 19504, loss is 4.826066970825195\n",
      "(64, 33)\n",
      "step 19505, loss is 4.921627044677734\n",
      "(64, 33)\n",
      "step 19506, loss is 4.70818567276001\n",
      "(64, 33)\n",
      "step 19507, loss is 4.827319622039795\n",
      "(64, 33)\n",
      "step 19508, loss is 4.789172172546387\n",
      "(64, 33)\n",
      "step 19509, loss is 4.784256458282471\n",
      "(64, 33)\n",
      "step 19510, loss is 4.581838130950928\n",
      "(64, 33)\n",
      "step 19511, loss is 4.86846399307251\n",
      "(64, 33)\n",
      "step 19512, loss is 4.801088809967041\n",
      "(64, 33)\n",
      "step 19513, loss is 4.78379487991333\n",
      "(64, 33)\n",
      "step 19514, loss is 4.683166980743408\n",
      "(64, 33)\n",
      "step 19515, loss is 4.739792346954346\n",
      "(64, 33)\n",
      "step 19516, loss is 4.702341079711914\n",
      "(64, 33)\n",
      "step 19517, loss is 4.824286460876465\n",
      "(64, 33)\n",
      "step 19518, loss is 4.629994869232178\n",
      "(64, 33)\n",
      "step 19519, loss is 4.985629081726074\n",
      "(64, 33)\n",
      "step 19520, loss is 4.721497058868408\n",
      "(64, 33)\n",
      "step 19521, loss is 4.937483787536621\n",
      "(64, 33)\n",
      "step 19522, loss is 4.856995105743408\n",
      "(64, 33)\n",
      "step 19523, loss is 4.7850751876831055\n",
      "(64, 33)\n",
      "step 19524, loss is 4.791958808898926\n",
      "(64, 33)\n",
      "step 19525, loss is 4.872165679931641\n",
      "(64, 33)\n",
      "step 19526, loss is 4.646456241607666\n",
      "(64, 33)\n",
      "step 19527, loss is 4.879599571228027\n",
      "(64, 33)\n",
      "step 19528, loss is 4.818604469299316\n",
      "(64, 33)\n",
      "step 19529, loss is 4.984292030334473\n",
      "(64, 33)\n",
      "step 19530, loss is 4.698267459869385\n",
      "(64, 33)\n",
      "step 19531, loss is 4.803050994873047\n",
      "(64, 33)\n",
      "step 19532, loss is 4.806314945220947\n",
      "(64, 33)\n",
      "step 19533, loss is 4.949316501617432\n",
      "(64, 33)\n",
      "step 19534, loss is 4.848621368408203\n",
      "(64, 33)\n",
      "step 19535, loss is 4.726753234863281\n",
      "(64, 33)\n",
      "step 19536, loss is 4.761610984802246\n",
      "(64, 33)\n",
      "step 19537, loss is 4.827779293060303\n",
      "(64, 33)\n",
      "step 19538, loss is 4.722663879394531\n",
      "(64, 33)\n",
      "step 19539, loss is 4.797703266143799\n",
      "(64, 33)\n",
      "step 19540, loss is 4.861796855926514\n",
      "(64, 33)\n",
      "step 19541, loss is 4.776697158813477\n",
      "(64, 33)\n",
      "step 19542, loss is 4.675861835479736\n",
      "(64, 33)\n",
      "step 19543, loss is 4.950838565826416\n",
      "(64, 33)\n",
      "step 19544, loss is 5.024942874908447\n",
      "(64, 33)\n",
      "step 19545, loss is 4.64857292175293\n",
      "(64, 33)\n",
      "step 19546, loss is 4.816792011260986\n",
      "(64, 33)\n",
      "step 19547, loss is 4.687654495239258\n",
      "(64, 33)\n",
      "step 19548, loss is 4.823834419250488\n",
      "(64, 33)\n",
      "step 19549, loss is 4.926773548126221\n",
      "(64, 33)\n",
      "step 19550, loss is 4.834415912628174\n",
      "(64, 33)\n",
      "step 19551, loss is 4.721961975097656\n",
      "(64, 33)\n",
      "step 19552, loss is 4.598281383514404\n",
      "(64, 33)\n",
      "step 19553, loss is 4.691037178039551\n",
      "(64, 33)\n",
      "step 19554, loss is 4.788961887359619\n",
      "(64, 33)\n",
      "step 19555, loss is 4.6774139404296875\n",
      "(64, 33)\n",
      "step 19556, loss is 4.901396751403809\n",
      "(64, 33)\n",
      "step 19557, loss is 4.762992858886719\n",
      "(64, 33)\n",
      "step 19558, loss is 4.804881572723389\n",
      "(64, 33)\n",
      "step 19559, loss is 4.770296573638916\n",
      "(64, 33)\n",
      "step 19560, loss is 4.790884494781494\n",
      "(64, 33)\n",
      "step 19561, loss is 4.855250835418701\n",
      "(64, 33)\n",
      "step 19562, loss is 4.700325012207031\n",
      "(64, 33)\n",
      "step 19563, loss is 4.770055770874023\n",
      "(64, 33)\n",
      "step 19564, loss is 4.895793437957764\n",
      "(64, 33)\n",
      "step 19565, loss is 4.831032752990723\n",
      "(64, 33)\n",
      "step 19566, loss is 4.6889567375183105\n",
      "(64, 33)\n",
      "step 19567, loss is 4.704115390777588\n",
      "(64, 33)\n",
      "step 19568, loss is 4.7726969718933105\n",
      "(64, 33)\n",
      "step 19569, loss is 4.757669925689697\n",
      "(64, 33)\n",
      "step 19570, loss is 4.6748576164245605\n",
      "(64, 33)\n",
      "step 19571, loss is 4.806953430175781\n",
      "(64, 33)\n",
      "step 19572, loss is 4.853760719299316\n",
      "(64, 33)\n",
      "step 19573, loss is 4.77575159072876\n",
      "(64, 33)\n",
      "step 19574, loss is 4.873260021209717\n",
      "(64, 33)\n",
      "step 19575, loss is 4.71162748336792\n",
      "(64, 33)\n",
      "step 19576, loss is 4.485172748565674\n",
      "(64, 33)\n",
      "step 19577, loss is 4.809288024902344\n",
      "(64, 33)\n",
      "step 19578, loss is 4.85502290725708\n",
      "(64, 33)\n",
      "step 19579, loss is 4.775982856750488\n",
      "(64, 33)\n",
      "step 19580, loss is 4.710542678833008\n",
      "(64, 33)\n",
      "step 19581, loss is 4.784204006195068\n",
      "(64, 33)\n",
      "step 19582, loss is 4.685523986816406\n",
      "(64, 33)\n",
      "step 19583, loss is 4.693295478820801\n",
      "(64, 33)\n",
      "step 19584, loss is 4.894741058349609\n",
      "(64, 33)\n",
      "step 19585, loss is 4.7790069580078125\n",
      "(64, 33)\n",
      "step 19586, loss is 4.786931991577148\n",
      "(64, 33)\n",
      "step 19587, loss is 4.7045488357543945\n",
      "(64, 33)\n",
      "step 19588, loss is 4.803225040435791\n",
      "(64, 33)\n",
      "step 19589, loss is 4.861278057098389\n",
      "(64, 33)\n",
      "step 19590, loss is 4.78592586517334\n",
      "(64, 33)\n",
      "step 19591, loss is 4.606381893157959\n",
      "(64, 33)\n",
      "step 19592, loss is 4.803576946258545\n",
      "(64, 33)\n",
      "step 19593, loss is 4.873651027679443\n",
      "(64, 33)\n",
      "step 19594, loss is 4.925987243652344\n",
      "(64, 33)\n",
      "step 19595, loss is 4.809318542480469\n",
      "(64, 33)\n",
      "step 19596, loss is 4.823888301849365\n",
      "(64, 33)\n",
      "step 19597, loss is 4.711410999298096\n",
      "(64, 33)\n",
      "step 19598, loss is 4.8654937744140625\n",
      "(64, 33)\n",
      "step 19599, loss is 4.803266525268555\n",
      "(64, 33)\n",
      "step 19600, loss is 4.907096862792969\n",
      "(64, 33)\n",
      "step 19601, loss is 4.787787914276123\n",
      "(64, 33)\n",
      "step 19602, loss is 4.731298446655273\n",
      "(64, 33)\n",
      "step 19603, loss is 4.720578193664551\n",
      "(64, 33)\n",
      "step 19604, loss is 4.783459663391113\n",
      "(64, 33)\n",
      "step 19605, loss is 4.816344261169434\n",
      "(64, 33)\n",
      "step 19606, loss is 4.738620758056641\n",
      "(64, 33)\n",
      "step 19607, loss is 4.636622905731201\n",
      "(64, 33)\n",
      "step 19608, loss is 4.657676696777344\n",
      "(64, 33)\n",
      "step 19609, loss is 4.69339656829834\n",
      "(64, 33)\n",
      "step 19610, loss is 4.769752502441406\n",
      "(64, 33)\n",
      "step 19611, loss is 4.593101501464844\n",
      "(64, 33)\n",
      "step 19612, loss is 4.85162353515625\n",
      "(64, 33)\n",
      "step 19613, loss is 4.868228912353516\n",
      "(64, 33)\n",
      "step 19614, loss is 5.014793395996094\n",
      "(64, 33)\n",
      "step 19615, loss is 4.806399345397949\n",
      "(64, 33)\n",
      "step 19616, loss is 4.78741455078125\n",
      "(64, 33)\n",
      "step 19617, loss is 4.824844837188721\n",
      "(64, 33)\n",
      "step 19618, loss is 4.868828773498535\n",
      "(64, 33)\n",
      "step 19619, loss is 4.988374710083008\n",
      "(64, 33)\n",
      "step 19620, loss is 4.684257984161377\n",
      "(64, 33)\n",
      "step 19621, loss is 4.782731056213379\n",
      "(64, 33)\n",
      "step 19622, loss is 4.798346996307373\n",
      "(64, 33)\n",
      "step 19623, loss is 4.866862773895264\n",
      "(64, 33)\n",
      "step 19624, loss is 4.767871379852295\n",
      "(64, 33)\n",
      "step 19625, loss is 4.719234943389893\n",
      "(64, 33)\n",
      "step 19626, loss is 4.7423248291015625\n",
      "(64, 33)\n",
      "step 19627, loss is 5.054765701293945\n",
      "(64, 33)\n",
      "step 19628, loss is 4.717231273651123\n",
      "(64, 33)\n",
      "step 19629, loss is 4.77909517288208\n",
      "(64, 33)\n",
      "step 19630, loss is 4.78861665725708\n",
      "(64, 33)\n",
      "step 19631, loss is 4.907088279724121\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19632, loss is 4.746846675872803\n",
      "(64, 33)\n",
      "step 19633, loss is 4.616189479827881\n",
      "(64, 33)\n",
      "step 19634, loss is 4.876906394958496\n",
      "(64, 33)\n",
      "step 19635, loss is 4.7581939697265625\n",
      "(64, 33)\n",
      "step 19636, loss is 4.801892280578613\n",
      "(64, 33)\n",
      "step 19637, loss is 5.075623512268066\n",
      "(64, 33)\n",
      "step 19638, loss is 4.702466011047363\n",
      "(64, 33)\n",
      "step 19639, loss is 4.721563816070557\n",
      "(64, 33)\n",
      "step 19640, loss is 4.75747013092041\n",
      "(64, 33)\n",
      "step 19641, loss is 4.768454074859619\n",
      "(64, 33)\n",
      "step 19642, loss is 4.979274749755859\n",
      "(64, 33)\n",
      "step 19643, loss is 4.73566198348999\n",
      "(64, 33)\n",
      "step 19644, loss is 4.739570617675781\n",
      "(64, 33)\n",
      "step 19645, loss is 4.688412189483643\n",
      "(64, 33)\n",
      "step 19646, loss is 4.693563938140869\n",
      "(64, 33)\n",
      "step 19647, loss is 4.572015762329102\n",
      "(64, 33)\n",
      "step 19648, loss is 4.866324424743652\n",
      "(64, 33)\n",
      "step 19649, loss is 4.701411724090576\n",
      "(64, 33)\n",
      "step 19650, loss is 4.822426795959473\n",
      "(64, 33)\n",
      "step 19651, loss is 4.816825866699219\n",
      "(64, 33)\n",
      "step 19652, loss is 4.784657001495361\n",
      "(64, 33)\n",
      "step 19653, loss is 4.682643413543701\n",
      "(64, 33)\n",
      "step 19654, loss is 4.802970886230469\n",
      "(64, 33)\n",
      "step 19655, loss is 4.74873161315918\n",
      "(64, 33)\n",
      "step 19656, loss is 4.808669090270996\n",
      "(64, 33)\n",
      "step 19657, loss is 4.86884069442749\n",
      "(64, 33)\n",
      "step 19658, loss is 4.991573810577393\n",
      "(64, 33)\n",
      "step 19659, loss is 4.8722662925720215\n",
      "(64, 33)\n",
      "step 19660, loss is 4.87755823135376\n",
      "(64, 33)\n",
      "step 19661, loss is 4.7929792404174805\n",
      "(64, 33)\n",
      "step 19662, loss is 4.763679027557373\n",
      "(64, 33)\n",
      "step 19663, loss is 4.571535110473633\n",
      "(64, 33)\n",
      "step 19664, loss is 4.811322212219238\n",
      "(64, 33)\n",
      "step 19665, loss is 4.809431076049805\n",
      "(64, 33)\n",
      "step 19666, loss is 4.6386942863464355\n",
      "(64, 33)\n",
      "step 19667, loss is 4.728341579437256\n",
      "(64, 33)\n",
      "step 19668, loss is 4.900747776031494\n",
      "(64, 33)\n",
      "step 19669, loss is 4.880105018615723\n",
      "(64, 33)\n",
      "step 19670, loss is 4.709243297576904\n",
      "(64, 33)\n",
      "step 19671, loss is 4.630864143371582\n",
      "(64, 33)\n",
      "step 19672, loss is 4.753566265106201\n",
      "(64, 33)\n",
      "step 19673, loss is 4.6783881187438965\n",
      "(64, 33)\n",
      "step 19674, loss is 4.767683982849121\n",
      "(64, 33)\n",
      "step 19675, loss is 4.787217140197754\n",
      "(64, 33)\n",
      "step 19676, loss is 4.879527568817139\n",
      "(64, 33)\n",
      "step 19677, loss is 4.736525535583496\n",
      "(64, 33)\n",
      "step 19678, loss is 4.858512878417969\n",
      "(64, 33)\n",
      "step 19679, loss is 4.752766132354736\n",
      "(64, 33)\n",
      "step 19680, loss is 4.862462520599365\n",
      "(64, 33)\n",
      "step 19681, loss is 4.8301682472229\n",
      "(64, 33)\n",
      "step 19682, loss is 4.739349842071533\n",
      "(64, 33)\n",
      "step 19683, loss is 4.86257266998291\n",
      "(64, 33)\n",
      "step 19684, loss is 4.929502964019775\n",
      "(64, 33)\n",
      "step 19685, loss is 4.853411674499512\n",
      "(64, 33)\n",
      "step 19686, loss is 4.7426300048828125\n",
      "(64, 33)\n",
      "step 19687, loss is 4.863160133361816\n",
      "(64, 33)\n",
      "step 19688, loss is 4.8745245933532715\n",
      "(64, 33)\n",
      "step 19689, loss is 4.7018723487854\n",
      "(64, 33)\n",
      "step 19690, loss is 4.9292988777160645\n",
      "(64, 33)\n",
      "step 19691, loss is 4.93196439743042\n",
      "(64, 33)\n",
      "step 19692, loss is 4.840690612792969\n",
      "(64, 33)\n",
      "step 19693, loss is 4.758118152618408\n",
      "(64, 33)\n",
      "step 19694, loss is 4.794268608093262\n",
      "(64, 33)\n",
      "step 19695, loss is 4.636684894561768\n",
      "(64, 33)\n",
      "step 19696, loss is 4.882884502410889\n",
      "(64, 33)\n",
      "step 19697, loss is 4.7397074699401855\n",
      "(64, 33)\n",
      "step 19698, loss is 4.723966598510742\n",
      "(64, 33)\n",
      "step 19699, loss is 4.965572357177734\n",
      "(64, 33)\n",
      "step 19700, loss is 4.671092987060547\n",
      "(64, 33)\n",
      "step 19701, loss is 4.78296422958374\n",
      "(64, 33)\n",
      "step 19702, loss is 4.7541117668151855\n",
      "(64, 33)\n",
      "step 19703, loss is 4.698667526245117\n",
      "(64, 33)\n",
      "step 19704, loss is 4.735644817352295\n",
      "(64, 33)\n",
      "step 19705, loss is 5.0112457275390625\n",
      "(64, 33)\n",
      "step 19706, loss is 4.818204879760742\n",
      "(64, 33)\n",
      "step 19707, loss is 4.725590705871582\n",
      "(64, 33)\n",
      "step 19708, loss is 4.726503372192383\n",
      "(64, 33)\n",
      "step 19709, loss is 4.8119683265686035\n",
      "(64, 33)\n",
      "step 19710, loss is 4.521255970001221\n",
      "(64, 33)\n",
      "step 19711, loss is 4.562007904052734\n",
      "(64, 33)\n",
      "step 19712, loss is 4.996817588806152\n",
      "(64, 33)\n",
      "step 19713, loss is 4.7396464347839355\n",
      "(64, 33)\n",
      "step 19714, loss is 4.8160719871521\n",
      "(64, 33)\n",
      "step 19715, loss is 4.743130683898926\n",
      "(64, 33)\n",
      "step 19716, loss is 4.87888765335083\n",
      "(64, 33)\n",
      "step 19717, loss is 4.503281593322754\n",
      "(64, 33)\n",
      "step 19718, loss is 4.79736328125\n",
      "(64, 33)\n",
      "step 19719, loss is 5.02650260925293\n",
      "(64, 33)\n",
      "step 19720, loss is 4.7302470207214355\n",
      "(64, 33)\n",
      "step 19721, loss is 5.043064117431641\n",
      "(64, 33)\n",
      "step 19722, loss is 4.769273281097412\n",
      "(64, 33)\n",
      "step 19723, loss is 4.820147514343262\n",
      "(64, 33)\n",
      "step 19724, loss is 4.682514667510986\n",
      "(64, 33)\n",
      "step 19725, loss is 4.62874698638916\n",
      "(64, 33)\n",
      "step 19726, loss is 4.9374775886535645\n",
      "(64, 33)\n",
      "step 19727, loss is 4.861783027648926\n",
      "(64, 33)\n",
      "step 19728, loss is 4.737997531890869\n",
      "(64, 33)\n",
      "step 19729, loss is 4.865811824798584\n",
      "(64, 33)\n",
      "step 19730, loss is 4.736409664154053\n",
      "(64, 33)\n",
      "step 19731, loss is 4.796292781829834\n",
      "(64, 33)\n",
      "step 19732, loss is 4.546701908111572\n",
      "(64, 33)\n",
      "step 19733, loss is 4.944891929626465\n",
      "(64, 33)\n",
      "step 19734, loss is 4.887768268585205\n",
      "(64, 33)\n",
      "step 19735, loss is 4.715566635131836\n",
      "(64, 33)\n",
      "step 19736, loss is 4.593635559082031\n",
      "(64, 33)\n",
      "step 19737, loss is 5.064988613128662\n",
      "(64, 33)\n",
      "step 19738, loss is 4.558746814727783\n",
      "(64, 33)\n",
      "step 19739, loss is 4.817577838897705\n",
      "(64, 33)\n",
      "step 19740, loss is 4.714365005493164\n",
      "(64, 33)\n",
      "step 19741, loss is 4.842723846435547\n",
      "(64, 33)\n",
      "step 19742, loss is 4.864521026611328\n",
      "(64, 33)\n",
      "step 19743, loss is 4.890792369842529\n",
      "(64, 33)\n",
      "step 19744, loss is 4.754876613616943\n",
      "(64, 33)\n",
      "step 19745, loss is 4.722104549407959\n",
      "(64, 33)\n",
      "step 19746, loss is 4.711074352264404\n",
      "(64, 33)\n",
      "step 19747, loss is 4.83707332611084\n",
      "(64, 33)\n",
      "step 19748, loss is 4.737338542938232\n",
      "(64, 33)\n",
      "step 19749, loss is 4.903999328613281\n",
      "(64, 33)\n",
      "step 19750, loss is 4.844851493835449\n",
      "(64, 33)\n",
      "step 19751, loss is 4.937061786651611\n",
      "(64, 33)\n",
      "step 19752, loss is 4.8638787269592285\n",
      "(64, 33)\n",
      "step 19753, loss is 4.665400505065918\n",
      "(64, 33)\n",
      "step 19754, loss is 4.916805267333984\n",
      "(64, 33)\n",
      "step 19755, loss is 4.932307720184326\n",
      "(64, 33)\n",
      "step 19756, loss is 4.767628192901611\n",
      "(64, 33)\n",
      "step 19757, loss is 4.647812366485596\n",
      "(64, 33)\n",
      "step 19758, loss is 4.711800575256348\n",
      "(64, 33)\n",
      "step 19759, loss is 4.722653388977051\n",
      "(64, 33)\n",
      "step 19760, loss is 4.7042131423950195\n",
      "(64, 33)\n",
      "step 19761, loss is 4.78036642074585\n",
      "(64, 33)\n",
      "step 19762, loss is 4.6878533363342285\n",
      "(64, 33)\n",
      "step 19763, loss is 4.727275371551514\n",
      "(64, 33)\n",
      "step 19764, loss is 4.707008361816406\n",
      "(64, 33)\n",
      "step 19765, loss is 4.684280872344971\n",
      "(64, 33)\n",
      "step 19766, loss is 4.740732669830322\n",
      "(64, 33)\n",
      "step 19767, loss is 4.781822204589844\n",
      "(64, 33)\n",
      "step 19768, loss is 4.889329433441162\n",
      "(64, 33)\n",
      "step 19769, loss is 4.623409271240234\n",
      "(64, 33)\n",
      "step 19770, loss is 4.670628070831299\n",
      "(64, 33)\n",
      "step 19771, loss is 4.969276428222656\n",
      "(64, 33)\n",
      "step 19772, loss is 4.976058483123779\n",
      "(64, 33)\n",
      "step 19773, loss is 4.85193395614624\n",
      "(64, 33)\n",
      "step 19774, loss is 4.844031810760498\n",
      "(64, 33)\n",
      "step 19775, loss is 4.70081090927124\n",
      "(64, 33)\n",
      "step 19776, loss is 4.817419052124023\n",
      "(64, 33)\n",
      "step 19777, loss is 4.694086074829102\n",
      "(64, 33)\n",
      "step 19778, loss is 4.832843780517578\n",
      "(64, 33)\n",
      "step 19779, loss is 4.761959552764893\n",
      "(64, 33)\n",
      "step 19780, loss is 4.927441596984863\n",
      "(64, 33)\n",
      "step 19781, loss is 4.616873264312744\n",
      "(64, 33)\n",
      "step 19782, loss is 4.790081024169922\n",
      "(64, 33)\n",
      "step 19783, loss is 4.6894850730896\n",
      "(64, 33)\n",
      "step 19784, loss is 4.900733947753906\n",
      "(64, 33)\n",
      "step 19785, loss is 4.695616722106934\n",
      "(64, 33)\n",
      "step 19786, loss is 4.717776775360107\n",
      "(64, 33)\n",
      "step 19787, loss is 4.818657875061035\n",
      "(64, 33)\n",
      "step 19788, loss is 4.639016151428223\n",
      "(64, 33)\n",
      "step 19789, loss is 4.937376976013184\n",
      "(64, 33)\n",
      "step 19790, loss is 4.8426737785339355\n",
      "(64, 33)\n",
      "step 19791, loss is 4.907537937164307\n",
      "(64, 33)\n",
      "step 19792, loss is 4.714962005615234\n",
      "(64, 33)\n",
      "step 19793, loss is 4.921175479888916\n",
      "(64, 33)\n",
      "step 19794, loss is 4.787931442260742\n",
      "(64, 33)\n",
      "step 19795, loss is 4.776633262634277\n",
      "(64, 33)\n",
      "step 19796, loss is 4.773660182952881\n",
      "(64, 33)\n",
      "step 19797, loss is 4.909224510192871\n",
      "(64, 33)\n",
      "step 19798, loss is 4.744718074798584\n",
      "(64, 33)\n",
      "step 19799, loss is 4.71990442276001\n",
      "(64, 33)\n",
      "step 19800, loss is 4.931728839874268\n",
      "(64, 33)\n",
      "step 19801, loss is 4.784594535827637\n",
      "(64, 33)\n",
      "step 19802, loss is 4.804039001464844\n",
      "(64, 33)\n",
      "step 19803, loss is 4.836048603057861\n",
      "(64, 33)\n",
      "step 19804, loss is 4.744985103607178\n",
      "(64, 33)\n",
      "step 19805, loss is 4.869175434112549\n",
      "(64, 33)\n",
      "step 19806, loss is 4.992098808288574\n",
      "(64, 33)\n",
      "step 19807, loss is 4.755317211151123\n",
      "(64, 33)\n",
      "step 19808, loss is 4.825173854827881\n",
      "(64, 33)\n",
      "step 19809, loss is 4.694456100463867\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19810, loss is 4.8544416427612305\n",
      "(64, 33)\n",
      "step 19811, loss is 5.007327556610107\n",
      "(64, 33)\n",
      "step 19812, loss is 4.6676154136657715\n",
      "(64, 33)\n",
      "step 19813, loss is 4.778975486755371\n",
      "(64, 33)\n",
      "step 19814, loss is 4.728160381317139\n",
      "(64, 33)\n",
      "step 19815, loss is 4.850107192993164\n",
      "(64, 33)\n",
      "step 19816, loss is 5.054999351501465\n",
      "(64, 33)\n",
      "step 19817, loss is 4.661505699157715\n",
      "(64, 33)\n",
      "step 19818, loss is 4.9571356773376465\n",
      "(64, 33)\n",
      "step 19819, loss is 4.902651786804199\n",
      "(64, 33)\n",
      "step 19820, loss is 4.858534336090088\n",
      "(64, 33)\n",
      "step 19821, loss is 4.846343994140625\n",
      "(64, 33)\n",
      "step 19822, loss is 4.7036333084106445\n",
      "(64, 33)\n",
      "step 19823, loss is 4.7360920906066895\n",
      "(64, 33)\n",
      "step 19824, loss is 4.61355447769165\n",
      "(64, 33)\n",
      "step 19825, loss is 4.591754913330078\n",
      "(64, 33)\n",
      "step 19826, loss is 5.022006988525391\n",
      "(64, 33)\n",
      "step 19827, loss is 4.93625020980835\n",
      "(64, 33)\n",
      "step 19828, loss is 4.7800164222717285\n",
      "(64, 33)\n",
      "step 19829, loss is 4.758721828460693\n",
      "(64, 33)\n",
      "step 19830, loss is 4.875328063964844\n",
      "(64, 33)\n",
      "step 19831, loss is 4.754082202911377\n",
      "(64, 33)\n",
      "step 19832, loss is 4.79187536239624\n",
      "(64, 33)\n",
      "step 19833, loss is 4.790710926055908\n",
      "(64, 33)\n",
      "step 19834, loss is 4.847718238830566\n",
      "(64, 33)\n",
      "step 19835, loss is 4.7504401206970215\n",
      "(64, 33)\n",
      "step 19836, loss is 4.621572971343994\n",
      "(64, 33)\n",
      "step 19837, loss is 4.927206516265869\n",
      "(64, 33)\n",
      "step 19838, loss is 4.686064720153809\n",
      "(64, 33)\n",
      "step 19839, loss is 4.735766410827637\n",
      "(64, 33)\n",
      "step 19840, loss is 4.832391262054443\n",
      "(64, 33)\n",
      "step 19841, loss is 4.751797676086426\n",
      "(64, 33)\n",
      "step 19842, loss is 4.7860894203186035\n",
      "(64, 33)\n",
      "step 19843, loss is 4.767858028411865\n",
      "(64, 33)\n",
      "step 19844, loss is 5.01253080368042\n",
      "(64, 33)\n",
      "step 19845, loss is 4.751580715179443\n",
      "(64, 33)\n",
      "step 19846, loss is 4.765439033508301\n",
      "(64, 33)\n",
      "step 19847, loss is 4.673844337463379\n",
      "(64, 33)\n",
      "step 19848, loss is 4.750265121459961\n",
      "(64, 33)\n",
      "step 19849, loss is 4.784412860870361\n",
      "(64, 33)\n",
      "step 19850, loss is 4.715922832489014\n",
      "(64, 33)\n",
      "step 19851, loss is 4.703420162200928\n",
      "(64, 33)\n",
      "step 19852, loss is 4.832871913909912\n",
      "(64, 33)\n",
      "step 19853, loss is 4.85288143157959\n",
      "(64, 33)\n",
      "step 19854, loss is 4.654604911804199\n",
      "(64, 33)\n",
      "step 19855, loss is 4.855500221252441\n",
      "(64, 33)\n",
      "step 19856, loss is 4.5855889320373535\n",
      "(64, 33)\n",
      "step 19857, loss is 4.7933502197265625\n",
      "(64, 33)\n",
      "step 19858, loss is 4.855556964874268\n",
      "(64, 33)\n",
      "step 19859, loss is 4.857840061187744\n",
      "(64, 33)\n",
      "step 19860, loss is 4.707603454589844\n",
      "(64, 33)\n",
      "step 19861, loss is 4.804043292999268\n",
      "(64, 33)\n",
      "step 19862, loss is 4.855177402496338\n",
      "(64, 33)\n",
      "step 19863, loss is 4.837618827819824\n",
      "(64, 33)\n",
      "step 19864, loss is 4.816501617431641\n",
      "(64, 33)\n",
      "step 19865, loss is 4.877742290496826\n",
      "(64, 33)\n",
      "step 19866, loss is 4.7868547439575195\n",
      "(64, 33)\n",
      "step 19867, loss is 4.970970153808594\n",
      "(64, 33)\n",
      "step 19868, loss is 4.8727707862854\n",
      "(64, 33)\n",
      "step 19869, loss is 4.914856910705566\n",
      "(64, 33)\n",
      "step 19870, loss is 4.746931552886963\n",
      "(64, 33)\n",
      "step 19871, loss is 4.65727424621582\n",
      "(64, 33)\n",
      "step 19872, loss is 4.862987518310547\n",
      "(64, 33)\n",
      "step 19873, loss is 4.868051052093506\n",
      "(64, 33)\n",
      "step 19874, loss is 4.661345481872559\n",
      "(64, 33)\n",
      "step 19875, loss is 4.688121318817139\n",
      "(64, 33)\n",
      "step 19876, loss is 4.72150182723999\n",
      "(64, 33)\n",
      "step 19877, loss is 4.896693229675293\n",
      "(64, 33)\n",
      "step 19878, loss is 4.980356693267822\n",
      "(64, 33)\n",
      "step 19879, loss is 4.788645267486572\n",
      "(64, 33)\n",
      "step 19880, loss is 4.751111030578613\n",
      "(64, 33)\n",
      "step 19881, loss is 4.798777103424072\n",
      "(64, 33)\n",
      "step 19882, loss is 4.778100967407227\n",
      "(64, 33)\n",
      "step 19883, loss is 4.846118927001953\n",
      "(64, 33)\n",
      "step 19884, loss is 4.744429111480713\n",
      "(64, 33)\n",
      "step 19885, loss is 4.904329299926758\n",
      "(64, 33)\n",
      "step 19886, loss is 4.846367359161377\n",
      "(64, 33)\n",
      "step 19887, loss is 4.5871686935424805\n",
      "(64, 33)\n",
      "step 19888, loss is 4.9027228355407715\n",
      "(64, 33)\n",
      "step 19889, loss is 4.650398254394531\n",
      "(64, 33)\n",
      "step 19890, loss is 4.877457141876221\n",
      "(64, 33)\n",
      "step 19891, loss is 4.903704643249512\n",
      "(64, 33)\n",
      "step 19892, loss is 4.852168560028076\n",
      "(64, 33)\n",
      "step 19893, loss is 4.775627613067627\n",
      "(64, 33)\n",
      "step 19894, loss is 4.758974075317383\n",
      "(64, 33)\n",
      "step 19895, loss is 4.796940803527832\n",
      "(64, 33)\n",
      "step 19896, loss is 4.879905700683594\n",
      "(64, 33)\n",
      "step 19897, loss is 4.608510494232178\n",
      "(64, 33)\n",
      "step 19898, loss is 4.753942012786865\n",
      "(64, 33)\n",
      "step 19899, loss is 5.006475448608398\n",
      "(64, 33)\n",
      "step 19900, loss is 4.862593650817871\n",
      "(64, 33)\n",
      "step 19901, loss is 4.867948532104492\n",
      "(64, 33)\n",
      "step 19902, loss is 4.778356552124023\n",
      "(64, 33)\n",
      "step 19903, loss is 4.6247992515563965\n",
      "(64, 33)\n",
      "step 19904, loss is 4.6579718589782715\n",
      "(64, 33)\n",
      "step 19905, loss is 5.046496868133545\n",
      "(64, 33)\n",
      "step 19906, loss is 4.786804676055908\n",
      "(64, 33)\n",
      "step 19907, loss is 4.7767205238342285\n",
      "(64, 33)\n",
      "step 19908, loss is 4.774590969085693\n",
      "(64, 33)\n",
      "step 19909, loss is 4.889406681060791\n",
      "(64, 33)\n",
      "step 19910, loss is 4.87939977645874\n",
      "(64, 33)\n",
      "step 19911, loss is 4.8501739501953125\n",
      "(64, 33)\n",
      "step 19912, loss is 4.915727615356445\n",
      "(64, 33)\n",
      "step 19913, loss is 4.749042987823486\n",
      "(64, 33)\n",
      "step 19914, loss is 4.499571800231934\n",
      "(64, 33)\n",
      "step 19915, loss is 4.830924034118652\n",
      "(64, 33)\n",
      "step 19916, loss is 4.83973503112793\n",
      "(64, 33)\n",
      "step 19917, loss is 4.920854091644287\n",
      "(64, 33)\n",
      "step 19918, loss is 4.872003078460693\n",
      "(64, 33)\n",
      "step 19919, loss is 4.631282806396484\n",
      "(64, 33)\n",
      "step 19920, loss is 4.908763885498047\n",
      "(64, 33)\n",
      "step 19921, loss is 4.709158897399902\n",
      "(64, 33)\n",
      "step 19922, loss is 4.730489730834961\n",
      "(64, 33)\n",
      "step 19923, loss is 4.8224263191223145\n",
      "(64, 33)\n",
      "step 19924, loss is 4.753868579864502\n",
      "(64, 33)\n",
      "step 19925, loss is 4.709985256195068\n",
      "(64, 33)\n",
      "step 19926, loss is 4.690390586853027\n",
      "(64, 33)\n",
      "step 19927, loss is 4.993706703186035\n",
      "(64, 33)\n",
      "step 19928, loss is 4.8232855796813965\n",
      "(64, 33)\n",
      "step 19929, loss is 4.83392858505249\n",
      "(64, 33)\n",
      "step 19930, loss is 4.863974571228027\n",
      "(64, 33)\n",
      "step 19931, loss is 4.776031970977783\n",
      "(64, 33)\n",
      "step 19932, loss is 4.8068928718566895\n",
      "(64, 33)\n",
      "step 19933, loss is 4.937027454376221\n",
      "(64, 33)\n",
      "step 19934, loss is 4.819375514984131\n",
      "(64, 33)\n",
      "step 19935, loss is 4.859836578369141\n",
      "(64, 33)\n",
      "step 19936, loss is 4.77821683883667\n",
      "(64, 33)\n",
      "step 19937, loss is 4.776238441467285\n",
      "(64, 33)\n",
      "step 19938, loss is 4.6903557777404785\n",
      "(64, 33)\n",
      "step 19939, loss is 4.75604248046875\n",
      "(64, 33)\n",
      "step 19940, loss is 4.7292561531066895\n",
      "(64, 33)\n",
      "step 19941, loss is 4.739395618438721\n",
      "(64, 33)\n",
      "step 19942, loss is 4.80019474029541\n",
      "(64, 33)\n",
      "step 19943, loss is 4.810270309448242\n",
      "(64, 33)\n",
      "step 19944, loss is 4.922080993652344\n",
      "(64, 33)\n",
      "step 19945, loss is 4.534265995025635\n",
      "(64, 33)\n",
      "step 19946, loss is 4.639198303222656\n",
      "(64, 33)\n",
      "step 19947, loss is 4.793582916259766\n",
      "(64, 33)\n",
      "step 19948, loss is 4.771269798278809\n",
      "(64, 33)\n",
      "step 19949, loss is 4.9102864265441895\n",
      "(64, 33)\n",
      "step 19950, loss is 4.69566011428833\n",
      "(64, 33)\n",
      "step 19951, loss is 4.961877346038818\n",
      "(64, 33)\n",
      "step 19952, loss is 4.683694839477539\n",
      "(64, 33)\n",
      "step 19953, loss is 4.613982200622559\n",
      "(64, 33)\n",
      "step 19954, loss is 4.755134105682373\n",
      "(64, 33)\n",
      "step 19955, loss is 4.777775764465332\n",
      "(64, 33)\n",
      "step 19956, loss is 4.734826564788818\n",
      "(64, 33)\n",
      "step 19957, loss is 4.836790084838867\n",
      "(64, 33)\n",
      "step 19958, loss is 4.83323335647583\n",
      "(64, 33)\n",
      "step 19959, loss is 4.987459182739258\n",
      "(64, 33)\n",
      "step 19960, loss is 4.800740718841553\n",
      "(64, 33)\n",
      "step 19961, loss is 4.781828880310059\n",
      "(64, 33)\n",
      "step 19962, loss is 4.754452228546143\n",
      "(64, 33)\n",
      "step 19963, loss is 4.720595359802246\n",
      "(64, 33)\n",
      "step 19964, loss is 4.6259074211120605\n",
      "(64, 33)\n",
      "step 19965, loss is 4.7665181159973145\n",
      "(64, 33)\n",
      "step 19966, loss is 4.631834506988525\n",
      "(64, 33)\n",
      "step 19967, loss is 4.806953430175781\n",
      "(64, 33)\n",
      "step 19968, loss is 4.7995285987854\n",
      "(64, 33)\n",
      "step 19969, loss is 4.817595481872559\n",
      "(64, 33)\n",
      "step 19970, loss is 4.9013285636901855\n",
      "(64, 33)\n",
      "step 19971, loss is 4.877886772155762\n",
      "(64, 33)\n",
      "step 19972, loss is 4.7894287109375\n",
      "(64, 33)\n",
      "step 19973, loss is 4.8623247146606445\n",
      "(64, 33)\n",
      "step 19974, loss is 4.709490776062012\n",
      "(64, 33)\n",
      "step 19975, loss is 4.922694206237793\n",
      "(64, 33)\n",
      "step 19976, loss is 4.821046829223633\n",
      "(64, 33)\n",
      "step 19977, loss is 4.689576148986816\n",
      "(64, 33)\n",
      "step 19978, loss is 4.818856716156006\n",
      "(64, 33)\n",
      "step 19979, loss is 4.9473981857299805\n",
      "(64, 33)\n",
      "step 19980, loss is 4.918002128601074\n",
      "(64, 33)\n",
      "step 19981, loss is 4.776858329772949\n",
      "(64, 33)\n",
      "step 19982, loss is 4.697256088256836\n",
      "(64, 33)\n",
      "step 19983, loss is 4.733440399169922\n",
      "(64, 33)\n",
      "step 19984, loss is 4.896369934082031\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19985, loss is 4.758815288543701\n",
      "(64, 33)\n",
      "step 19986, loss is 4.6946234703063965\n",
      "(64, 33)\n",
      "step 19987, loss is 4.7290520668029785\n",
      "(64, 33)\n",
      "step 19988, loss is 4.817621231079102\n",
      "(64, 33)\n",
      "step 19989, loss is 5.086556434631348\n",
      "(64, 33)\n",
      "step 19990, loss is 4.740711688995361\n",
      "(64, 33)\n",
      "step 19991, loss is 4.853148937225342\n",
      "(64, 33)\n",
      "step 19992, loss is 4.686857223510742\n",
      "(64, 33)\n",
      "step 19993, loss is 4.771348476409912\n",
      "(64, 33)\n",
      "step 19994, loss is 4.738490581512451\n",
      "(64, 33)\n",
      "step 19995, loss is 4.659322738647461\n",
      "(64, 33)\n",
      "step 19996, loss is 4.712077617645264\n",
      "(64, 33)\n",
      "step 19997, loss is 4.784573554992676\n",
      "(64, 33)\n",
      "step 19998, loss is 4.819597244262695\n",
      "(64, 33)\n",
      "step 19999, loss is 4.889838218688965\n",
      "(64, 33)\n",
      "step 20000, loss is 4.734463214874268\n",
      "(64, 33)\n",
      "step 20001, loss is 4.777158260345459\n",
      "(64, 33)\n",
      "step 20002, loss is 4.750513076782227\n",
      "(64, 33)\n",
      "step 20003, loss is 4.716485500335693\n",
      "(64, 33)\n",
      "step 20004, loss is 4.714817523956299\n",
      "(64, 33)\n",
      "step 20005, loss is 4.6874871253967285\n",
      "(64, 33)\n",
      "step 20006, loss is 4.674005508422852\n",
      "(64, 33)\n",
      "step 20007, loss is 4.902851581573486\n",
      "(64, 33)\n",
      "step 20008, loss is 4.830409049987793\n",
      "(64, 33)\n",
      "step 20009, loss is 4.645926475524902\n",
      "(64, 33)\n",
      "step 20010, loss is 4.9019622802734375\n",
      "(64, 33)\n",
      "step 20011, loss is 4.687325477600098\n",
      "(64, 33)\n",
      "step 20012, loss is 4.707271099090576\n",
      "(64, 33)\n",
      "step 20013, loss is 4.819947242736816\n",
      "(64, 33)\n",
      "step 20014, loss is 4.743898391723633\n",
      "(64, 33)\n",
      "step 20015, loss is 4.763088703155518\n",
      "(64, 33)\n",
      "step 20016, loss is 4.856692314147949\n",
      "(64, 33)\n",
      "step 20017, loss is 4.84357213973999\n",
      "(64, 33)\n",
      "step 20018, loss is 4.876058578491211\n",
      "(64, 33)\n",
      "step 20019, loss is 4.751579284667969\n",
      "(64, 33)\n",
      "step 20020, loss is 4.630993366241455\n",
      "(64, 33)\n",
      "step 20021, loss is 4.885633945465088\n",
      "(64, 33)\n",
      "step 20022, loss is 4.742984294891357\n",
      "(64, 33)\n",
      "step 20023, loss is 4.795942306518555\n",
      "(64, 33)\n",
      "step 20024, loss is 4.667396545410156\n",
      "(64, 33)\n",
      "step 20025, loss is 4.88324499130249\n",
      "(64, 33)\n",
      "step 20026, loss is 4.762320518493652\n",
      "(64, 33)\n",
      "step 20027, loss is 4.836133003234863\n",
      "(64, 33)\n",
      "step 20028, loss is 4.8337178230285645\n",
      "(64, 33)\n",
      "step 20029, loss is 4.915236949920654\n",
      "(64, 33)\n",
      "step 20030, loss is 4.660084247589111\n",
      "(64, 33)\n",
      "step 20031, loss is 4.822781562805176\n",
      "(64, 33)\n",
      "step 20032, loss is 4.835537910461426\n",
      "(64, 33)\n",
      "step 20033, loss is 4.684460163116455\n",
      "(64, 33)\n",
      "step 20034, loss is 4.814765930175781\n",
      "(64, 33)\n",
      "step 20035, loss is 5.024881362915039\n",
      "(64, 33)\n",
      "step 20036, loss is 4.798625469207764\n",
      "(64, 33)\n",
      "step 20037, loss is 4.68487548828125\n",
      "(64, 33)\n",
      "step 20038, loss is 4.759790420532227\n",
      "(64, 33)\n",
      "step 20039, loss is 4.786232948303223\n",
      "(64, 33)\n",
      "step 20040, loss is 4.742720127105713\n",
      "(64, 33)\n",
      "step 20041, loss is 4.634119987487793\n",
      "(64, 33)\n",
      "step 20042, loss is 4.846583366394043\n",
      "(64, 33)\n",
      "step 20043, loss is 4.648573398590088\n",
      "(64, 33)\n",
      "step 20044, loss is 4.893202304840088\n",
      "(64, 33)\n",
      "step 20045, loss is 4.836477279663086\n",
      "(64, 33)\n",
      "step 20046, loss is 4.757804870605469\n",
      "(64, 33)\n",
      "step 20047, loss is 4.787396430969238\n",
      "(64, 33)\n",
      "step 20048, loss is 4.792013645172119\n",
      "(64, 33)\n",
      "step 20049, loss is 4.718483924865723\n",
      "(64, 33)\n",
      "step 20050, loss is 4.723762512207031\n",
      "(64, 33)\n",
      "step 20051, loss is 4.792921543121338\n",
      "(64, 33)\n",
      "step 20052, loss is 4.966373920440674\n",
      "(64, 33)\n",
      "step 20053, loss is 4.916736602783203\n",
      "(64, 33)\n",
      "step 20054, loss is 4.581272602081299\n",
      "(64, 33)\n",
      "step 20055, loss is 4.867074966430664\n",
      "(64, 33)\n",
      "step 20056, loss is 4.9342570304870605\n",
      "(64, 33)\n",
      "step 20057, loss is 4.711039066314697\n",
      "(64, 33)\n",
      "step 20058, loss is 4.750871181488037\n",
      "(64, 33)\n",
      "step 20059, loss is 4.861040115356445\n",
      "(64, 33)\n",
      "step 20060, loss is 4.8590168952941895\n",
      "(64, 33)\n",
      "step 20061, loss is 4.883110046386719\n",
      "(64, 33)\n",
      "step 20062, loss is 5.009943008422852\n",
      "(64, 33)\n",
      "step 20063, loss is 4.949676513671875\n",
      "(64, 33)\n",
      "step 20064, loss is 4.842222213745117\n",
      "(64, 33)\n",
      "step 20065, loss is 4.77635383605957\n",
      "(64, 33)\n",
      "step 20066, loss is 4.798548698425293\n",
      "(64, 33)\n",
      "step 20067, loss is 4.79939603805542\n",
      "(64, 33)\n",
      "step 20068, loss is 4.759369850158691\n",
      "(64, 33)\n",
      "step 20069, loss is 4.910597324371338\n",
      "(64, 33)\n",
      "step 20070, loss is 4.800127029418945\n",
      "(64, 33)\n",
      "step 20071, loss is 4.797075271606445\n",
      "(64, 33)\n",
      "step 20072, loss is 4.8937482833862305\n",
      "(64, 33)\n",
      "step 20073, loss is 4.643609046936035\n",
      "(64, 33)\n",
      "step 20074, loss is 4.8534135818481445\n",
      "(64, 33)\n",
      "step 20075, loss is 4.763153553009033\n",
      "(64, 33)\n",
      "step 20076, loss is 4.764928340911865\n",
      "(64, 33)\n",
      "step 20077, loss is 4.73399019241333\n",
      "(64, 33)\n",
      "step 20078, loss is 4.668845176696777\n",
      "(64, 33)\n",
      "step 20079, loss is 4.775801181793213\n",
      "(64, 33)\n",
      "step 20080, loss is 4.728715419769287\n",
      "(64, 33)\n",
      "step 20081, loss is 4.893604755401611\n",
      "(64, 33)\n",
      "step 20082, loss is 4.941327095031738\n",
      "(64, 33)\n",
      "step 20083, loss is 4.724688529968262\n",
      "(64, 33)\n",
      "step 20084, loss is 4.7573933601379395\n",
      "(64, 33)\n",
      "step 20085, loss is 4.983327388763428\n",
      "(64, 33)\n",
      "step 20086, loss is 4.75270414352417\n",
      "(64, 33)\n",
      "step 20087, loss is 4.815525054931641\n",
      "(64, 33)\n",
      "step 20088, loss is 4.659604072570801\n",
      "(64, 33)\n",
      "step 20089, loss is 4.665699481964111\n",
      "(64, 33)\n",
      "step 20090, loss is 4.695101261138916\n",
      "(64, 33)\n",
      "step 20091, loss is 4.741387844085693\n",
      "(64, 33)\n",
      "step 20092, loss is 4.926840305328369\n",
      "(64, 33)\n",
      "step 20093, loss is 4.746401309967041\n",
      "(64, 33)\n",
      "step 20094, loss is 4.8833489418029785\n",
      "(64, 33)\n",
      "step 20095, loss is 4.63997220993042\n",
      "(64, 33)\n",
      "step 20096, loss is 4.9352030754089355\n",
      "(64, 33)\n",
      "step 20097, loss is 4.907256603240967\n",
      "(64, 33)\n",
      "step 20098, loss is 4.958062171936035\n",
      "(64, 33)\n",
      "step 20099, loss is 4.710304260253906\n",
      "(64, 33)\n",
      "step 20100, loss is 4.872903823852539\n",
      "(64, 33)\n",
      "step 20101, loss is 4.842452526092529\n",
      "(64, 33)\n",
      "step 20102, loss is 4.853253364562988\n",
      "(64, 33)\n",
      "step 20103, loss is 4.784043788909912\n",
      "(64, 33)\n",
      "step 20104, loss is 4.76099157333374\n",
      "(64, 33)\n",
      "step 20105, loss is 4.786297798156738\n",
      "(64, 33)\n",
      "step 20106, loss is 4.753284454345703\n",
      "(64, 33)\n",
      "step 20107, loss is 4.8216047286987305\n",
      "(64, 33)\n",
      "step 20108, loss is 4.776459693908691\n",
      "(64, 33)\n",
      "step 20109, loss is 4.785351276397705\n",
      "(64, 33)\n",
      "step 20110, loss is 4.813963413238525\n",
      "(64, 33)\n",
      "step 20111, loss is 4.755895614624023\n",
      "(64, 33)\n",
      "step 20112, loss is 4.732802867889404\n",
      "(64, 33)\n",
      "step 20113, loss is 4.629386901855469\n",
      "(64, 33)\n",
      "step 20114, loss is 4.817381381988525\n",
      "(64, 33)\n",
      "step 20115, loss is 4.795181751251221\n",
      "(64, 33)\n",
      "step 20116, loss is 4.758751392364502\n",
      "(64, 33)\n",
      "step 20117, loss is 4.618109226226807\n",
      "(64, 33)\n",
      "step 20118, loss is 4.678679466247559\n",
      "(64, 33)\n",
      "step 20119, loss is 4.737766265869141\n",
      "(64, 33)\n",
      "step 20120, loss is 4.854829788208008\n",
      "(64, 33)\n",
      "step 20121, loss is 4.857902526855469\n",
      "(64, 33)\n",
      "step 20122, loss is 4.601005554199219\n",
      "(64, 33)\n",
      "step 20123, loss is 4.782748699188232\n",
      "(64, 33)\n",
      "step 20124, loss is 4.938693523406982\n",
      "(64, 33)\n",
      "step 20125, loss is 4.691075801849365\n",
      "(64, 33)\n",
      "step 20126, loss is 5.0921525955200195\n",
      "(64, 33)\n",
      "step 20127, loss is 4.720208644866943\n",
      "(64, 33)\n",
      "step 20128, loss is 4.635861873626709\n",
      "(64, 33)\n",
      "step 20129, loss is 4.8456950187683105\n",
      "(64, 33)\n",
      "step 20130, loss is 4.602991104125977\n",
      "(64, 33)\n",
      "step 20131, loss is 5.083354949951172\n",
      "(64, 33)\n",
      "step 20132, loss is 4.773545742034912\n",
      "(64, 33)\n",
      "step 20133, loss is 4.814193248748779\n",
      "(64, 33)\n",
      "step 20134, loss is 4.975889205932617\n",
      "(64, 33)\n",
      "step 20135, loss is 4.971651077270508\n",
      "(64, 33)\n",
      "step 20136, loss is 4.728031158447266\n",
      "(64, 33)\n",
      "step 20137, loss is 4.794242858886719\n",
      "(64, 33)\n",
      "step 20138, loss is 4.8187408447265625\n",
      "(64, 33)\n",
      "step 20139, loss is 4.502109050750732\n",
      "(64, 33)\n",
      "step 20140, loss is 4.975826740264893\n",
      "(64, 33)\n",
      "step 20141, loss is 4.817975044250488\n",
      "(64, 33)\n",
      "step 20142, loss is 5.026576519012451\n",
      "(64, 33)\n",
      "step 20143, loss is 4.840762138366699\n",
      "(64, 33)\n",
      "step 20144, loss is 4.680668354034424\n",
      "(64, 33)\n",
      "step 20145, loss is 4.741336345672607\n",
      "(64, 33)\n",
      "step 20146, loss is 4.757289886474609\n",
      "(64, 33)\n",
      "step 20147, loss is 4.800233840942383\n",
      "(64, 33)\n",
      "step 20148, loss is 4.738071918487549\n",
      "(64, 33)\n",
      "step 20149, loss is 4.954585075378418\n",
      "(64, 33)\n",
      "step 20150, loss is 4.577996730804443\n",
      "(64, 33)\n",
      "step 20151, loss is 4.8654704093933105\n",
      "(64, 33)\n",
      "step 20152, loss is 4.878684043884277\n",
      "(64, 33)\n",
      "step 20153, loss is 4.739664554595947\n",
      "(64, 33)\n",
      "step 20154, loss is 4.608548164367676\n",
      "(64, 33)\n",
      "step 20155, loss is 4.934854984283447\n",
      "(64, 33)\n",
      "step 20156, loss is 4.7652387619018555\n",
      "(64, 33)\n",
      "step 20157, loss is 4.551305294036865\n",
      "(64, 33)\n",
      "step 20158, loss is 4.612475872039795\n",
      "(64, 33)\n",
      "step 20159, loss is 4.670849323272705\n",
      "(64, 33)\n",
      "step 20160, loss is 4.747036933898926\n",
      "(64, 33)\n",
      "step 20161, loss is 4.7804179191589355\n",
      "(64, 33)\n",
      "step 20162, loss is 4.698543071746826\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20163, loss is 4.865170478820801\n",
      "(64, 33)\n",
      "step 20164, loss is 4.804503440856934\n",
      "(64, 33)\n",
      "step 20165, loss is 4.85054349899292\n",
      "(64, 33)\n",
      "step 20166, loss is 4.822356224060059\n",
      "(64, 33)\n",
      "step 20167, loss is 4.923168659210205\n",
      "(64, 33)\n",
      "step 20168, loss is 4.859447479248047\n",
      "(64, 33)\n",
      "step 20169, loss is 4.722007751464844\n",
      "(64, 33)\n",
      "step 20170, loss is 4.697789669036865\n",
      "(64, 33)\n",
      "step 20171, loss is 4.604434490203857\n",
      "(64, 33)\n",
      "step 20172, loss is 4.792708396911621\n",
      "(64, 33)\n",
      "step 20173, loss is 4.69634485244751\n",
      "(64, 33)\n",
      "step 20174, loss is 4.7014994621276855\n",
      "(64, 33)\n",
      "step 20175, loss is 4.7301530838012695\n",
      "(64, 33)\n",
      "step 20176, loss is 5.045430660247803\n",
      "(64, 33)\n",
      "step 20177, loss is 4.790473937988281\n",
      "(64, 33)\n",
      "step 20178, loss is 4.8580827713012695\n",
      "(64, 33)\n",
      "step 20179, loss is 4.781793117523193\n",
      "(64, 33)\n",
      "step 20180, loss is 4.617143154144287\n",
      "(64, 33)\n",
      "step 20181, loss is 4.8307390213012695\n",
      "(64, 33)\n",
      "step 20182, loss is 4.849228858947754\n",
      "(64, 33)\n",
      "step 20183, loss is 4.60677433013916\n",
      "(64, 33)\n",
      "step 20184, loss is 4.832108497619629\n",
      "(64, 33)\n",
      "step 20185, loss is 4.798245429992676\n",
      "(64, 33)\n",
      "step 20186, loss is 4.811599254608154\n",
      "(64, 33)\n",
      "step 20187, loss is 4.770665645599365\n",
      "(64, 33)\n",
      "step 20188, loss is 4.858825206756592\n",
      "(64, 33)\n",
      "step 20189, loss is 4.772370338439941\n",
      "(64, 33)\n",
      "step 20190, loss is 4.793156147003174\n",
      "(64, 33)\n",
      "step 20191, loss is 5.039769172668457\n",
      "(64, 33)\n",
      "step 20192, loss is 4.739675998687744\n",
      "(64, 33)\n",
      "step 20193, loss is 4.812320709228516\n",
      "(64, 33)\n",
      "step 20194, loss is 4.724809169769287\n",
      "(64, 33)\n",
      "step 20195, loss is 4.612753868103027\n",
      "(64, 33)\n",
      "step 20196, loss is 4.826695442199707\n",
      "(64, 33)\n",
      "step 20197, loss is 4.777094841003418\n",
      "(64, 33)\n",
      "step 20198, loss is 4.653532028198242\n",
      "(64, 33)\n",
      "step 20199, loss is 4.833594799041748\n",
      "(64, 33)\n",
      "step 20200, loss is 4.614784240722656\n",
      "(64, 33)\n",
      "step 20201, loss is 4.821563243865967\n",
      "(64, 33)\n",
      "step 20202, loss is 4.6171674728393555\n",
      "(64, 33)\n",
      "step 20203, loss is 4.946341514587402\n",
      "(64, 33)\n",
      "step 20204, loss is 4.779402256011963\n",
      "(64, 33)\n",
      "step 20205, loss is 4.858491897583008\n",
      "(64, 33)\n",
      "step 20206, loss is 4.644378185272217\n",
      "(64, 33)\n",
      "step 20207, loss is 4.861935615539551\n",
      "(64, 33)\n",
      "step 20208, loss is 4.898290157318115\n",
      "(64, 33)\n",
      "step 20209, loss is 4.927888870239258\n",
      "(64, 33)\n",
      "step 20210, loss is 4.8059186935424805\n",
      "(64, 33)\n",
      "step 20211, loss is 4.842994689941406\n",
      "(64, 33)\n",
      "step 20212, loss is 4.706572532653809\n",
      "(64, 33)\n",
      "step 20213, loss is 4.79502534866333\n",
      "(64, 33)\n",
      "step 20214, loss is 4.888160228729248\n",
      "(64, 33)\n",
      "step 20215, loss is 4.842103958129883\n",
      "(64, 33)\n",
      "step 20216, loss is 4.911587715148926\n",
      "(64, 33)\n",
      "step 20217, loss is 4.6718645095825195\n",
      "(64, 33)\n",
      "step 20218, loss is 4.900180816650391\n",
      "(64, 33)\n",
      "step 20219, loss is 4.772982120513916\n",
      "(64, 33)\n",
      "step 20220, loss is 4.682330131530762\n",
      "(64, 33)\n",
      "step 20221, loss is 4.775580406188965\n",
      "(64, 33)\n",
      "step 20222, loss is 4.600656032562256\n",
      "(64, 33)\n",
      "step 20223, loss is 4.668488025665283\n",
      "(64, 33)\n",
      "step 20224, loss is 4.72451114654541\n",
      "(64, 33)\n",
      "step 20225, loss is 4.73628044128418\n",
      "(64, 33)\n",
      "step 20226, loss is 4.914757251739502\n",
      "(64, 33)\n",
      "step 20227, loss is 4.750258445739746\n",
      "(64, 33)\n",
      "step 20228, loss is 4.727197170257568\n",
      "(64, 33)\n",
      "step 20229, loss is 4.661705017089844\n",
      "(64, 33)\n",
      "step 20230, loss is 4.729047775268555\n",
      "(64, 33)\n",
      "step 20231, loss is 4.905965328216553\n",
      "(64, 33)\n",
      "step 20232, loss is 4.691456317901611\n",
      "(64, 33)\n",
      "step 20233, loss is 4.71248197555542\n",
      "(64, 33)\n",
      "step 20234, loss is 4.690469741821289\n",
      "(64, 33)\n",
      "step 20235, loss is 4.5967020988464355\n",
      "(64, 33)\n",
      "step 20236, loss is 4.906052589416504\n",
      "(64, 33)\n",
      "step 20237, loss is 4.786369323730469\n",
      "(64, 33)\n",
      "step 20238, loss is 5.053693771362305\n",
      "(64, 33)\n",
      "step 20239, loss is 4.767751216888428\n",
      "(64, 33)\n",
      "step 20240, loss is 4.896873950958252\n",
      "(64, 33)\n",
      "step 20241, loss is 4.723992824554443\n",
      "(64, 33)\n",
      "step 20242, loss is 4.728601932525635\n",
      "(64, 33)\n",
      "step 20243, loss is 4.6762261390686035\n",
      "(64, 33)\n",
      "step 20244, loss is 4.724837779998779\n",
      "(64, 33)\n",
      "step 20245, loss is 4.926589012145996\n",
      "(64, 33)\n",
      "step 20246, loss is 4.838754177093506\n",
      "(64, 33)\n",
      "step 20247, loss is 4.614558219909668\n",
      "(64, 33)\n",
      "step 20248, loss is 4.7552361488342285\n",
      "(64, 33)\n",
      "step 20249, loss is 4.669745922088623\n",
      "(64, 33)\n",
      "step 20250, loss is 4.815588474273682\n",
      "(64, 33)\n",
      "step 20251, loss is 4.846977233886719\n",
      "(64, 33)\n",
      "step 20252, loss is 4.783644199371338\n",
      "(64, 33)\n",
      "step 20253, loss is 4.807663917541504\n",
      "(64, 33)\n",
      "step 20254, loss is 4.8643107414245605\n",
      "(64, 33)\n",
      "step 20255, loss is 4.724648952484131\n",
      "(64, 33)\n",
      "step 20256, loss is 4.886434555053711\n",
      "(64, 33)\n",
      "step 20257, loss is 4.706303596496582\n",
      "(64, 33)\n",
      "step 20258, loss is 4.763701915740967\n",
      "(64, 33)\n",
      "step 20259, loss is 4.9857497215271\n",
      "(64, 33)\n",
      "step 20260, loss is 4.653596878051758\n",
      "(64, 33)\n",
      "step 20261, loss is 4.7811198234558105\n",
      "(64, 33)\n",
      "step 20262, loss is 4.5972771644592285\n",
      "(64, 33)\n",
      "step 20263, loss is 4.761031627655029\n",
      "(64, 33)\n",
      "step 20264, loss is 4.981034278869629\n",
      "(64, 33)\n",
      "step 20265, loss is 4.9098076820373535\n",
      "(64, 33)\n",
      "step 20266, loss is 4.617534160614014\n",
      "(64, 33)\n",
      "step 20267, loss is 4.696989059448242\n",
      "(64, 33)\n",
      "step 20268, loss is 4.86470890045166\n",
      "(64, 33)\n",
      "step 20269, loss is 4.825724124908447\n",
      "(64, 33)\n",
      "step 20270, loss is 4.474454402923584\n",
      "(64, 33)\n",
      "step 20271, loss is 4.83507776260376\n",
      "(64, 33)\n",
      "step 20272, loss is 4.734597682952881\n",
      "(64, 33)\n",
      "step 20273, loss is 4.639886856079102\n",
      "(64, 33)\n",
      "step 20274, loss is 4.729501724243164\n",
      "(64, 33)\n",
      "step 20275, loss is 4.716128826141357\n",
      "(64, 33)\n",
      "step 20276, loss is 4.82883358001709\n",
      "(64, 33)\n",
      "step 20277, loss is 4.944541931152344\n",
      "(64, 33)\n",
      "step 20278, loss is 4.7636942863464355\n",
      "(64, 33)\n",
      "step 20279, loss is 4.637909412384033\n",
      "(64, 33)\n",
      "step 20280, loss is 4.766101837158203\n",
      "(64, 33)\n",
      "step 20281, loss is 4.817016124725342\n",
      "(64, 33)\n",
      "step 20282, loss is 4.920443534851074\n",
      "(64, 33)\n",
      "step 20283, loss is 4.729607582092285\n",
      "(64, 33)\n",
      "step 20284, loss is 4.749715328216553\n",
      "(64, 33)\n",
      "step 20285, loss is 4.70693302154541\n",
      "(64, 33)\n",
      "step 20286, loss is 4.879734992980957\n",
      "(64, 33)\n",
      "step 20287, loss is 4.692338466644287\n",
      "(64, 33)\n",
      "step 20288, loss is 4.769692897796631\n",
      "(64, 33)\n",
      "step 20289, loss is 4.842338562011719\n",
      "(64, 33)\n",
      "step 20290, loss is 4.753855228424072\n",
      "(64, 33)\n",
      "step 20291, loss is 4.749478816986084\n",
      "(64, 33)\n",
      "step 20292, loss is 4.794378280639648\n",
      "(64, 33)\n",
      "step 20293, loss is 4.840026378631592\n",
      "(64, 33)\n",
      "step 20294, loss is 4.647643089294434\n",
      "(64, 33)\n",
      "step 20295, loss is 4.865237236022949\n",
      "(64, 33)\n",
      "step 20296, loss is 4.794312477111816\n",
      "(64, 33)\n",
      "step 20297, loss is 4.95523738861084\n",
      "(64, 33)\n",
      "step 20298, loss is 4.671257019042969\n",
      "(64, 33)\n",
      "step 20299, loss is 4.942620754241943\n",
      "(64, 33)\n",
      "step 20300, loss is 4.716060638427734\n",
      "(64, 33)\n",
      "step 20301, loss is 4.896356582641602\n",
      "(64, 33)\n",
      "step 20302, loss is 4.734253406524658\n",
      "(64, 33)\n",
      "step 20303, loss is 4.752864360809326\n",
      "(64, 33)\n",
      "step 20304, loss is 4.688755989074707\n",
      "(64, 33)\n",
      "step 20305, loss is 4.729954719543457\n",
      "(64, 33)\n",
      "step 20306, loss is 4.625174522399902\n",
      "(64, 33)\n",
      "step 20307, loss is 4.616726875305176\n",
      "(64, 33)\n",
      "step 20308, loss is 4.973448276519775\n",
      "(64, 33)\n",
      "step 20309, loss is 4.788886070251465\n",
      "(64, 33)\n",
      "step 20310, loss is 4.730098247528076\n",
      "(64, 33)\n",
      "step 20311, loss is 4.919670104980469\n",
      "(64, 33)\n",
      "step 20312, loss is 4.7676520347595215\n",
      "(64, 33)\n",
      "step 20313, loss is 4.768040180206299\n",
      "(64, 33)\n",
      "step 20314, loss is 4.657290935516357\n",
      "(64, 33)\n",
      "step 20315, loss is 4.744978427886963\n",
      "(64, 33)\n",
      "step 20316, loss is 4.7009148597717285\n",
      "(64, 33)\n",
      "step 20317, loss is 4.799412727355957\n",
      "(64, 33)\n",
      "step 20318, loss is 4.776618003845215\n",
      "(64, 33)\n",
      "step 20319, loss is 4.897025108337402\n",
      "(64, 33)\n",
      "step 20320, loss is 4.6441779136657715\n",
      "(64, 33)\n",
      "step 20321, loss is 4.702174186706543\n",
      "(64, 33)\n",
      "step 20322, loss is 4.770507335662842\n",
      "(64, 33)\n",
      "step 20323, loss is 4.64942741394043\n",
      "(64, 33)\n",
      "step 20324, loss is 4.881049156188965\n",
      "(64, 33)\n",
      "step 20325, loss is 4.83306884765625\n",
      "(64, 33)\n",
      "step 20326, loss is 4.757009506225586\n",
      "(64, 33)\n",
      "step 20327, loss is 4.870079517364502\n",
      "(64, 33)\n",
      "step 20328, loss is 4.72219705581665\n",
      "(64, 33)\n",
      "step 20329, loss is 4.738443851470947\n",
      "(64, 33)\n",
      "step 20330, loss is 4.6828532218933105\n",
      "(64, 33)\n",
      "step 20331, loss is 4.757883071899414\n",
      "(64, 33)\n",
      "step 20332, loss is 4.783985614776611\n",
      "(64, 33)\n",
      "step 20333, loss is 4.630273342132568\n",
      "(64, 33)\n",
      "step 20334, loss is 4.778875827789307\n",
      "(64, 33)\n",
      "step 20335, loss is 4.716340065002441\n",
      "(64, 33)\n",
      "step 20336, loss is 4.727771759033203\n",
      "(64, 33)\n",
      "step 20337, loss is 4.7604756355285645\n",
      "(64, 33)\n",
      "step 20338, loss is 4.782907962799072\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20339, loss is 4.812361717224121\n",
      "(64, 33)\n",
      "step 20340, loss is 4.7861809730529785\n",
      "(64, 33)\n",
      "step 20341, loss is 4.739871501922607\n",
      "(64, 33)\n",
      "step 20342, loss is 4.8658833503723145\n",
      "(64, 33)\n",
      "step 20343, loss is 4.927947044372559\n",
      "(64, 33)\n",
      "step 20344, loss is 4.649506092071533\n",
      "(64, 33)\n",
      "step 20345, loss is 4.840448379516602\n",
      "(64, 33)\n",
      "step 20346, loss is 4.536064147949219\n",
      "(64, 33)\n",
      "step 20347, loss is 4.977202415466309\n",
      "(64, 33)\n",
      "step 20348, loss is 4.725698947906494\n",
      "(64, 33)\n",
      "step 20349, loss is 4.664257526397705\n",
      "(64, 33)\n",
      "step 20350, loss is 4.802947044372559\n",
      "(64, 33)\n",
      "step 20351, loss is 4.761800289154053\n",
      "(64, 33)\n",
      "step 20352, loss is 4.768520355224609\n",
      "(64, 33)\n",
      "step 20353, loss is 4.947256088256836\n",
      "(64, 33)\n",
      "step 20354, loss is 4.734715461730957\n",
      "(64, 33)\n",
      "step 20355, loss is 4.831628799438477\n",
      "(64, 33)\n",
      "step 20356, loss is 4.860522747039795\n",
      "(64, 33)\n",
      "step 20357, loss is 4.72150993347168\n",
      "(64, 33)\n",
      "step 20358, loss is 4.742848873138428\n",
      "(64, 33)\n",
      "step 20359, loss is 4.731865882873535\n",
      "(64, 33)\n",
      "step 20360, loss is 4.860592842102051\n",
      "(64, 33)\n",
      "step 20361, loss is 4.713464260101318\n",
      "(64, 33)\n",
      "step 20362, loss is 4.495273113250732\n",
      "(64, 33)\n",
      "step 20363, loss is 4.813470363616943\n",
      "(64, 33)\n",
      "step 20364, loss is 4.787201881408691\n",
      "(64, 33)\n",
      "step 20365, loss is 4.94700288772583\n",
      "(64, 33)\n",
      "step 20366, loss is 4.867379665374756\n",
      "(64, 33)\n",
      "step 20367, loss is 4.80780029296875\n",
      "(64, 33)\n",
      "step 20368, loss is 4.767826557159424\n",
      "(64, 33)\n",
      "step 20369, loss is 4.739029407501221\n",
      "(64, 33)\n",
      "step 20370, loss is 4.7004780769348145\n",
      "(64, 33)\n",
      "step 20371, loss is 4.794548034667969\n",
      "(64, 33)\n",
      "step 20372, loss is 4.765192985534668\n",
      "(64, 33)\n",
      "step 20373, loss is 4.89986515045166\n",
      "(64, 33)\n",
      "step 20374, loss is 4.749147891998291\n",
      "(64, 33)\n",
      "step 20375, loss is 4.772066116333008\n",
      "(64, 33)\n",
      "step 20376, loss is 4.664741516113281\n",
      "(64, 33)\n",
      "step 20377, loss is 4.851593494415283\n",
      "(64, 33)\n",
      "step 20378, loss is 4.626883029937744\n",
      "(64, 33)\n",
      "step 20379, loss is 4.8834547996521\n",
      "(64, 33)\n",
      "step 20380, loss is 4.769279479980469\n",
      "(64, 33)\n",
      "step 20381, loss is 4.717252254486084\n",
      "(64, 33)\n",
      "step 20382, loss is 5.016453742980957\n",
      "(64, 33)\n",
      "step 20383, loss is 4.7444987297058105\n",
      "(64, 33)\n",
      "step 20384, loss is 4.977540493011475\n",
      "(64, 33)\n",
      "step 20385, loss is 4.837996959686279\n",
      "(64, 33)\n",
      "step 20386, loss is 4.945077419281006\n",
      "(64, 33)\n",
      "step 20387, loss is 4.636446475982666\n",
      "(64, 33)\n",
      "step 20388, loss is 4.942408561706543\n",
      "(64, 33)\n",
      "step 20389, loss is 4.634679317474365\n",
      "(64, 33)\n",
      "step 20390, loss is 4.6803669929504395\n",
      "(64, 33)\n",
      "step 20391, loss is 4.737251281738281\n",
      "(64, 33)\n",
      "step 20392, loss is 4.697817802429199\n",
      "(64, 33)\n",
      "step 20393, loss is 5.058059215545654\n",
      "(64, 33)\n",
      "step 20394, loss is 4.978219032287598\n",
      "(64, 33)\n",
      "step 20395, loss is 4.896483898162842\n",
      "(64, 33)\n",
      "step 20396, loss is 4.766518592834473\n",
      "(64, 33)\n",
      "step 20397, loss is 4.844391822814941\n",
      "(64, 33)\n",
      "step 20398, loss is 4.83079195022583\n",
      "(64, 33)\n",
      "step 20399, loss is 4.623076915740967\n",
      "(64, 33)\n",
      "step 20400, loss is 4.612422943115234\n",
      "(64, 33)\n",
      "step 20401, loss is 4.459961891174316\n",
      "(64, 33)\n",
      "step 20402, loss is 4.896607875823975\n",
      "(64, 33)\n",
      "step 20403, loss is 4.808806419372559\n",
      "(64, 33)\n",
      "step 20404, loss is 4.76917028427124\n",
      "(64, 33)\n",
      "step 20405, loss is 4.906651973724365\n",
      "(64, 33)\n",
      "step 20406, loss is 4.8343377113342285\n",
      "(64, 33)\n",
      "step 20407, loss is 4.722304344177246\n",
      "(64, 33)\n",
      "step 20408, loss is 4.640653610229492\n",
      "(64, 33)\n",
      "step 20409, loss is 4.736734390258789\n",
      "(64, 33)\n",
      "step 20410, loss is 4.663802146911621\n",
      "(64, 33)\n",
      "step 20411, loss is 4.8605146408081055\n",
      "(64, 33)\n",
      "step 20412, loss is 4.894243240356445\n",
      "(64, 33)\n",
      "step 20413, loss is 4.667514801025391\n",
      "(64, 33)\n",
      "step 20414, loss is 4.750720500946045\n",
      "(64, 33)\n",
      "step 20415, loss is 4.752600193023682\n",
      "(64, 33)\n",
      "step 20416, loss is 4.751834869384766\n",
      "(64, 33)\n",
      "step 20417, loss is 4.794183254241943\n",
      "(64, 33)\n",
      "step 20418, loss is 4.92572546005249\n",
      "(64, 33)\n",
      "step 20419, loss is 4.766186237335205\n",
      "(64, 33)\n",
      "step 20420, loss is 4.817998886108398\n",
      "(64, 33)\n",
      "step 20421, loss is 4.786793231964111\n",
      "(64, 33)\n",
      "step 20422, loss is 4.705014228820801\n",
      "(64, 33)\n",
      "step 20423, loss is 4.8857951164245605\n",
      "(64, 33)\n",
      "step 20424, loss is 4.870631217956543\n",
      "(64, 33)\n",
      "step 20425, loss is 4.7187981605529785\n",
      "(64, 33)\n",
      "step 20426, loss is 4.731420993804932\n",
      "(64, 33)\n",
      "step 20427, loss is 4.877587795257568\n",
      "(64, 33)\n",
      "step 20428, loss is 4.697754383087158\n",
      "(64, 33)\n",
      "step 20429, loss is 4.841578483581543\n",
      "(64, 33)\n",
      "step 20430, loss is 4.794096946716309\n",
      "(64, 33)\n",
      "step 20431, loss is 4.758907794952393\n",
      "(64, 33)\n",
      "step 20432, loss is 4.762741565704346\n",
      "(64, 33)\n",
      "step 20433, loss is 4.697968482971191\n",
      "(64, 33)\n",
      "step 20434, loss is 4.693172931671143\n",
      "(64, 33)\n",
      "step 20435, loss is 4.966679573059082\n",
      "(64, 33)\n",
      "step 20436, loss is 4.860984802246094\n",
      "(64, 33)\n",
      "step 20437, loss is 4.796505451202393\n",
      "(64, 33)\n",
      "step 20438, loss is 4.916689395904541\n",
      "(64, 33)\n",
      "step 20439, loss is 4.923561096191406\n",
      "(64, 33)\n",
      "step 20440, loss is 4.786861896514893\n",
      "(64, 33)\n",
      "step 20441, loss is 4.599708557128906\n",
      "(64, 33)\n",
      "step 20442, loss is 4.609044075012207\n",
      "(64, 33)\n",
      "step 20443, loss is 4.802278518676758\n",
      "(64, 33)\n",
      "step 20444, loss is 4.848762035369873\n",
      "(64, 33)\n",
      "step 20445, loss is 4.844813823699951\n",
      "(64, 33)\n",
      "step 20446, loss is 4.533369541168213\n",
      "(64, 33)\n",
      "step 20447, loss is 4.9569010734558105\n",
      "(64, 33)\n",
      "step 20448, loss is 4.83900785446167\n",
      "(64, 33)\n",
      "step 20449, loss is 4.77167272567749\n",
      "(64, 33)\n",
      "step 20450, loss is 4.960751533508301\n",
      "(64, 33)\n",
      "step 20451, loss is 4.762779235839844\n",
      "(64, 33)\n",
      "step 20452, loss is 4.982375621795654\n",
      "(64, 33)\n",
      "step 20453, loss is 4.828339576721191\n",
      "(64, 33)\n",
      "step 20454, loss is 4.805098533630371\n",
      "(64, 33)\n",
      "step 20455, loss is 4.791446208953857\n",
      "(64, 33)\n",
      "step 20456, loss is 4.781446933746338\n",
      "(64, 33)\n",
      "step 20457, loss is 4.856436252593994\n",
      "(64, 33)\n",
      "step 20458, loss is 4.824507236480713\n",
      "(64, 33)\n",
      "step 20459, loss is 5.024934768676758\n",
      "(64, 33)\n",
      "step 20460, loss is 4.792763710021973\n",
      "(64, 33)\n",
      "step 20461, loss is 4.713799953460693\n",
      "(64, 33)\n",
      "step 20462, loss is 4.8130574226379395\n",
      "(64, 33)\n",
      "step 20463, loss is 4.843722343444824\n",
      "(64, 33)\n",
      "step 20464, loss is 4.773931503295898\n",
      "(64, 33)\n",
      "step 20465, loss is 4.814445495605469\n",
      "(64, 33)\n",
      "step 20466, loss is 4.937594413757324\n",
      "(64, 33)\n",
      "step 20467, loss is 4.721158504486084\n",
      "(64, 33)\n",
      "step 20468, loss is 4.691347122192383\n",
      "(64, 33)\n",
      "step 20469, loss is 4.814978122711182\n",
      "(64, 33)\n",
      "step 20470, loss is 4.659171104431152\n",
      "(64, 33)\n",
      "step 20471, loss is 4.917421340942383\n",
      "(64, 33)\n",
      "step 20472, loss is 4.770470142364502\n",
      "(64, 33)\n",
      "step 20473, loss is 4.748426914215088\n",
      "(64, 33)\n",
      "step 20474, loss is 4.743440628051758\n",
      "(64, 33)\n",
      "step 20475, loss is 4.651537895202637\n",
      "(64, 33)\n",
      "step 20476, loss is 4.889834880828857\n",
      "(64, 33)\n",
      "step 20477, loss is 4.831381797790527\n",
      "(64, 33)\n",
      "step 20478, loss is 4.925879955291748\n",
      "(64, 33)\n",
      "step 20479, loss is 4.584323406219482\n",
      "(64, 33)\n",
      "step 20480, loss is 4.827015399932861\n",
      "(64, 33)\n",
      "step 20481, loss is 4.803705215454102\n",
      "(64, 33)\n",
      "step 20482, loss is 4.615405082702637\n",
      "(64, 33)\n",
      "step 20483, loss is 4.605812072753906\n",
      "(64, 33)\n",
      "step 20484, loss is 4.735665321350098\n",
      "(64, 33)\n",
      "step 20485, loss is 4.709342002868652\n",
      "(64, 33)\n",
      "step 20486, loss is 4.690978527069092\n",
      "(64, 33)\n",
      "step 20487, loss is 4.765066146850586\n",
      "(64, 33)\n",
      "step 20488, loss is 4.732982158660889\n",
      "(64, 33)\n",
      "step 20489, loss is 4.705162048339844\n",
      "(64, 33)\n",
      "step 20490, loss is 4.830731391906738\n",
      "(64, 33)\n",
      "step 20491, loss is 4.837027549743652\n",
      "(64, 33)\n",
      "step 20492, loss is 5.042514801025391\n",
      "(64, 33)\n",
      "step 20493, loss is 4.8919501304626465\n",
      "(64, 33)\n",
      "step 20494, loss is 4.851442813873291\n",
      "(64, 33)\n",
      "step 20495, loss is 4.698935031890869\n",
      "(64, 33)\n",
      "step 20496, loss is 4.746429920196533\n",
      "(64, 33)\n",
      "step 20497, loss is 4.8247809410095215\n",
      "(64, 33)\n",
      "step 20498, loss is 4.929116725921631\n",
      "(64, 33)\n",
      "step 20499, loss is 4.918110370635986\n",
      "(64, 33)\n",
      "step 20500, loss is 4.871766090393066\n",
      "(64, 33)\n",
      "step 20501, loss is 4.85784387588501\n",
      "(64, 33)\n",
      "step 20502, loss is 4.815742492675781\n",
      "(64, 33)\n",
      "step 20503, loss is 4.78082275390625\n",
      "(64, 33)\n",
      "step 20504, loss is 4.845846652984619\n",
      "(64, 33)\n",
      "step 20505, loss is 4.7447638511657715\n",
      "(64, 33)\n",
      "step 20506, loss is 4.7705979347229\n",
      "(64, 33)\n",
      "step 20507, loss is 4.6921000480651855\n",
      "(64, 33)\n",
      "step 20508, loss is 4.926730155944824\n",
      "(64, 33)\n",
      "step 20509, loss is 4.7796125411987305\n",
      "(64, 33)\n",
      "step 20510, loss is 4.873629570007324\n",
      "(64, 33)\n",
      "step 20511, loss is 4.699150085449219\n",
      "(64, 33)\n",
      "step 20512, loss is 4.665211200714111\n",
      "(64, 33)\n",
      "step 20513, loss is 4.790675163269043\n",
      "(64, 33)\n",
      "step 20514, loss is 4.657047271728516\n",
      "(64, 33)\n",
      "step 20515, loss is 4.871743679046631\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20516, loss is 4.707147121429443\n",
      "(64, 33)\n",
      "step 20517, loss is 4.7308478355407715\n",
      "(64, 33)\n",
      "step 20518, loss is 4.807656764984131\n",
      "(64, 33)\n",
      "step 20519, loss is 4.803952217102051\n",
      "(64, 33)\n",
      "step 20520, loss is 4.681111812591553\n",
      "(64, 33)\n",
      "step 20521, loss is 4.940147399902344\n",
      "(64, 33)\n",
      "step 20522, loss is 4.771372318267822\n",
      "(64, 33)\n",
      "step 20523, loss is 4.9153523445129395\n",
      "(64, 33)\n",
      "step 20524, loss is 4.663252353668213\n",
      "(64, 33)\n",
      "step 20525, loss is 4.850277900695801\n",
      "(64, 33)\n",
      "step 20526, loss is 4.785179138183594\n",
      "(64, 33)\n",
      "step 20527, loss is 4.735492706298828\n",
      "(64, 33)\n",
      "step 20528, loss is 5.105291366577148\n",
      "(64, 33)\n",
      "step 20529, loss is 4.481471538543701\n",
      "(64, 33)\n",
      "step 20530, loss is 4.742863178253174\n",
      "(64, 33)\n",
      "step 20531, loss is 4.653759002685547\n",
      "(64, 33)\n",
      "step 20532, loss is 4.875060081481934\n",
      "(64, 33)\n",
      "step 20533, loss is 4.698261260986328\n",
      "(64, 33)\n",
      "step 20534, loss is 4.852476119995117\n",
      "(64, 33)\n",
      "step 20535, loss is 4.672261714935303\n",
      "(64, 33)\n",
      "step 20536, loss is 4.749211311340332\n",
      "(64, 33)\n",
      "step 20537, loss is 4.745312690734863\n",
      "(64, 33)\n",
      "step 20538, loss is 4.741709232330322\n",
      "(64, 33)\n",
      "step 20539, loss is 4.684989929199219\n",
      "(64, 33)\n",
      "step 20540, loss is 4.680975914001465\n",
      "(64, 33)\n",
      "step 20541, loss is 4.951647758483887\n",
      "(64, 33)\n",
      "step 20542, loss is 4.846000671386719\n",
      "(64, 33)\n",
      "step 20543, loss is 4.926363945007324\n",
      "(64, 33)\n",
      "step 20544, loss is 4.728379726409912\n",
      "(64, 33)\n",
      "step 20545, loss is 4.854896545410156\n",
      "(64, 33)\n",
      "step 20546, loss is 4.903751850128174\n",
      "(64, 33)\n",
      "step 20547, loss is 4.830234527587891\n",
      "(64, 33)\n",
      "step 20548, loss is 4.907437801361084\n",
      "(64, 33)\n",
      "step 20549, loss is 4.749048233032227\n",
      "(64, 33)\n",
      "step 20550, loss is 4.984576225280762\n",
      "(64, 33)\n",
      "step 20551, loss is 4.757441997528076\n",
      "(64, 33)\n",
      "step 20552, loss is 4.939610004425049\n",
      "(64, 33)\n",
      "step 20553, loss is 4.8437347412109375\n",
      "(64, 33)\n",
      "step 20554, loss is 4.715449810028076\n",
      "(64, 33)\n",
      "step 20555, loss is 4.989930152893066\n",
      "(64, 33)\n",
      "step 20556, loss is 4.812265872955322\n",
      "(64, 33)\n",
      "step 20557, loss is 4.875219821929932\n",
      "(64, 33)\n",
      "step 20558, loss is 4.75448751449585\n",
      "(64, 33)\n",
      "step 20559, loss is 4.8215765953063965\n",
      "(64, 33)\n",
      "step 20560, loss is 4.860357761383057\n",
      "(64, 33)\n",
      "step 20561, loss is 4.835037708282471\n",
      "(64, 33)\n",
      "step 20562, loss is 4.825101852416992\n",
      "(64, 33)\n",
      "step 20563, loss is 4.779993534088135\n",
      "(64, 33)\n",
      "step 20564, loss is 4.685518741607666\n",
      "(64, 33)\n",
      "step 20565, loss is 4.602843761444092\n",
      "(64, 33)\n",
      "step 20566, loss is 4.79212760925293\n",
      "(64, 33)\n",
      "step 20567, loss is 5.067439556121826\n",
      "(64, 33)\n",
      "step 20568, loss is 4.717827796936035\n",
      "(64, 33)\n",
      "step 20569, loss is 4.7635111808776855\n",
      "(64, 33)\n",
      "step 20570, loss is 4.819444179534912\n",
      "(64, 33)\n",
      "step 20571, loss is 4.680356025695801\n",
      "(64, 33)\n",
      "step 20572, loss is 4.7182817459106445\n",
      "(64, 33)\n",
      "step 20573, loss is 4.888801574707031\n",
      "(64, 33)\n",
      "step 20574, loss is 4.871706008911133\n",
      "(64, 33)\n",
      "step 20575, loss is 4.692686080932617\n",
      "(64, 33)\n",
      "step 20576, loss is 4.634294509887695\n",
      "(64, 33)\n",
      "step 20577, loss is 4.739855766296387\n",
      "(64, 33)\n",
      "step 20578, loss is 4.899002552032471\n",
      "(64, 33)\n",
      "step 20579, loss is 4.684333801269531\n",
      "(64, 33)\n",
      "step 20580, loss is 4.8231072425842285\n",
      "(64, 33)\n",
      "step 20581, loss is 4.824802875518799\n",
      "(64, 33)\n",
      "step 20582, loss is 4.696981906890869\n",
      "(64, 33)\n",
      "step 20583, loss is 4.726363658905029\n",
      "(64, 33)\n",
      "step 20584, loss is 4.895522117614746\n",
      "(64, 33)\n",
      "step 20585, loss is 4.720326900482178\n",
      "(64, 33)\n",
      "step 20586, loss is 4.704730987548828\n",
      "(64, 33)\n",
      "step 20587, loss is 4.851505756378174\n",
      "(64, 33)\n",
      "step 20588, loss is 4.877206325531006\n",
      "(64, 33)\n",
      "step 20589, loss is 4.749589443206787\n",
      "(64, 33)\n",
      "step 20590, loss is 4.840487480163574\n",
      "(64, 33)\n",
      "step 20591, loss is 4.758434772491455\n",
      "(64, 33)\n",
      "step 20592, loss is 4.8168134689331055\n",
      "(64, 33)\n",
      "step 20593, loss is 4.711174964904785\n",
      "(64, 33)\n",
      "step 20594, loss is 5.021491050720215\n",
      "(64, 33)\n",
      "step 20595, loss is 4.7730302810668945\n",
      "(64, 33)\n",
      "step 20596, loss is 4.796630859375\n",
      "(64, 33)\n",
      "step 20597, loss is 4.636458396911621\n",
      "(64, 33)\n",
      "step 20598, loss is 4.944472312927246\n",
      "(64, 33)\n",
      "step 20599, loss is 4.84281063079834\n",
      "(64, 33)\n",
      "step 20600, loss is 4.825397968292236\n",
      "(64, 33)\n",
      "step 20601, loss is 4.691435813903809\n",
      "(64, 33)\n",
      "step 20602, loss is 4.8410749435424805\n",
      "(64, 33)\n",
      "step 20603, loss is 4.968476295471191\n",
      "(64, 33)\n",
      "step 20604, loss is 4.8447651863098145\n",
      "(64, 33)\n",
      "step 20605, loss is 4.826767921447754\n",
      "(64, 33)\n",
      "step 20606, loss is 4.794459819793701\n",
      "(64, 33)\n",
      "step 20607, loss is 4.843591690063477\n",
      "(64, 33)\n",
      "step 20608, loss is 4.820784091949463\n",
      "(64, 33)\n",
      "step 20609, loss is 4.791510105133057\n",
      "(64, 33)\n",
      "step 20610, loss is 4.659090042114258\n",
      "(64, 33)\n",
      "step 20611, loss is 4.658799648284912\n",
      "(64, 33)\n",
      "step 20612, loss is 4.7180094718933105\n",
      "(64, 33)\n",
      "step 20613, loss is 4.818272113800049\n",
      "(64, 33)\n",
      "step 20614, loss is 4.995526313781738\n",
      "(64, 33)\n",
      "step 20615, loss is 4.8350629806518555\n",
      "(64, 33)\n",
      "step 20616, loss is 4.994129180908203\n",
      "(64, 33)\n",
      "step 20617, loss is 4.792390823364258\n",
      "(64, 33)\n",
      "step 20618, loss is 4.879006862640381\n",
      "(64, 33)\n",
      "step 20619, loss is 4.716065406799316\n",
      "(64, 33)\n",
      "step 20620, loss is 4.819987773895264\n",
      "(64, 33)\n",
      "step 20621, loss is 4.688212871551514\n",
      "(64, 33)\n",
      "step 20622, loss is 4.6309967041015625\n",
      "(64, 33)\n",
      "step 20623, loss is 4.748451232910156\n",
      "(64, 33)\n",
      "step 20624, loss is 4.788389682769775\n",
      "(64, 33)\n",
      "step 20625, loss is 4.709147930145264\n",
      "(64, 33)\n",
      "step 20626, loss is 4.736954689025879\n",
      "(64, 33)\n",
      "step 20627, loss is 4.76185941696167\n",
      "(64, 33)\n",
      "step 20628, loss is 4.733409404754639\n",
      "(64, 33)\n",
      "step 20629, loss is 4.885902404785156\n",
      "(64, 33)\n",
      "step 20630, loss is 4.821418762207031\n",
      "(64, 33)\n",
      "step 20631, loss is 4.863385200500488\n",
      "(64, 33)\n",
      "step 20632, loss is 4.791028022766113\n",
      "(64, 33)\n",
      "step 20633, loss is 4.759324550628662\n",
      "(64, 33)\n",
      "step 20634, loss is 4.760740756988525\n",
      "(64, 33)\n",
      "step 20635, loss is 4.942324638366699\n",
      "(64, 33)\n",
      "step 20636, loss is 4.932888984680176\n",
      "(64, 33)\n",
      "step 20637, loss is 4.692245006561279\n",
      "(64, 33)\n",
      "step 20638, loss is 4.905436992645264\n",
      "(64, 33)\n",
      "step 20639, loss is 4.7600178718566895\n",
      "(64, 33)\n",
      "step 20640, loss is 4.710195064544678\n",
      "(64, 33)\n",
      "step 20641, loss is 4.708516597747803\n",
      "(64, 33)\n",
      "step 20642, loss is 4.650299072265625\n",
      "(64, 33)\n",
      "step 20643, loss is 4.841331958770752\n",
      "(64, 33)\n",
      "step 20644, loss is 4.912771701812744\n",
      "(64, 33)\n",
      "step 20645, loss is 4.577609539031982\n",
      "(64, 33)\n",
      "step 20646, loss is 4.703869342803955\n",
      "(64, 33)\n",
      "step 20647, loss is 4.8897318840026855\n",
      "(64, 33)\n",
      "step 20648, loss is 4.799278736114502\n",
      "(64, 33)\n",
      "step 20649, loss is 4.91640567779541\n",
      "(64, 33)\n",
      "step 20650, loss is 4.855260372161865\n",
      "(64, 33)\n",
      "step 20651, loss is 4.703144550323486\n",
      "(64, 33)\n",
      "step 20652, loss is 5.0163187980651855\n",
      "(64, 33)\n",
      "step 20653, loss is 4.872497081756592\n",
      "(64, 33)\n",
      "step 20654, loss is 4.739233493804932\n",
      "(64, 33)\n",
      "step 20655, loss is 4.671358108520508\n",
      "(64, 33)\n",
      "step 20656, loss is 4.782222747802734\n",
      "(64, 33)\n",
      "step 20657, loss is 4.658155918121338\n",
      "(64, 33)\n",
      "step 20658, loss is 4.8686089515686035\n",
      "(64, 33)\n",
      "step 20659, loss is 4.608924865722656\n",
      "(64, 33)\n",
      "step 20660, loss is 4.787745952606201\n",
      "(64, 33)\n",
      "step 20661, loss is 4.837885856628418\n",
      "(64, 33)\n",
      "step 20662, loss is 4.822264671325684\n",
      "(64, 33)\n",
      "step 20663, loss is 4.755178928375244\n",
      "(64, 33)\n",
      "step 20664, loss is 4.710258960723877\n",
      "(64, 33)\n",
      "step 20665, loss is 4.766283988952637\n",
      "(64, 33)\n",
      "step 20666, loss is 4.817497253417969\n",
      "(64, 33)\n",
      "step 20667, loss is 4.8025641441345215\n",
      "(64, 33)\n",
      "step 20668, loss is 4.734343528747559\n",
      "(64, 33)\n",
      "step 20669, loss is 4.90574836730957\n",
      "(64, 33)\n",
      "step 20670, loss is 4.712489128112793\n",
      "(64, 33)\n",
      "step 20671, loss is 4.810952186584473\n",
      "(64, 33)\n",
      "step 20672, loss is 4.772634983062744\n",
      "(64, 33)\n",
      "step 20673, loss is 4.887411594390869\n",
      "(64, 33)\n",
      "step 20674, loss is 4.79478645324707\n",
      "(64, 33)\n",
      "step 20675, loss is 4.807788848876953\n",
      "(64, 33)\n",
      "step 20676, loss is 4.933974266052246\n",
      "(64, 33)\n",
      "step 20677, loss is 4.7516045570373535\n",
      "(64, 33)\n",
      "step 20678, loss is 4.793459415435791\n",
      "(64, 33)\n",
      "step 20679, loss is 4.643753528594971\n",
      "(64, 33)\n",
      "step 20680, loss is 4.836986064910889\n",
      "(64, 33)\n",
      "step 20681, loss is 4.64547061920166\n",
      "(64, 33)\n",
      "step 20682, loss is 4.618654251098633\n",
      "(64, 33)\n",
      "step 20683, loss is 4.7144389152526855\n",
      "(64, 33)\n",
      "step 20684, loss is 4.831999778747559\n",
      "(64, 33)\n",
      "step 20685, loss is 4.742321968078613\n",
      "(64, 33)\n",
      "step 20686, loss is 4.860246658325195\n",
      "(64, 33)\n",
      "step 20687, loss is 4.891284465789795\n",
      "(64, 33)\n",
      "step 20688, loss is 4.762095928192139\n",
      "(64, 33)\n",
      "step 20689, loss is 4.696617126464844\n",
      "(64, 33)\n",
      "step 20690, loss is 4.841597080230713\n",
      "(64, 33)\n",
      "step 20691, loss is 4.774464130401611\n",
      "(64, 33)\n",
      "step 20692, loss is 4.774815559387207\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20693, loss is 4.698609828948975\n",
      "(64, 33)\n",
      "step 20694, loss is 4.784527778625488\n",
      "(64, 33)\n",
      "step 20695, loss is 4.7526326179504395\n",
      "(64, 33)\n",
      "step 20696, loss is 4.855195999145508\n",
      "(64, 33)\n",
      "step 20697, loss is 4.798490524291992\n",
      "(64, 33)\n",
      "step 20698, loss is 4.890153884887695\n",
      "(64, 33)\n",
      "step 20699, loss is 4.858386039733887\n",
      "(64, 33)\n",
      "step 20700, loss is 4.86231803894043\n",
      "(64, 33)\n",
      "step 20701, loss is 4.8659467697143555\n",
      "(64, 33)\n",
      "step 20702, loss is 4.846792697906494\n",
      "(64, 33)\n",
      "step 20703, loss is 4.680458068847656\n",
      "(64, 33)\n",
      "step 20704, loss is 4.838756084442139\n",
      "(64, 33)\n",
      "step 20705, loss is 4.719431400299072\n",
      "(64, 33)\n",
      "step 20706, loss is 4.958338737487793\n",
      "(64, 33)\n",
      "step 20707, loss is 4.744353771209717\n",
      "(64, 33)\n",
      "step 20708, loss is 4.794394493103027\n",
      "(64, 33)\n",
      "step 20709, loss is 4.709707736968994\n",
      "(64, 33)\n",
      "step 20710, loss is 4.706989765167236\n",
      "(64, 33)\n",
      "step 20711, loss is 4.895802021026611\n",
      "(64, 33)\n",
      "step 20712, loss is 4.7477803230285645\n",
      "(64, 33)\n",
      "step 20713, loss is 4.738883972167969\n",
      "(64, 33)\n",
      "step 20714, loss is 4.704275608062744\n",
      "(64, 33)\n",
      "step 20715, loss is 4.879124641418457\n",
      "(64, 33)\n",
      "step 20716, loss is 4.699455261230469\n",
      "(64, 33)\n",
      "step 20717, loss is 4.843905448913574\n",
      "(64, 33)\n",
      "step 20718, loss is 4.840423583984375\n",
      "(64, 33)\n",
      "step 20719, loss is 4.769942283630371\n",
      "(64, 33)\n",
      "step 20720, loss is 4.764085292816162\n",
      "(64, 33)\n",
      "step 20721, loss is 4.756156921386719\n",
      "(64, 33)\n",
      "step 20722, loss is 4.93094539642334\n",
      "(64, 33)\n",
      "step 20723, loss is 4.930138111114502\n",
      "(64, 33)\n",
      "step 20724, loss is 4.888140678405762\n",
      "(64, 33)\n",
      "step 20725, loss is 4.924628734588623\n",
      "(64, 33)\n",
      "step 20726, loss is 4.7409892082214355\n",
      "(64, 33)\n",
      "step 20727, loss is 4.799531936645508\n",
      "(64, 33)\n",
      "step 20728, loss is 4.738795280456543\n",
      "(64, 33)\n",
      "step 20729, loss is 4.916603088378906\n",
      "(64, 33)\n",
      "step 20730, loss is 4.837684154510498\n",
      "(64, 33)\n",
      "step 20731, loss is 4.676300048828125\n",
      "(64, 33)\n",
      "step 20732, loss is 4.6842570304870605\n",
      "(64, 33)\n",
      "step 20733, loss is 4.705414772033691\n",
      "(64, 33)\n",
      "step 20734, loss is 4.7474188804626465\n",
      "(64, 33)\n",
      "step 20735, loss is 4.787130832672119\n",
      "(64, 33)\n",
      "step 20736, loss is 4.770143985748291\n",
      "(64, 33)\n",
      "step 20737, loss is 4.910965919494629\n",
      "(64, 33)\n",
      "step 20738, loss is 4.648866176605225\n",
      "(64, 33)\n",
      "step 20739, loss is 4.751546382904053\n",
      "(64, 33)\n",
      "step 20740, loss is 4.861711025238037\n",
      "(64, 33)\n",
      "step 20741, loss is 4.909225940704346\n",
      "(64, 33)\n",
      "step 20742, loss is 4.7142486572265625\n",
      "(64, 33)\n",
      "step 20743, loss is 4.653309345245361\n",
      "(64, 33)\n",
      "step 20744, loss is 4.792511463165283\n",
      "(64, 33)\n",
      "step 20745, loss is 4.884640693664551\n",
      "(64, 33)\n",
      "step 20746, loss is 4.901047706604004\n",
      "(64, 33)\n",
      "step 20747, loss is 4.726600646972656\n",
      "(64, 33)\n",
      "step 20748, loss is 4.770377159118652\n",
      "(64, 33)\n",
      "step 20749, loss is 4.7574872970581055\n",
      "(64, 33)\n",
      "step 20750, loss is 4.790339469909668\n",
      "(64, 33)\n",
      "step 20751, loss is 4.757258415222168\n",
      "(64, 33)\n",
      "step 20752, loss is 4.819552421569824\n",
      "(64, 33)\n",
      "step 20753, loss is 4.578941345214844\n",
      "(64, 33)\n",
      "step 20754, loss is 4.613909721374512\n",
      "(64, 33)\n",
      "step 20755, loss is 4.610637187957764\n",
      "(64, 33)\n",
      "step 20756, loss is 4.836691856384277\n",
      "(64, 33)\n",
      "step 20757, loss is 4.823858261108398\n",
      "(64, 33)\n",
      "step 20758, loss is 4.943411827087402\n",
      "(64, 33)\n",
      "step 20759, loss is 4.7048845291137695\n",
      "(64, 33)\n",
      "step 20760, loss is 4.862198829650879\n",
      "(64, 33)\n",
      "step 20761, loss is 4.947141647338867\n",
      "(64, 33)\n",
      "step 20762, loss is 4.769731044769287\n",
      "(64, 33)\n",
      "step 20763, loss is 4.609141826629639\n",
      "(64, 33)\n",
      "step 20764, loss is 4.853058338165283\n",
      "(64, 33)\n",
      "step 20765, loss is 4.708207607269287\n",
      "(64, 33)\n",
      "step 20766, loss is 4.79935359954834\n",
      "(64, 33)\n",
      "step 20767, loss is 4.850779056549072\n",
      "(64, 33)\n",
      "step 20768, loss is 4.645939826965332\n",
      "(64, 33)\n",
      "step 20769, loss is 4.858527183532715\n",
      "(64, 33)\n",
      "step 20770, loss is 4.814977169036865\n",
      "(64, 33)\n",
      "step 20771, loss is 4.87454080581665\n",
      "(64, 33)\n",
      "step 20772, loss is 4.5074687004089355\n",
      "(64, 33)\n",
      "step 20773, loss is 4.7929768562316895\n",
      "(64, 33)\n",
      "step 20774, loss is 4.974795818328857\n",
      "(64, 33)\n",
      "step 20775, loss is 5.043313980102539\n",
      "(64, 33)\n",
      "step 20776, loss is 4.693812370300293\n",
      "(64, 33)\n",
      "step 20777, loss is 4.612578868865967\n",
      "(64, 33)\n",
      "step 20778, loss is 4.531591892242432\n",
      "(64, 33)\n",
      "step 20779, loss is 4.781406879425049\n",
      "(64, 33)\n",
      "step 20780, loss is 4.730669021606445\n",
      "(64, 33)\n",
      "step 20781, loss is 4.86998176574707\n",
      "(64, 33)\n",
      "step 20782, loss is 4.869874000549316\n",
      "(64, 33)\n",
      "step 20783, loss is 4.7086310386657715\n",
      "(64, 33)\n",
      "step 20784, loss is 4.751142501831055\n",
      "(64, 33)\n",
      "step 20785, loss is 4.602973937988281\n",
      "(64, 33)\n",
      "step 20786, loss is 4.840885639190674\n",
      "(64, 33)\n",
      "step 20787, loss is 4.908437252044678\n",
      "(64, 33)\n",
      "step 20788, loss is 4.864442825317383\n",
      "(64, 33)\n",
      "step 20789, loss is 4.933566093444824\n",
      "(64, 33)\n",
      "step 20790, loss is 4.612745761871338\n",
      "(64, 33)\n",
      "step 20791, loss is 4.686827659606934\n",
      "(64, 33)\n",
      "step 20792, loss is 4.8149094581604\n",
      "(64, 33)\n",
      "step 20793, loss is 4.701244831085205\n",
      "(64, 33)\n",
      "step 20794, loss is 4.788897514343262\n",
      "(64, 33)\n",
      "step 20795, loss is 4.813634872436523\n",
      "(64, 33)\n",
      "step 20796, loss is 4.781229496002197\n",
      "(64, 33)\n",
      "step 20797, loss is 4.921854496002197\n",
      "(64, 33)\n",
      "step 20798, loss is 4.730638027191162\n",
      "(64, 33)\n",
      "step 20799, loss is 4.918504238128662\n",
      "(64, 33)\n",
      "step 20800, loss is 4.7444305419921875\n",
      "(64, 33)\n",
      "step 20801, loss is 4.883194446563721\n",
      "(64, 33)\n",
      "step 20802, loss is 4.753355979919434\n",
      "(64, 33)\n",
      "step 20803, loss is 5.030218601226807\n",
      "(64, 33)\n",
      "step 20804, loss is 4.721871852874756\n",
      "(64, 33)\n",
      "step 20805, loss is 5.00553035736084\n",
      "(64, 33)\n",
      "step 20806, loss is 4.720827102661133\n",
      "(64, 33)\n",
      "step 20807, loss is 4.697956562042236\n",
      "(64, 33)\n",
      "step 20808, loss is 4.809498310089111\n",
      "(64, 33)\n",
      "step 20809, loss is 4.60788631439209\n",
      "(64, 33)\n",
      "step 20810, loss is 5.018437385559082\n",
      "(64, 33)\n",
      "step 20811, loss is 4.8735504150390625\n",
      "(64, 33)\n",
      "step 20812, loss is 4.73209285736084\n",
      "(64, 33)\n",
      "step 20813, loss is 4.832571029663086\n",
      "(64, 33)\n",
      "step 20814, loss is 4.803481101989746\n",
      "(64, 33)\n",
      "step 20815, loss is 4.753190040588379\n",
      "(64, 33)\n",
      "step 20816, loss is 4.782723903656006\n",
      "(64, 33)\n",
      "step 20817, loss is 4.755458831787109\n",
      "(64, 33)\n",
      "step 20818, loss is 4.692780494689941\n",
      "(64, 33)\n",
      "step 20819, loss is 4.797266483306885\n",
      "(64, 33)\n",
      "step 20820, loss is 4.879508972167969\n",
      "(64, 33)\n",
      "step 20821, loss is 4.746630668640137\n",
      "(64, 33)\n",
      "step 20822, loss is 4.981021404266357\n",
      "(64, 33)\n",
      "step 20823, loss is 4.71626615524292\n",
      "(64, 33)\n",
      "step 20824, loss is 4.876060485839844\n",
      "(64, 33)\n",
      "step 20825, loss is 4.753395080566406\n",
      "(64, 33)\n",
      "step 20826, loss is 4.488731384277344\n",
      "(64, 33)\n",
      "step 20827, loss is 4.777485370635986\n",
      "(64, 33)\n",
      "step 20828, loss is 4.766143321990967\n",
      "(64, 33)\n",
      "step 20829, loss is 4.734708309173584\n",
      "(64, 33)\n",
      "step 20830, loss is 4.741397857666016\n",
      "(64, 33)\n",
      "step 20831, loss is 4.881824016571045\n",
      "(64, 33)\n",
      "step 20832, loss is 4.867591857910156\n",
      "(64, 33)\n",
      "step 20833, loss is 4.69704008102417\n",
      "(64, 33)\n",
      "step 20834, loss is 4.81580924987793\n",
      "(64, 33)\n",
      "step 20835, loss is 4.7599077224731445\n",
      "(64, 33)\n",
      "step 20836, loss is 4.923544406890869\n",
      "(64, 33)\n",
      "step 20837, loss is 4.787359714508057\n",
      "(64, 33)\n",
      "step 20838, loss is 4.868278503417969\n",
      "(64, 33)\n",
      "step 20839, loss is 4.663061141967773\n",
      "(64, 33)\n",
      "step 20840, loss is 4.696793556213379\n",
      "(64, 33)\n",
      "step 20841, loss is 4.684885501861572\n",
      "(64, 33)\n",
      "step 20842, loss is 4.849808216094971\n",
      "(64, 33)\n",
      "step 20843, loss is 4.75006103515625\n",
      "(64, 33)\n",
      "step 20844, loss is 4.84590482711792\n",
      "(64, 33)\n",
      "step 20845, loss is 4.6677069664001465\n",
      "(64, 33)\n",
      "step 20846, loss is 4.755213737487793\n",
      "(64, 33)\n",
      "step 20847, loss is 4.868134021759033\n",
      "(64, 33)\n",
      "step 20848, loss is 4.716841220855713\n",
      "(64, 33)\n",
      "step 20849, loss is 4.788992881774902\n",
      "(64, 33)\n",
      "step 20850, loss is 4.748798847198486\n",
      "(64, 33)\n",
      "step 20851, loss is 4.8984761238098145\n",
      "(64, 33)\n",
      "step 20852, loss is 4.720821857452393\n",
      "(64, 33)\n",
      "step 20853, loss is 4.76662540435791\n",
      "(64, 33)\n",
      "step 20854, loss is 4.948441505432129\n",
      "(64, 33)\n",
      "step 20855, loss is 4.87502384185791\n",
      "(64, 33)\n",
      "step 20856, loss is 4.706266403198242\n",
      "(64, 33)\n",
      "step 20857, loss is 4.825327396392822\n",
      "(64, 33)\n",
      "step 20858, loss is 4.886102199554443\n",
      "(64, 33)\n",
      "step 20859, loss is 4.720351219177246\n",
      "(64, 33)\n",
      "step 20860, loss is 4.7296881675720215\n",
      "(64, 33)\n",
      "step 20861, loss is 4.977447032928467\n",
      "(64, 33)\n",
      "step 20862, loss is 4.890139579772949\n",
      "(64, 33)\n",
      "step 20863, loss is 4.857181549072266\n",
      "(64, 33)\n",
      "step 20864, loss is 4.945838928222656\n",
      "(64, 33)\n",
      "step 20865, loss is 4.945058345794678\n",
      "(64, 33)\n",
      "step 20866, loss is 4.768651962280273\n",
      "(64, 33)\n",
      "step 20867, loss is 4.5886969566345215\n",
      "(64, 33)\n",
      "step 20868, loss is 4.831798076629639\n",
      "(64, 33)\n",
      "step 20869, loss is 4.683563709259033\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20870, loss is 4.929049491882324\n",
      "(64, 33)\n",
      "step 20871, loss is 4.915063381195068\n",
      "(64, 33)\n",
      "step 20872, loss is 4.934962272644043\n",
      "(64, 33)\n",
      "step 20873, loss is 4.857191562652588\n",
      "(64, 33)\n",
      "step 20874, loss is 4.721818923950195\n",
      "(64, 33)\n",
      "step 20875, loss is 4.7353010177612305\n",
      "(64, 33)\n",
      "step 20876, loss is 4.864946365356445\n",
      "(64, 33)\n",
      "step 20877, loss is 4.8018059730529785\n",
      "(64, 33)\n",
      "step 20878, loss is 4.777154922485352\n",
      "(64, 33)\n",
      "step 20879, loss is 4.837845802307129\n",
      "(64, 33)\n",
      "step 20880, loss is 4.660257339477539\n",
      "(64, 33)\n",
      "step 20881, loss is 4.799154281616211\n",
      "(64, 33)\n",
      "step 20882, loss is 4.8033294677734375\n",
      "(64, 33)\n",
      "step 20883, loss is 4.611564636230469\n",
      "(64, 33)\n",
      "step 20884, loss is 4.827328681945801\n",
      "(64, 33)\n",
      "step 20885, loss is 4.649311065673828\n",
      "(64, 33)\n",
      "step 20886, loss is 4.736349105834961\n",
      "(64, 33)\n",
      "step 20887, loss is 4.657776355743408\n",
      "(64, 33)\n",
      "step 20888, loss is 4.76009464263916\n",
      "(64, 33)\n",
      "step 20889, loss is 4.917624473571777\n",
      "(64, 33)\n",
      "step 20890, loss is 4.782618522644043\n",
      "(64, 33)\n",
      "step 20891, loss is 4.8174872398376465\n",
      "(64, 33)\n",
      "step 20892, loss is 4.724418640136719\n",
      "(64, 33)\n",
      "step 20893, loss is 4.6154961585998535\n",
      "(64, 33)\n",
      "step 20894, loss is 4.749551773071289\n",
      "(64, 33)\n",
      "step 20895, loss is 4.79124641418457\n",
      "(64, 33)\n",
      "step 20896, loss is 4.885974407196045\n",
      "(64, 33)\n",
      "step 20897, loss is 4.738318920135498\n",
      "(64, 33)\n",
      "step 20898, loss is 4.732127666473389\n",
      "(64, 33)\n",
      "step 20899, loss is 4.881460666656494\n",
      "(64, 33)\n",
      "step 20900, loss is 4.709882736206055\n",
      "(64, 33)\n",
      "step 20901, loss is 4.806168079376221\n",
      "(64, 33)\n",
      "step 20902, loss is 4.736023902893066\n",
      "(64, 33)\n",
      "step 20903, loss is 4.737354278564453\n",
      "(64, 33)\n",
      "step 20904, loss is 4.923947811126709\n",
      "(64, 33)\n",
      "step 20905, loss is 4.639810562133789\n",
      "(64, 33)\n",
      "step 20906, loss is 4.6843366622924805\n",
      "(64, 33)\n",
      "step 20907, loss is 4.768274784088135\n",
      "(64, 33)\n",
      "step 20908, loss is 4.823707580566406\n",
      "(64, 33)\n",
      "step 20909, loss is 4.846426486968994\n",
      "(64, 33)\n",
      "step 20910, loss is 4.833746910095215\n",
      "(64, 33)\n",
      "step 20911, loss is 4.851134777069092\n",
      "(64, 33)\n",
      "step 20912, loss is 4.622668266296387\n",
      "(64, 33)\n",
      "step 20913, loss is 4.825028896331787\n",
      "(64, 33)\n",
      "step 20914, loss is 4.894155025482178\n",
      "(64, 33)\n",
      "step 20915, loss is 4.9402031898498535\n",
      "(64, 33)\n",
      "step 20916, loss is 4.804512023925781\n",
      "(64, 33)\n",
      "step 20917, loss is 4.755865097045898\n",
      "(64, 33)\n",
      "step 20918, loss is 4.546128273010254\n",
      "(64, 33)\n",
      "step 20919, loss is 4.685163497924805\n",
      "(64, 33)\n",
      "step 20920, loss is 4.69656229019165\n",
      "(64, 33)\n",
      "step 20921, loss is 4.831567287445068\n",
      "(64, 33)\n",
      "step 20922, loss is 4.915605545043945\n",
      "(64, 33)\n",
      "step 20923, loss is 4.693635940551758\n",
      "(64, 33)\n",
      "step 20924, loss is 4.8275861740112305\n",
      "(64, 33)\n",
      "step 20925, loss is 4.714649200439453\n",
      "(64, 33)\n",
      "step 20926, loss is 4.730754375457764\n",
      "(64, 33)\n",
      "step 20927, loss is 4.855033874511719\n",
      "(64, 33)\n",
      "step 20928, loss is 5.008641719818115\n",
      "(64, 33)\n",
      "step 20929, loss is 4.679502010345459\n",
      "(64, 33)\n",
      "step 20930, loss is 4.752834320068359\n",
      "(64, 33)\n",
      "step 20931, loss is 4.811098098754883\n",
      "(64, 33)\n",
      "step 20932, loss is 4.9664740562438965\n",
      "(64, 33)\n",
      "step 20933, loss is 4.72899055480957\n",
      "(64, 33)\n",
      "step 20934, loss is 4.701680660247803\n",
      "(64, 33)\n",
      "step 20935, loss is 4.799680709838867\n",
      "(64, 33)\n",
      "step 20936, loss is 4.967006206512451\n",
      "(64, 33)\n",
      "step 20937, loss is 4.766704082489014\n",
      "(64, 33)\n",
      "step 20938, loss is 4.879997253417969\n",
      "(64, 33)\n",
      "step 20939, loss is 4.9162397384643555\n",
      "(64, 33)\n",
      "step 20940, loss is 4.888003826141357\n",
      "(64, 33)\n",
      "step 20941, loss is 4.864252090454102\n",
      "(64, 33)\n",
      "step 20942, loss is 4.7668280601501465\n",
      "(64, 33)\n",
      "step 20943, loss is 4.599300861358643\n",
      "(64, 33)\n",
      "step 20944, loss is 4.691551685333252\n",
      "(64, 33)\n",
      "step 20945, loss is 4.713099956512451\n",
      "(64, 33)\n",
      "step 20946, loss is 4.856330871582031\n",
      "(64, 33)\n",
      "step 20947, loss is 4.659609794616699\n",
      "(64, 33)\n",
      "step 20948, loss is 4.732478141784668\n",
      "(64, 33)\n",
      "step 20949, loss is 4.803194522857666\n",
      "(64, 33)\n",
      "step 20950, loss is 5.028741836547852\n",
      "(64, 33)\n",
      "step 20951, loss is 4.906613826751709\n",
      "(64, 33)\n",
      "step 20952, loss is 4.883852005004883\n",
      "(64, 33)\n",
      "step 20953, loss is 4.7733635902404785\n",
      "(64, 33)\n",
      "step 20954, loss is 4.787224292755127\n",
      "(64, 33)\n",
      "step 20955, loss is 4.813761234283447\n",
      "(64, 33)\n",
      "step 20956, loss is 4.9880690574646\n",
      "(64, 33)\n",
      "step 20957, loss is 4.844862461090088\n",
      "(64, 33)\n",
      "step 20958, loss is 4.830863952636719\n",
      "(64, 33)\n",
      "step 20959, loss is 4.9903106689453125\n",
      "(64, 33)\n",
      "step 20960, loss is 4.797434329986572\n",
      "(64, 33)\n",
      "step 20961, loss is 4.81400728225708\n",
      "(64, 33)\n",
      "step 20962, loss is 4.8492584228515625\n",
      "(64, 33)\n",
      "step 20963, loss is 4.9200239181518555\n",
      "(64, 33)\n",
      "step 20964, loss is 4.948397636413574\n",
      "(64, 33)\n",
      "step 20965, loss is 4.741291522979736\n",
      "(64, 33)\n",
      "step 20966, loss is 4.829572677612305\n",
      "(64, 33)\n",
      "step 20967, loss is 4.772264003753662\n",
      "(64, 33)\n",
      "step 20968, loss is 4.846765041351318\n",
      "(64, 33)\n",
      "step 20969, loss is 4.9155168533325195\n",
      "(64, 33)\n",
      "step 20970, loss is 4.788811683654785\n",
      "(64, 33)\n",
      "step 20971, loss is 4.898124694824219\n",
      "(64, 33)\n",
      "step 20972, loss is 4.803347110748291\n",
      "(64, 33)\n",
      "step 20973, loss is 4.831109523773193\n",
      "(64, 33)\n",
      "step 20974, loss is 4.930239200592041\n",
      "(64, 33)\n",
      "step 20975, loss is 4.851719856262207\n",
      "(64, 33)\n",
      "step 20976, loss is 4.935760974884033\n",
      "(64, 33)\n",
      "step 20977, loss is 4.818729400634766\n",
      "(64, 33)\n",
      "step 20978, loss is 4.880577087402344\n",
      "(64, 33)\n",
      "step 20979, loss is 4.834602355957031\n",
      "(64, 33)\n",
      "step 20980, loss is 4.786734104156494\n",
      "(64, 33)\n",
      "step 20981, loss is 4.620600700378418\n",
      "(64, 33)\n",
      "step 20982, loss is 4.925357341766357\n",
      "(64, 33)\n",
      "step 20983, loss is 4.926259994506836\n",
      "(64, 33)\n",
      "step 20984, loss is 4.768091678619385\n",
      "(64, 33)\n",
      "step 20985, loss is 4.841464996337891\n",
      "(64, 33)\n",
      "step 20986, loss is 4.7764692306518555\n",
      "(64, 33)\n",
      "step 20987, loss is 4.845056056976318\n",
      "(64, 33)\n",
      "step 20988, loss is 4.733169078826904\n",
      "(64, 33)\n",
      "step 20989, loss is 5.0535197257995605\n",
      "(64, 33)\n",
      "step 20990, loss is 4.834152698516846\n",
      "(64, 33)\n",
      "step 20991, loss is 4.778677463531494\n",
      "(64, 33)\n",
      "step 20992, loss is 4.829041957855225\n",
      "(64, 33)\n",
      "step 20993, loss is 4.972729206085205\n",
      "(64, 33)\n",
      "step 20994, loss is 4.659329414367676\n",
      "(64, 33)\n",
      "step 20995, loss is 4.848479270935059\n",
      "(64, 33)\n",
      "step 20996, loss is 4.734519004821777\n",
      "(64, 33)\n",
      "step 20997, loss is 4.802184581756592\n",
      "(64, 33)\n",
      "step 20998, loss is 5.010018348693848\n",
      "(64, 33)\n",
      "step 20999, loss is 4.833850860595703\n",
      "(64, 33)\n",
      "step 21000, loss is 4.610354900360107\n",
      "(64, 33)\n",
      "step 21001, loss is 4.777336120605469\n",
      "(64, 33)\n",
      "step 21002, loss is 4.997838973999023\n",
      "(64, 33)\n",
      "step 21003, loss is 4.769776344299316\n",
      "(64, 33)\n",
      "step 21004, loss is 4.741168975830078\n",
      "(64, 33)\n",
      "step 21005, loss is 4.713249206542969\n",
      "(64, 33)\n",
      "step 21006, loss is 4.799960136413574\n",
      "(64, 33)\n",
      "step 21007, loss is 4.8806891441345215\n",
      "(64, 33)\n",
      "step 21008, loss is 4.824661731719971\n",
      "(64, 33)\n",
      "step 21009, loss is 4.665591716766357\n",
      "(64, 33)\n",
      "step 21010, loss is 4.787777423858643\n",
      "(64, 33)\n",
      "step 21011, loss is 4.964791297912598\n",
      "(64, 33)\n",
      "step 21012, loss is 4.752849102020264\n",
      "(64, 33)\n",
      "step 21013, loss is 4.85770320892334\n",
      "(64, 33)\n",
      "step 21014, loss is 4.724411487579346\n",
      "(64, 33)\n",
      "step 21015, loss is 4.790331840515137\n",
      "(64, 33)\n",
      "step 21016, loss is 4.733928203582764\n",
      "(64, 33)\n",
      "step 21017, loss is 4.726271152496338\n",
      "(64, 33)\n",
      "step 21018, loss is 4.770567893981934\n",
      "(64, 33)\n",
      "step 21019, loss is 4.467008113861084\n",
      "(64, 33)\n",
      "step 21020, loss is 4.813069820404053\n",
      "(64, 33)\n",
      "step 21021, loss is 4.599411487579346\n",
      "(64, 33)\n",
      "step 21022, loss is 4.908489227294922\n",
      "(64, 33)\n",
      "step 21023, loss is 4.7947611808776855\n",
      "(64, 33)\n",
      "step 21024, loss is 5.11671781539917\n",
      "(64, 33)\n",
      "step 21025, loss is 4.77913236618042\n",
      "(64, 33)\n",
      "step 21026, loss is 4.897744178771973\n",
      "(64, 33)\n",
      "step 21027, loss is 4.822875022888184\n",
      "(64, 33)\n",
      "step 21028, loss is 4.75038480758667\n",
      "(64, 33)\n",
      "step 21029, loss is 4.864492416381836\n",
      "(64, 33)\n",
      "step 21030, loss is 4.8665452003479\n",
      "(64, 33)\n",
      "step 21031, loss is 4.892210006713867\n",
      "(64, 33)\n",
      "step 21032, loss is 4.523329257965088\n",
      "(64, 33)\n",
      "step 21033, loss is 4.808899879455566\n",
      "(64, 33)\n",
      "step 21034, loss is 4.870790958404541\n",
      "(64, 33)\n",
      "step 21035, loss is 4.6737751960754395\n",
      "(64, 33)\n",
      "step 21036, loss is 4.8023271560668945\n",
      "(64, 33)\n",
      "step 21037, loss is 4.884237289428711\n",
      "(64, 33)\n",
      "step 21038, loss is 4.769934177398682\n",
      "(64, 33)\n",
      "step 21039, loss is 4.869012355804443\n",
      "(64, 33)\n",
      "step 21040, loss is 4.814085960388184\n",
      "(64, 33)\n",
      "step 21041, loss is 4.8001227378845215\n",
      "(64, 33)\n",
      "step 21042, loss is 4.924219608306885\n",
      "(64, 33)\n",
      "step 21043, loss is 4.895213603973389\n",
      "(64, 33)\n",
      "step 21044, loss is 4.870571136474609\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21045, loss is 4.888028144836426\n",
      "(64, 33)\n",
      "step 21046, loss is 4.7888569831848145\n",
      "(64, 33)\n",
      "step 21047, loss is 4.845944881439209\n",
      "(64, 33)\n",
      "step 21048, loss is 4.926475524902344\n",
      "(64, 33)\n",
      "step 21049, loss is 4.634847164154053\n",
      "(64, 33)\n",
      "step 21050, loss is 4.875058174133301\n",
      "(64, 33)\n",
      "step 21051, loss is 4.887533187866211\n",
      "(64, 33)\n",
      "step 21052, loss is 4.894786357879639\n",
      "(64, 33)\n",
      "step 21053, loss is 5.021186828613281\n",
      "(64, 33)\n",
      "step 21054, loss is 4.614206790924072\n",
      "(64, 33)\n",
      "step 21055, loss is 4.752828598022461\n",
      "(64, 33)\n",
      "step 21056, loss is 4.736248970031738\n",
      "(64, 33)\n",
      "step 21057, loss is 4.732816696166992\n",
      "(64, 33)\n",
      "step 21058, loss is 4.79763126373291\n",
      "(64, 33)\n",
      "step 21059, loss is 4.985902786254883\n",
      "(64, 33)\n",
      "step 21060, loss is 4.648585319519043\n",
      "(64, 33)\n",
      "step 21061, loss is 4.809177398681641\n",
      "(64, 33)\n",
      "step 21062, loss is 4.751287460327148\n",
      "(64, 33)\n",
      "step 21063, loss is 4.697405815124512\n",
      "(64, 33)\n",
      "step 21064, loss is 4.7082977294921875\n",
      "(64, 33)\n",
      "step 21065, loss is 4.812028884887695\n",
      "(64, 33)\n",
      "step 21066, loss is 4.965240001678467\n",
      "(64, 33)\n",
      "step 21067, loss is 4.805532455444336\n",
      "(64, 33)\n",
      "step 21068, loss is 4.641480445861816\n",
      "(64, 33)\n",
      "step 21069, loss is 4.864823341369629\n",
      "(64, 33)\n",
      "step 21070, loss is 4.557921409606934\n",
      "(64, 33)\n",
      "step 21071, loss is 4.830210208892822\n",
      "(64, 33)\n",
      "step 21072, loss is 4.745795249938965\n",
      "(64, 33)\n",
      "step 21073, loss is 4.754047393798828\n",
      "(64, 33)\n",
      "step 21074, loss is 4.55996561050415\n",
      "(64, 33)\n",
      "step 21075, loss is 4.725775718688965\n",
      "(64, 33)\n",
      "step 21076, loss is 4.794387340545654\n",
      "(64, 33)\n",
      "step 21077, loss is 4.831836700439453\n",
      "(64, 33)\n",
      "step 21078, loss is 4.626514434814453\n",
      "(64, 33)\n",
      "step 21079, loss is 4.849122524261475\n",
      "(64, 33)\n",
      "step 21080, loss is 4.7894816398620605\n",
      "(64, 33)\n",
      "step 21081, loss is 4.726433753967285\n",
      "(64, 33)\n",
      "step 21082, loss is 4.8190155029296875\n",
      "(64, 33)\n",
      "step 21083, loss is 4.825963973999023\n",
      "(64, 33)\n",
      "step 21084, loss is 4.644798755645752\n",
      "(64, 33)\n",
      "step 21085, loss is 4.759683609008789\n",
      "(64, 33)\n",
      "step 21086, loss is 4.682788372039795\n",
      "(64, 33)\n",
      "step 21087, loss is 4.889430999755859\n",
      "(64, 33)\n",
      "step 21088, loss is 4.799269676208496\n",
      "(64, 33)\n",
      "step 21089, loss is 4.8693366050720215\n",
      "(64, 33)\n",
      "step 21090, loss is 4.72037410736084\n",
      "(64, 33)\n",
      "step 21091, loss is 4.622152328491211\n",
      "(64, 33)\n",
      "step 21092, loss is 4.933218002319336\n",
      "(64, 33)\n",
      "step 21093, loss is 4.868293762207031\n",
      "(64, 33)\n",
      "step 21094, loss is 4.640926837921143\n",
      "(64, 33)\n",
      "step 21095, loss is 4.708980560302734\n",
      "(64, 33)\n",
      "step 21096, loss is 4.884230613708496\n",
      "(64, 33)\n",
      "step 21097, loss is 4.900539398193359\n",
      "(64, 33)\n",
      "step 21098, loss is 4.857349872589111\n",
      "(64, 33)\n",
      "step 21099, loss is 4.770017147064209\n",
      "(64, 33)\n",
      "step 21100, loss is 4.76022481918335\n",
      "(64, 33)\n",
      "step 21101, loss is 4.6261796951293945\n",
      "(64, 33)\n",
      "step 21102, loss is 4.884645462036133\n",
      "(64, 33)\n",
      "step 21103, loss is 4.858334541320801\n",
      "(64, 33)\n",
      "step 21104, loss is 4.839240074157715\n",
      "(64, 33)\n",
      "step 21105, loss is 4.976860523223877\n",
      "(64, 33)\n",
      "step 21106, loss is 4.718722820281982\n",
      "(64, 33)\n",
      "step 21107, loss is 4.7972917556762695\n",
      "(64, 33)\n",
      "step 21108, loss is 4.79097318649292\n",
      "(64, 33)\n",
      "step 21109, loss is 4.709489345550537\n",
      "(64, 33)\n",
      "step 21110, loss is 4.782516956329346\n",
      "(64, 33)\n",
      "step 21111, loss is 4.761654853820801\n",
      "(64, 33)\n",
      "step 21112, loss is 4.69276762008667\n",
      "(64, 33)\n",
      "step 21113, loss is 4.732080459594727\n",
      "(64, 33)\n",
      "step 21114, loss is 4.984485149383545\n",
      "(64, 33)\n",
      "step 21115, loss is 4.785022735595703\n",
      "(64, 33)\n",
      "step 21116, loss is 4.713785648345947\n",
      "(64, 33)\n",
      "step 21117, loss is 4.769223213195801\n",
      "(64, 33)\n",
      "step 21118, loss is 4.8488898277282715\n",
      "(64, 33)\n",
      "step 21119, loss is 4.842461109161377\n",
      "(64, 33)\n",
      "step 21120, loss is 4.89605712890625\n",
      "(64, 33)\n",
      "step 21121, loss is 4.616365909576416\n",
      "(64, 33)\n",
      "step 21122, loss is 4.713411331176758\n",
      "(64, 33)\n",
      "step 21123, loss is 4.968648910522461\n",
      "(64, 33)\n",
      "step 21124, loss is 4.833071231842041\n",
      "(64, 33)\n",
      "step 21125, loss is 4.865843296051025\n",
      "(64, 33)\n",
      "step 21126, loss is 4.947978496551514\n",
      "(64, 33)\n",
      "step 21127, loss is 4.696977138519287\n",
      "(64, 33)\n",
      "step 21128, loss is 4.696416854858398\n",
      "(64, 33)\n",
      "step 21129, loss is 4.980647563934326\n",
      "(64, 33)\n",
      "step 21130, loss is 4.986743450164795\n",
      "(64, 33)\n",
      "step 21131, loss is 4.737292766571045\n",
      "(64, 33)\n",
      "step 21132, loss is 4.733509063720703\n",
      "(64, 33)\n",
      "step 21133, loss is 4.814807415008545\n",
      "(64, 33)\n",
      "step 21134, loss is 4.6676411628723145\n",
      "(64, 33)\n",
      "step 21135, loss is 5.065288543701172\n",
      "(64, 33)\n",
      "step 21136, loss is 4.828152656555176\n",
      "(64, 33)\n",
      "step 21137, loss is 4.931711196899414\n",
      "(64, 33)\n",
      "step 21138, loss is 4.55076789855957\n",
      "(64, 33)\n",
      "step 21139, loss is 4.792399883270264\n",
      "(64, 33)\n",
      "step 21140, loss is 4.703802585601807\n",
      "(64, 33)\n",
      "step 21141, loss is 4.869816780090332\n",
      "(64, 33)\n",
      "step 21142, loss is 4.717141151428223\n",
      "(64, 33)\n",
      "step 21143, loss is 4.683191299438477\n",
      "(64, 33)\n",
      "step 21144, loss is 4.649013519287109\n",
      "(64, 33)\n",
      "step 21145, loss is 4.929069519042969\n",
      "(64, 33)\n",
      "step 21146, loss is 5.017906188964844\n",
      "(64, 33)\n",
      "step 21147, loss is 4.705150127410889\n",
      "(64, 33)\n",
      "step 21148, loss is 4.700708389282227\n",
      "(64, 33)\n",
      "step 21149, loss is 4.770707130432129\n",
      "(64, 33)\n",
      "step 21150, loss is 4.738151550292969\n",
      "(64, 33)\n",
      "step 21151, loss is 4.9118452072143555\n",
      "(64, 33)\n",
      "step 21152, loss is 4.727262496948242\n",
      "(64, 33)\n",
      "step 21153, loss is 4.771478176116943\n",
      "(64, 33)\n",
      "step 21154, loss is 4.82450008392334\n",
      "(64, 33)\n",
      "step 21155, loss is 4.767063140869141\n",
      "(64, 33)\n",
      "step 21156, loss is 5.0106706619262695\n",
      "(64, 33)\n",
      "step 21157, loss is 4.667866230010986\n",
      "(64, 33)\n",
      "step 21158, loss is 4.879936695098877\n",
      "(64, 33)\n",
      "step 21159, loss is 4.665716648101807\n",
      "(64, 33)\n",
      "step 21160, loss is 4.770681381225586\n",
      "(64, 33)\n",
      "step 21161, loss is 4.732937335968018\n",
      "(64, 33)\n",
      "step 21162, loss is 4.771419525146484\n",
      "(64, 33)\n",
      "step 21163, loss is 4.756762504577637\n",
      "(64, 33)\n",
      "step 21164, loss is 4.7519330978393555\n",
      "(64, 33)\n",
      "step 21165, loss is 4.877605438232422\n",
      "(64, 33)\n",
      "step 21166, loss is 4.593166828155518\n",
      "(64, 33)\n",
      "step 21167, loss is 4.880770206451416\n",
      "(64, 33)\n",
      "step 21168, loss is 4.614224433898926\n",
      "(64, 33)\n",
      "step 21169, loss is 4.580179691314697\n",
      "(64, 33)\n",
      "step 21170, loss is 4.8456711769104\n",
      "(64, 33)\n",
      "step 21171, loss is 4.760921001434326\n",
      "(64, 33)\n",
      "step 21172, loss is 4.787505626678467\n",
      "(64, 33)\n",
      "step 21173, loss is 4.7018723487854\n",
      "(64, 33)\n",
      "step 21174, loss is 4.885880947113037\n",
      "(64, 33)\n",
      "step 21175, loss is 4.845666885375977\n",
      "(64, 33)\n",
      "step 21176, loss is 4.896614074707031\n",
      "(64, 33)\n",
      "step 21177, loss is 4.745368957519531\n",
      "(64, 33)\n",
      "step 21178, loss is 4.578057289123535\n",
      "(64, 33)\n",
      "step 21179, loss is 4.592332363128662\n",
      "(64, 33)\n",
      "step 21180, loss is 4.957671642303467\n",
      "(64, 33)\n",
      "step 21181, loss is 4.866751194000244\n",
      "(64, 33)\n",
      "step 21182, loss is 4.919654369354248\n",
      "(64, 33)\n",
      "step 21183, loss is 4.751150131225586\n",
      "(64, 33)\n",
      "step 21184, loss is 4.656118392944336\n",
      "(64, 33)\n",
      "step 21185, loss is 4.652652740478516\n",
      "(64, 33)\n",
      "step 21186, loss is 4.751501560211182\n",
      "(64, 33)\n",
      "step 21187, loss is 4.773637294769287\n",
      "(64, 33)\n",
      "step 21188, loss is 4.7645263671875\n",
      "(64, 33)\n",
      "step 21189, loss is 4.89068603515625\n",
      "(64, 33)\n",
      "step 21190, loss is 4.906772136688232\n",
      "(64, 33)\n",
      "step 21191, loss is 4.570465087890625\n",
      "(64, 33)\n",
      "step 21192, loss is 4.8515305519104\n",
      "(64, 33)\n",
      "step 21193, loss is 4.878441333770752\n",
      "(64, 33)\n",
      "step 21194, loss is 4.615177154541016\n",
      "(64, 33)\n",
      "step 21195, loss is 4.973538875579834\n",
      "(64, 33)\n",
      "step 21196, loss is 4.754543304443359\n",
      "(64, 33)\n",
      "step 21197, loss is 4.710155010223389\n",
      "(64, 33)\n",
      "step 21198, loss is 4.872351169586182\n",
      "(64, 33)\n",
      "step 21199, loss is 5.0353217124938965\n",
      "(64, 33)\n",
      "step 21200, loss is 4.750396728515625\n",
      "(64, 33)\n",
      "step 21201, loss is 4.900155544281006\n",
      "(64, 33)\n",
      "step 21202, loss is 4.808359622955322\n",
      "(64, 33)\n",
      "step 21203, loss is 4.842235565185547\n",
      "(64, 33)\n",
      "step 21204, loss is 4.781105041503906\n",
      "(64, 33)\n",
      "step 21205, loss is 4.847413063049316\n",
      "(64, 33)\n",
      "step 21206, loss is 4.7032151222229\n",
      "(64, 33)\n",
      "step 21207, loss is 4.670900821685791\n",
      "(64, 33)\n",
      "step 21208, loss is 4.836523532867432\n",
      "(64, 33)\n",
      "step 21209, loss is 4.9879631996154785\n",
      "(64, 33)\n",
      "step 21210, loss is 4.694450378417969\n",
      "(64, 33)\n",
      "step 21211, loss is 4.584083080291748\n",
      "(64, 33)\n",
      "step 21212, loss is 4.750478744506836\n",
      "(64, 33)\n",
      "step 21213, loss is 4.683657646179199\n",
      "(64, 33)\n",
      "step 21214, loss is 4.755455017089844\n",
      "(64, 33)\n",
      "step 21215, loss is 4.867345809936523\n",
      "(64, 33)\n",
      "step 21216, loss is 4.877296447753906\n",
      "(64, 33)\n",
      "step 21217, loss is 4.916894912719727\n",
      "(64, 33)\n",
      "step 21218, loss is 4.678235054016113\n",
      "(64, 33)\n",
      "step 21219, loss is 4.85684871673584\n",
      "(64, 33)\n",
      "step 21220, loss is 4.670323371887207\n",
      "(64, 33)\n",
      "step 21221, loss is 4.7354736328125\n",
      "(64, 33)\n",
      "step 21222, loss is 4.917799949645996\n",
      "(64, 33)\n",
      "step 21223, loss is 4.730628967285156\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21224, loss is 4.87451171875\n",
      "(64, 33)\n",
      "step 21225, loss is 4.907321453094482\n",
      "(64, 33)\n",
      "step 21226, loss is 4.739957809448242\n",
      "(64, 33)\n",
      "step 21227, loss is 4.763736724853516\n",
      "(64, 33)\n",
      "step 21228, loss is 4.806156158447266\n",
      "(64, 33)\n",
      "step 21229, loss is 4.699360370635986\n",
      "(64, 33)\n",
      "step 21230, loss is 4.691131114959717\n",
      "(64, 33)\n",
      "step 21231, loss is 4.903611183166504\n",
      "(64, 33)\n",
      "step 21232, loss is 4.711810111999512\n",
      "(64, 33)\n",
      "step 21233, loss is 4.882922649383545\n",
      "(64, 33)\n",
      "step 21234, loss is 4.793407917022705\n",
      "(64, 33)\n",
      "step 21235, loss is 4.662441730499268\n",
      "(64, 33)\n",
      "step 21236, loss is 4.740692138671875\n",
      "(64, 33)\n",
      "step 21237, loss is 4.8454132080078125\n",
      "(64, 33)\n",
      "step 21238, loss is 4.737146377563477\n",
      "(64, 33)\n",
      "step 21239, loss is 4.591921329498291\n",
      "(64, 33)\n",
      "step 21240, loss is 4.722297668457031\n",
      "(64, 33)\n",
      "step 21241, loss is 4.879902362823486\n",
      "(64, 33)\n",
      "step 21242, loss is 4.648375511169434\n",
      "(64, 33)\n",
      "step 21243, loss is 4.772394180297852\n",
      "(64, 33)\n",
      "step 21244, loss is 4.612945079803467\n",
      "(64, 33)\n",
      "step 21245, loss is 4.747772216796875\n",
      "(64, 33)\n",
      "step 21246, loss is 4.78308629989624\n",
      "(64, 33)\n",
      "step 21247, loss is 4.825787544250488\n",
      "(64, 33)\n",
      "step 21248, loss is 4.987473011016846\n",
      "(64, 33)\n",
      "step 21249, loss is 4.855532169342041\n",
      "(64, 33)\n",
      "step 21250, loss is 4.810716152191162\n",
      "(64, 33)\n",
      "step 21251, loss is 4.815552711486816\n",
      "(64, 33)\n",
      "step 21252, loss is 4.674220561981201\n",
      "(64, 33)\n",
      "step 21253, loss is 4.587446212768555\n",
      "(64, 33)\n",
      "step 21254, loss is 4.842889785766602\n",
      "(64, 33)\n",
      "step 21255, loss is 4.631059646606445\n",
      "(64, 33)\n",
      "step 21256, loss is 4.762821197509766\n",
      "(64, 33)\n",
      "step 21257, loss is 4.760697841644287\n",
      "(64, 33)\n",
      "step 21258, loss is 4.653388023376465\n",
      "(64, 33)\n",
      "step 21259, loss is 4.659073352813721\n",
      "(64, 33)\n",
      "step 21260, loss is 4.771349906921387\n",
      "(64, 33)\n",
      "step 21261, loss is 4.848365783691406\n",
      "(64, 33)\n",
      "step 21262, loss is 4.898481845855713\n",
      "(64, 33)\n",
      "step 21263, loss is 4.82580041885376\n",
      "(64, 33)\n",
      "step 21264, loss is 4.623828887939453\n",
      "(64, 33)\n",
      "step 21265, loss is 4.705857276916504\n",
      "(64, 33)\n",
      "step 21266, loss is 4.667490482330322\n",
      "(64, 33)\n",
      "step 21267, loss is 4.900907516479492\n",
      "(64, 33)\n",
      "step 21268, loss is 4.807449817657471\n",
      "(64, 33)\n",
      "step 21269, loss is 4.720871448516846\n",
      "(64, 33)\n",
      "step 21270, loss is 4.804115295410156\n",
      "(64, 33)\n",
      "step 21271, loss is 4.779466152191162\n",
      "(64, 33)\n",
      "step 21272, loss is 4.683439254760742\n",
      "(64, 33)\n",
      "step 21273, loss is 4.792830944061279\n",
      "(64, 33)\n",
      "step 21274, loss is 4.698110103607178\n",
      "(64, 33)\n",
      "step 21275, loss is 4.655571937561035\n",
      "(64, 33)\n",
      "step 21276, loss is 4.828067779541016\n",
      "(64, 33)\n",
      "step 21277, loss is 4.768823623657227\n",
      "(64, 33)\n",
      "step 21278, loss is 4.877699375152588\n",
      "(64, 33)\n",
      "step 21279, loss is 4.908904075622559\n",
      "(64, 33)\n",
      "step 21280, loss is 4.6934895515441895\n",
      "(64, 33)\n",
      "step 21281, loss is 4.811141490936279\n",
      "(64, 33)\n",
      "step 21282, loss is 4.810615062713623\n",
      "(64, 33)\n",
      "step 21283, loss is 4.908685684204102\n",
      "(64, 33)\n",
      "step 21284, loss is 4.862617492675781\n",
      "(64, 33)\n",
      "step 21285, loss is 4.818413257598877\n",
      "(64, 33)\n",
      "step 21286, loss is 4.725972652435303\n",
      "(64, 33)\n",
      "step 21287, loss is 4.983467102050781\n",
      "(64, 33)\n",
      "step 21288, loss is 4.702497482299805\n",
      "(64, 33)\n",
      "step 21289, loss is 4.845130920410156\n",
      "(64, 33)\n",
      "step 21290, loss is 4.7784953117370605\n",
      "(64, 33)\n",
      "step 21291, loss is 4.875129222869873\n",
      "(64, 33)\n",
      "step 21292, loss is 4.713323593139648\n",
      "(64, 33)\n",
      "step 21293, loss is 4.800390720367432\n",
      "(64, 33)\n",
      "step 21294, loss is 4.7642035484313965\n",
      "(64, 33)\n",
      "step 21295, loss is 4.847164630889893\n",
      "(64, 33)\n",
      "step 21296, loss is 4.776325702667236\n",
      "(64, 33)\n",
      "step 21297, loss is 4.674971103668213\n",
      "(64, 33)\n",
      "step 21298, loss is 4.611076354980469\n",
      "(64, 33)\n",
      "step 21299, loss is 4.751828193664551\n",
      "(64, 33)\n",
      "step 21300, loss is 4.659760475158691\n",
      "(64, 33)\n",
      "step 21301, loss is 4.753331184387207\n",
      "(64, 33)\n",
      "step 21302, loss is 4.744363784790039\n",
      "(64, 33)\n",
      "step 21303, loss is 4.815595626831055\n",
      "(64, 33)\n",
      "step 21304, loss is 4.91942834854126\n",
      "(64, 33)\n",
      "step 21305, loss is 4.970994472503662\n",
      "(64, 33)\n",
      "step 21306, loss is 4.588435173034668\n",
      "(64, 33)\n",
      "step 21307, loss is 4.786989212036133\n",
      "(64, 33)\n",
      "step 21308, loss is 4.640909671783447\n",
      "(64, 33)\n",
      "step 21309, loss is 4.669835567474365\n",
      "(64, 33)\n",
      "step 21310, loss is 4.928643226623535\n",
      "(64, 33)\n",
      "step 21311, loss is 4.509567737579346\n",
      "(64, 33)\n",
      "step 21312, loss is 4.617682456970215\n",
      "(64, 33)\n",
      "step 21313, loss is 4.896849632263184\n",
      "(64, 33)\n",
      "step 21314, loss is 4.585939884185791\n",
      "(64, 33)\n",
      "step 21315, loss is 4.745739936828613\n",
      "(64, 33)\n",
      "step 21316, loss is 4.708106517791748\n",
      "(64, 33)\n",
      "step 21317, loss is 4.662051677703857\n",
      "(64, 33)\n",
      "step 21318, loss is 4.807918071746826\n",
      "(64, 33)\n",
      "step 21319, loss is 4.694328308105469\n",
      "(64, 33)\n",
      "step 21320, loss is 4.641745090484619\n",
      "(64, 33)\n",
      "step 21321, loss is 4.878885269165039\n",
      "(64, 33)\n",
      "step 21322, loss is 4.789448261260986\n",
      "(64, 33)\n",
      "step 21323, loss is 4.881936550140381\n",
      "(64, 33)\n",
      "step 21324, loss is 4.886425018310547\n",
      "(64, 33)\n",
      "step 21325, loss is 4.820413112640381\n",
      "(64, 33)\n",
      "step 21326, loss is 4.741433143615723\n",
      "(64, 33)\n",
      "step 21327, loss is 4.876000881195068\n",
      "(64, 33)\n",
      "step 21328, loss is 4.935911178588867\n",
      "(64, 33)\n",
      "step 21329, loss is 4.6800713539123535\n",
      "(64, 33)\n",
      "step 21330, loss is 4.639458656311035\n",
      "(64, 33)\n",
      "step 21331, loss is 4.759929180145264\n",
      "(64, 33)\n",
      "step 21332, loss is 4.9293904304504395\n",
      "(64, 33)\n",
      "step 21333, loss is 4.7022786140441895\n",
      "(64, 33)\n",
      "step 21334, loss is 4.750072956085205\n",
      "(64, 33)\n",
      "step 21335, loss is 4.802596092224121\n",
      "(64, 33)\n",
      "step 21336, loss is 4.824596881866455\n",
      "(64, 33)\n",
      "step 21337, loss is 4.8516106605529785\n",
      "(64, 33)\n",
      "step 21338, loss is 4.683876991271973\n",
      "(64, 33)\n",
      "step 21339, loss is 4.740752220153809\n",
      "(64, 33)\n",
      "step 21340, loss is 4.843635559082031\n",
      "(64, 33)\n",
      "step 21341, loss is 4.873665809631348\n",
      "(64, 33)\n",
      "step 21342, loss is 4.616454601287842\n",
      "(64, 33)\n",
      "step 21343, loss is 4.8404083251953125\n",
      "(64, 33)\n",
      "step 21344, loss is 4.917865753173828\n",
      "(64, 33)\n",
      "step 21345, loss is 4.872450351715088\n",
      "(64, 33)\n",
      "step 21346, loss is 4.773730754852295\n",
      "(64, 33)\n",
      "step 21347, loss is 4.779082775115967\n",
      "(64, 33)\n",
      "step 21348, loss is 4.78426456451416\n",
      "(64, 33)\n",
      "step 21349, loss is 4.727614402770996\n",
      "(64, 33)\n",
      "step 21350, loss is 4.733405113220215\n",
      "(64, 33)\n",
      "step 21351, loss is 4.835712909698486\n",
      "(64, 33)\n",
      "step 21352, loss is 4.8594818115234375\n",
      "(64, 33)\n",
      "step 21353, loss is 4.573421955108643\n",
      "(64, 33)\n",
      "step 21354, loss is 4.691554069519043\n",
      "(64, 33)\n",
      "step 21355, loss is 4.929702281951904\n",
      "(64, 33)\n",
      "step 21356, loss is 4.849893569946289\n",
      "(64, 33)\n",
      "step 21357, loss is 4.597079753875732\n",
      "(64, 33)\n",
      "step 21358, loss is 4.853914737701416\n",
      "(64, 33)\n",
      "step 21359, loss is 4.841948986053467\n",
      "(64, 33)\n",
      "step 21360, loss is 4.6884589195251465\n",
      "(64, 33)\n",
      "step 21361, loss is 4.88212776184082\n",
      "(64, 33)\n",
      "step 21362, loss is 4.688554286956787\n",
      "(64, 33)\n",
      "step 21363, loss is 4.682517051696777\n",
      "(64, 33)\n",
      "step 21364, loss is 4.642895221710205\n",
      "(64, 33)\n",
      "step 21365, loss is 4.827914714813232\n",
      "(64, 33)\n",
      "step 21366, loss is 4.704866886138916\n",
      "(64, 33)\n",
      "step 21367, loss is 4.836205959320068\n",
      "(64, 33)\n",
      "step 21368, loss is 4.749381065368652\n",
      "(64, 33)\n",
      "step 21369, loss is 4.865848541259766\n",
      "(64, 33)\n",
      "step 21370, loss is 4.794346332550049\n",
      "(64, 33)\n",
      "step 21371, loss is 4.814999580383301\n",
      "(64, 33)\n",
      "step 21372, loss is 4.806852340698242\n",
      "(64, 33)\n",
      "step 21373, loss is 4.6302595138549805\n",
      "(64, 33)\n",
      "step 21374, loss is 4.731643199920654\n",
      "(64, 33)\n",
      "step 21375, loss is 4.669197082519531\n",
      "(64, 33)\n",
      "step 21376, loss is 4.805006504058838\n",
      "(64, 33)\n",
      "step 21377, loss is 4.81156587600708\n",
      "(64, 33)\n",
      "step 21378, loss is 4.890232086181641\n",
      "(64, 33)\n",
      "step 21379, loss is 4.810225963592529\n",
      "(64, 33)\n",
      "step 21380, loss is 4.790432929992676\n",
      "(64, 33)\n",
      "step 21381, loss is 4.744563579559326\n",
      "(64, 33)\n",
      "step 21382, loss is 4.8756537437438965\n",
      "(64, 33)\n",
      "step 21383, loss is 4.777765274047852\n",
      "(64, 33)\n",
      "step 21384, loss is 4.682653427124023\n",
      "(64, 33)\n",
      "step 21385, loss is 4.647039413452148\n",
      "(64, 33)\n",
      "step 21386, loss is 4.867141246795654\n",
      "(64, 33)\n",
      "step 21387, loss is 4.878517150878906\n",
      "(64, 33)\n",
      "step 21388, loss is 4.785954475402832\n",
      "(64, 33)\n",
      "step 21389, loss is 4.7003560066223145\n",
      "(64, 33)\n",
      "step 21390, loss is 4.7921342849731445\n",
      "(64, 33)\n",
      "step 21391, loss is 4.769180774688721\n",
      "(64, 33)\n",
      "step 21392, loss is 4.602443695068359\n",
      "(64, 33)\n",
      "step 21393, loss is 4.884031772613525\n",
      "(64, 33)\n",
      "step 21394, loss is 4.764130592346191\n",
      "(64, 33)\n",
      "step 21395, loss is 4.817793369293213\n",
      "(64, 33)\n",
      "step 21396, loss is 4.752139091491699\n",
      "(64, 33)\n",
      "step 21397, loss is 4.701329231262207\n",
      "(64, 33)\n",
      "step 21398, loss is 4.924064636230469\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21399, loss is 4.890861511230469\n",
      "(64, 33)\n",
      "step 21400, loss is 4.724874973297119\n",
      "(64, 33)\n",
      "step 21401, loss is 4.7199530601501465\n",
      "(64, 33)\n",
      "step 21402, loss is 4.840576648712158\n",
      "(64, 33)\n",
      "step 21403, loss is 4.71737003326416\n",
      "(64, 33)\n",
      "step 21404, loss is 4.804024696350098\n",
      "(64, 33)\n",
      "step 21405, loss is 4.942558765411377\n",
      "(64, 33)\n",
      "step 21406, loss is 4.780221462249756\n",
      "(64, 33)\n",
      "step 21407, loss is 4.714487552642822\n",
      "(64, 33)\n",
      "step 21408, loss is 4.795231819152832\n",
      "(64, 33)\n",
      "step 21409, loss is 4.771033763885498\n",
      "(64, 33)\n",
      "step 21410, loss is 4.77092170715332\n",
      "(64, 33)\n",
      "step 21411, loss is 4.9202165603637695\n",
      "(64, 33)\n",
      "step 21412, loss is 4.849055290222168\n",
      "(64, 33)\n",
      "step 21413, loss is 4.654371738433838\n",
      "(64, 33)\n",
      "step 21414, loss is 4.593655109405518\n",
      "(64, 33)\n",
      "step 21415, loss is 4.863550662994385\n",
      "(64, 33)\n",
      "step 21416, loss is 4.917233467102051\n",
      "(64, 33)\n",
      "step 21417, loss is 4.820019721984863\n",
      "(64, 33)\n",
      "step 21418, loss is 4.929126739501953\n",
      "(64, 33)\n",
      "step 21419, loss is 4.775989055633545\n",
      "(64, 33)\n",
      "step 21420, loss is 4.640626907348633\n",
      "(64, 33)\n",
      "step 21421, loss is 4.809286117553711\n",
      "(64, 33)\n",
      "step 21422, loss is 4.894115924835205\n",
      "(64, 33)\n",
      "step 21423, loss is 4.835960388183594\n",
      "(64, 33)\n",
      "step 21424, loss is 4.74202823638916\n",
      "(64, 33)\n",
      "step 21425, loss is 4.781266212463379\n",
      "(64, 33)\n",
      "step 21426, loss is 4.771290302276611\n",
      "(64, 33)\n",
      "step 21427, loss is 4.765771865844727\n",
      "(64, 33)\n",
      "step 21428, loss is 4.850056171417236\n",
      "(64, 33)\n",
      "step 21429, loss is 4.879713535308838\n",
      "(64, 33)\n",
      "step 21430, loss is 4.716377258300781\n",
      "(64, 33)\n",
      "step 21431, loss is 4.809286594390869\n",
      "(64, 33)\n",
      "step 21432, loss is 4.7058610916137695\n",
      "(64, 33)\n",
      "step 21433, loss is 4.667168140411377\n",
      "(64, 33)\n",
      "step 21434, loss is 4.800646781921387\n",
      "(64, 33)\n",
      "step 21435, loss is 4.684545516967773\n",
      "(64, 33)\n",
      "step 21436, loss is 4.722667694091797\n",
      "(64, 33)\n",
      "step 21437, loss is 4.730460166931152\n",
      "(64, 33)\n",
      "step 21438, loss is 4.843942165374756\n",
      "(64, 33)\n",
      "step 21439, loss is 4.999353408813477\n",
      "(64, 33)\n",
      "step 21440, loss is 4.8624958992004395\n",
      "(64, 33)\n",
      "step 21441, loss is 4.882858753204346\n",
      "(64, 33)\n",
      "step 21442, loss is 4.863857269287109\n",
      "(64, 33)\n",
      "step 21443, loss is 4.623448848724365\n",
      "(64, 33)\n",
      "step 21444, loss is 4.884772300720215\n",
      "(64, 33)\n",
      "step 21445, loss is 4.722166538238525\n",
      "(64, 33)\n",
      "step 21446, loss is 4.708095550537109\n",
      "(64, 33)\n",
      "step 21447, loss is 4.923768520355225\n",
      "(64, 33)\n",
      "step 21448, loss is 4.776318073272705\n",
      "(64, 33)\n",
      "step 21449, loss is 4.781370639801025\n",
      "(64, 33)\n",
      "step 21450, loss is 4.871159076690674\n",
      "(64, 33)\n",
      "step 21451, loss is 4.738907337188721\n",
      "(64, 33)\n",
      "step 21452, loss is 4.707264423370361\n",
      "(64, 33)\n",
      "step 21453, loss is 4.782010555267334\n",
      "(64, 33)\n",
      "step 21454, loss is 4.744782447814941\n",
      "(64, 33)\n",
      "step 21455, loss is 4.751723766326904\n",
      "(64, 33)\n",
      "step 21456, loss is 4.889537811279297\n",
      "(64, 33)\n",
      "step 21457, loss is 4.93257474899292\n",
      "(64, 33)\n",
      "step 21458, loss is 4.788095951080322\n",
      "(64, 33)\n",
      "step 21459, loss is 4.844143867492676\n",
      "(64, 33)\n",
      "step 21460, loss is 4.49383544921875\n",
      "(64, 33)\n",
      "step 21461, loss is 4.713522434234619\n",
      "(64, 33)\n",
      "step 21462, loss is 4.682337284088135\n",
      "(64, 33)\n",
      "step 21463, loss is 4.735357284545898\n",
      "(64, 33)\n",
      "step 21464, loss is 4.9137725830078125\n",
      "(64, 33)\n",
      "step 21465, loss is 4.894120693206787\n",
      "(64, 33)\n",
      "step 21466, loss is 4.9431986808776855\n",
      "(64, 33)\n",
      "step 21467, loss is 4.808077335357666\n",
      "(64, 33)\n",
      "step 21468, loss is 4.78297758102417\n",
      "(64, 33)\n",
      "step 21469, loss is 4.788504600524902\n",
      "(64, 33)\n",
      "step 21470, loss is 4.911742210388184\n",
      "(64, 33)\n",
      "step 21471, loss is 4.669583320617676\n",
      "(64, 33)\n",
      "step 21472, loss is 4.799445629119873\n",
      "(64, 33)\n",
      "step 21473, loss is 4.924530029296875\n",
      "(64, 33)\n",
      "step 21474, loss is 4.842323303222656\n",
      "(64, 33)\n",
      "step 21475, loss is 4.63603401184082\n",
      "(64, 33)\n",
      "step 21476, loss is 4.717448711395264\n",
      "(64, 33)\n",
      "step 21477, loss is 4.829972267150879\n",
      "(64, 33)\n",
      "step 21478, loss is 4.705592632293701\n",
      "(64, 33)\n",
      "step 21479, loss is 4.753112316131592\n",
      "(64, 33)\n",
      "step 21480, loss is 4.653779029846191\n",
      "(64, 33)\n",
      "step 21481, loss is 4.806950569152832\n",
      "(64, 33)\n",
      "step 21482, loss is 4.662208557128906\n",
      "(64, 33)\n",
      "step 21483, loss is 4.783130168914795\n",
      "(64, 33)\n",
      "step 21484, loss is 4.650974273681641\n",
      "(64, 33)\n",
      "step 21485, loss is 4.722438812255859\n",
      "(64, 33)\n",
      "step 21486, loss is 4.649806022644043\n",
      "(64, 33)\n",
      "step 21487, loss is 4.817336082458496\n",
      "(64, 33)\n",
      "step 21488, loss is 4.903081893920898\n",
      "(64, 33)\n",
      "step 21489, loss is 4.787967681884766\n",
      "(64, 33)\n",
      "step 21490, loss is 4.710202693939209\n",
      "(64, 33)\n",
      "step 21491, loss is 4.655126094818115\n",
      "(64, 33)\n",
      "step 21492, loss is 4.871091365814209\n",
      "(64, 33)\n",
      "step 21493, loss is 4.724738121032715\n",
      "(64, 33)\n",
      "step 21494, loss is 4.752310276031494\n",
      "(64, 33)\n",
      "step 21495, loss is 4.623309135437012\n",
      "(64, 33)\n",
      "step 21496, loss is 4.912474155426025\n",
      "(64, 33)\n",
      "step 21497, loss is 4.926802158355713\n",
      "(64, 33)\n",
      "step 21498, loss is 4.953752517700195\n",
      "(64, 33)\n",
      "step 21499, loss is 4.752084255218506\n",
      "(64, 33)\n",
      "step 21500, loss is 4.903294563293457\n",
      "(64, 33)\n",
      "step 21501, loss is 4.6934967041015625\n",
      "(64, 33)\n",
      "step 21502, loss is 4.815465450286865\n",
      "(64, 33)\n",
      "step 21503, loss is 4.976771831512451\n",
      "(64, 33)\n",
      "step 21504, loss is 4.713236331939697\n",
      "(64, 33)\n",
      "step 21505, loss is 4.719314098358154\n",
      "(64, 33)\n",
      "step 21506, loss is 4.872602462768555\n",
      "(64, 33)\n",
      "step 21507, loss is 4.720816612243652\n",
      "(64, 33)\n",
      "step 21508, loss is 4.8164801597595215\n",
      "(64, 33)\n",
      "step 21509, loss is 4.6932854652404785\n",
      "(64, 33)\n",
      "step 21510, loss is 4.723944187164307\n",
      "(64, 33)\n",
      "step 21511, loss is 4.867577075958252\n",
      "(64, 33)\n",
      "step 21512, loss is 4.985030651092529\n",
      "(64, 33)\n",
      "step 21513, loss is 4.86873722076416\n",
      "(64, 33)\n",
      "step 21514, loss is 4.896292209625244\n",
      "(64, 33)\n",
      "step 21515, loss is 4.853415012359619\n",
      "(64, 33)\n",
      "step 21516, loss is 4.845795154571533\n",
      "(64, 33)\n",
      "step 21517, loss is 4.938826560974121\n",
      "(64, 33)\n",
      "step 21518, loss is 4.801550388336182\n",
      "(64, 33)\n",
      "step 21519, loss is 4.76141357421875\n",
      "(64, 33)\n",
      "step 21520, loss is 4.762847423553467\n",
      "(64, 33)\n",
      "step 21521, loss is 4.757850646972656\n",
      "(64, 33)\n",
      "step 21522, loss is 4.869243144989014\n",
      "(64, 33)\n",
      "step 21523, loss is 4.786670684814453\n",
      "(64, 33)\n",
      "step 21524, loss is 4.848487854003906\n",
      "(64, 33)\n",
      "step 21525, loss is 4.802832126617432\n",
      "(64, 33)\n",
      "step 21526, loss is 4.798455238342285\n",
      "(64, 33)\n",
      "step 21527, loss is 4.86630392074585\n",
      "(64, 33)\n",
      "step 21528, loss is 4.883750915527344\n",
      "(64, 33)\n",
      "step 21529, loss is 4.65986442565918\n",
      "(64, 33)\n",
      "step 21530, loss is 4.826868057250977\n",
      "(64, 33)\n",
      "step 21531, loss is 4.7154436111450195\n",
      "(64, 33)\n",
      "step 21532, loss is 4.876997947692871\n",
      "(64, 33)\n",
      "step 21533, loss is 4.722211837768555\n",
      "(64, 33)\n",
      "step 21534, loss is 4.802884101867676\n",
      "(64, 33)\n",
      "step 21535, loss is 4.667861461639404\n",
      "(64, 33)\n",
      "step 21536, loss is 4.798439025878906\n",
      "(64, 33)\n",
      "step 21537, loss is 4.632012367248535\n",
      "(64, 33)\n",
      "step 21538, loss is 4.904990196228027\n",
      "(64, 33)\n",
      "step 21539, loss is 4.683228969573975\n",
      "(64, 33)\n",
      "step 21540, loss is 4.627198219299316\n",
      "(64, 33)\n",
      "step 21541, loss is 4.863837718963623\n",
      "(64, 33)\n",
      "step 21542, loss is 4.556528568267822\n",
      "(64, 33)\n",
      "step 21543, loss is 4.6297478675842285\n",
      "(64, 33)\n",
      "step 21544, loss is 4.704047679901123\n",
      "(64, 33)\n",
      "step 21545, loss is 4.838772773742676\n",
      "(64, 33)\n",
      "step 21546, loss is 4.741292953491211\n",
      "(64, 33)\n",
      "step 21547, loss is 4.721911430358887\n",
      "(64, 33)\n",
      "step 21548, loss is 4.5633769035339355\n",
      "(64, 33)\n",
      "step 21549, loss is 4.673459529876709\n",
      "(64, 33)\n",
      "step 21550, loss is 4.856502056121826\n",
      "(64, 33)\n",
      "step 21551, loss is 4.722437858581543\n",
      "(64, 33)\n",
      "step 21552, loss is 4.793458938598633\n",
      "(64, 33)\n",
      "step 21553, loss is 4.65648078918457\n",
      "(64, 33)\n",
      "step 21554, loss is 4.646378040313721\n",
      "(64, 33)\n",
      "step 21555, loss is 4.96544075012207\n",
      "(64, 33)\n",
      "step 21556, loss is 4.798205375671387\n",
      "(64, 33)\n",
      "step 21557, loss is 4.846582412719727\n",
      "(64, 33)\n",
      "step 21558, loss is 4.7394795417785645\n",
      "(64, 33)\n",
      "step 21559, loss is 4.727830410003662\n",
      "(64, 33)\n",
      "step 21560, loss is 4.838122844696045\n",
      "(64, 33)\n",
      "step 21561, loss is 4.829832553863525\n",
      "(64, 33)\n",
      "step 21562, loss is 4.835373401641846\n",
      "(64, 33)\n",
      "step 21563, loss is 4.751889228820801\n",
      "(64, 33)\n",
      "step 21564, loss is 4.752523899078369\n",
      "(64, 33)\n",
      "step 21565, loss is 4.762880325317383\n",
      "(64, 33)\n",
      "step 21566, loss is 4.833118915557861\n",
      "(64, 33)\n",
      "step 21567, loss is 4.907588481903076\n",
      "(64, 33)\n",
      "step 21568, loss is 4.822019577026367\n",
      "(64, 33)\n",
      "step 21569, loss is 4.748606204986572\n",
      "(64, 33)\n",
      "step 21570, loss is 4.832909107208252\n",
      "(64, 33)\n",
      "step 21571, loss is 4.923434734344482\n",
      "(64, 33)\n",
      "step 21572, loss is 4.982840061187744\n",
      "(64, 33)\n",
      "step 21573, loss is 4.822209358215332\n",
      "(64, 33)\n",
      "step 21574, loss is 4.857041835784912\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21575, loss is 4.727425575256348\n",
      "(64, 33)\n",
      "step 21576, loss is 4.728236675262451\n",
      "(64, 33)\n",
      "step 21577, loss is 4.78291130065918\n",
      "(64, 33)\n",
      "step 21578, loss is 4.783706188201904\n",
      "(64, 33)\n",
      "step 21579, loss is 4.943324089050293\n",
      "(64, 33)\n",
      "step 21580, loss is 4.687869071960449\n",
      "(64, 33)\n",
      "step 21581, loss is 4.963485240936279\n",
      "(64, 33)\n",
      "step 21582, loss is 4.682713508605957\n",
      "(64, 33)\n",
      "step 21583, loss is 4.864748001098633\n",
      "(64, 33)\n",
      "step 21584, loss is 4.642430305480957\n",
      "(64, 33)\n",
      "step 21585, loss is 4.686214923858643\n",
      "(64, 33)\n",
      "step 21586, loss is 4.769974231719971\n",
      "(64, 33)\n",
      "step 21587, loss is 4.7689900398254395\n",
      "(64, 33)\n",
      "step 21588, loss is 4.924752235412598\n",
      "(64, 33)\n",
      "step 21589, loss is 4.861729145050049\n",
      "(64, 33)\n",
      "step 21590, loss is 4.927234649658203\n",
      "(64, 33)\n",
      "step 21591, loss is 4.713672637939453\n",
      "(64, 33)\n",
      "step 21592, loss is 4.885291576385498\n",
      "(64, 33)\n",
      "step 21593, loss is 4.688994884490967\n",
      "(64, 33)\n",
      "step 21594, loss is 4.724658012390137\n",
      "(64, 33)\n",
      "step 21595, loss is 4.7987565994262695\n",
      "(64, 33)\n",
      "step 21596, loss is 4.794622898101807\n",
      "(64, 33)\n",
      "step 21597, loss is 4.914000511169434\n",
      "(64, 33)\n",
      "step 21598, loss is 4.885946750640869\n",
      "(64, 33)\n",
      "step 21599, loss is 4.635375022888184\n",
      "(64, 33)\n",
      "step 21600, loss is 4.901086807250977\n",
      "(64, 33)\n",
      "step 21601, loss is 4.821750640869141\n",
      "(64, 33)\n",
      "step 21602, loss is 4.795814037322998\n",
      "(64, 33)\n",
      "step 21603, loss is 4.704659938812256\n",
      "(64, 33)\n",
      "step 21604, loss is 4.913646697998047\n",
      "(64, 33)\n",
      "step 21605, loss is 4.9205002784729\n",
      "(64, 33)\n",
      "step 21606, loss is 4.9395880699157715\n",
      "(64, 33)\n",
      "step 21607, loss is 4.757312297821045\n",
      "(64, 33)\n",
      "step 21608, loss is 4.67885160446167\n",
      "(64, 33)\n",
      "step 21609, loss is 4.715968608856201\n",
      "(64, 33)\n",
      "step 21610, loss is 4.808474063873291\n",
      "(64, 33)\n",
      "step 21611, loss is 4.986743927001953\n",
      "(64, 33)\n",
      "step 21612, loss is 4.620976448059082\n",
      "(64, 33)\n",
      "step 21613, loss is 4.829063415527344\n",
      "(64, 33)\n",
      "step 21614, loss is 4.866291522979736\n",
      "(64, 33)\n",
      "step 21615, loss is 4.866493225097656\n",
      "(64, 33)\n",
      "step 21616, loss is 4.646503925323486\n",
      "(64, 33)\n",
      "step 21617, loss is 4.946222305297852\n",
      "(64, 33)\n",
      "step 21618, loss is 5.021212577819824\n",
      "(64, 33)\n",
      "step 21619, loss is 4.925376892089844\n",
      "(64, 33)\n",
      "step 21620, loss is 4.916769504547119\n",
      "(64, 33)\n",
      "step 21621, loss is 4.800140380859375\n",
      "(64, 33)\n",
      "step 21622, loss is 4.898329734802246\n",
      "(64, 33)\n",
      "step 21623, loss is 4.876126289367676\n",
      "(64, 33)\n",
      "step 21624, loss is 4.730118274688721\n",
      "(64, 33)\n",
      "step 21625, loss is 4.4338274002075195\n",
      "(64, 33)\n",
      "step 21626, loss is 4.789919853210449\n",
      "(64, 33)\n",
      "step 21627, loss is 4.789943695068359\n",
      "(64, 33)\n",
      "step 21628, loss is 4.818850040435791\n",
      "(64, 33)\n",
      "step 21629, loss is 4.606480598449707\n",
      "(64, 33)\n",
      "step 21630, loss is 4.729923248291016\n",
      "(64, 33)\n",
      "step 21631, loss is 4.78961706161499\n",
      "(64, 33)\n",
      "step 21632, loss is 4.963265895843506\n",
      "(64, 33)\n",
      "step 21633, loss is 4.766036510467529\n",
      "(64, 33)\n",
      "step 21634, loss is 4.803875923156738\n",
      "(64, 33)\n",
      "step 21635, loss is 4.706297874450684\n",
      "(64, 33)\n",
      "step 21636, loss is 4.717200756072998\n",
      "(64, 33)\n",
      "step 21637, loss is 4.786867141723633\n",
      "(64, 33)\n",
      "step 21638, loss is 4.737545013427734\n",
      "(64, 33)\n",
      "step 21639, loss is 4.679653167724609\n",
      "(64, 33)\n",
      "step 21640, loss is 4.811232089996338\n",
      "(64, 33)\n",
      "step 21641, loss is 4.69659423828125\n",
      "(64, 33)\n",
      "step 21642, loss is 4.769537448883057\n",
      "(64, 33)\n",
      "step 21643, loss is 4.655035495758057\n",
      "(64, 33)\n",
      "step 21644, loss is 4.79153299331665\n",
      "(64, 33)\n",
      "step 21645, loss is 4.573590278625488\n",
      "(64, 33)\n",
      "step 21646, loss is 4.812984943389893\n",
      "(64, 33)\n",
      "step 21647, loss is 4.701617240905762\n",
      "(64, 33)\n",
      "step 21648, loss is 4.946932792663574\n",
      "(64, 33)\n",
      "step 21649, loss is 4.64249324798584\n",
      "(64, 33)\n",
      "step 21650, loss is 4.694210529327393\n",
      "(64, 33)\n",
      "step 21651, loss is 4.852103233337402\n",
      "(64, 33)\n",
      "step 21652, loss is 4.873239994049072\n",
      "(64, 33)\n",
      "step 21653, loss is 4.6510090827941895\n",
      "(64, 33)\n",
      "step 21654, loss is 4.695514678955078\n",
      "(64, 33)\n",
      "step 21655, loss is 4.846352577209473\n",
      "(64, 33)\n",
      "step 21656, loss is 4.837676048278809\n",
      "(64, 33)\n",
      "step 21657, loss is 4.748524188995361\n",
      "(64, 33)\n",
      "step 21658, loss is 4.854762077331543\n",
      "(64, 33)\n",
      "step 21659, loss is 4.866732120513916\n",
      "(64, 33)\n",
      "step 21660, loss is 4.835153579711914\n",
      "(64, 33)\n",
      "step 21661, loss is 4.713029384613037\n",
      "(64, 33)\n",
      "step 21662, loss is 4.84174919128418\n",
      "(64, 33)\n",
      "step 21663, loss is 4.842143535614014\n",
      "(64, 33)\n",
      "step 21664, loss is 4.774328231811523\n",
      "(64, 33)\n",
      "step 21665, loss is 4.776952266693115\n",
      "(64, 33)\n",
      "step 21666, loss is 4.746309280395508\n",
      "(64, 33)\n",
      "step 21667, loss is 4.894946098327637\n",
      "(64, 33)\n",
      "step 21668, loss is 4.825716018676758\n",
      "(64, 33)\n",
      "step 21669, loss is 4.6276726722717285\n",
      "(64, 33)\n",
      "step 21670, loss is 4.747818946838379\n",
      "(64, 33)\n",
      "step 21671, loss is 5.054348945617676\n",
      "(64, 33)\n",
      "step 21672, loss is 4.79240083694458\n",
      "(64, 33)\n",
      "step 21673, loss is 4.795835971832275\n",
      "(64, 33)\n",
      "step 21674, loss is 4.769089221954346\n",
      "(64, 33)\n",
      "step 21675, loss is 4.714795112609863\n",
      "(64, 33)\n",
      "step 21676, loss is 4.84053897857666\n",
      "(64, 33)\n",
      "step 21677, loss is 4.772139072418213\n",
      "(64, 33)\n",
      "step 21678, loss is 4.7054877281188965\n",
      "(64, 33)\n",
      "step 21679, loss is 4.660531520843506\n",
      "(64, 33)\n",
      "step 21680, loss is 4.814177513122559\n",
      "(64, 33)\n",
      "step 21681, loss is 4.712100982666016\n",
      "(64, 33)\n",
      "step 21682, loss is 4.750572204589844\n",
      "(64, 33)\n",
      "step 21683, loss is 4.751635551452637\n",
      "(64, 33)\n",
      "step 21684, loss is 4.65032958984375\n",
      "(64, 33)\n",
      "step 21685, loss is 4.744770526885986\n",
      "(64, 33)\n",
      "step 21686, loss is 4.8170485496521\n",
      "(64, 33)\n",
      "step 21687, loss is 4.858452796936035\n",
      "(64, 33)\n",
      "step 21688, loss is 4.821581840515137\n",
      "(64, 33)\n",
      "step 21689, loss is 4.683049201965332\n",
      "(64, 33)\n",
      "step 21690, loss is 4.798732280731201\n",
      "(64, 33)\n",
      "step 21691, loss is 4.805448532104492\n",
      "(64, 33)\n",
      "step 21692, loss is 4.822165489196777\n",
      "(64, 33)\n",
      "step 21693, loss is 4.66683292388916\n",
      "(64, 33)\n",
      "step 21694, loss is 4.871580600738525\n",
      "(64, 33)\n",
      "step 21695, loss is 4.731100559234619\n",
      "(64, 33)\n",
      "step 21696, loss is 4.812092304229736\n",
      "(64, 33)\n",
      "step 21697, loss is 4.956568717956543\n",
      "(64, 33)\n",
      "step 21698, loss is 4.687968730926514\n",
      "(64, 33)\n",
      "step 21699, loss is 4.871777057647705\n",
      "(64, 33)\n",
      "step 21700, loss is 4.7902727127075195\n",
      "(64, 33)\n",
      "step 21701, loss is 4.559678554534912\n",
      "(64, 33)\n",
      "step 21702, loss is 4.770495414733887\n",
      "(64, 33)\n",
      "step 21703, loss is 4.911622047424316\n",
      "(64, 33)\n",
      "step 21704, loss is 4.710443496704102\n",
      "(64, 33)\n",
      "step 21705, loss is 4.675412178039551\n",
      "(64, 33)\n",
      "step 21706, loss is 4.8266520500183105\n",
      "(64, 33)\n",
      "step 21707, loss is 4.810922145843506\n",
      "(64, 33)\n",
      "step 21708, loss is 4.905221939086914\n",
      "(64, 33)\n",
      "step 21709, loss is 4.633111000061035\n",
      "(64, 33)\n",
      "step 21710, loss is 4.874899387359619\n",
      "(64, 33)\n",
      "step 21711, loss is 4.7087225914001465\n",
      "(64, 33)\n",
      "step 21712, loss is 4.786159515380859\n",
      "(64, 33)\n",
      "step 21713, loss is 4.838924407958984\n",
      "(64, 33)\n",
      "step 21714, loss is 4.776297092437744\n",
      "(64, 33)\n",
      "step 21715, loss is 4.799408435821533\n",
      "(64, 33)\n",
      "step 21716, loss is 4.76181697845459\n",
      "(64, 33)\n",
      "step 21717, loss is 4.554018974304199\n",
      "(64, 33)\n",
      "step 21718, loss is 4.693116188049316\n",
      "(64, 33)\n",
      "step 21719, loss is 4.849935054779053\n",
      "(64, 33)\n",
      "step 21720, loss is 4.8532795906066895\n",
      "(64, 33)\n",
      "step 21721, loss is 4.706182479858398\n",
      "(64, 33)\n",
      "step 21722, loss is 4.7315850257873535\n",
      "(64, 33)\n",
      "step 21723, loss is 4.804600715637207\n",
      "(64, 33)\n",
      "step 21724, loss is 4.855261325836182\n",
      "(64, 33)\n",
      "step 21725, loss is 4.775329113006592\n",
      "(64, 33)\n",
      "step 21726, loss is 4.667699337005615\n",
      "(64, 33)\n",
      "step 21727, loss is 4.760047912597656\n",
      "(64, 33)\n",
      "step 21728, loss is 4.8444719314575195\n",
      "(64, 33)\n",
      "step 21729, loss is 4.709568023681641\n",
      "(64, 33)\n",
      "step 21730, loss is 4.829063892364502\n",
      "(64, 33)\n",
      "step 21731, loss is 4.610337257385254\n",
      "(64, 33)\n",
      "step 21732, loss is 4.770750045776367\n",
      "(64, 33)\n",
      "step 21733, loss is 4.778672218322754\n",
      "(64, 33)\n",
      "step 21734, loss is 4.89577054977417\n",
      "(64, 33)\n",
      "step 21735, loss is 4.721739768981934\n",
      "(64, 33)\n",
      "step 21736, loss is 4.8264689445495605\n",
      "(64, 33)\n",
      "step 21737, loss is 4.9266839027404785\n",
      "(64, 33)\n",
      "step 21738, loss is 4.9851603507995605\n",
      "(64, 33)\n",
      "step 21739, loss is 4.877208232879639\n",
      "(64, 33)\n",
      "step 21740, loss is 4.782991886138916\n",
      "(64, 33)\n",
      "step 21741, loss is 4.749876022338867\n",
      "(64, 33)\n",
      "step 21742, loss is 4.910274982452393\n",
      "(64, 33)\n",
      "step 21743, loss is 4.786686897277832\n",
      "(64, 33)\n",
      "step 21744, loss is 4.602470397949219\n",
      "(64, 33)\n",
      "step 21745, loss is 4.859955787658691\n",
      "(64, 33)\n",
      "step 21746, loss is 4.913000583648682\n",
      "(64, 33)\n",
      "step 21747, loss is 4.5697760581970215\n",
      "(64, 33)\n",
      "step 21748, loss is 4.969978332519531\n",
      "(64, 33)\n",
      "step 21749, loss is 4.79252815246582\n",
      "(64, 33)\n",
      "step 21750, loss is 5.056027412414551\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21751, loss is 4.588724613189697\n",
      "(64, 33)\n",
      "step 21752, loss is 4.760496139526367\n",
      "(64, 33)\n",
      "step 21753, loss is 4.821536540985107\n",
      "(64, 33)\n",
      "step 21754, loss is 4.796055793762207\n",
      "(64, 33)\n",
      "step 21755, loss is 4.794266700744629\n",
      "(64, 33)\n",
      "step 21756, loss is 4.879613876342773\n",
      "(64, 33)\n",
      "step 21757, loss is 4.764858245849609\n",
      "(64, 33)\n",
      "step 21758, loss is 4.9900922775268555\n",
      "(64, 33)\n",
      "step 21759, loss is 4.764573574066162\n",
      "(64, 33)\n",
      "step 21760, loss is 4.775557041168213\n",
      "(64, 33)\n",
      "step 21761, loss is 4.7106099128723145\n",
      "(64, 33)\n",
      "step 21762, loss is 4.74281644821167\n",
      "(64, 33)\n",
      "step 21763, loss is 4.940118312835693\n",
      "(64, 33)\n",
      "step 21764, loss is 4.800321102142334\n",
      "(64, 33)\n",
      "step 21765, loss is 4.920912742614746\n",
      "(64, 33)\n",
      "step 21766, loss is 4.6402587890625\n",
      "(64, 33)\n",
      "step 21767, loss is 4.823776721954346\n",
      "(64, 33)\n",
      "step 21768, loss is 4.777652740478516\n",
      "(64, 33)\n",
      "step 21769, loss is 4.730386257171631\n",
      "(64, 33)\n",
      "step 21770, loss is 4.899886608123779\n",
      "(64, 33)\n",
      "step 21771, loss is 4.826194763183594\n",
      "(64, 33)\n",
      "step 21772, loss is 5.046812057495117\n",
      "(64, 33)\n",
      "step 21773, loss is 4.875709056854248\n",
      "(64, 33)\n",
      "step 21774, loss is 4.9288153648376465\n",
      "(64, 33)\n",
      "step 21775, loss is 4.721367359161377\n",
      "(64, 33)\n",
      "step 21776, loss is 4.705974578857422\n",
      "(64, 33)\n",
      "step 21777, loss is 4.76637601852417\n",
      "(64, 33)\n",
      "step 21778, loss is 4.847140789031982\n",
      "(64, 33)\n",
      "step 21779, loss is 4.762483596801758\n",
      "(64, 33)\n",
      "step 21780, loss is 4.793891906738281\n",
      "(64, 33)\n",
      "step 21781, loss is 4.857509613037109\n",
      "(64, 33)\n",
      "step 21782, loss is 4.578619956970215\n",
      "(64, 33)\n",
      "step 21783, loss is 4.750671863555908\n",
      "(64, 33)\n",
      "step 21784, loss is 4.909547805786133\n",
      "(64, 33)\n",
      "step 21785, loss is 4.7077202796936035\n",
      "(64, 33)\n",
      "step 21786, loss is 4.791356086730957\n",
      "(64, 33)\n",
      "step 21787, loss is 4.761650085449219\n",
      "(64, 33)\n",
      "step 21788, loss is 4.955758094787598\n",
      "(64, 33)\n",
      "step 21789, loss is 4.791376113891602\n",
      "(64, 33)\n",
      "step 21790, loss is 4.722345352172852\n",
      "(64, 33)\n",
      "step 21791, loss is 4.891382217407227\n",
      "(64, 33)\n",
      "step 21792, loss is 4.807924747467041\n",
      "(64, 33)\n",
      "step 21793, loss is 4.751819133758545\n",
      "(64, 33)\n",
      "step 21794, loss is 4.81876802444458\n",
      "(64, 33)\n",
      "step 21795, loss is 4.873095512390137\n",
      "(64, 33)\n",
      "step 21796, loss is 4.852575778961182\n",
      "(64, 33)\n",
      "step 21797, loss is 4.911557674407959\n",
      "(64, 33)\n",
      "step 21798, loss is 4.8691792488098145\n",
      "(64, 33)\n",
      "step 21799, loss is 4.853813648223877\n",
      "(64, 33)\n",
      "step 21800, loss is 4.763816833496094\n",
      "(64, 33)\n",
      "step 21801, loss is 4.736993312835693\n",
      "(64, 33)\n",
      "step 21802, loss is 4.757455825805664\n",
      "(64, 33)\n",
      "step 21803, loss is 4.691388130187988\n",
      "(64, 33)\n",
      "step 21804, loss is 4.688892841339111\n",
      "(64, 33)\n",
      "step 21805, loss is 4.709290504455566\n",
      "(64, 33)\n",
      "step 21806, loss is 4.910887241363525\n",
      "(64, 33)\n",
      "step 21807, loss is 4.685432434082031\n",
      "(64, 33)\n",
      "step 21808, loss is 4.759498596191406\n",
      "(64, 33)\n",
      "step 21809, loss is 4.9395623207092285\n",
      "(64, 33)\n",
      "step 21810, loss is 4.7460150718688965\n",
      "(64, 33)\n",
      "step 21811, loss is 4.854109287261963\n",
      "(64, 33)\n",
      "step 21812, loss is 4.800098419189453\n",
      "(64, 33)\n",
      "step 21813, loss is 5.039261341094971\n",
      "(64, 33)\n",
      "step 21814, loss is 4.712502479553223\n",
      "(64, 33)\n",
      "step 21815, loss is 4.721949577331543\n",
      "(64, 33)\n",
      "step 21816, loss is 4.663751602172852\n",
      "(64, 33)\n",
      "step 21817, loss is 4.655614852905273\n",
      "(64, 33)\n",
      "step 21818, loss is 4.798119068145752\n",
      "(64, 33)\n",
      "step 21819, loss is 4.73547887802124\n",
      "(64, 33)\n",
      "step 21820, loss is 4.703710079193115\n",
      "(64, 33)\n",
      "step 21821, loss is 4.744278430938721\n",
      "(64, 33)\n",
      "step 21822, loss is 4.7930402755737305\n",
      "(64, 33)\n",
      "step 21823, loss is 4.80751371383667\n",
      "(64, 33)\n",
      "step 21824, loss is 4.874084949493408\n",
      "(64, 33)\n",
      "step 21825, loss is 4.799563884735107\n",
      "(64, 33)\n",
      "step 21826, loss is 4.670231342315674\n",
      "(64, 33)\n",
      "step 21827, loss is 4.75979471206665\n",
      "(64, 33)\n",
      "step 21828, loss is 4.7803850173950195\n",
      "(64, 33)\n",
      "step 21829, loss is 4.679264545440674\n",
      "(64, 33)\n",
      "step 21830, loss is 4.923463821411133\n",
      "(64, 33)\n",
      "step 21831, loss is 4.918570518493652\n",
      "(64, 33)\n",
      "step 21832, loss is 4.718233108520508\n",
      "(64, 33)\n",
      "step 21833, loss is 4.610555648803711\n",
      "(64, 33)\n",
      "step 21834, loss is 4.931697845458984\n",
      "(64, 33)\n",
      "step 21835, loss is 4.789971351623535\n",
      "(64, 33)\n",
      "step 21836, loss is 4.890105247497559\n",
      "(64, 33)\n",
      "step 21837, loss is 4.678944110870361\n",
      "(64, 33)\n",
      "step 21838, loss is 4.851438045501709\n",
      "(64, 33)\n",
      "step 21839, loss is 4.792085647583008\n",
      "(64, 33)\n",
      "step 21840, loss is 4.684034824371338\n",
      "(64, 33)\n",
      "step 21841, loss is 4.576671600341797\n",
      "(64, 33)\n",
      "step 21842, loss is 4.877227306365967\n",
      "(64, 33)\n",
      "step 21843, loss is 4.687595367431641\n",
      "(64, 33)\n",
      "step 21844, loss is 4.9994354248046875\n",
      "(64, 33)\n",
      "step 21845, loss is 4.8212199211120605\n",
      "(64, 33)\n",
      "step 21846, loss is 4.827601909637451\n",
      "(64, 33)\n",
      "step 21847, loss is 4.7266154289245605\n",
      "(64, 33)\n",
      "step 21848, loss is 4.867562294006348\n",
      "(64, 33)\n",
      "step 21849, loss is 4.760390281677246\n",
      "(64, 33)\n",
      "step 21850, loss is 4.741642951965332\n",
      "(64, 33)\n",
      "step 21851, loss is 5.0154128074646\n",
      "(64, 33)\n",
      "step 21852, loss is 4.6553497314453125\n",
      "(64, 33)\n",
      "step 21853, loss is 4.788309097290039\n",
      "(64, 33)\n",
      "step 21854, loss is 4.730751037597656\n",
      "(64, 33)\n",
      "step 21855, loss is 4.730363368988037\n",
      "(64, 33)\n",
      "step 21856, loss is 4.597719192504883\n",
      "(64, 33)\n",
      "step 21857, loss is 4.694907188415527\n",
      "(64, 33)\n",
      "step 21858, loss is 4.936642646789551\n",
      "(64, 33)\n",
      "step 21859, loss is 4.709399223327637\n",
      "(64, 33)\n",
      "step 21860, loss is 4.716926574707031\n",
      "(64, 33)\n",
      "step 21861, loss is 4.779623508453369\n",
      "(64, 33)\n",
      "step 21862, loss is 4.336075305938721\n",
      "(64, 33)\n",
      "step 21863, loss is 4.61386775970459\n",
      "(64, 33)\n",
      "step 21864, loss is 4.6468024253845215\n",
      "(64, 33)\n",
      "step 21865, loss is 4.896337985992432\n",
      "(64, 33)\n",
      "step 21866, loss is 4.59639835357666\n",
      "(64, 33)\n",
      "step 21867, loss is 4.810331344604492\n",
      "(64, 33)\n",
      "step 21868, loss is 4.707736492156982\n",
      "(64, 33)\n",
      "step 21869, loss is 4.680873870849609\n",
      "(64, 33)\n",
      "step 21870, loss is 4.822948932647705\n",
      "(64, 33)\n",
      "step 21871, loss is 4.791790008544922\n",
      "(64, 33)\n",
      "step 21872, loss is 4.801730155944824\n",
      "(64, 33)\n",
      "step 21873, loss is 4.8342719078063965\n",
      "(64, 33)\n",
      "step 21874, loss is 4.939854145050049\n",
      "(64, 33)\n",
      "step 21875, loss is 4.813931941986084\n",
      "(64, 33)\n",
      "step 21876, loss is 4.860972881317139\n",
      "(64, 33)\n",
      "step 21877, loss is 4.621699333190918\n",
      "(64, 33)\n",
      "step 21878, loss is 4.738338470458984\n",
      "(64, 33)\n",
      "step 21879, loss is 4.8381524085998535\n",
      "(64, 33)\n",
      "step 21880, loss is 4.969512462615967\n",
      "(64, 33)\n",
      "step 21881, loss is 4.88685941696167\n",
      "(64, 33)\n",
      "step 21882, loss is 4.671308994293213\n",
      "(64, 33)\n",
      "step 21883, loss is 4.7360076904296875\n",
      "(64, 33)\n",
      "step 21884, loss is 4.813406467437744\n",
      "(64, 33)\n",
      "step 21885, loss is 4.850593090057373\n",
      "(64, 33)\n",
      "step 21886, loss is 4.862149715423584\n",
      "(64, 33)\n",
      "step 21887, loss is 4.5026092529296875\n",
      "(64, 33)\n",
      "step 21888, loss is 4.807613849639893\n",
      "(64, 33)\n",
      "step 21889, loss is 4.673590660095215\n",
      "(64, 33)\n",
      "step 21890, loss is 4.6648406982421875\n",
      "(64, 33)\n",
      "step 21891, loss is 4.921089172363281\n",
      "(64, 33)\n",
      "step 21892, loss is 4.90022611618042\n",
      "(64, 33)\n",
      "step 21893, loss is 4.861349105834961\n",
      "(64, 33)\n",
      "step 21894, loss is 4.79805326461792\n",
      "(64, 33)\n",
      "step 21895, loss is 4.676107883453369\n",
      "(64, 33)\n",
      "step 21896, loss is 4.642440319061279\n",
      "(64, 33)\n",
      "step 21897, loss is 4.806857585906982\n",
      "(64, 33)\n",
      "step 21898, loss is 4.888969421386719\n",
      "(64, 33)\n",
      "step 21899, loss is 4.855827331542969\n",
      "(64, 33)\n",
      "step 21900, loss is 4.8673930168151855\n",
      "(64, 33)\n",
      "step 21901, loss is 4.939680099487305\n",
      "(64, 33)\n",
      "step 21902, loss is 4.8997344970703125\n",
      "(64, 33)\n",
      "step 21903, loss is 4.764016151428223\n",
      "(64, 33)\n",
      "step 21904, loss is 4.747706413269043\n",
      "(64, 33)\n",
      "step 21905, loss is 4.669618606567383\n",
      "(64, 33)\n",
      "step 21906, loss is 4.783088207244873\n",
      "(64, 33)\n",
      "step 21907, loss is 4.568127155303955\n",
      "(64, 33)\n",
      "step 21908, loss is 4.847997188568115\n",
      "(64, 33)\n",
      "step 21909, loss is 4.842586517333984\n",
      "(64, 33)\n",
      "step 21910, loss is 4.753580570220947\n",
      "(64, 33)\n",
      "step 21911, loss is 4.867038726806641\n",
      "(64, 33)\n",
      "step 21912, loss is 4.706309795379639\n",
      "(64, 33)\n",
      "step 21913, loss is 4.711825370788574\n",
      "(64, 33)\n",
      "step 21914, loss is 4.776855945587158\n",
      "(64, 33)\n",
      "step 21915, loss is 4.689822673797607\n",
      "(64, 33)\n",
      "step 21916, loss is 4.670483112335205\n",
      "(64, 33)\n",
      "step 21917, loss is 4.757437705993652\n",
      "(64, 33)\n",
      "step 21918, loss is 4.63272762298584\n",
      "(64, 33)\n",
      "step 21919, loss is 4.69613790512085\n",
      "(64, 33)\n",
      "step 21920, loss is 4.641615867614746\n",
      "(64, 33)\n",
      "step 21921, loss is 4.774824142456055\n",
      "(64, 33)\n",
      "step 21922, loss is 5.082251071929932\n",
      "(64, 33)\n",
      "step 21923, loss is 5.065195560455322\n",
      "(64, 33)\n",
      "step 21924, loss is 4.652399063110352\n",
      "(64, 33)\n",
      "step 21925, loss is 4.5870161056518555\n",
      "(64, 33)\n",
      "step 21926, loss is 4.891854286193848\n",
      "(64, 33)\n",
      "step 21927, loss is 4.725113868713379\n",
      "(64, 33)\n",
      "step 21928, loss is 5.013830184936523\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21929, loss is 4.598780632019043\n",
      "(64, 33)\n",
      "step 21930, loss is 4.803468704223633\n",
      "(64, 33)\n",
      "step 21931, loss is 4.75618839263916\n",
      "(64, 33)\n",
      "step 21932, loss is 4.793107509613037\n",
      "(64, 33)\n",
      "step 21933, loss is 4.816831588745117\n",
      "(64, 33)\n",
      "step 21934, loss is 4.923921585083008\n",
      "(64, 33)\n",
      "step 21935, loss is 4.678370952606201\n",
      "(64, 33)\n",
      "step 21936, loss is 4.576067924499512\n",
      "(64, 33)\n",
      "step 21937, loss is 4.727855682373047\n",
      "(64, 33)\n",
      "step 21938, loss is 4.767872333526611\n",
      "(64, 33)\n",
      "step 21939, loss is 4.905616283416748\n",
      "(64, 33)\n",
      "step 21940, loss is 4.454535484313965\n",
      "(64, 33)\n",
      "step 21941, loss is 4.6684041023254395\n",
      "(64, 33)\n",
      "step 21942, loss is 4.8801069259643555\n",
      "(64, 33)\n",
      "step 21943, loss is 4.743401050567627\n",
      "(64, 33)\n",
      "step 21944, loss is 4.708791255950928\n",
      "(64, 33)\n",
      "step 21945, loss is 4.7769575119018555\n",
      "(64, 33)\n",
      "step 21946, loss is 4.781571865081787\n",
      "(64, 33)\n",
      "step 21947, loss is 4.5268754959106445\n",
      "(64, 33)\n",
      "step 21948, loss is 4.793641090393066\n",
      "(64, 33)\n",
      "step 21949, loss is 4.892266273498535\n",
      "(64, 33)\n",
      "step 21950, loss is 4.81783390045166\n",
      "(64, 33)\n",
      "step 21951, loss is 4.844213962554932\n",
      "(64, 33)\n",
      "step 21952, loss is 4.698777198791504\n",
      "(64, 33)\n",
      "step 21953, loss is 4.704832077026367\n",
      "(64, 33)\n",
      "step 21954, loss is 4.710329055786133\n",
      "(64, 33)\n",
      "step 21955, loss is 4.761248588562012\n",
      "(64, 33)\n",
      "step 21956, loss is 4.7426018714904785\n",
      "(64, 33)\n",
      "step 21957, loss is 4.873963356018066\n",
      "(64, 33)\n",
      "step 21958, loss is 4.778781890869141\n",
      "(64, 33)\n",
      "step 21959, loss is 4.716843605041504\n",
      "(64, 33)\n",
      "step 21960, loss is 4.832993507385254\n",
      "(64, 33)\n",
      "step 21961, loss is 4.787894248962402\n",
      "(64, 33)\n",
      "step 21962, loss is 4.647810459136963\n",
      "(64, 33)\n",
      "step 21963, loss is 4.671387195587158\n",
      "(64, 33)\n",
      "step 21964, loss is 4.848485946655273\n",
      "(64, 33)\n",
      "step 21965, loss is 4.695740699768066\n",
      "(64, 33)\n",
      "step 21966, loss is 4.712011337280273\n",
      "(64, 33)\n",
      "step 21967, loss is 4.654386520385742\n",
      "(64, 33)\n",
      "step 21968, loss is 4.754415512084961\n",
      "(64, 33)\n",
      "step 21969, loss is 4.759522438049316\n",
      "(64, 33)\n",
      "step 21970, loss is 4.646161079406738\n",
      "(64, 33)\n",
      "step 21971, loss is 4.828126907348633\n",
      "(64, 33)\n",
      "step 21972, loss is 4.7549848556518555\n",
      "(64, 33)\n",
      "step 21973, loss is 4.764427185058594\n",
      "(64, 33)\n",
      "step 21974, loss is 4.824269771575928\n",
      "(64, 33)\n",
      "step 21975, loss is 4.749466419219971\n",
      "(64, 33)\n",
      "step 21976, loss is 4.636527061462402\n",
      "(64, 33)\n",
      "step 21977, loss is 4.812944412231445\n",
      "(64, 33)\n",
      "step 21978, loss is 4.863734722137451\n",
      "(64, 33)\n",
      "step 21979, loss is 4.795477867126465\n",
      "(64, 33)\n",
      "step 21980, loss is 4.6460700035095215\n",
      "(64, 33)\n",
      "step 21981, loss is 4.728527545928955\n",
      "(64, 33)\n",
      "step 21982, loss is 4.685421943664551\n",
      "(64, 33)\n",
      "step 21983, loss is 4.632573127746582\n",
      "(64, 33)\n",
      "step 21984, loss is 4.797708988189697\n",
      "(64, 33)\n",
      "step 21985, loss is 4.721869945526123\n",
      "(64, 33)\n",
      "step 21986, loss is 4.811691761016846\n",
      "(64, 33)\n",
      "step 21987, loss is 4.6639404296875\n",
      "(64, 33)\n",
      "step 21988, loss is 4.716374397277832\n",
      "(64, 33)\n",
      "step 21989, loss is 4.951071262359619\n",
      "(64, 33)\n",
      "step 21990, loss is 4.752572536468506\n",
      "(64, 33)\n",
      "step 21991, loss is 4.620412349700928\n",
      "(64, 33)\n",
      "step 21992, loss is 4.745840549468994\n",
      "(64, 33)\n",
      "step 21993, loss is 4.919076919555664\n",
      "(64, 33)\n",
      "step 21994, loss is 4.600351810455322\n",
      "(64, 33)\n",
      "step 21995, loss is 4.808295726776123\n",
      "(64, 33)\n",
      "step 21996, loss is 4.6645188331604\n",
      "(64, 33)\n",
      "step 21997, loss is 4.978425979614258\n",
      "(64, 33)\n",
      "step 21998, loss is 4.8433966636657715\n",
      "(64, 33)\n",
      "step 21999, loss is 4.588191509246826\n",
      "(64, 33)\n",
      "step 22000, loss is 4.8541364669799805\n",
      "(64, 33)\n",
      "step 22001, loss is 4.6561665534973145\n",
      "(64, 33)\n",
      "step 22002, loss is 4.636770725250244\n",
      "(64, 33)\n",
      "step 22003, loss is 4.966148376464844\n",
      "(64, 33)\n",
      "step 22004, loss is 4.790281772613525\n",
      "(64, 33)\n",
      "step 22005, loss is 4.708442687988281\n",
      "(64, 33)\n",
      "step 22006, loss is 4.774288654327393\n",
      "(64, 33)\n",
      "step 22007, loss is 5.039642810821533\n",
      "(64, 33)\n",
      "step 22008, loss is 4.674424648284912\n",
      "(64, 33)\n",
      "step 22009, loss is 4.526424884796143\n",
      "(64, 33)\n",
      "step 22010, loss is 4.920351505279541\n",
      "(64, 33)\n",
      "step 22011, loss is 4.797485828399658\n",
      "(64, 33)\n",
      "step 22012, loss is 4.659953594207764\n",
      "(64, 33)\n",
      "step 22013, loss is 4.822054862976074\n",
      "(64, 33)\n",
      "step 22014, loss is 4.730344295501709\n",
      "(64, 33)\n",
      "step 22015, loss is 4.577831745147705\n",
      "(64, 33)\n",
      "step 22016, loss is 4.678859233856201\n",
      "(64, 33)\n",
      "step 22017, loss is 4.637950420379639\n",
      "(64, 33)\n",
      "step 22018, loss is 4.8868489265441895\n",
      "(64, 33)\n",
      "step 22019, loss is 4.89333438873291\n",
      "(64, 33)\n",
      "step 22020, loss is 4.671666622161865\n",
      "(64, 33)\n",
      "step 22021, loss is 4.880303382873535\n",
      "(64, 33)\n",
      "step 22022, loss is 4.7965569496154785\n",
      "(64, 33)\n",
      "step 22023, loss is 4.7727274894714355\n",
      "(64, 33)\n",
      "step 22024, loss is 4.825839042663574\n",
      "(64, 33)\n",
      "step 22025, loss is 4.848668098449707\n",
      "(64, 33)\n",
      "step 22026, loss is 4.88385009765625\n",
      "(64, 33)\n",
      "step 22027, loss is 4.831877708435059\n",
      "(64, 33)\n",
      "step 22028, loss is 4.972001075744629\n",
      "(64, 33)\n",
      "step 22029, loss is 4.804800510406494\n",
      "(64, 33)\n",
      "step 22030, loss is 4.8182477951049805\n",
      "(64, 33)\n",
      "step 22031, loss is 4.738149642944336\n",
      "(64, 33)\n",
      "step 22032, loss is 4.819426536560059\n",
      "(64, 33)\n",
      "step 22033, loss is 4.6229071617126465\n",
      "(64, 33)\n",
      "step 22034, loss is 4.6415629386901855\n",
      "(64, 33)\n",
      "step 22035, loss is 4.8581156730651855\n",
      "(64, 33)\n",
      "step 22036, loss is 4.6999287605285645\n",
      "(64, 33)\n",
      "step 22037, loss is 5.1115264892578125\n",
      "(64, 33)\n",
      "step 22038, loss is 4.705383777618408\n",
      "(64, 33)\n",
      "step 22039, loss is 4.677585601806641\n",
      "(64, 33)\n",
      "step 22040, loss is 4.79667329788208\n",
      "(64, 33)\n",
      "step 22041, loss is 4.584813117980957\n",
      "(64, 33)\n",
      "step 22042, loss is 4.599891185760498\n",
      "(64, 33)\n",
      "step 22043, loss is 4.810625076293945\n",
      "(64, 33)\n",
      "step 22044, loss is 4.842559814453125\n",
      "(64, 33)\n",
      "step 22045, loss is 4.720909118652344\n",
      "(64, 33)\n",
      "step 22046, loss is 4.967437744140625\n",
      "(64, 33)\n",
      "step 22047, loss is 4.695330619812012\n",
      "(64, 33)\n",
      "step 22048, loss is 4.853737831115723\n",
      "(64, 33)\n",
      "step 22049, loss is 4.910179138183594\n",
      "(64, 33)\n",
      "step 22050, loss is 4.784797191619873\n",
      "(64, 33)\n",
      "step 22051, loss is 4.848083019256592\n",
      "(64, 33)\n",
      "step 22052, loss is 4.669856071472168\n",
      "(64, 33)\n",
      "step 22053, loss is 4.798595428466797\n",
      "(64, 33)\n",
      "step 22054, loss is 4.80944299697876\n",
      "(64, 33)\n",
      "step 22055, loss is 4.780594825744629\n",
      "(64, 33)\n",
      "step 22056, loss is 4.7896952629089355\n",
      "(64, 33)\n",
      "step 22057, loss is 4.901944160461426\n",
      "(64, 33)\n",
      "step 22058, loss is 4.541858196258545\n",
      "(64, 33)\n",
      "step 22059, loss is 4.73927640914917\n",
      "(64, 33)\n",
      "step 22060, loss is 4.768887996673584\n",
      "(64, 33)\n",
      "step 22061, loss is 4.878092288970947\n",
      "(64, 33)\n",
      "step 22062, loss is 4.730692386627197\n",
      "(64, 33)\n",
      "step 22063, loss is 4.55681037902832\n",
      "(64, 33)\n",
      "step 22064, loss is 4.728010654449463\n",
      "(64, 33)\n",
      "step 22065, loss is 4.765326023101807\n",
      "(64, 33)\n",
      "step 22066, loss is 4.903051853179932\n",
      "(64, 33)\n",
      "step 22067, loss is 4.538392066955566\n",
      "(64, 33)\n",
      "step 22068, loss is 4.8148369789123535\n",
      "(64, 33)\n",
      "step 22069, loss is 4.784630298614502\n",
      "(64, 33)\n",
      "step 22070, loss is 4.919943332672119\n",
      "(64, 33)\n",
      "step 22071, loss is 4.822485446929932\n",
      "(64, 33)\n",
      "step 22072, loss is 4.682932376861572\n",
      "(64, 33)\n",
      "step 22073, loss is 4.699040412902832\n",
      "(64, 33)\n",
      "step 22074, loss is 4.693192481994629\n",
      "(64, 33)\n",
      "step 22075, loss is 4.555850505828857\n",
      "(64, 33)\n",
      "step 22076, loss is 4.890246868133545\n",
      "(64, 33)\n",
      "step 22077, loss is 4.886173248291016\n",
      "(64, 33)\n",
      "step 22078, loss is 4.7006635665893555\n",
      "(64, 33)\n",
      "step 22079, loss is 4.746486186981201\n",
      "(64, 33)\n",
      "step 22080, loss is 4.638669967651367\n",
      "(64, 33)\n",
      "step 22081, loss is 4.819745063781738\n",
      "(64, 33)\n",
      "step 22082, loss is 4.92506217956543\n",
      "(64, 33)\n",
      "step 22083, loss is 4.8807902336120605\n",
      "(64, 33)\n",
      "step 22084, loss is 4.745665073394775\n",
      "(64, 33)\n",
      "step 22085, loss is 4.893812656402588\n",
      "(64, 33)\n",
      "step 22086, loss is 4.671487331390381\n",
      "(64, 33)\n",
      "step 22087, loss is 4.916192531585693\n",
      "(64, 33)\n",
      "step 22088, loss is 4.9483160972595215\n",
      "(64, 33)\n",
      "step 22089, loss is 4.794885635375977\n",
      "(64, 33)\n",
      "step 22090, loss is 4.965466022491455\n",
      "(64, 33)\n",
      "step 22091, loss is 4.833535671234131\n",
      "(64, 33)\n",
      "step 22092, loss is 4.9708685874938965\n",
      "(64, 33)\n",
      "step 22093, loss is 4.7639641761779785\n",
      "(64, 33)\n",
      "step 22094, loss is 4.842721462249756\n",
      "(64, 33)\n",
      "step 22095, loss is 4.844728469848633\n",
      "(64, 33)\n",
      "step 22096, loss is 4.610845565795898\n",
      "(64, 33)\n",
      "step 22097, loss is 4.876825332641602\n",
      "(64, 33)\n",
      "step 22098, loss is 4.894007205963135\n",
      "(64, 33)\n",
      "step 22099, loss is 4.867790699005127\n",
      "(64, 33)\n",
      "step 22100, loss is 4.775393009185791\n",
      "(64, 33)\n",
      "step 22101, loss is 4.65964937210083\n",
      "(64, 33)\n",
      "step 22102, loss is 4.717141628265381\n",
      "(64, 33)\n",
      "step 22103, loss is 4.679388046264648\n",
      "(64, 33)\n",
      "step 22104, loss is 4.7680745124816895\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22105, loss is 4.6467695236206055\n",
      "(64, 33)\n",
      "step 22106, loss is 4.765167713165283\n",
      "(64, 33)\n",
      "step 22107, loss is 4.883728981018066\n",
      "(64, 33)\n",
      "step 22108, loss is 4.955397129058838\n",
      "(64, 33)\n",
      "step 22109, loss is 4.975058555603027\n",
      "(64, 33)\n",
      "step 22110, loss is 4.814414978027344\n",
      "(64, 33)\n",
      "step 22111, loss is 4.713039875030518\n",
      "(64, 33)\n",
      "step 22112, loss is 4.687367916107178\n",
      "(64, 33)\n",
      "step 22113, loss is 4.753119945526123\n",
      "(64, 33)\n",
      "step 22114, loss is 4.727427959442139\n",
      "(64, 33)\n",
      "step 22115, loss is 5.018195152282715\n",
      "(64, 33)\n",
      "step 22116, loss is 4.707056045532227\n",
      "(64, 33)\n",
      "step 22117, loss is 4.976701259613037\n",
      "(64, 33)\n",
      "step 22118, loss is 4.801597595214844\n",
      "(64, 33)\n",
      "step 22119, loss is 4.715675354003906\n",
      "(64, 33)\n",
      "step 22120, loss is 4.793509006500244\n",
      "(64, 33)\n",
      "step 22121, loss is 4.743839263916016\n",
      "(64, 33)\n",
      "step 22122, loss is 4.845631122589111\n",
      "(64, 33)\n",
      "step 22123, loss is 4.715638160705566\n",
      "(64, 33)\n",
      "step 22124, loss is 4.70549201965332\n",
      "(64, 33)\n",
      "step 22125, loss is 4.669821262359619\n",
      "(64, 33)\n",
      "step 22126, loss is 4.910438537597656\n",
      "(64, 33)\n",
      "step 22127, loss is 4.929630279541016\n",
      "(64, 33)\n",
      "step 22128, loss is 4.725906848907471\n",
      "(64, 33)\n",
      "step 22129, loss is 4.833277225494385\n",
      "(64, 33)\n",
      "step 22130, loss is 4.7773942947387695\n",
      "(64, 33)\n",
      "step 22131, loss is 4.720975875854492\n",
      "(64, 33)\n",
      "step 22132, loss is 4.827034950256348\n",
      "(64, 33)\n",
      "step 22133, loss is 4.886603832244873\n",
      "(64, 33)\n",
      "step 22134, loss is 4.95399808883667\n",
      "(64, 33)\n",
      "step 22135, loss is 4.651122093200684\n",
      "(64, 33)\n",
      "step 22136, loss is 4.740188121795654\n",
      "(64, 33)\n",
      "step 22137, loss is 4.697062969207764\n",
      "(64, 33)\n",
      "step 22138, loss is 4.501388072967529\n",
      "(64, 33)\n",
      "step 22139, loss is 4.764143466949463\n",
      "(64, 33)\n",
      "step 22140, loss is 4.7776360511779785\n",
      "(64, 33)\n",
      "step 22141, loss is 4.873504638671875\n",
      "(64, 33)\n",
      "step 22142, loss is 4.763147354125977\n",
      "(64, 33)\n",
      "step 22143, loss is 4.819654941558838\n",
      "(64, 33)\n",
      "step 22144, loss is 5.051721572875977\n",
      "(64, 33)\n",
      "step 22145, loss is 4.984158039093018\n",
      "(64, 33)\n",
      "step 22146, loss is 4.6768269538879395\n",
      "(64, 33)\n",
      "step 22147, loss is 4.862398624420166\n",
      "(64, 33)\n",
      "step 22148, loss is 4.672615051269531\n",
      "(64, 33)\n",
      "step 22149, loss is 4.788425445556641\n",
      "(64, 33)\n",
      "step 22150, loss is 4.828522205352783\n",
      "(64, 33)\n",
      "step 22151, loss is 4.831151485443115\n",
      "(64, 33)\n",
      "step 22152, loss is 4.716831684112549\n",
      "(64, 33)\n",
      "step 22153, loss is 4.863009452819824\n",
      "(64, 33)\n",
      "step 22154, loss is 4.766209602355957\n",
      "(64, 33)\n",
      "step 22155, loss is 4.7256059646606445\n",
      "(64, 33)\n",
      "step 22156, loss is 4.84218692779541\n",
      "(64, 33)\n",
      "step 22157, loss is 4.74428129196167\n",
      "(64, 33)\n",
      "step 22158, loss is 4.914267063140869\n",
      "(64, 33)\n",
      "step 22159, loss is 4.889078140258789\n",
      "(64, 33)\n",
      "step 22160, loss is 4.75676155090332\n",
      "(64, 33)\n",
      "step 22161, loss is 4.772732734680176\n",
      "(64, 33)\n",
      "step 22162, loss is 4.745842456817627\n",
      "(64, 33)\n",
      "step 22163, loss is 4.737157821655273\n",
      "(64, 33)\n",
      "step 22164, loss is 4.863916397094727\n",
      "(64, 33)\n",
      "step 22165, loss is 4.737245559692383\n",
      "(64, 33)\n",
      "step 22166, loss is 4.853597640991211\n",
      "(64, 33)\n",
      "step 22167, loss is 4.786742210388184\n",
      "(64, 33)\n",
      "step 22168, loss is 4.671387195587158\n",
      "(64, 33)\n",
      "step 22169, loss is 4.551949977874756\n",
      "(64, 33)\n",
      "step 22170, loss is 4.780832767486572\n",
      "(64, 33)\n",
      "step 22171, loss is 4.80259370803833\n",
      "(64, 33)\n",
      "step 22172, loss is 4.886721134185791\n",
      "(64, 33)\n",
      "step 22173, loss is 4.7673234939575195\n",
      "(64, 33)\n",
      "step 22174, loss is 4.60455322265625\n",
      "(64, 33)\n",
      "step 22175, loss is 4.759735584259033\n",
      "(64, 33)\n",
      "step 22176, loss is 4.814330101013184\n",
      "(64, 33)\n",
      "step 22177, loss is 4.76518440246582\n",
      "(64, 33)\n",
      "step 22178, loss is 4.89643669128418\n",
      "(64, 33)\n",
      "step 22179, loss is 4.809756755828857\n",
      "(64, 33)\n",
      "step 22180, loss is 4.775325775146484\n",
      "(64, 33)\n",
      "step 22181, loss is 4.855967998504639\n",
      "(64, 33)\n",
      "step 22182, loss is 4.993611812591553\n",
      "(64, 33)\n",
      "step 22183, loss is 4.874793529510498\n",
      "(64, 33)\n",
      "step 22184, loss is 5.010859489440918\n",
      "(64, 33)\n",
      "step 22185, loss is 4.733283996582031\n",
      "(64, 33)\n",
      "step 22186, loss is 4.826043605804443\n",
      "(64, 33)\n",
      "step 22187, loss is 4.766658782958984\n",
      "(64, 33)\n",
      "step 22188, loss is 4.673529624938965\n",
      "(64, 33)\n",
      "step 22189, loss is 4.79699182510376\n",
      "(64, 33)\n",
      "step 22190, loss is 4.761224269866943\n",
      "(64, 33)\n",
      "step 22191, loss is 4.964288711547852\n",
      "(64, 33)\n",
      "step 22192, loss is 4.636263847351074\n",
      "(64, 33)\n",
      "step 22193, loss is 4.641005039215088\n",
      "(64, 33)\n",
      "step 22194, loss is 4.838677406311035\n",
      "(64, 33)\n",
      "step 22195, loss is 4.847525119781494\n",
      "(64, 33)\n",
      "step 22196, loss is 4.711073398590088\n",
      "(64, 33)\n",
      "step 22197, loss is 4.8806471824646\n",
      "(64, 33)\n",
      "step 22198, loss is 4.770622253417969\n",
      "(64, 33)\n",
      "step 22199, loss is 4.881533145904541\n",
      "(64, 33)\n",
      "step 22200, loss is 4.931343078613281\n",
      "(64, 33)\n",
      "step 22201, loss is 4.72202730178833\n",
      "(64, 33)\n",
      "step 22202, loss is 4.966639518737793\n",
      "(64, 33)\n",
      "step 22203, loss is 4.717528343200684\n",
      "(64, 33)\n",
      "step 22204, loss is 4.8518900871276855\n",
      "(64, 33)\n",
      "step 22205, loss is 4.751203536987305\n",
      "(64, 33)\n",
      "step 22206, loss is 4.731951713562012\n",
      "(64, 33)\n",
      "step 22207, loss is 4.831161975860596\n",
      "(64, 33)\n",
      "step 22208, loss is 4.874291896820068\n",
      "(64, 33)\n",
      "step 22209, loss is 4.821844100952148\n",
      "(64, 33)\n",
      "step 22210, loss is 4.805030822753906\n",
      "(64, 33)\n",
      "step 22211, loss is 4.705233097076416\n",
      "(64, 33)\n",
      "step 22212, loss is 4.714875221252441\n",
      "(64, 33)\n",
      "step 22213, loss is 5.015834331512451\n",
      "(64, 33)\n",
      "step 22214, loss is 4.807957649230957\n",
      "(64, 33)\n",
      "step 22215, loss is 4.873115539550781\n",
      "(64, 33)\n",
      "step 22216, loss is 4.747940540313721\n",
      "(64, 33)\n",
      "step 22217, loss is 4.8928399085998535\n",
      "(64, 33)\n",
      "step 22218, loss is 4.794235706329346\n",
      "(64, 33)\n",
      "step 22219, loss is 4.918033599853516\n",
      "(64, 33)\n",
      "step 22220, loss is 4.987462520599365\n",
      "(64, 33)\n",
      "step 22221, loss is 4.9229936599731445\n",
      "(64, 33)\n",
      "step 22222, loss is 4.737212181091309\n",
      "(64, 33)\n",
      "step 22223, loss is 5.027439117431641\n",
      "(64, 33)\n",
      "step 22224, loss is 4.628756523132324\n",
      "(64, 33)\n",
      "step 22225, loss is 4.700570583343506\n",
      "(64, 33)\n",
      "step 22226, loss is 4.601789474487305\n",
      "(64, 33)\n",
      "step 22227, loss is 4.979706287384033\n",
      "(64, 33)\n",
      "step 22228, loss is 4.865842819213867\n",
      "(64, 33)\n",
      "step 22229, loss is 4.741631031036377\n",
      "(64, 33)\n",
      "step 22230, loss is 4.871104717254639\n",
      "(64, 33)\n",
      "step 22231, loss is 4.856760025024414\n",
      "(64, 33)\n",
      "step 22232, loss is 4.949953079223633\n",
      "(64, 33)\n",
      "step 22233, loss is 4.659472465515137\n",
      "(64, 33)\n",
      "step 22234, loss is 4.9543280601501465\n",
      "(64, 33)\n",
      "step 22235, loss is 4.867027282714844\n",
      "(64, 33)\n",
      "step 22236, loss is 4.975288391113281\n",
      "(64, 33)\n",
      "step 22237, loss is 4.792389869689941\n",
      "(64, 33)\n",
      "step 22238, loss is 4.648966312408447\n",
      "(64, 33)\n",
      "step 22239, loss is 4.936485290527344\n",
      "(64, 33)\n",
      "step 22240, loss is 4.850400924682617\n",
      "(64, 33)\n",
      "step 22241, loss is 4.74362325668335\n",
      "(64, 33)\n",
      "step 22242, loss is 4.788717269897461\n",
      "(64, 33)\n",
      "step 22243, loss is 4.8034257888793945\n",
      "(64, 33)\n",
      "step 22244, loss is 4.605749130249023\n",
      "(64, 33)\n",
      "step 22245, loss is 4.951899528503418\n",
      "(64, 33)\n",
      "step 22246, loss is 4.732342720031738\n",
      "(64, 33)\n",
      "step 22247, loss is 4.666431427001953\n",
      "(64, 33)\n",
      "step 22248, loss is 4.809412479400635\n",
      "(64, 33)\n",
      "step 22249, loss is 4.66402006149292\n",
      "(64, 33)\n",
      "step 22250, loss is 4.6848673820495605\n",
      "(64, 33)\n",
      "step 22251, loss is 4.871740341186523\n",
      "(64, 33)\n",
      "step 22252, loss is 4.704641819000244\n",
      "(64, 33)\n",
      "step 22253, loss is 4.78096866607666\n",
      "(64, 33)\n",
      "step 22254, loss is 5.020997524261475\n",
      "(64, 33)\n",
      "step 22255, loss is 4.703430652618408\n",
      "(64, 33)\n",
      "step 22256, loss is 4.795371055603027\n",
      "(64, 33)\n",
      "step 22257, loss is 4.7432403564453125\n",
      "(64, 33)\n",
      "step 22258, loss is 4.62797212600708\n",
      "(64, 33)\n",
      "step 22259, loss is 4.823849678039551\n",
      "(64, 33)\n",
      "step 22260, loss is 4.778629779815674\n",
      "(64, 33)\n",
      "step 22261, loss is 4.925117492675781\n",
      "(64, 33)\n",
      "step 22262, loss is 4.799444198608398\n",
      "(64, 33)\n",
      "step 22263, loss is 4.637150287628174\n",
      "(64, 33)\n",
      "step 22264, loss is 4.798072814941406\n",
      "(64, 33)\n",
      "step 22265, loss is 4.620473384857178\n",
      "(64, 33)\n",
      "step 22266, loss is 4.797349452972412\n",
      "(64, 33)\n",
      "step 22267, loss is 4.810312747955322\n",
      "(64, 33)\n",
      "step 22268, loss is 4.703310012817383\n",
      "(64, 33)\n",
      "step 22269, loss is 4.993597984313965\n",
      "(64, 33)\n",
      "step 22270, loss is 4.7739434242248535\n",
      "(64, 33)\n",
      "step 22271, loss is 4.7448248863220215\n",
      "(64, 33)\n",
      "step 22272, loss is 4.726190567016602\n",
      "(64, 33)\n",
      "step 22273, loss is 4.6263933181762695\n",
      "(64, 33)\n",
      "step 22274, loss is 4.638505458831787\n",
      "(64, 33)\n",
      "step 22275, loss is 4.807875156402588\n",
      "(64, 33)\n",
      "step 22276, loss is 4.929995536804199\n",
      "(64, 33)\n",
      "step 22277, loss is 4.90585470199585\n",
      "(64, 33)\n",
      "step 22278, loss is 4.67574405670166\n",
      "(64, 33)\n",
      "step 22279, loss is 4.958977222442627\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22280, loss is 5.0269083976745605\n",
      "(64, 33)\n",
      "step 22281, loss is 4.8815226554870605\n",
      "(64, 33)\n",
      "step 22282, loss is 4.833016395568848\n",
      "(64, 33)\n",
      "step 22283, loss is 4.83201789855957\n",
      "(64, 33)\n",
      "step 22284, loss is 4.756748199462891\n",
      "(64, 33)\n",
      "step 22285, loss is 4.573126316070557\n",
      "(64, 33)\n",
      "step 22286, loss is 4.787635326385498\n",
      "(64, 33)\n",
      "step 22287, loss is 4.7645978927612305\n",
      "(64, 33)\n",
      "step 22288, loss is 4.809701442718506\n",
      "(64, 33)\n",
      "step 22289, loss is 4.909769535064697\n",
      "(64, 33)\n",
      "step 22290, loss is 4.863908767700195\n",
      "(64, 33)\n",
      "step 22291, loss is 4.853649139404297\n",
      "(64, 33)\n",
      "step 22292, loss is 4.694681644439697\n",
      "(64, 33)\n",
      "step 22293, loss is 4.728638172149658\n",
      "(64, 33)\n",
      "step 22294, loss is 4.657721042633057\n",
      "(64, 33)\n",
      "step 22295, loss is 4.811060905456543\n",
      "(64, 33)\n",
      "step 22296, loss is 4.765532970428467\n",
      "(64, 33)\n",
      "step 22297, loss is 4.577587604522705\n",
      "(64, 33)\n",
      "step 22298, loss is 4.860010147094727\n",
      "(64, 33)\n",
      "step 22299, loss is 4.844992637634277\n",
      "(64, 33)\n",
      "step 22300, loss is 4.745422840118408\n",
      "(64, 33)\n",
      "step 22301, loss is 4.886379241943359\n",
      "(64, 33)\n",
      "step 22302, loss is 4.748083114624023\n",
      "(64, 33)\n",
      "step 22303, loss is 4.86441707611084\n",
      "(64, 33)\n",
      "step 22304, loss is 4.934588432312012\n",
      "(64, 33)\n",
      "step 22305, loss is 4.722590923309326\n",
      "(64, 33)\n",
      "step 22306, loss is 4.928398132324219\n",
      "(64, 33)\n",
      "step 22307, loss is 4.626697063446045\n",
      "(64, 33)\n",
      "step 22308, loss is 4.863110542297363\n",
      "(64, 33)\n",
      "step 22309, loss is 4.650655746459961\n",
      "(64, 33)\n",
      "step 22310, loss is 4.797483921051025\n",
      "(64, 33)\n",
      "step 22311, loss is 4.790956974029541\n",
      "(64, 33)\n",
      "step 22312, loss is 4.696882247924805\n",
      "(64, 33)\n",
      "step 22313, loss is 4.804849147796631\n",
      "(64, 33)\n",
      "step 22314, loss is 4.918519020080566\n",
      "(64, 33)\n",
      "step 22315, loss is 4.703105449676514\n",
      "(64, 33)\n",
      "step 22316, loss is 4.659395217895508\n",
      "(64, 33)\n",
      "step 22317, loss is 4.575448513031006\n",
      "(64, 33)\n",
      "step 22318, loss is 4.813424110412598\n",
      "(64, 33)\n",
      "step 22319, loss is 4.817122936248779\n",
      "(64, 33)\n",
      "step 22320, loss is 4.8671441078186035\n",
      "(64, 33)\n",
      "step 22321, loss is 4.733232498168945\n",
      "(64, 33)\n",
      "step 22322, loss is 4.785995006561279\n",
      "(64, 33)\n",
      "step 22323, loss is 4.737016201019287\n",
      "(64, 33)\n",
      "step 22324, loss is 4.559928894042969\n",
      "(64, 33)\n",
      "step 22325, loss is 4.686799049377441\n",
      "(64, 33)\n",
      "step 22326, loss is 4.794419765472412\n",
      "(64, 33)\n",
      "step 22327, loss is 4.925882339477539\n",
      "(64, 33)\n",
      "step 22328, loss is 4.737208366394043\n",
      "(64, 33)\n",
      "step 22329, loss is 4.716187477111816\n",
      "(64, 33)\n",
      "step 22330, loss is 4.894550800323486\n",
      "(64, 33)\n",
      "step 22331, loss is 4.678396224975586\n",
      "(64, 33)\n",
      "step 22332, loss is 4.698603630065918\n",
      "(64, 33)\n",
      "step 22333, loss is 4.738801002502441\n",
      "(64, 33)\n",
      "step 22334, loss is 4.734508037567139\n",
      "(64, 33)\n",
      "step 22335, loss is 4.736542224884033\n",
      "(64, 33)\n",
      "step 22336, loss is 4.9004130363464355\n",
      "(64, 33)\n",
      "step 22337, loss is 4.885009765625\n",
      "(64, 33)\n",
      "step 22338, loss is 4.80782413482666\n",
      "(64, 33)\n",
      "step 22339, loss is 4.859007835388184\n",
      "(64, 33)\n",
      "step 22340, loss is 4.939903259277344\n",
      "(64, 33)\n",
      "step 22341, loss is 4.83818244934082\n",
      "(64, 33)\n",
      "step 22342, loss is 4.854113578796387\n",
      "(64, 33)\n",
      "step 22343, loss is 5.015833854675293\n",
      "(64, 33)\n",
      "step 22344, loss is 4.807547092437744\n",
      "(64, 33)\n",
      "step 22345, loss is 4.802495956420898\n",
      "(64, 33)\n",
      "step 22346, loss is 4.813520908355713\n",
      "(64, 33)\n",
      "step 22347, loss is 4.774080753326416\n",
      "(64, 33)\n",
      "step 22348, loss is 4.9655232429504395\n",
      "(64, 33)\n",
      "step 22349, loss is 4.974105358123779\n",
      "(64, 33)\n",
      "step 22350, loss is 5.006446838378906\n",
      "(64, 33)\n",
      "step 22351, loss is 4.915951728820801\n",
      "(64, 33)\n",
      "step 22352, loss is 4.908371448516846\n",
      "(64, 33)\n",
      "step 22353, loss is 4.852360248565674\n",
      "(64, 33)\n",
      "step 22354, loss is 4.896385192871094\n",
      "(64, 33)\n",
      "step 22355, loss is 4.777933597564697\n",
      "(64, 33)\n",
      "step 22356, loss is 4.8896803855896\n",
      "(64, 33)\n",
      "step 22357, loss is 4.805913925170898\n",
      "(64, 33)\n",
      "step 22358, loss is 4.836282253265381\n",
      "(64, 33)\n",
      "step 22359, loss is 4.854461669921875\n",
      "(64, 33)\n",
      "step 22360, loss is 4.796273231506348\n",
      "(64, 33)\n",
      "step 22361, loss is 4.59851598739624\n",
      "(64, 33)\n",
      "step 22362, loss is 4.81113862991333\n",
      "(64, 33)\n",
      "step 22363, loss is 4.768466472625732\n",
      "(64, 33)\n",
      "step 22364, loss is 4.700089454650879\n",
      "(64, 33)\n",
      "step 22365, loss is 4.833500862121582\n",
      "(64, 33)\n",
      "step 22366, loss is 4.734896183013916\n",
      "(64, 33)\n",
      "step 22367, loss is 4.879232883453369\n",
      "(64, 33)\n",
      "step 22368, loss is 4.651823043823242\n",
      "(64, 33)\n",
      "step 22369, loss is 4.690293312072754\n",
      "(64, 33)\n",
      "step 22370, loss is 4.65157413482666\n",
      "(64, 33)\n",
      "step 22371, loss is 4.87217378616333\n",
      "(64, 33)\n",
      "step 22372, loss is 4.651151180267334\n",
      "(64, 33)\n",
      "step 22373, loss is 4.778134822845459\n",
      "(64, 33)\n",
      "step 22374, loss is 4.966968059539795\n",
      "(64, 33)\n",
      "step 22375, loss is 4.8158860206604\n",
      "(64, 33)\n",
      "step 22376, loss is 4.833342552185059\n",
      "(64, 33)\n",
      "step 22377, loss is 4.657643795013428\n",
      "(64, 33)\n",
      "step 22378, loss is 4.939467906951904\n",
      "(64, 33)\n",
      "step 22379, loss is 4.814126968383789\n",
      "(64, 33)\n",
      "step 22380, loss is 4.842774391174316\n",
      "(64, 33)\n",
      "step 22381, loss is 4.827494144439697\n",
      "(64, 33)\n",
      "step 22382, loss is 4.888103008270264\n",
      "(64, 33)\n",
      "step 22383, loss is 4.862112998962402\n",
      "(64, 33)\n",
      "step 22384, loss is 4.7714152336120605\n",
      "(64, 33)\n",
      "step 22385, loss is 4.9479875564575195\n",
      "(64, 33)\n",
      "step 22386, loss is 4.657581806182861\n",
      "(64, 33)\n",
      "step 22387, loss is 4.822056770324707\n",
      "(64, 33)\n",
      "step 22388, loss is 4.745452404022217\n",
      "(64, 33)\n",
      "step 22389, loss is 4.734708786010742\n",
      "(64, 33)\n",
      "step 22390, loss is 4.766642093658447\n",
      "(64, 33)\n",
      "step 22391, loss is 4.813990116119385\n",
      "(64, 33)\n",
      "step 22392, loss is 4.9169840812683105\n",
      "(64, 33)\n",
      "step 22393, loss is 4.882545471191406\n",
      "(64, 33)\n",
      "step 22394, loss is 4.9345293045043945\n",
      "(64, 33)\n",
      "step 22395, loss is 4.7869486808776855\n",
      "(64, 33)\n",
      "step 22396, loss is 4.804635524749756\n",
      "(64, 33)\n",
      "step 22397, loss is 4.9618659019470215\n",
      "(64, 33)\n",
      "step 22398, loss is 4.922722339630127\n",
      "(64, 33)\n",
      "step 22399, loss is 4.6094441413879395\n",
      "(64, 33)\n",
      "step 22400, loss is 4.842781066894531\n",
      "(64, 33)\n",
      "step 22401, loss is 4.729337215423584\n",
      "(64, 33)\n",
      "step 22402, loss is 4.9237141609191895\n",
      "(64, 33)\n",
      "step 22403, loss is 4.581330299377441\n",
      "(64, 33)\n",
      "step 22404, loss is 4.91708517074585\n",
      "(64, 33)\n",
      "step 22405, loss is 4.6757941246032715\n",
      "(64, 33)\n",
      "step 22406, loss is 4.91147518157959\n",
      "(64, 33)\n",
      "step 22407, loss is 4.869284629821777\n",
      "(64, 33)\n",
      "step 22408, loss is 4.649764537811279\n",
      "(64, 33)\n",
      "step 22409, loss is 4.673112392425537\n",
      "(64, 33)\n",
      "step 22410, loss is 4.8062052726745605\n",
      "(64, 33)\n",
      "step 22411, loss is 4.863458633422852\n",
      "(64, 33)\n",
      "step 22412, loss is 4.741997241973877\n",
      "(64, 33)\n",
      "step 22413, loss is 4.812657833099365\n",
      "(64, 33)\n",
      "step 22414, loss is 4.739030838012695\n",
      "(64, 33)\n",
      "step 22415, loss is 4.535857677459717\n",
      "(64, 33)\n",
      "step 22416, loss is 4.869026184082031\n",
      "(64, 33)\n",
      "step 22417, loss is 4.900216102600098\n",
      "(64, 33)\n",
      "step 22418, loss is 4.649509906768799\n",
      "(64, 33)\n",
      "step 22419, loss is 4.779712200164795\n",
      "(64, 33)\n",
      "step 22420, loss is 4.806704998016357\n",
      "(64, 33)\n",
      "step 22421, loss is 4.816294193267822\n",
      "(64, 33)\n",
      "step 22422, loss is 4.7263617515563965\n",
      "(64, 33)\n",
      "step 22423, loss is 4.965399742126465\n",
      "(64, 33)\n",
      "step 22424, loss is 4.6313652992248535\n",
      "(64, 33)\n",
      "step 22425, loss is 4.871331691741943\n",
      "(64, 33)\n",
      "step 22426, loss is 4.750739097595215\n",
      "(64, 33)\n",
      "step 22427, loss is 4.86409854888916\n",
      "(64, 33)\n",
      "step 22428, loss is 4.776595115661621\n",
      "(64, 33)\n",
      "step 22429, loss is 4.839469909667969\n",
      "(64, 33)\n",
      "step 22430, loss is 4.721621990203857\n",
      "(64, 33)\n",
      "step 22431, loss is 4.7579522132873535\n",
      "(64, 33)\n",
      "step 22432, loss is 4.950775623321533\n",
      "(64, 33)\n",
      "step 22433, loss is 4.948119163513184\n",
      "(64, 33)\n",
      "step 22434, loss is 4.737191677093506\n",
      "(64, 33)\n",
      "step 22435, loss is 4.943777561187744\n",
      "(64, 33)\n",
      "step 22436, loss is 4.690987586975098\n",
      "(64, 33)\n",
      "step 22437, loss is 4.777422904968262\n",
      "(64, 33)\n",
      "step 22438, loss is 4.8684539794921875\n",
      "(64, 33)\n",
      "step 22439, loss is 4.8055243492126465\n",
      "(64, 33)\n",
      "step 22440, loss is 4.881924629211426\n",
      "(64, 33)\n",
      "step 22441, loss is 4.8647332191467285\n",
      "(64, 33)\n",
      "step 22442, loss is 4.853325843811035\n",
      "(64, 33)\n",
      "step 22443, loss is 4.791610240936279\n",
      "(64, 33)\n",
      "step 22444, loss is 4.8515496253967285\n",
      "(64, 33)\n",
      "step 22445, loss is 4.653693675994873\n",
      "(64, 33)\n",
      "step 22446, loss is 4.922830581665039\n",
      "(64, 33)\n",
      "step 22447, loss is 4.8863525390625\n",
      "(64, 33)\n",
      "step 22448, loss is 4.815891265869141\n",
      "(64, 33)\n",
      "step 22449, loss is 4.718146800994873\n",
      "(64, 33)\n",
      "step 22450, loss is 4.762709140777588\n",
      "(64, 33)\n",
      "step 22451, loss is 4.843108177185059\n",
      "(64, 33)\n",
      "step 22452, loss is 4.707242488861084\n",
      "(64, 33)\n",
      "step 22453, loss is 4.894458293914795\n",
      "(64, 33)\n",
      "step 22454, loss is 4.733998775482178\n",
      "(64, 33)\n",
      "step 22455, loss is 4.920920372009277\n",
      "(64, 33)\n",
      "step 22456, loss is 4.880341053009033\n",
      "(64, 33)\n",
      "step 22457, loss is 4.691853046417236\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22458, loss is 4.8075852394104\n",
      "(64, 33)\n",
      "step 22459, loss is 4.779001235961914\n",
      "(64, 33)\n",
      "step 22460, loss is 4.794549465179443\n",
      "(64, 33)\n",
      "step 22461, loss is 4.544233798980713\n",
      "(64, 33)\n",
      "step 22462, loss is 4.6834797859191895\n",
      "(64, 33)\n",
      "step 22463, loss is 4.8726582527160645\n",
      "(64, 33)\n",
      "step 22464, loss is 4.789829730987549\n",
      "(64, 33)\n",
      "step 22465, loss is 4.705171585083008\n",
      "(64, 33)\n",
      "step 22466, loss is 4.6877312660217285\n",
      "(64, 33)\n",
      "step 22467, loss is 4.701489448547363\n",
      "(64, 33)\n",
      "step 22468, loss is 4.7471442222595215\n",
      "(64, 33)\n",
      "step 22469, loss is 4.8898186683654785\n",
      "(64, 33)\n",
      "step 22470, loss is 4.9321160316467285\n",
      "(64, 33)\n",
      "step 22471, loss is 4.760344982147217\n",
      "(64, 33)\n",
      "step 22472, loss is 4.787389755249023\n",
      "(64, 33)\n",
      "step 22473, loss is 4.613773822784424\n",
      "(64, 33)\n",
      "step 22474, loss is 4.72672700881958\n",
      "(64, 33)\n",
      "step 22475, loss is 4.7918806076049805\n",
      "(64, 33)\n",
      "step 22476, loss is 4.8183465003967285\n",
      "(64, 33)\n",
      "step 22477, loss is 4.728277206420898\n",
      "(64, 33)\n",
      "step 22478, loss is 4.903088092803955\n",
      "(64, 33)\n",
      "step 22479, loss is 4.86311149597168\n",
      "(64, 33)\n",
      "step 22480, loss is 4.818362236022949\n",
      "(64, 33)\n",
      "step 22481, loss is 4.864627838134766\n",
      "(64, 33)\n",
      "step 22482, loss is 4.599272727966309\n",
      "(64, 33)\n",
      "step 22483, loss is 4.738243579864502\n",
      "(64, 33)\n",
      "step 22484, loss is 4.617798805236816\n",
      "(64, 33)\n",
      "step 22485, loss is 4.903637409210205\n",
      "(64, 33)\n",
      "step 22486, loss is 4.894697189331055\n",
      "(64, 33)\n",
      "step 22487, loss is 4.841133117675781\n",
      "(64, 33)\n",
      "step 22488, loss is 4.742432594299316\n",
      "(64, 33)\n",
      "step 22489, loss is 4.803126335144043\n",
      "(64, 33)\n",
      "step 22490, loss is 4.808647632598877\n",
      "(64, 33)\n",
      "step 22491, loss is 4.803304672241211\n",
      "(64, 33)\n",
      "step 22492, loss is 4.758735179901123\n",
      "(64, 33)\n",
      "step 22493, loss is 4.656333923339844\n",
      "(64, 33)\n",
      "step 22494, loss is 4.769464492797852\n",
      "(64, 33)\n",
      "step 22495, loss is 4.730615615844727\n",
      "(64, 33)\n",
      "step 22496, loss is 4.787572860717773\n",
      "(64, 33)\n",
      "step 22497, loss is 4.747981071472168\n",
      "(64, 33)\n",
      "step 22498, loss is 4.626481056213379\n",
      "(64, 33)\n",
      "step 22499, loss is 4.750078201293945\n",
      "(64, 33)\n",
      "step 22500, loss is 4.6983418464660645\n",
      "(64, 33)\n",
      "step 22501, loss is 4.8897480964660645\n",
      "(64, 33)\n",
      "step 22502, loss is 4.817846775054932\n",
      "(64, 33)\n",
      "step 22503, loss is 4.822824478149414\n",
      "(64, 33)\n",
      "step 22504, loss is 4.709444046020508\n",
      "(64, 33)\n",
      "step 22505, loss is 4.710581302642822\n",
      "(64, 33)\n",
      "step 22506, loss is 4.654062747955322\n",
      "(64, 33)\n",
      "step 22507, loss is 4.74894905090332\n",
      "(64, 33)\n",
      "step 22508, loss is 4.823036193847656\n",
      "(64, 33)\n",
      "step 22509, loss is 4.660833835601807\n",
      "(64, 33)\n",
      "step 22510, loss is 4.761777400970459\n",
      "(64, 33)\n",
      "step 22511, loss is 4.744678020477295\n",
      "(64, 33)\n",
      "step 22512, loss is 4.7575788497924805\n",
      "(64, 33)\n",
      "step 22513, loss is 4.6651835441589355\n",
      "(64, 33)\n",
      "step 22514, loss is 4.627530097961426\n",
      "(64, 33)\n",
      "step 22515, loss is 4.998891830444336\n",
      "(64, 33)\n",
      "step 22516, loss is 4.770298957824707\n",
      "(64, 33)\n",
      "step 22517, loss is 4.672264099121094\n",
      "(64, 33)\n",
      "step 22518, loss is 4.651705741882324\n",
      "(64, 33)\n",
      "step 22519, loss is 4.68852424621582\n",
      "(64, 33)\n",
      "step 22520, loss is 4.714703559875488\n",
      "(64, 33)\n",
      "step 22521, loss is 4.739759922027588\n",
      "(64, 33)\n",
      "step 22522, loss is 4.7924323081970215\n",
      "(64, 33)\n",
      "step 22523, loss is 4.811370849609375\n",
      "(64, 33)\n",
      "step 22524, loss is 4.8230156898498535\n",
      "(64, 33)\n",
      "step 22525, loss is 4.714585304260254\n",
      "(64, 33)\n",
      "step 22526, loss is 4.691723346710205\n",
      "(64, 33)\n",
      "step 22527, loss is 4.920697212219238\n",
      "(64, 33)\n",
      "step 22528, loss is 4.75293493270874\n",
      "(64, 33)\n",
      "step 22529, loss is 4.849637985229492\n",
      "(64, 33)\n",
      "step 22530, loss is 4.743931293487549\n",
      "(64, 33)\n",
      "step 22531, loss is 4.771627902984619\n",
      "(64, 33)\n",
      "step 22532, loss is 4.656525611877441\n",
      "(64, 33)\n",
      "step 22533, loss is 4.72572660446167\n",
      "(64, 33)\n",
      "step 22534, loss is 4.7859787940979\n",
      "(64, 33)\n",
      "step 22535, loss is 4.751547336578369\n",
      "(64, 33)\n",
      "step 22536, loss is 4.882444858551025\n",
      "(64, 33)\n",
      "step 22537, loss is 4.674933433532715\n",
      "(64, 33)\n",
      "step 22538, loss is 4.669670104980469\n",
      "(64, 33)\n",
      "step 22539, loss is 4.869682788848877\n",
      "(64, 33)\n",
      "step 22540, loss is 4.815673828125\n",
      "(64, 33)\n",
      "step 22541, loss is 4.6523756980896\n",
      "(64, 33)\n",
      "step 22542, loss is 4.737421989440918\n",
      "(64, 33)\n",
      "step 22543, loss is 4.691503047943115\n",
      "(64, 33)\n",
      "step 22544, loss is 4.752906799316406\n",
      "(64, 33)\n",
      "step 22545, loss is 4.800825119018555\n",
      "(64, 33)\n",
      "step 22546, loss is 4.661520957946777\n",
      "(64, 33)\n",
      "step 22547, loss is 4.728359222412109\n",
      "(64, 33)\n",
      "step 22548, loss is 4.738390922546387\n",
      "(64, 33)\n",
      "step 22549, loss is 4.761445999145508\n",
      "(64, 33)\n",
      "step 22550, loss is 4.981196403503418\n",
      "(64, 33)\n",
      "step 22551, loss is 4.747564792633057\n",
      "(64, 33)\n",
      "step 22552, loss is 4.8646769523620605\n",
      "(64, 33)\n",
      "step 22553, loss is 4.712614059448242\n",
      "(64, 33)\n",
      "step 22554, loss is 4.969001770019531\n",
      "(64, 33)\n",
      "step 22555, loss is 4.838597297668457\n",
      "(64, 33)\n",
      "step 22556, loss is 4.627209663391113\n",
      "(64, 33)\n",
      "step 22557, loss is 4.839558124542236\n",
      "(64, 33)\n",
      "step 22558, loss is 4.775860786437988\n",
      "(64, 33)\n",
      "step 22559, loss is 4.8812360763549805\n",
      "(64, 33)\n",
      "step 22560, loss is 4.79251766204834\n",
      "(64, 33)\n",
      "step 22561, loss is 4.505797386169434\n",
      "(64, 33)\n",
      "step 22562, loss is 4.696444511413574\n",
      "(64, 33)\n",
      "step 22563, loss is 4.718237400054932\n",
      "(64, 33)\n",
      "step 22564, loss is 4.761224269866943\n",
      "(64, 33)\n",
      "step 22565, loss is 4.626228332519531\n",
      "(64, 33)\n",
      "step 22566, loss is 4.821035385131836\n",
      "(64, 33)\n",
      "step 22567, loss is 4.52511739730835\n",
      "(64, 33)\n",
      "step 22568, loss is 4.7971906661987305\n",
      "(64, 33)\n",
      "step 22569, loss is 4.761196136474609\n",
      "(64, 33)\n",
      "step 22570, loss is 4.722553253173828\n",
      "(64, 33)\n",
      "step 22571, loss is 4.732614040374756\n",
      "(64, 33)\n",
      "step 22572, loss is 4.7331061363220215\n",
      "(64, 33)\n",
      "step 22573, loss is 4.896432876586914\n",
      "(64, 33)\n",
      "step 22574, loss is 4.766739368438721\n",
      "(64, 33)\n",
      "step 22575, loss is 4.630088806152344\n",
      "(64, 33)\n",
      "step 22576, loss is 4.735477447509766\n",
      "(64, 33)\n",
      "step 22577, loss is 4.905571460723877\n",
      "(64, 33)\n",
      "step 22578, loss is 4.770139694213867\n",
      "(64, 33)\n",
      "step 22579, loss is 4.825389385223389\n",
      "(64, 33)\n",
      "step 22580, loss is 4.755741119384766\n",
      "(64, 33)\n",
      "step 22581, loss is 4.797535419464111\n",
      "(64, 33)\n",
      "step 22582, loss is 4.7850799560546875\n",
      "(64, 33)\n",
      "step 22583, loss is 4.7289605140686035\n",
      "(64, 33)\n",
      "step 22584, loss is 4.838737487792969\n",
      "(64, 33)\n",
      "step 22585, loss is 4.858190536499023\n",
      "(64, 33)\n",
      "step 22586, loss is 4.990412712097168\n",
      "(64, 33)\n",
      "step 22587, loss is 4.823627471923828\n",
      "(64, 33)\n",
      "step 22588, loss is 4.68652868270874\n",
      "(64, 33)\n",
      "step 22589, loss is 4.781787395477295\n",
      "(64, 33)\n",
      "step 22590, loss is 4.786166667938232\n",
      "(64, 33)\n",
      "step 22591, loss is 4.740235805511475\n",
      "(64, 33)\n",
      "step 22592, loss is 4.840824604034424\n",
      "(64, 33)\n",
      "step 22593, loss is 4.695034503936768\n",
      "(64, 33)\n",
      "step 22594, loss is 4.673167705535889\n",
      "(64, 33)\n",
      "step 22595, loss is 4.823837757110596\n",
      "(64, 33)\n",
      "step 22596, loss is 4.911686420440674\n",
      "(64, 33)\n",
      "step 22597, loss is 4.809872150421143\n",
      "(64, 33)\n",
      "step 22598, loss is 4.876533031463623\n",
      "(64, 33)\n",
      "step 22599, loss is 4.80305814743042\n",
      "(64, 33)\n",
      "step 22600, loss is 4.678785800933838\n",
      "(64, 33)\n",
      "step 22601, loss is 4.849075794219971\n",
      "(64, 33)\n",
      "step 22602, loss is 4.733465194702148\n",
      "(64, 33)\n",
      "step 22603, loss is 4.9425225257873535\n",
      "(64, 33)\n",
      "step 22604, loss is 4.6648149490356445\n",
      "(64, 33)\n",
      "step 22605, loss is 4.82636022567749\n",
      "(64, 33)\n",
      "step 22606, loss is 4.7820048332214355\n",
      "(64, 33)\n",
      "step 22607, loss is 4.96329927444458\n",
      "(64, 33)\n",
      "step 22608, loss is 4.844791889190674\n",
      "(64, 33)\n",
      "step 22609, loss is 4.609696388244629\n",
      "(64, 33)\n",
      "step 22610, loss is 4.841930389404297\n",
      "(64, 33)\n",
      "step 22611, loss is 4.740565776824951\n",
      "(64, 33)\n",
      "step 22612, loss is 4.891486167907715\n",
      "(64, 33)\n",
      "step 22613, loss is 4.726616382598877\n",
      "(64, 33)\n",
      "step 22614, loss is 4.903933048248291\n",
      "(64, 33)\n",
      "step 22615, loss is 4.827555179595947\n",
      "(64, 33)\n",
      "step 22616, loss is 4.893927574157715\n",
      "(64, 33)\n",
      "step 22617, loss is 4.801399230957031\n",
      "(64, 33)\n",
      "step 22618, loss is 4.763317584991455\n",
      "(64, 33)\n",
      "step 22619, loss is 4.8068952560424805\n",
      "(64, 33)\n",
      "step 22620, loss is 4.871186256408691\n",
      "(64, 33)\n",
      "step 22621, loss is 4.912680149078369\n",
      "(64, 33)\n",
      "step 22622, loss is 4.753446578979492\n",
      "(64, 33)\n",
      "step 22623, loss is 4.647872447967529\n",
      "(64, 33)\n",
      "step 22624, loss is 4.813548564910889\n",
      "(64, 33)\n",
      "step 22625, loss is 4.832653522491455\n",
      "(64, 33)\n",
      "step 22626, loss is 4.890982151031494\n",
      "(64, 33)\n",
      "step 22627, loss is 4.710231781005859\n",
      "(64, 33)\n",
      "step 22628, loss is 4.6533966064453125\n",
      "(64, 33)\n",
      "step 22629, loss is 4.8754777908325195\n",
      "(64, 33)\n",
      "step 22630, loss is 4.7174577713012695\n",
      "(64, 33)\n",
      "step 22631, loss is 4.921819686889648\n",
      "(64, 33)\n",
      "step 22632, loss is 4.837071895599365\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22633, loss is 4.787455081939697\n",
      "(64, 33)\n",
      "step 22634, loss is 4.758459091186523\n",
      "(64, 33)\n",
      "step 22635, loss is 4.881522178649902\n",
      "(64, 33)\n",
      "step 22636, loss is 4.61270809173584\n",
      "(64, 33)\n",
      "step 22637, loss is 4.662789344787598\n",
      "(64, 33)\n",
      "step 22638, loss is 4.868418216705322\n",
      "(64, 33)\n",
      "step 22639, loss is 4.643755912780762\n",
      "(64, 33)\n",
      "step 22640, loss is 4.837657928466797\n",
      "(64, 33)\n",
      "step 22641, loss is 4.942683696746826\n",
      "(64, 33)\n",
      "step 22642, loss is 4.728063583374023\n",
      "(64, 33)\n",
      "step 22643, loss is 4.705972194671631\n",
      "(64, 33)\n",
      "step 22644, loss is 4.91039514541626\n",
      "(64, 33)\n",
      "step 22645, loss is 4.809055328369141\n",
      "(64, 33)\n",
      "step 22646, loss is 4.971364974975586\n",
      "(64, 33)\n",
      "step 22647, loss is 4.704050540924072\n",
      "(64, 33)\n",
      "step 22648, loss is 4.791665554046631\n",
      "(64, 33)\n",
      "step 22649, loss is 4.765549182891846\n",
      "(64, 33)\n",
      "step 22650, loss is 4.68795108795166\n",
      "(64, 33)\n",
      "step 22651, loss is 4.843544006347656\n",
      "(64, 33)\n",
      "step 22652, loss is 4.797830104827881\n",
      "(64, 33)\n",
      "step 22653, loss is 4.844611167907715\n",
      "(64, 33)\n",
      "step 22654, loss is 4.892041206359863\n",
      "(64, 33)\n",
      "step 22655, loss is 4.675189971923828\n",
      "(64, 33)\n",
      "step 22656, loss is 4.82329797744751\n",
      "(64, 33)\n",
      "step 22657, loss is 4.778858184814453\n",
      "(64, 33)\n",
      "step 22658, loss is 4.905629634857178\n",
      "(64, 33)\n",
      "step 22659, loss is 4.85908317565918\n",
      "(64, 33)\n",
      "step 22660, loss is 4.782530307769775\n",
      "(64, 33)\n",
      "step 22661, loss is 4.810206413269043\n",
      "(64, 33)\n",
      "step 22662, loss is 4.8402533531188965\n",
      "(64, 33)\n",
      "step 22663, loss is 4.839920520782471\n",
      "(64, 33)\n",
      "step 22664, loss is 4.722417831420898\n",
      "(64, 33)\n",
      "step 22665, loss is 4.851098537445068\n",
      "(64, 33)\n",
      "step 22666, loss is 4.82247257232666\n",
      "(64, 33)\n",
      "step 22667, loss is 4.812224388122559\n",
      "(64, 33)\n",
      "step 22668, loss is 4.881461143493652\n",
      "(64, 33)\n",
      "step 22669, loss is 4.854778289794922\n",
      "(64, 33)\n",
      "step 22670, loss is 4.667695045471191\n",
      "(64, 33)\n",
      "step 22671, loss is 4.986423969268799\n",
      "(64, 33)\n",
      "step 22672, loss is 4.987936019897461\n",
      "(64, 33)\n",
      "step 22673, loss is 4.64148473739624\n",
      "(64, 33)\n",
      "step 22674, loss is 4.78255558013916\n",
      "(64, 33)\n",
      "step 22675, loss is 4.920895576477051\n",
      "(64, 33)\n",
      "step 22676, loss is 4.674488544464111\n",
      "(64, 33)\n",
      "step 22677, loss is 4.903888702392578\n",
      "(64, 33)\n",
      "step 22678, loss is 4.9589619636535645\n",
      "(64, 33)\n",
      "step 22679, loss is 4.827870845794678\n",
      "(64, 33)\n",
      "step 22680, loss is 4.906582832336426\n",
      "(64, 33)\n",
      "step 22681, loss is 4.7378764152526855\n",
      "(64, 33)\n",
      "step 22682, loss is 4.720894813537598\n",
      "(64, 33)\n",
      "step 22683, loss is 4.594501972198486\n",
      "(64, 33)\n",
      "step 22684, loss is 4.631239414215088\n",
      "(64, 33)\n",
      "step 22685, loss is 4.927767276763916\n",
      "(64, 33)\n",
      "step 22686, loss is 4.798804759979248\n",
      "(64, 33)\n",
      "step 22687, loss is 4.75197172164917\n",
      "(64, 33)\n",
      "step 22688, loss is 4.775798320770264\n",
      "(64, 33)\n",
      "step 22689, loss is 4.9411139488220215\n",
      "(64, 33)\n",
      "step 22690, loss is 4.938694477081299\n",
      "(64, 33)\n",
      "step 22691, loss is 4.87700891494751\n",
      "(64, 33)\n",
      "step 22692, loss is 4.786874294281006\n",
      "(64, 33)\n",
      "step 22693, loss is 4.816091537475586\n",
      "(64, 33)\n",
      "step 22694, loss is 4.786109447479248\n",
      "(64, 33)\n",
      "step 22695, loss is 4.8520827293396\n",
      "(64, 33)\n",
      "step 22696, loss is 4.783894062042236\n",
      "(64, 33)\n",
      "step 22697, loss is 4.8660454750061035\n",
      "(64, 33)\n",
      "step 22698, loss is 4.586032390594482\n",
      "(64, 33)\n",
      "step 22699, loss is 4.744693279266357\n",
      "(64, 33)\n",
      "step 22700, loss is 4.763688087463379\n",
      "(64, 33)\n",
      "step 22701, loss is 4.856278419494629\n",
      "(64, 33)\n",
      "step 22702, loss is 4.622669696807861\n",
      "(64, 33)\n",
      "step 22703, loss is 4.884665489196777\n",
      "(64, 33)\n",
      "step 22704, loss is 4.742923736572266\n",
      "(64, 33)\n",
      "step 22705, loss is 4.917564868927002\n",
      "(64, 33)\n",
      "step 22706, loss is 4.850225925445557\n",
      "(64, 33)\n",
      "step 22707, loss is 4.6527581214904785\n",
      "(64, 33)\n",
      "step 22708, loss is 4.62274169921875\n",
      "(64, 33)\n",
      "step 22709, loss is 4.765052795410156\n",
      "(64, 33)\n",
      "step 22710, loss is 4.639920234680176\n",
      "(64, 33)\n",
      "step 22711, loss is 4.824643611907959\n",
      "(64, 33)\n",
      "step 22712, loss is 4.752687454223633\n",
      "(64, 33)\n",
      "step 22713, loss is 4.740646839141846\n",
      "(64, 33)\n",
      "step 22714, loss is 4.863937854766846\n",
      "(64, 33)\n",
      "step 22715, loss is 4.89848518371582\n",
      "(64, 33)\n",
      "step 22716, loss is 4.960351467132568\n",
      "(64, 33)\n",
      "step 22717, loss is 4.827917575836182\n",
      "(64, 33)\n",
      "step 22718, loss is 5.016152381896973\n",
      "(64, 33)\n",
      "step 22719, loss is 4.797519207000732\n",
      "(64, 33)\n",
      "step 22720, loss is 4.903278827667236\n",
      "(64, 33)\n",
      "step 22721, loss is 5.036499500274658\n",
      "(64, 33)\n",
      "step 22722, loss is 4.878533363342285\n",
      "(64, 33)\n",
      "step 22723, loss is 4.706071376800537\n",
      "(64, 33)\n",
      "step 22724, loss is 4.700326919555664\n",
      "(64, 33)\n",
      "step 22725, loss is 4.593650817871094\n",
      "(64, 33)\n",
      "step 22726, loss is 4.821830749511719\n",
      "(64, 33)\n",
      "step 22727, loss is 4.7287139892578125\n",
      "(64, 33)\n",
      "step 22728, loss is 4.983669757843018\n",
      "(64, 33)\n",
      "step 22729, loss is 4.79910945892334\n",
      "(64, 33)\n",
      "step 22730, loss is 4.471964359283447\n",
      "(64, 33)\n",
      "step 22731, loss is 4.758841037750244\n",
      "(64, 33)\n",
      "step 22732, loss is 4.542845249176025\n",
      "(64, 33)\n",
      "step 22733, loss is 4.711334705352783\n",
      "(64, 33)\n",
      "step 22734, loss is 4.661245822906494\n",
      "(64, 33)\n",
      "step 22735, loss is 4.812104225158691\n",
      "(64, 33)\n",
      "step 22736, loss is 4.814797878265381\n",
      "(64, 33)\n",
      "step 22737, loss is 4.631315231323242\n",
      "(64, 33)\n",
      "step 22738, loss is 4.9348063468933105\n",
      "(64, 33)\n",
      "step 22739, loss is 4.83929443359375\n",
      "(64, 33)\n",
      "step 22740, loss is 4.687900543212891\n",
      "(64, 33)\n",
      "step 22741, loss is 4.7995524406433105\n",
      "(64, 33)\n",
      "step 22742, loss is 4.861941337585449\n",
      "(64, 33)\n",
      "step 22743, loss is 4.895092487335205\n",
      "(64, 33)\n",
      "step 22744, loss is 4.775412559509277\n",
      "(64, 33)\n",
      "step 22745, loss is 4.741020202636719\n",
      "(64, 33)\n",
      "step 22746, loss is 4.9324750900268555\n",
      "(64, 33)\n",
      "step 22747, loss is 4.779543399810791\n",
      "(64, 33)\n",
      "step 22748, loss is 4.849908351898193\n",
      "(64, 33)\n",
      "step 22749, loss is 4.846807956695557\n",
      "(64, 33)\n",
      "step 22750, loss is 4.753820419311523\n",
      "(64, 33)\n",
      "step 22751, loss is 4.917971134185791\n",
      "(64, 33)\n",
      "step 22752, loss is 5.036401271820068\n",
      "(64, 33)\n",
      "step 22753, loss is 4.721775054931641\n",
      "(64, 33)\n",
      "step 22754, loss is 4.672563552856445\n",
      "(64, 33)\n",
      "step 22755, loss is 4.806027889251709\n",
      "(64, 33)\n",
      "step 22756, loss is 4.719801425933838\n",
      "(64, 33)\n",
      "step 22757, loss is 4.67055082321167\n",
      "(64, 33)\n",
      "step 22758, loss is 4.632493495941162\n",
      "(64, 33)\n",
      "step 22759, loss is 4.560072898864746\n",
      "(64, 33)\n",
      "step 22760, loss is 4.638814926147461\n",
      "(64, 33)\n",
      "step 22761, loss is 4.763814449310303\n",
      "(64, 33)\n",
      "step 22762, loss is 4.614642143249512\n",
      "(64, 33)\n",
      "step 22763, loss is 4.702442646026611\n",
      "(64, 33)\n",
      "step 22764, loss is 4.683238506317139\n",
      "(64, 33)\n",
      "step 22765, loss is 4.8211894035339355\n",
      "(64, 33)\n",
      "step 22766, loss is 4.766862392425537\n",
      "(64, 33)\n",
      "step 22767, loss is 4.663914680480957\n",
      "(64, 33)\n",
      "step 22768, loss is 4.897836208343506\n",
      "(64, 33)\n",
      "step 22769, loss is 4.788758277893066\n",
      "(64, 33)\n",
      "step 22770, loss is 4.961182594299316\n",
      "(64, 33)\n",
      "step 22771, loss is 4.846470355987549\n",
      "(64, 33)\n",
      "step 22772, loss is 4.874427795410156\n",
      "(64, 33)\n",
      "step 22773, loss is 4.80507755279541\n",
      "(64, 33)\n",
      "step 22774, loss is 4.812446594238281\n",
      "(64, 33)\n",
      "step 22775, loss is 4.776065826416016\n",
      "(64, 33)\n",
      "step 22776, loss is 4.730735778808594\n",
      "(64, 33)\n",
      "step 22777, loss is 4.718387603759766\n",
      "(64, 33)\n",
      "step 22778, loss is 4.599815845489502\n",
      "(64, 33)\n",
      "step 22779, loss is 4.482822418212891\n",
      "(64, 33)\n",
      "step 22780, loss is 4.799093246459961\n",
      "(64, 33)\n",
      "step 22781, loss is 4.659485816955566\n",
      "(64, 33)\n",
      "step 22782, loss is 4.811689853668213\n",
      "(64, 33)\n",
      "step 22783, loss is 4.871511459350586\n",
      "(64, 33)\n",
      "step 22784, loss is 4.753722667694092\n",
      "(64, 33)\n",
      "step 22785, loss is 4.5379180908203125\n",
      "(64, 33)\n",
      "step 22786, loss is 4.704892158508301\n",
      "(64, 33)\n",
      "step 22787, loss is 4.815667152404785\n",
      "(64, 33)\n",
      "step 22788, loss is 4.653446674346924\n",
      "(64, 33)\n",
      "step 22789, loss is 4.822336196899414\n",
      "(64, 33)\n",
      "step 22790, loss is 4.653517723083496\n",
      "(64, 33)\n",
      "step 22791, loss is 4.8309326171875\n",
      "(64, 33)\n",
      "step 22792, loss is 4.927809238433838\n",
      "(64, 33)\n",
      "step 22793, loss is 4.826938629150391\n",
      "(64, 33)\n",
      "step 22794, loss is 4.838983535766602\n",
      "(64, 33)\n",
      "step 22795, loss is 4.72809362411499\n",
      "(64, 33)\n",
      "step 22796, loss is 4.641507148742676\n",
      "(64, 33)\n",
      "step 22797, loss is 4.809157848358154\n",
      "(64, 33)\n",
      "step 22798, loss is 4.620267868041992\n",
      "(64, 33)\n",
      "step 22799, loss is 4.822090148925781\n",
      "(64, 33)\n",
      "step 22800, loss is 4.637570858001709\n",
      "(64, 33)\n",
      "step 22801, loss is 4.841082572937012\n",
      "(64, 33)\n",
      "step 22802, loss is 4.684877872467041\n",
      "(64, 33)\n",
      "step 22803, loss is 4.809893608093262\n",
      "(64, 33)\n",
      "step 22804, loss is 4.887099742889404\n",
      "(64, 33)\n",
      "step 22805, loss is 4.748605251312256\n",
      "(64, 33)\n",
      "step 22806, loss is 4.728741645812988\n",
      "(64, 33)\n",
      "step 22807, loss is 4.750417709350586\n",
      "(64, 33)\n",
      "step 22808, loss is 4.847910404205322\n",
      "(64, 33)\n",
      "step 22809, loss is 4.8503098487854\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22810, loss is 4.766615390777588\n",
      "(64, 33)\n",
      "step 22811, loss is 4.660832405090332\n",
      "(64, 33)\n",
      "step 22812, loss is 4.490795135498047\n",
      "(64, 33)\n",
      "step 22813, loss is 4.998222351074219\n",
      "(64, 33)\n",
      "step 22814, loss is 4.871572971343994\n",
      "(64, 33)\n",
      "step 22815, loss is 4.629735469818115\n",
      "(64, 33)\n",
      "step 22816, loss is 4.862529277801514\n",
      "(64, 33)\n",
      "step 22817, loss is 4.653746604919434\n",
      "(64, 33)\n",
      "step 22818, loss is 4.8400421142578125\n",
      "(64, 33)\n",
      "step 22819, loss is 4.726591110229492\n",
      "(64, 33)\n",
      "step 22820, loss is 4.780804634094238\n",
      "(64, 33)\n",
      "step 22821, loss is 4.705923557281494\n",
      "(64, 33)\n",
      "step 22822, loss is 4.888603687286377\n",
      "(64, 33)\n",
      "step 22823, loss is 4.801575183868408\n",
      "(64, 33)\n",
      "step 22824, loss is 4.885725498199463\n",
      "(64, 33)\n",
      "step 22825, loss is 4.72914457321167\n",
      "(64, 33)\n",
      "step 22826, loss is 4.802659511566162\n",
      "(64, 33)\n",
      "step 22827, loss is 4.696571350097656\n",
      "(64, 33)\n",
      "step 22828, loss is 4.723219394683838\n",
      "(64, 33)\n",
      "step 22829, loss is 4.834617614746094\n",
      "(64, 33)\n",
      "step 22830, loss is 4.80775260925293\n",
      "(64, 33)\n",
      "step 22831, loss is 4.686905384063721\n",
      "(64, 33)\n",
      "step 22832, loss is 4.8345232009887695\n",
      "(64, 33)\n",
      "step 22833, loss is 4.600099086761475\n",
      "(64, 33)\n",
      "step 22834, loss is 4.549453258514404\n",
      "(64, 33)\n",
      "step 22835, loss is 4.796632766723633\n",
      "(64, 33)\n",
      "step 22836, loss is 4.783987045288086\n",
      "(64, 33)\n",
      "step 22837, loss is 4.808276176452637\n",
      "(64, 33)\n",
      "step 22838, loss is 4.597470283508301\n",
      "(64, 33)\n",
      "step 22839, loss is 4.781052112579346\n",
      "(64, 33)\n",
      "step 22840, loss is 4.70143985748291\n",
      "(64, 33)\n",
      "step 22841, loss is 4.574498176574707\n",
      "(64, 33)\n",
      "step 22842, loss is 4.660343170166016\n",
      "(64, 33)\n",
      "step 22843, loss is 4.819199562072754\n",
      "(64, 33)\n",
      "step 22844, loss is 4.78737211227417\n",
      "(64, 33)\n",
      "step 22845, loss is 4.757314205169678\n",
      "(64, 33)\n",
      "step 22846, loss is 4.664972305297852\n",
      "(64, 33)\n",
      "step 22847, loss is 4.7014923095703125\n",
      "(64, 33)\n",
      "step 22848, loss is 4.859527587890625\n",
      "(64, 33)\n",
      "step 22849, loss is 4.777549743652344\n",
      "(64, 33)\n",
      "step 22850, loss is 4.739634990692139\n",
      "(64, 33)\n",
      "step 22851, loss is 4.668388366699219\n",
      "(64, 33)\n",
      "step 22852, loss is 4.807079315185547\n",
      "(64, 33)\n",
      "step 22853, loss is 4.714927673339844\n",
      "(64, 33)\n",
      "step 22854, loss is 4.687991619110107\n",
      "(64, 33)\n",
      "step 22855, loss is 4.730366230010986\n",
      "(64, 33)\n",
      "step 22856, loss is 4.679037570953369\n",
      "(64, 33)\n",
      "step 22857, loss is 4.866631031036377\n",
      "(64, 33)\n",
      "step 22858, loss is 4.648829460144043\n",
      "(64, 33)\n",
      "step 22859, loss is 4.848516941070557\n",
      "(64, 33)\n",
      "step 22860, loss is 4.8173298835754395\n",
      "(64, 33)\n",
      "step 22861, loss is 4.800419330596924\n",
      "(64, 33)\n",
      "step 22862, loss is 4.787841796875\n",
      "(64, 33)\n",
      "step 22863, loss is 4.679620265960693\n",
      "(64, 33)\n",
      "step 22864, loss is 4.851776123046875\n",
      "(64, 33)\n",
      "step 22865, loss is 4.831260681152344\n",
      "(64, 33)\n",
      "step 22866, loss is 4.783872604370117\n",
      "(64, 33)\n",
      "step 22867, loss is 4.766489505767822\n",
      "(64, 33)\n",
      "step 22868, loss is 4.952339172363281\n",
      "(64, 33)\n",
      "step 22869, loss is 4.8253865242004395\n",
      "(64, 33)\n",
      "step 22870, loss is 4.773857593536377\n",
      "(64, 33)\n",
      "step 22871, loss is 4.7296271324157715\n",
      "(64, 33)\n",
      "step 22872, loss is 4.771409511566162\n",
      "(64, 33)\n",
      "step 22873, loss is 4.636264801025391\n",
      "(64, 33)\n",
      "step 22874, loss is 4.891368389129639\n",
      "(64, 33)\n",
      "step 22875, loss is 4.634040355682373\n",
      "(64, 33)\n",
      "step 22876, loss is 4.812372207641602\n",
      "(64, 33)\n",
      "step 22877, loss is 4.8616414070129395\n",
      "(64, 33)\n",
      "step 22878, loss is 4.932552337646484\n",
      "(64, 33)\n",
      "step 22879, loss is 4.740572452545166\n",
      "(64, 33)\n",
      "step 22880, loss is 4.740702152252197\n",
      "(64, 33)\n",
      "step 22881, loss is 4.727687835693359\n",
      "(64, 33)\n",
      "step 22882, loss is 4.9318437576293945\n",
      "(64, 33)\n",
      "step 22883, loss is 4.75404691696167\n",
      "(64, 33)\n",
      "step 22884, loss is 4.864573001861572\n",
      "(64, 33)\n",
      "step 22885, loss is 4.814926624298096\n",
      "(64, 33)\n",
      "step 22886, loss is 4.902298927307129\n",
      "(64, 33)\n",
      "step 22887, loss is 4.791404724121094\n",
      "(64, 33)\n",
      "step 22888, loss is 4.808022975921631\n",
      "(64, 33)\n",
      "step 22889, loss is 4.723115921020508\n",
      "(64, 33)\n",
      "step 22890, loss is 4.826692581176758\n",
      "(64, 33)\n",
      "step 22891, loss is 4.797970771789551\n",
      "(64, 33)\n",
      "step 22892, loss is 4.8455657958984375\n",
      "(64, 33)\n",
      "step 22893, loss is 4.864026069641113\n",
      "(64, 33)\n",
      "step 22894, loss is 4.770185470581055\n",
      "(64, 33)\n",
      "step 22895, loss is 4.657599449157715\n",
      "(64, 33)\n",
      "step 22896, loss is 4.833893775939941\n",
      "(64, 33)\n",
      "step 22897, loss is 4.647225379943848\n",
      "(64, 33)\n",
      "step 22898, loss is 4.901731967926025\n",
      "(64, 33)\n",
      "step 22899, loss is 4.79093599319458\n",
      "(64, 33)\n",
      "step 22900, loss is 4.9608235359191895\n",
      "(64, 33)\n",
      "step 22901, loss is 4.884920120239258\n",
      "(64, 33)\n",
      "step 22902, loss is 4.92024040222168\n",
      "(64, 33)\n",
      "step 22903, loss is 4.835990905761719\n",
      "(64, 33)\n",
      "step 22904, loss is 4.685329437255859\n",
      "(64, 33)\n",
      "step 22905, loss is 4.694210052490234\n",
      "(64, 33)\n",
      "step 22906, loss is 4.897369861602783\n",
      "(64, 33)\n",
      "step 22907, loss is 4.788169860839844\n",
      "(64, 33)\n",
      "step 22908, loss is 4.7426886558532715\n",
      "(64, 33)\n",
      "step 22909, loss is 4.549602031707764\n",
      "(64, 33)\n",
      "step 22910, loss is 4.881422519683838\n",
      "(64, 33)\n",
      "step 22911, loss is 4.626247406005859\n",
      "(64, 33)\n",
      "step 22912, loss is 4.501646518707275\n",
      "(64, 33)\n",
      "step 22913, loss is 4.7715163230896\n",
      "(64, 33)\n",
      "step 22914, loss is 4.770031452178955\n",
      "(64, 33)\n",
      "step 22915, loss is 4.789487361907959\n",
      "(64, 33)\n",
      "step 22916, loss is 4.8048014640808105\n",
      "(64, 33)\n",
      "step 22917, loss is 4.935484886169434\n",
      "(64, 33)\n",
      "step 22918, loss is 4.978461742401123\n",
      "(64, 33)\n",
      "step 22919, loss is 4.854048252105713\n",
      "(64, 33)\n",
      "step 22920, loss is 4.753634929656982\n",
      "(64, 33)\n",
      "step 22921, loss is 4.794381618499756\n",
      "(64, 33)\n",
      "step 22922, loss is 4.702529430389404\n",
      "(64, 33)\n",
      "step 22923, loss is 4.723846435546875\n",
      "(64, 33)\n",
      "step 22924, loss is 4.891019344329834\n",
      "(64, 33)\n",
      "step 22925, loss is 4.942198753356934\n",
      "(64, 33)\n",
      "step 22926, loss is 4.627303600311279\n",
      "(64, 33)\n",
      "step 22927, loss is 4.6349196434021\n",
      "(64, 33)\n",
      "step 22928, loss is 4.837402820587158\n",
      "(64, 33)\n",
      "step 22929, loss is 4.813119888305664\n",
      "(64, 33)\n",
      "step 22930, loss is 4.919684410095215\n",
      "(64, 33)\n",
      "step 22931, loss is 4.756064414978027\n",
      "(64, 33)\n",
      "step 22932, loss is 4.759977340698242\n",
      "(64, 33)\n",
      "step 22933, loss is 4.741319179534912\n",
      "(64, 33)\n",
      "step 22934, loss is 4.593368053436279\n",
      "(64, 33)\n",
      "step 22935, loss is 4.899470329284668\n",
      "(64, 33)\n",
      "step 22936, loss is 4.6437835693359375\n",
      "(64, 33)\n",
      "step 22937, loss is 4.827106952667236\n",
      "(64, 33)\n",
      "step 22938, loss is 4.811816692352295\n",
      "(64, 33)\n",
      "step 22939, loss is 4.823397159576416\n",
      "(64, 33)\n",
      "step 22940, loss is 4.654882907867432\n",
      "(64, 33)\n",
      "step 22941, loss is 4.776222229003906\n",
      "(64, 33)\n",
      "step 22942, loss is 4.712837219238281\n",
      "(64, 33)\n",
      "step 22943, loss is 4.6774678230285645\n",
      "(64, 33)\n",
      "step 22944, loss is 4.781304836273193\n",
      "(64, 33)\n",
      "step 22945, loss is 4.796689510345459\n",
      "(64, 33)\n",
      "step 22946, loss is 4.899630069732666\n",
      "(64, 33)\n",
      "step 22947, loss is 4.79667329788208\n",
      "(64, 33)\n",
      "step 22948, loss is 4.886753082275391\n",
      "(64, 33)\n",
      "step 22949, loss is 4.432155132293701\n",
      "(64, 33)\n",
      "step 22950, loss is 4.797845840454102\n",
      "(64, 33)\n",
      "step 22951, loss is 4.724660396575928\n",
      "(64, 33)\n",
      "step 22952, loss is 4.742994785308838\n",
      "(64, 33)\n",
      "step 22953, loss is 4.684288024902344\n",
      "(64, 33)\n",
      "step 22954, loss is 4.6919474601745605\n",
      "(64, 33)\n",
      "step 22955, loss is 4.698078632354736\n",
      "(64, 33)\n",
      "step 22956, loss is 4.823182582855225\n",
      "(64, 33)\n",
      "step 22957, loss is 4.86594295501709\n",
      "(64, 33)\n",
      "step 22958, loss is 4.774178981781006\n",
      "(64, 33)\n",
      "step 22959, loss is 4.7769975662231445\n",
      "(64, 33)\n",
      "step 22960, loss is 4.6691083908081055\n",
      "(64, 33)\n",
      "step 22961, loss is 4.765600204467773\n",
      "(64, 33)\n",
      "step 22962, loss is 5.047679424285889\n",
      "(64, 33)\n",
      "step 22963, loss is 4.710336208343506\n",
      "(64, 33)\n",
      "step 22964, loss is 4.859547138214111\n",
      "(64, 33)\n",
      "step 22965, loss is 4.875709533691406\n",
      "(64, 33)\n",
      "step 22966, loss is 4.665619850158691\n",
      "(64, 33)\n",
      "step 22967, loss is 4.90015983581543\n",
      "(64, 33)\n",
      "step 22968, loss is 4.79162073135376\n",
      "(64, 33)\n",
      "step 22969, loss is 4.924118995666504\n",
      "(64, 33)\n",
      "step 22970, loss is 4.824400424957275\n",
      "(64, 33)\n",
      "step 22971, loss is 4.765707969665527\n",
      "(64, 33)\n",
      "step 22972, loss is 4.807122707366943\n",
      "(64, 33)\n",
      "step 22973, loss is 4.741006851196289\n",
      "(64, 33)\n",
      "step 22974, loss is 4.782503604888916\n",
      "(64, 33)\n",
      "step 22975, loss is 4.707544803619385\n",
      "(64, 33)\n",
      "step 22976, loss is 4.756868839263916\n",
      "(64, 33)\n",
      "step 22977, loss is 4.79414701461792\n",
      "(64, 33)\n",
      "step 22978, loss is 4.8951416015625\n",
      "(64, 33)\n",
      "step 22979, loss is 4.8029279708862305\n",
      "(64, 33)\n",
      "step 22980, loss is 4.603372097015381\n",
      "(64, 33)\n",
      "step 22981, loss is 4.7905449867248535\n",
      "(64, 33)\n",
      "step 22982, loss is 4.835679531097412\n",
      "(64, 33)\n",
      "step 22983, loss is 4.725624084472656\n",
      "(64, 33)\n",
      "step 22984, loss is 4.710910797119141\n",
      "(64, 33)\n",
      "step 22985, loss is 4.645055770874023\n",
      "(64, 33)\n",
      "step 22986, loss is 4.726222991943359\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22987, loss is 4.713990688323975\n",
      "(64, 33)\n",
      "step 22988, loss is 4.865233421325684\n",
      "(64, 33)\n",
      "step 22989, loss is 4.6758646965026855\n",
      "(64, 33)\n",
      "step 22990, loss is 4.618448257446289\n",
      "(64, 33)\n",
      "step 22991, loss is 4.788476943969727\n",
      "(64, 33)\n",
      "step 22992, loss is 4.856367588043213\n",
      "(64, 33)\n",
      "step 22993, loss is 4.827785491943359\n",
      "(64, 33)\n",
      "step 22994, loss is 4.698632717132568\n",
      "(64, 33)\n",
      "step 22995, loss is 4.8640031814575195\n",
      "(64, 33)\n",
      "step 22996, loss is 4.800632476806641\n",
      "(64, 33)\n",
      "step 22997, loss is 4.570162773132324\n",
      "(64, 33)\n",
      "step 22998, loss is 4.697290420532227\n",
      "(64, 33)\n",
      "step 22999, loss is 4.902626037597656\n",
      "(64, 33)\n",
      "step 23000, loss is 4.932816028594971\n",
      "(64, 33)\n",
      "step 23001, loss is 4.8619384765625\n",
      "(64, 33)\n",
      "step 23002, loss is 4.699591636657715\n",
      "(64, 33)\n",
      "step 23003, loss is 4.944260597229004\n",
      "(64, 33)\n",
      "step 23004, loss is 4.763887882232666\n",
      "(64, 33)\n",
      "step 23005, loss is 4.9186296463012695\n",
      "(64, 33)\n",
      "step 23006, loss is 4.839470863342285\n",
      "(64, 33)\n",
      "step 23007, loss is 4.695523262023926\n",
      "(64, 33)\n",
      "step 23008, loss is 4.809872150421143\n",
      "(64, 33)\n",
      "step 23009, loss is 4.731897354125977\n",
      "(64, 33)\n",
      "step 23010, loss is 4.834304332733154\n",
      "(64, 33)\n",
      "step 23011, loss is 4.909054756164551\n",
      "(64, 33)\n",
      "step 23012, loss is 4.7473673820495605\n",
      "(64, 33)\n",
      "step 23013, loss is 4.767751216888428\n",
      "(64, 33)\n",
      "step 23014, loss is 4.800906658172607\n",
      "(64, 33)\n",
      "step 23015, loss is 4.820594310760498\n",
      "(64, 33)\n",
      "step 23016, loss is 4.902915954589844\n",
      "(64, 33)\n",
      "step 23017, loss is 4.8191633224487305\n",
      "(64, 33)\n",
      "step 23018, loss is 4.793689250946045\n",
      "(64, 33)\n",
      "step 23019, loss is 4.768556118011475\n",
      "(64, 33)\n",
      "step 23020, loss is 4.935433387756348\n",
      "(64, 33)\n",
      "step 23021, loss is 4.804373264312744\n",
      "(64, 33)\n",
      "step 23022, loss is 4.722911357879639\n",
      "(64, 33)\n",
      "step 23023, loss is 4.8534770011901855\n",
      "(64, 33)\n",
      "step 23024, loss is 4.8526082038879395\n",
      "(64, 33)\n",
      "step 23025, loss is 4.710058212280273\n",
      "(64, 33)\n",
      "step 23026, loss is 4.777894020080566\n",
      "(64, 33)\n",
      "step 23027, loss is 4.706115245819092\n",
      "(64, 33)\n",
      "step 23028, loss is 4.906805992126465\n",
      "(64, 33)\n",
      "step 23029, loss is 4.685768127441406\n",
      "(64, 33)\n",
      "step 23030, loss is 4.780239582061768\n",
      "(64, 33)\n",
      "step 23031, loss is 4.687739372253418\n",
      "(64, 33)\n",
      "step 23032, loss is 4.545009613037109\n",
      "(64, 33)\n",
      "step 23033, loss is 4.8584418296813965\n",
      "(64, 33)\n",
      "step 23034, loss is 4.616435527801514\n",
      "(64, 33)\n",
      "step 23035, loss is 4.773580074310303\n",
      "(64, 33)\n",
      "step 23036, loss is 4.609710216522217\n",
      "(64, 33)\n",
      "step 23037, loss is 4.987677097320557\n",
      "(64, 33)\n",
      "step 23038, loss is 4.704363822937012\n",
      "(64, 33)\n",
      "step 23039, loss is 4.8950018882751465\n",
      "(64, 33)\n",
      "step 23040, loss is 4.618409156799316\n",
      "(64, 33)\n",
      "step 23041, loss is 4.776608943939209\n",
      "(64, 33)\n",
      "step 23042, loss is 4.7533040046691895\n",
      "(64, 33)\n",
      "step 23043, loss is 4.743325233459473\n",
      "(64, 33)\n",
      "step 23044, loss is 4.704606533050537\n",
      "(64, 33)\n",
      "step 23045, loss is 4.786731243133545\n",
      "(64, 33)\n",
      "step 23046, loss is 4.864707946777344\n",
      "(64, 33)\n",
      "step 23047, loss is 4.624690532684326\n",
      "(64, 33)\n",
      "step 23048, loss is 4.922699928283691\n",
      "(64, 33)\n",
      "step 23049, loss is 4.73763370513916\n",
      "(64, 33)\n",
      "step 23050, loss is 4.712559223175049\n",
      "(64, 33)\n",
      "step 23051, loss is 4.937163829803467\n",
      "(64, 33)\n",
      "step 23052, loss is 4.813787937164307\n",
      "(64, 33)\n",
      "step 23053, loss is 4.7638678550720215\n",
      "(64, 33)\n",
      "step 23054, loss is 4.613967418670654\n",
      "(64, 33)\n",
      "step 23055, loss is 4.862180709838867\n",
      "(64, 33)\n",
      "step 23056, loss is 4.810540199279785\n",
      "(64, 33)\n",
      "step 23057, loss is 4.81625509262085\n",
      "(64, 33)\n",
      "step 23058, loss is 4.728281021118164\n",
      "(64, 33)\n",
      "step 23059, loss is 4.729459762573242\n",
      "(64, 33)\n",
      "step 23060, loss is 4.629725456237793\n",
      "(64, 33)\n",
      "step 23061, loss is 4.760307788848877\n",
      "(64, 33)\n",
      "step 23062, loss is 4.7837653160095215\n",
      "(64, 33)\n",
      "step 23063, loss is 4.936130523681641\n",
      "(64, 33)\n",
      "step 23064, loss is 4.752567291259766\n",
      "(64, 33)\n",
      "step 23065, loss is 4.827040195465088\n",
      "(64, 33)\n",
      "step 23066, loss is 4.6669230461120605\n",
      "(64, 33)\n",
      "step 23067, loss is 4.834178924560547\n",
      "(64, 33)\n",
      "step 23068, loss is 4.753612041473389\n",
      "(64, 33)\n",
      "step 23069, loss is 4.888400077819824\n",
      "(64, 33)\n",
      "step 23070, loss is 4.826982021331787\n",
      "(64, 33)\n",
      "step 23071, loss is 4.744148254394531\n",
      "(64, 33)\n",
      "step 23072, loss is 4.789677619934082\n",
      "(64, 33)\n",
      "step 23073, loss is 4.519713401794434\n",
      "(64, 33)\n",
      "step 23074, loss is 4.870231628417969\n",
      "(64, 33)\n",
      "step 23075, loss is 4.747223377227783\n",
      "(64, 33)\n",
      "step 23076, loss is 4.867255210876465\n",
      "(64, 33)\n",
      "step 23077, loss is 4.615323543548584\n",
      "(64, 33)\n",
      "step 23078, loss is 4.898967266082764\n",
      "(64, 33)\n",
      "step 23079, loss is 4.696775913238525\n",
      "(64, 33)\n",
      "step 23080, loss is 4.917897701263428\n",
      "(64, 33)\n",
      "step 23081, loss is 4.6589813232421875\n",
      "(64, 33)\n",
      "step 23082, loss is 4.832026481628418\n",
      "(64, 33)\n",
      "step 23083, loss is 4.551484107971191\n",
      "(64, 33)\n",
      "step 23084, loss is 4.730613708496094\n",
      "(64, 33)\n",
      "step 23085, loss is 4.8026652336120605\n",
      "(64, 33)\n",
      "step 23086, loss is 4.8414764404296875\n",
      "(64, 33)\n",
      "step 23087, loss is 4.780815601348877\n",
      "(64, 33)\n",
      "step 23088, loss is 4.816830158233643\n",
      "(64, 33)\n",
      "step 23089, loss is 4.801288604736328\n",
      "(64, 33)\n",
      "step 23090, loss is 4.780014514923096\n",
      "(64, 33)\n",
      "step 23091, loss is 4.795139312744141\n",
      "(64, 33)\n",
      "step 23092, loss is 4.572218894958496\n",
      "(64, 33)\n",
      "step 23093, loss is 4.7743754386901855\n",
      "(64, 33)\n",
      "step 23094, loss is 4.821287631988525\n",
      "(64, 33)\n",
      "step 23095, loss is 4.7469353675842285\n",
      "(64, 33)\n",
      "step 23096, loss is 4.842403888702393\n",
      "(64, 33)\n",
      "step 23097, loss is 4.685718536376953\n",
      "(64, 33)\n",
      "step 23098, loss is 4.62476921081543\n",
      "(64, 33)\n",
      "step 23099, loss is 4.7204365730285645\n",
      "(64, 33)\n",
      "step 23100, loss is 5.054903507232666\n",
      "(64, 33)\n",
      "step 23101, loss is 4.802555561065674\n",
      "(64, 33)\n",
      "step 23102, loss is 4.79097843170166\n",
      "(64, 33)\n",
      "step 23103, loss is 4.518129348754883\n",
      "(64, 33)\n",
      "step 23104, loss is 5.037223815917969\n",
      "(64, 33)\n",
      "step 23105, loss is 4.6981730461120605\n",
      "(64, 33)\n",
      "step 23106, loss is 4.927637100219727\n",
      "(64, 33)\n",
      "step 23107, loss is 4.77721643447876\n",
      "(64, 33)\n",
      "step 23108, loss is 4.9845499992370605\n",
      "(64, 33)\n",
      "step 23109, loss is 4.799467086791992\n",
      "(64, 33)\n",
      "step 23110, loss is 4.761955738067627\n",
      "(64, 33)\n",
      "step 23111, loss is 4.848252773284912\n",
      "(64, 33)\n",
      "step 23112, loss is 4.527154922485352\n",
      "(64, 33)\n",
      "step 23113, loss is 4.63412618637085\n",
      "(64, 33)\n",
      "step 23114, loss is 4.759302616119385\n",
      "(64, 33)\n",
      "step 23115, loss is 4.773437023162842\n",
      "(64, 33)\n",
      "step 23116, loss is 4.672330856323242\n",
      "(64, 33)\n",
      "step 23117, loss is 4.716689586639404\n",
      "(64, 33)\n",
      "step 23118, loss is 4.840137004852295\n",
      "(64, 33)\n",
      "step 23119, loss is 4.740553855895996\n",
      "(64, 33)\n",
      "step 23120, loss is 4.76187801361084\n",
      "(64, 33)\n",
      "step 23121, loss is 4.654734134674072\n",
      "(64, 33)\n",
      "step 23122, loss is 4.7820658683776855\n",
      "(64, 33)\n",
      "step 23123, loss is 4.761979103088379\n",
      "(64, 33)\n",
      "step 23124, loss is 4.703260898590088\n",
      "(64, 33)\n",
      "step 23125, loss is 4.576446533203125\n",
      "(64, 33)\n",
      "step 23126, loss is 4.86281681060791\n",
      "(64, 33)\n",
      "step 23127, loss is 4.872568130493164\n",
      "(64, 33)\n",
      "step 23128, loss is 4.771998405456543\n",
      "(64, 33)\n",
      "step 23129, loss is 4.813788414001465\n",
      "(64, 33)\n",
      "step 23130, loss is 4.755575656890869\n",
      "(64, 33)\n",
      "step 23131, loss is 4.786498069763184\n",
      "(64, 33)\n",
      "step 23132, loss is 4.780843257904053\n",
      "(64, 33)\n",
      "step 23133, loss is 4.83398962020874\n",
      "(64, 33)\n",
      "step 23134, loss is 4.872211933135986\n",
      "(64, 33)\n",
      "step 23135, loss is 4.7763261795043945\n",
      "(64, 33)\n",
      "step 23136, loss is 4.950479984283447\n",
      "(64, 33)\n",
      "step 23137, loss is 4.808076858520508\n",
      "(64, 33)\n",
      "step 23138, loss is 4.559828758239746\n",
      "(64, 33)\n",
      "step 23139, loss is 4.822399139404297\n",
      "(64, 33)\n",
      "step 23140, loss is 4.922449111938477\n",
      "(64, 33)\n",
      "step 23141, loss is 4.693215847015381\n",
      "(64, 33)\n",
      "step 23142, loss is 4.810024738311768\n",
      "(64, 33)\n",
      "step 23143, loss is 4.787124156951904\n",
      "(64, 33)\n",
      "step 23144, loss is 4.777453899383545\n",
      "(64, 33)\n",
      "step 23145, loss is 4.593674182891846\n",
      "(64, 33)\n",
      "step 23146, loss is 4.868105888366699\n",
      "(64, 33)\n",
      "step 23147, loss is 4.796629428863525\n",
      "(64, 33)\n",
      "step 23148, loss is 4.784258842468262\n",
      "(64, 33)\n",
      "step 23149, loss is 4.691906452178955\n",
      "(64, 33)\n",
      "step 23150, loss is 4.736306190490723\n",
      "(64, 33)\n",
      "step 23151, loss is 4.700948715209961\n",
      "(64, 33)\n",
      "step 23152, loss is 4.820400714874268\n",
      "(64, 33)\n",
      "step 23153, loss is 4.6175456047058105\n",
      "(64, 33)\n",
      "step 23154, loss is 4.96148681640625\n",
      "(64, 33)\n",
      "step 23155, loss is 4.7117109298706055\n",
      "(64, 33)\n",
      "step 23156, loss is 4.948676586151123\n",
      "(64, 33)\n",
      "step 23157, loss is 4.874593257904053\n",
      "(64, 33)\n",
      "step 23158, loss is 4.77463960647583\n",
      "(64, 33)\n",
      "step 23159, loss is 4.7782368659973145\n",
      "(64, 33)\n",
      "step 23160, loss is 4.878789901733398\n",
      "(64, 33)\n",
      "step 23161, loss is 4.620649814605713\n",
      "(64, 33)\n",
      "step 23162, loss is 4.86986780166626\n",
      "(64, 33)\n",
      "step 23163, loss is 4.7961931228637695\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23164, loss is 4.971856594085693\n",
      "(64, 33)\n",
      "step 23165, loss is 4.696390628814697\n",
      "(64, 33)\n",
      "step 23166, loss is 4.795614719390869\n",
      "(64, 33)\n",
      "step 23167, loss is 4.8160905838012695\n",
      "(64, 33)\n",
      "step 23168, loss is 4.936270713806152\n",
      "(64, 33)\n",
      "step 23169, loss is 4.837735176086426\n",
      "(64, 33)\n",
      "step 23170, loss is 4.721842288970947\n",
      "(64, 33)\n",
      "step 23171, loss is 4.749590873718262\n",
      "(64, 33)\n",
      "step 23172, loss is 4.816902160644531\n",
      "(64, 33)\n",
      "step 23173, loss is 4.691350936889648\n",
      "(64, 33)\n",
      "step 23174, loss is 4.804495811462402\n",
      "(64, 33)\n",
      "step 23175, loss is 4.848203659057617\n",
      "(64, 33)\n",
      "step 23176, loss is 4.763050079345703\n",
      "(64, 33)\n",
      "step 23177, loss is 4.666017055511475\n",
      "(64, 33)\n",
      "step 23178, loss is 4.934220314025879\n",
      "(64, 33)\n",
      "step 23179, loss is 5.024693965911865\n",
      "(64, 33)\n",
      "step 23180, loss is 4.637513160705566\n",
      "(64, 33)\n",
      "step 23181, loss is 4.8185296058654785\n",
      "(64, 33)\n",
      "step 23182, loss is 4.692215919494629\n",
      "(64, 33)\n",
      "step 23183, loss is 4.83076810836792\n",
      "(64, 33)\n",
      "step 23184, loss is 4.91900634765625\n",
      "(64, 33)\n",
      "step 23185, loss is 4.814911842346191\n",
      "(64, 33)\n",
      "step 23186, loss is 4.715084075927734\n",
      "(64, 33)\n",
      "step 23187, loss is 4.602514266967773\n",
      "(64, 33)\n",
      "step 23188, loss is 4.673582553863525\n",
      "(64, 33)\n",
      "step 23189, loss is 4.769407272338867\n",
      "(64, 33)\n",
      "step 23190, loss is 4.669569969177246\n",
      "(64, 33)\n",
      "step 23191, loss is 4.900521278381348\n",
      "(64, 33)\n",
      "step 23192, loss is 4.771501064300537\n",
      "(64, 33)\n",
      "step 23193, loss is 4.800858497619629\n",
      "(64, 33)\n",
      "step 23194, loss is 4.772012233734131\n",
      "(64, 33)\n",
      "step 23195, loss is 4.782190322875977\n",
      "(64, 33)\n",
      "step 23196, loss is 4.848484992980957\n",
      "(64, 33)\n",
      "step 23197, loss is 4.702247619628906\n",
      "(64, 33)\n",
      "step 23198, loss is 4.764393329620361\n",
      "(64, 33)\n",
      "step 23199, loss is 4.893560409545898\n",
      "(64, 33)\n",
      "step 23200, loss is 4.826556205749512\n",
      "(64, 33)\n",
      "step 23201, loss is 4.689629077911377\n",
      "(64, 33)\n",
      "step 23202, loss is 4.703032970428467\n",
      "(64, 33)\n",
      "step 23203, loss is 4.758846282958984\n",
      "(64, 33)\n",
      "step 23204, loss is 4.755386829376221\n",
      "(64, 33)\n",
      "step 23205, loss is 4.670533657073975\n",
      "(64, 33)\n",
      "step 23206, loss is 4.79490852355957\n",
      "(64, 33)\n",
      "step 23207, loss is 4.844501972198486\n",
      "(64, 33)\n",
      "step 23208, loss is 4.770676612854004\n",
      "(64, 33)\n",
      "step 23209, loss is 4.877138137817383\n",
      "(64, 33)\n",
      "step 23210, loss is 4.69471549987793\n",
      "(64, 33)\n",
      "step 23211, loss is 4.472390651702881\n",
      "(64, 33)\n",
      "step 23212, loss is 4.811172962188721\n",
      "(64, 33)\n",
      "step 23213, loss is 4.833926200866699\n",
      "(64, 33)\n",
      "step 23214, loss is 4.763786315917969\n",
      "(64, 33)\n",
      "step 23215, loss is 4.71121883392334\n",
      "(64, 33)\n",
      "step 23216, loss is 4.771206378936768\n",
      "(64, 33)\n",
      "step 23217, loss is 4.681392192840576\n",
      "(64, 33)\n",
      "step 23218, loss is 4.68594217300415\n",
      "(64, 33)\n",
      "step 23219, loss is 4.906055927276611\n",
      "(64, 33)\n",
      "step 23220, loss is 4.776796340942383\n",
      "(64, 33)\n",
      "step 23221, loss is 4.784251689910889\n",
      "(64, 33)\n",
      "step 23222, loss is 4.6970977783203125\n",
      "(64, 33)\n",
      "step 23223, loss is 4.799273490905762\n",
      "(64, 33)\n",
      "step 23224, loss is 4.857231616973877\n",
      "(64, 33)\n",
      "step 23225, loss is 4.7949113845825195\n",
      "(64, 33)\n",
      "step 23226, loss is 4.596595764160156\n",
      "(64, 33)\n",
      "step 23227, loss is 4.805980205535889\n",
      "(64, 33)\n",
      "step 23228, loss is 4.87228536605835\n",
      "(64, 33)\n",
      "step 23229, loss is 4.919568061828613\n",
      "(64, 33)\n",
      "step 23230, loss is 4.816919803619385\n",
      "(64, 33)\n",
      "step 23231, loss is 4.813869476318359\n",
      "(64, 33)\n",
      "step 23232, loss is 4.701957702636719\n",
      "(64, 33)\n",
      "step 23233, loss is 4.8644633293151855\n",
      "(64, 33)\n",
      "step 23234, loss is 4.79719352722168\n",
      "(64, 33)\n",
      "step 23235, loss is 4.9024271965026855\n",
      "(64, 33)\n",
      "step 23236, loss is 4.777038097381592\n",
      "(64, 33)\n",
      "step 23237, loss is 4.716169834136963\n",
      "(64, 33)\n",
      "step 23238, loss is 4.711182117462158\n",
      "(64, 33)\n",
      "step 23239, loss is 4.781514644622803\n",
      "(64, 33)\n",
      "step 23240, loss is 4.799580097198486\n",
      "(64, 33)\n",
      "step 23241, loss is 4.731550216674805\n",
      "(64, 33)\n",
      "step 23242, loss is 4.618536949157715\n",
      "(64, 33)\n",
      "step 23243, loss is 4.652952194213867\n",
      "(64, 33)\n",
      "step 23244, loss is 4.697343826293945\n",
      "(64, 33)\n",
      "step 23245, loss is 4.761666297912598\n",
      "(64, 33)\n",
      "step 23246, loss is 4.592358112335205\n",
      "(64, 33)\n",
      "step 23247, loss is 4.855334758758545\n",
      "(64, 33)\n",
      "step 23248, loss is 4.845801830291748\n",
      "(64, 33)\n",
      "step 23249, loss is 4.999382972717285\n",
      "(64, 33)\n",
      "step 23250, loss is 4.7888922691345215\n",
      "(64, 33)\n",
      "step 23251, loss is 4.784148693084717\n",
      "(64, 33)\n",
      "step 23252, loss is 4.824065208435059\n",
      "(64, 33)\n",
      "step 23253, loss is 4.874266624450684\n",
      "(64, 33)\n",
      "step 23254, loss is 4.985077857971191\n",
      "(64, 33)\n",
      "step 23255, loss is 4.6756672859191895\n",
      "(64, 33)\n",
      "step 23256, loss is 4.777756214141846\n",
      "(64, 33)\n",
      "step 23257, loss is 4.777513027191162\n",
      "(64, 33)\n",
      "step 23258, loss is 4.856588840484619\n",
      "(64, 33)\n",
      "step 23259, loss is 4.757870674133301\n",
      "(64, 33)\n",
      "step 23260, loss is 4.713081359863281\n",
      "(64, 33)\n",
      "step 23261, loss is 4.730678558349609\n",
      "(64, 33)\n",
      "step 23262, loss is 5.046690464019775\n",
      "(64, 33)\n",
      "step 23263, loss is 4.708567142486572\n",
      "(64, 33)\n",
      "step 23264, loss is 4.797358512878418\n",
      "(64, 33)\n",
      "step 23265, loss is 4.784706115722656\n",
      "(64, 33)\n",
      "step 23266, loss is 4.915384292602539\n",
      "(64, 33)\n",
      "step 23267, loss is 4.7485246658325195\n",
      "(64, 33)\n",
      "step 23268, loss is 4.6021342277526855\n",
      "(64, 33)\n",
      "step 23269, loss is 4.8621826171875\n",
      "(64, 33)\n",
      "step 23270, loss is 4.745952129364014\n",
      "(64, 33)\n",
      "step 23271, loss is 4.783827304840088\n",
      "(64, 33)\n",
      "step 23272, loss is 5.080031394958496\n",
      "(64, 33)\n",
      "step 23273, loss is 4.683527946472168\n",
      "(64, 33)\n",
      "step 23274, loss is 4.7072434425354\n",
      "(64, 33)\n",
      "step 23275, loss is 4.7676286697387695\n",
      "(64, 33)\n",
      "step 23276, loss is 4.7733988761901855\n",
      "(64, 33)\n",
      "step 23277, loss is 4.961269378662109\n",
      "(64, 33)\n",
      "step 23278, loss is 4.728217124938965\n",
      "(64, 33)\n",
      "step 23279, loss is 4.737159729003906\n",
      "(64, 33)\n",
      "step 23280, loss is 4.697261810302734\n",
      "(64, 33)\n",
      "step 23281, loss is 4.694511413574219\n",
      "(64, 33)\n",
      "step 23282, loss is 4.565924167633057\n",
      "(64, 33)\n",
      "step 23283, loss is 4.874202728271484\n",
      "(64, 33)\n",
      "step 23284, loss is 4.697308540344238\n",
      "(64, 33)\n",
      "step 23285, loss is 4.79908561706543\n",
      "(64, 33)\n",
      "step 23286, loss is 4.818785190582275\n",
      "(64, 33)\n",
      "step 23287, loss is 4.7768120765686035\n",
      "(64, 33)\n",
      "step 23288, loss is 4.660429954528809\n",
      "(64, 33)\n",
      "step 23289, loss is 4.789412975311279\n",
      "(64, 33)\n",
      "step 23290, loss is 4.732220649719238\n",
      "(64, 33)\n",
      "step 23291, loss is 4.813767910003662\n",
      "(64, 33)\n",
      "step 23292, loss is 4.857950687408447\n",
      "(64, 33)\n",
      "step 23293, loss is 4.987787246704102\n",
      "(64, 33)\n",
      "step 23294, loss is 4.863977432250977\n",
      "(64, 33)\n",
      "step 23295, loss is 4.867808818817139\n",
      "(64, 33)\n",
      "step 23296, loss is 4.780377388000488\n",
      "(64, 33)\n",
      "step 23297, loss is 4.738777160644531\n",
      "(64, 33)\n",
      "step 23298, loss is 4.577271938323975\n",
      "(64, 33)\n",
      "step 23299, loss is 4.811574459075928\n",
      "(64, 33)\n",
      "step 23300, loss is 4.793267250061035\n",
      "(64, 33)\n",
      "step 23301, loss is 4.627755641937256\n",
      "(64, 33)\n",
      "step 23302, loss is 4.728318691253662\n",
      "(64, 33)\n",
      "step 23303, loss is 4.902500152587891\n",
      "(64, 33)\n",
      "step 23304, loss is 4.8736796379089355\n",
      "(64, 33)\n",
      "step 23305, loss is 4.687586784362793\n",
      "(64, 33)\n",
      "step 23306, loss is 4.610629558563232\n",
      "(64, 33)\n",
      "step 23307, loss is 4.73155403137207\n",
      "(64, 33)\n",
      "step 23308, loss is 4.691201210021973\n",
      "(64, 33)\n",
      "step 23309, loss is 4.745517253875732\n",
      "(64, 33)\n",
      "step 23310, loss is 4.7811360359191895\n",
      "(64, 33)\n",
      "step 23311, loss is 4.870802402496338\n",
      "(64, 33)\n",
      "step 23312, loss is 4.721131324768066\n",
      "(64, 33)\n",
      "step 23313, loss is 4.8569464683532715\n",
      "(64, 33)\n",
      "step 23314, loss is 4.759843349456787\n",
      "(64, 33)\n",
      "step 23315, loss is 4.859546184539795\n",
      "(64, 33)\n",
      "step 23316, loss is 4.825159549713135\n",
      "(64, 33)\n",
      "step 23317, loss is 4.734582424163818\n",
      "(64, 33)\n",
      "step 23318, loss is 4.8373026847839355\n",
      "(64, 33)\n",
      "step 23319, loss is 4.923249244689941\n",
      "(64, 33)\n",
      "step 23320, loss is 4.853407382965088\n",
      "(64, 33)\n",
      "step 23321, loss is 4.735918998718262\n",
      "(64, 33)\n",
      "step 23322, loss is 4.849972724914551\n",
      "(64, 33)\n",
      "step 23323, loss is 4.879755973815918\n",
      "(64, 33)\n",
      "step 23324, loss is 4.685787677764893\n",
      "(64, 33)\n",
      "step 23325, loss is 4.907253742218018\n",
      "(64, 33)\n",
      "step 23326, loss is 4.937447547912598\n",
      "(64, 33)\n",
      "step 23327, loss is 4.82122278213501\n",
      "(64, 33)\n",
      "step 23328, loss is 4.768876552581787\n",
      "(64, 33)\n",
      "step 23329, loss is 4.790242671966553\n",
      "(64, 33)\n",
      "step 23330, loss is 4.6206159591674805\n",
      "(64, 33)\n",
      "step 23331, loss is 4.878364562988281\n",
      "(64, 33)\n",
      "step 23332, loss is 4.721927642822266\n",
      "(64, 33)\n",
      "step 23333, loss is 4.703085899353027\n",
      "(64, 33)\n",
      "step 23334, loss is 4.968546390533447\n",
      "(64, 33)\n",
      "step 23335, loss is 4.660690784454346\n",
      "(64, 33)\n",
      "step 23336, loss is 4.770029067993164\n",
      "(64, 33)\n",
      "step 23337, loss is 4.75634765625\n",
      "(64, 33)\n",
      "step 23338, loss is 4.686477184295654\n",
      "(64, 33)\n",
      "step 23339, loss is 4.729825973510742\n",
      "(64, 33)\n",
      "step 23340, loss is 4.999716281890869\n",
      "(64, 33)\n",
      "step 23341, loss is 4.808958530426025\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23342, loss is 4.733868598937988\n",
      "(64, 33)\n",
      "step 23343, loss is 4.709844589233398\n",
      "(64, 33)\n",
      "step 23344, loss is 4.816000461578369\n",
      "(64, 33)\n",
      "step 23345, loss is 4.529858589172363\n",
      "(64, 33)\n",
      "step 23346, loss is 4.570456504821777\n",
      "(64, 33)\n",
      "step 23347, loss is 4.9852681159973145\n",
      "(64, 33)\n",
      "step 23348, loss is 4.7351975440979\n",
      "(64, 33)\n",
      "step 23349, loss is 4.818679332733154\n",
      "(64, 33)\n",
      "step 23350, loss is 4.736608028411865\n",
      "(64, 33)\n",
      "step 23351, loss is 4.8595781326293945\n",
      "(64, 33)\n",
      "step 23352, loss is 4.4910407066345215\n",
      "(64, 33)\n",
      "step 23353, loss is 4.806091785430908\n",
      "(64, 33)\n",
      "step 23354, loss is 5.010831356048584\n",
      "(64, 33)\n",
      "step 23355, loss is 4.7122087478637695\n",
      "(64, 33)\n",
      "step 23356, loss is 5.032709121704102\n",
      "(64, 33)\n",
      "step 23357, loss is 4.7745747566223145\n",
      "(64, 33)\n",
      "step 23358, loss is 4.816710948944092\n",
      "(64, 33)\n",
      "step 23359, loss is 4.669121742248535\n",
      "(64, 33)\n",
      "step 23360, loss is 4.6142191886901855\n",
      "(64, 33)\n",
      "step 23361, loss is 4.911634922027588\n",
      "(64, 33)\n",
      "step 23362, loss is 4.848895072937012\n",
      "(64, 33)\n",
      "step 23363, loss is 4.738906383514404\n",
      "(64, 33)\n",
      "step 23364, loss is 4.850991725921631\n",
      "(64, 33)\n",
      "step 23365, loss is 4.722932815551758\n",
      "(64, 33)\n",
      "step 23366, loss is 4.783862590789795\n",
      "(64, 33)\n",
      "step 23367, loss is 4.541083335876465\n",
      "(64, 33)\n",
      "step 23368, loss is 4.937945365905762\n",
      "(64, 33)\n",
      "step 23369, loss is 4.877572536468506\n",
      "(64, 33)\n",
      "step 23370, loss is 4.694142818450928\n",
      "(64, 33)\n",
      "step 23371, loss is 4.609633922576904\n",
      "(64, 33)\n",
      "step 23372, loss is 5.055655002593994\n",
      "(64, 33)\n",
      "step 23373, loss is 4.565797805786133\n",
      "(64, 33)\n",
      "step 23374, loss is 4.813883304595947\n",
      "(64, 33)\n",
      "step 23375, loss is 4.706778049468994\n",
      "(64, 33)\n",
      "step 23376, loss is 4.834957122802734\n",
      "(64, 33)\n",
      "step 23377, loss is 4.854779243469238\n",
      "(64, 33)\n",
      "step 23378, loss is 4.883852958679199\n",
      "(64, 33)\n",
      "step 23379, loss is 4.749523162841797\n",
      "(64, 33)\n",
      "step 23380, loss is 4.713308811187744\n",
      "(64, 33)\n",
      "step 23381, loss is 4.709953308105469\n",
      "(64, 33)\n",
      "step 23382, loss is 4.8219170570373535\n",
      "(64, 33)\n",
      "step 23383, loss is 4.725627422332764\n",
      "(64, 33)\n",
      "step 23384, loss is 4.914219379425049\n",
      "(64, 33)\n",
      "step 23385, loss is 4.833652019500732\n",
      "(64, 33)\n",
      "step 23386, loss is 4.927075386047363\n",
      "(64, 33)\n",
      "step 23387, loss is 4.854704856872559\n",
      "(64, 33)\n",
      "step 23388, loss is 4.659665584564209\n",
      "(64, 33)\n",
      "step 23389, loss is 4.92058801651001\n",
      "(64, 33)\n",
      "step 23390, loss is 4.927635192871094\n",
      "(64, 33)\n",
      "step 23391, loss is 4.758003234863281\n",
      "(64, 33)\n",
      "step 23392, loss is 4.657546043395996\n",
      "(64, 33)\n",
      "step 23393, loss is 4.7015380859375\n",
      "(64, 33)\n",
      "step 23394, loss is 4.721561431884766\n",
      "(64, 33)\n",
      "step 23395, loss is 4.701999664306641\n",
      "(64, 33)\n",
      "step 23396, loss is 4.782275676727295\n",
      "(64, 33)\n",
      "step 23397, loss is 4.681990146636963\n",
      "(64, 33)\n",
      "step 23398, loss is 4.7175984382629395\n",
      "(64, 33)\n",
      "step 23399, loss is 4.706679344177246\n",
      "(64, 33)\n",
      "step 23400, loss is 4.680943012237549\n",
      "(64, 33)\n",
      "step 23401, loss is 4.7204670906066895\n",
      "(64, 33)\n",
      "step 23402, loss is 4.760786533355713\n",
      "(64, 33)\n",
      "step 23403, loss is 4.88183069229126\n",
      "(64, 33)\n",
      "step 23404, loss is 4.610373497009277\n",
      "(64, 33)\n",
      "step 23405, loss is 4.6538777351379395\n",
      "(64, 33)\n",
      "step 23406, loss is 4.963756561279297\n",
      "(64, 33)\n",
      "step 23407, loss is 4.970158100128174\n",
      "(64, 33)\n",
      "step 23408, loss is 4.848300933837891\n",
      "(64, 33)\n",
      "step 23409, loss is 4.852660179138184\n",
      "(64, 33)\n",
      "step 23410, loss is 4.701286315917969\n",
      "(64, 33)\n",
      "step 23411, loss is 4.808050155639648\n",
      "(64, 33)\n",
      "step 23412, loss is 4.7072434425354\n",
      "(64, 33)\n",
      "step 23413, loss is 4.809534549713135\n",
      "(64, 33)\n",
      "step 23414, loss is 4.757260799407959\n",
      "(64, 33)\n",
      "step 23415, loss is 4.890474796295166\n",
      "(64, 33)\n",
      "step 23416, loss is 4.615963459014893\n",
      "(64, 33)\n",
      "step 23417, loss is 4.774167060852051\n",
      "(64, 33)\n",
      "step 23418, loss is 4.692747116088867\n",
      "(64, 33)\n",
      "step 23419, loss is 4.892817974090576\n",
      "(64, 33)\n",
      "step 23420, loss is 4.69736909866333\n",
      "(64, 33)\n",
      "step 23421, loss is 4.712695121765137\n",
      "(64, 33)\n",
      "step 23422, loss is 4.8100409507751465\n",
      "(64, 33)\n",
      "step 23423, loss is 4.641147613525391\n",
      "(64, 33)\n",
      "step 23424, loss is 4.913395881652832\n",
      "(64, 33)\n",
      "step 23425, loss is 4.838552474975586\n",
      "(64, 33)\n",
      "step 23426, loss is 4.907516956329346\n",
      "(64, 33)\n",
      "step 23427, loss is 4.700194835662842\n",
      "(64, 33)\n",
      "step 23428, loss is 4.918483734130859\n",
      "(64, 33)\n",
      "step 23429, loss is 4.77687406539917\n",
      "(64, 33)\n",
      "step 23430, loss is 4.769270420074463\n",
      "(64, 33)\n",
      "step 23431, loss is 4.749054431915283\n",
      "(64, 33)\n",
      "step 23432, loss is 4.919758319854736\n",
      "(64, 33)\n",
      "step 23433, loss is 4.751679420471191\n",
      "(64, 33)\n",
      "step 23434, loss is 4.704148292541504\n",
      "(64, 33)\n",
      "step 23435, loss is 4.913228511810303\n",
      "(64, 33)\n",
      "step 23436, loss is 4.791390895843506\n",
      "(64, 33)\n",
      "step 23437, loss is 4.799173355102539\n",
      "(64, 33)\n",
      "step 23438, loss is 4.827357769012451\n",
      "(64, 33)\n",
      "step 23439, loss is 4.743258476257324\n",
      "(64, 33)\n",
      "step 23440, loss is 4.861832141876221\n",
      "(64, 33)\n",
      "step 23441, loss is 4.999676704406738\n",
      "(64, 33)\n",
      "step 23442, loss is 4.746486663818359\n",
      "(64, 33)\n",
      "step 23443, loss is 4.810311794281006\n",
      "(64, 33)\n",
      "step 23444, loss is 4.68471097946167\n",
      "(64, 33)\n",
      "step 23445, loss is 4.862530708312988\n",
      "(64, 33)\n",
      "step 23446, loss is 4.992645263671875\n",
      "(64, 33)\n",
      "step 23447, loss is 4.652042388916016\n",
      "(64, 33)\n",
      "step 23448, loss is 4.790666103363037\n",
      "(64, 33)\n",
      "step 23449, loss is 4.727288246154785\n",
      "(64, 33)\n",
      "step 23450, loss is 4.857208251953125\n",
      "(64, 33)\n",
      "step 23451, loss is 5.055419445037842\n",
      "(64, 33)\n",
      "step 23452, loss is 4.64862060546875\n",
      "(64, 33)\n",
      "step 23453, loss is 4.948620796203613\n",
      "(64, 33)\n",
      "step 23454, loss is 4.89737606048584\n",
      "(64, 33)\n",
      "step 23455, loss is 4.848956108093262\n",
      "(64, 33)\n",
      "step 23456, loss is 4.844549179077148\n",
      "(64, 33)\n",
      "step 23457, loss is 4.692775249481201\n",
      "(64, 33)\n",
      "step 23458, loss is 4.719828128814697\n",
      "(64, 33)\n",
      "step 23459, loss is 4.6110968589782715\n",
      "(64, 33)\n",
      "step 23460, loss is 4.569856643676758\n",
      "(64, 33)\n",
      "step 23461, loss is 5.046018123626709\n",
      "(64, 33)\n",
      "step 23462, loss is 4.926523208618164\n",
      "(64, 33)\n",
      "step 23463, loss is 4.771903038024902\n",
      "(64, 33)\n",
      "step 23464, loss is 4.731444835662842\n",
      "(64, 33)\n",
      "step 23465, loss is 4.876878261566162\n",
      "(64, 33)\n",
      "step 23466, loss is 4.760490417480469\n",
      "(64, 33)\n",
      "step 23467, loss is 4.761841773986816\n",
      "(64, 33)\n",
      "step 23468, loss is 4.782161712646484\n",
      "(64, 33)\n",
      "step 23469, loss is 4.838772773742676\n",
      "(64, 33)\n",
      "step 23470, loss is 4.737887859344482\n",
      "(64, 33)\n",
      "step 23471, loss is 4.60197639465332\n",
      "(64, 33)\n",
      "step 23472, loss is 4.912137031555176\n",
      "(64, 33)\n",
      "step 23473, loss is 4.674051761627197\n",
      "(64, 33)\n",
      "step 23474, loss is 4.74359655380249\n",
      "(64, 33)\n",
      "step 23475, loss is 4.7976298332214355\n",
      "(64, 33)\n",
      "step 23476, loss is 4.75875186920166\n",
      "(64, 33)\n",
      "step 23477, loss is 4.775651931762695\n",
      "(64, 33)\n",
      "step 23478, loss is 4.744701385498047\n",
      "(64, 33)\n",
      "step 23479, loss is 4.995202541351318\n",
      "(64, 33)\n",
      "step 23480, loss is 4.739655494689941\n",
      "(64, 33)\n",
      "step 23481, loss is 4.75473165512085\n",
      "(64, 33)\n",
      "step 23482, loss is 4.674562454223633\n",
      "(64, 33)\n",
      "step 23483, loss is 4.739474296569824\n",
      "(64, 33)\n",
      "step 23484, loss is 4.76666259765625\n",
      "(64, 33)\n",
      "step 23485, loss is 4.703640937805176\n",
      "(64, 33)\n",
      "step 23486, loss is 4.7082390785217285\n",
      "(64, 33)\n",
      "step 23487, loss is 4.831289291381836\n",
      "(64, 33)\n",
      "step 23488, loss is 4.852119445800781\n",
      "(64, 33)\n",
      "step 23489, loss is 4.661402702331543\n",
      "(64, 33)\n",
      "step 23490, loss is 4.837880611419678\n",
      "(64, 33)\n",
      "step 23491, loss is 4.570475101470947\n",
      "(64, 33)\n",
      "step 23492, loss is 4.797112941741943\n",
      "(64, 33)\n",
      "step 23493, loss is 4.8548665046691895\n",
      "(64, 33)\n",
      "step 23494, loss is 4.843722343444824\n",
      "(64, 33)\n",
      "step 23495, loss is 4.699501037597656\n",
      "(64, 33)\n",
      "step 23496, loss is 4.791563987731934\n",
      "(64, 33)\n",
      "step 23497, loss is 4.846382141113281\n",
      "(64, 33)\n",
      "step 23498, loss is 4.83610200881958\n",
      "(64, 33)\n",
      "step 23499, loss is 4.829048156738281\n",
      "(64, 33)\n",
      "step 23500, loss is 4.877119541168213\n",
      "(64, 33)\n",
      "step 23501, loss is 4.7796807289123535\n",
      "(64, 33)\n",
      "step 23502, loss is 4.954773426055908\n",
      "(64, 33)\n",
      "step 23503, loss is 4.867136001586914\n",
      "(64, 33)\n",
      "step 23504, loss is 4.91342306137085\n",
      "(64, 33)\n",
      "step 23505, loss is 4.744316101074219\n",
      "(64, 33)\n",
      "step 23506, loss is 4.65602970123291\n",
      "(64, 33)\n",
      "step 23507, loss is 4.839291572570801\n",
      "(64, 33)\n",
      "step 23508, loss is 4.848611354827881\n",
      "(64, 33)\n",
      "step 23509, loss is 4.654900074005127\n",
      "(64, 33)\n",
      "step 23510, loss is 4.682487964630127\n",
      "(64, 33)\n",
      "step 23511, loss is 4.712887287139893\n",
      "(64, 33)\n",
      "step 23512, loss is 4.891092777252197\n",
      "(64, 33)\n",
      "step 23513, loss is 4.947936534881592\n",
      "(64, 33)\n",
      "step 23514, loss is 4.783042907714844\n",
      "(64, 33)\n",
      "step 23515, loss is 4.739189147949219\n",
      "(64, 33)\n",
      "step 23516, loss is 4.7969183921813965\n",
      "(64, 33)\n",
      "step 23517, loss is 4.775271415710449\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23518, loss is 4.84916353225708\n",
      "(64, 33)\n",
      "step 23519, loss is 4.731582164764404\n",
      "(64, 33)\n",
      "step 23520, loss is 4.906226634979248\n",
      "(64, 33)\n",
      "step 23521, loss is 4.843233585357666\n",
      "(64, 33)\n",
      "step 23522, loss is 4.580570697784424\n",
      "(64, 33)\n",
      "step 23523, loss is 4.890141010284424\n",
      "(64, 33)\n",
      "step 23524, loss is 4.642011642456055\n",
      "(64, 33)\n",
      "step 23525, loss is 4.875167369842529\n",
      "(64, 33)\n",
      "step 23526, loss is 4.879123687744141\n",
      "(64, 33)\n",
      "step 23527, loss is 4.8471879959106445\n",
      "(64, 33)\n",
      "step 23528, loss is 4.760334491729736\n",
      "(64, 33)\n",
      "step 23529, loss is 4.744772911071777\n",
      "(64, 33)\n",
      "step 23530, loss is 4.79756498336792\n",
      "(64, 33)\n",
      "step 23531, loss is 4.8664703369140625\n",
      "(64, 33)\n",
      "step 23532, loss is 4.587259769439697\n",
      "(64, 33)\n",
      "step 23533, loss is 4.750132083892822\n",
      "(64, 33)\n",
      "step 23534, loss is 4.98900032043457\n",
      "(64, 33)\n",
      "step 23535, loss is 4.842876434326172\n",
      "(64, 33)\n",
      "step 23536, loss is 4.857299327850342\n",
      "(64, 33)\n",
      "step 23537, loss is 4.7623724937438965\n",
      "(64, 33)\n",
      "step 23538, loss is 4.643793106079102\n",
      "(64, 33)\n",
      "step 23539, loss is 4.655529975891113\n",
      "(64, 33)\n",
      "step 23540, loss is 5.039505481719971\n",
      "(64, 33)\n",
      "step 23541, loss is 4.779750823974609\n",
      "(64, 33)\n",
      "step 23542, loss is 4.765373229980469\n",
      "(64, 33)\n",
      "step 23543, loss is 4.779192924499512\n",
      "(64, 33)\n",
      "step 23544, loss is 4.875026226043701\n",
      "(64, 33)\n",
      "step 23545, loss is 4.880810737609863\n",
      "(64, 33)\n",
      "step 23546, loss is 4.852528095245361\n",
      "(64, 33)\n",
      "step 23547, loss is 4.9131693840026855\n",
      "(64, 33)\n",
      "step 23548, loss is 4.751405715942383\n",
      "(64, 33)\n",
      "step 23549, loss is 4.492697238922119\n",
      "(64, 33)\n",
      "step 23550, loss is 4.8335280418396\n",
      "(64, 33)\n",
      "step 23551, loss is 4.836485385894775\n",
      "(64, 33)\n",
      "step 23552, loss is 4.923104286193848\n",
      "(64, 33)\n",
      "step 23553, loss is 4.864226818084717\n",
      "(64, 33)\n",
      "step 23554, loss is 4.631058216094971\n",
      "(64, 33)\n",
      "step 23555, loss is 4.897895812988281\n",
      "(64, 33)\n",
      "step 23556, loss is 4.7066755294799805\n",
      "(64, 33)\n",
      "step 23557, loss is 4.718780517578125\n",
      "(64, 33)\n",
      "step 23558, loss is 4.816019535064697\n",
      "(64, 33)\n",
      "step 23559, loss is 4.742222785949707\n",
      "(64, 33)\n",
      "step 23560, loss is 4.695505619049072\n",
      "(64, 33)\n",
      "step 23561, loss is 4.683574199676514\n",
      "(64, 33)\n",
      "step 23562, loss is 4.978968143463135\n",
      "(64, 33)\n",
      "step 23563, loss is 4.812831878662109\n",
      "(64, 33)\n",
      "step 23564, loss is 4.818446636199951\n",
      "(64, 33)\n",
      "step 23565, loss is 4.879369735717773\n",
      "(64, 33)\n",
      "step 23566, loss is 4.769932746887207\n",
      "(64, 33)\n",
      "step 23567, loss is 4.784889221191406\n",
      "(64, 33)\n",
      "step 23568, loss is 4.932465076446533\n",
      "(64, 33)\n",
      "step 23569, loss is 4.81749153137207\n",
      "(64, 33)\n",
      "step 23570, loss is 4.859562873840332\n",
      "(64, 33)\n",
      "step 23571, loss is 4.77600622177124\n",
      "(64, 33)\n",
      "step 23572, loss is 4.775462627410889\n",
      "(64, 33)\n",
      "step 23573, loss is 4.691043853759766\n",
      "(64, 33)\n",
      "step 23574, loss is 4.759280681610107\n",
      "(64, 33)\n",
      "step 23575, loss is 4.720383167266846\n",
      "(64, 33)\n",
      "step 23576, loss is 4.733578681945801\n",
      "(64, 33)\n",
      "step 23577, loss is 4.800938129425049\n",
      "(64, 33)\n",
      "step 23578, loss is 4.803138732910156\n",
      "(64, 33)\n",
      "step 23579, loss is 4.8877034187316895\n",
      "(64, 33)\n",
      "step 23580, loss is 4.5103349685668945\n",
      "(64, 33)\n",
      "step 23581, loss is 4.63932991027832\n",
      "(64, 33)\n",
      "step 23582, loss is 4.799013137817383\n",
      "(64, 33)\n",
      "step 23583, loss is 4.768939971923828\n",
      "(64, 33)\n",
      "step 23584, loss is 4.892658233642578\n",
      "(64, 33)\n",
      "step 23585, loss is 4.695053577423096\n",
      "(64, 33)\n",
      "step 23586, loss is 4.96123743057251\n",
      "(64, 33)\n",
      "step 23587, loss is 4.676231384277344\n",
      "(64, 33)\n",
      "step 23588, loss is 4.604434490203857\n",
      "(64, 33)\n",
      "step 23589, loss is 4.767105579376221\n",
      "(64, 33)\n",
      "step 23590, loss is 4.756690979003906\n",
      "(64, 33)\n",
      "step 23591, loss is 4.729986190795898\n",
      "(64, 33)\n",
      "step 23592, loss is 4.832405090332031\n",
      "(64, 33)\n",
      "step 23593, loss is 4.829501152038574\n",
      "(64, 33)\n",
      "step 23594, loss is 4.984617233276367\n",
      "(64, 33)\n",
      "step 23595, loss is 4.8003058433532715\n",
      "(64, 33)\n",
      "step 23596, loss is 4.780651569366455\n",
      "(64, 33)\n",
      "step 23597, loss is 4.728604316711426\n",
      "(64, 33)\n",
      "step 23598, loss is 4.722929000854492\n",
      "(64, 33)\n",
      "step 23599, loss is 4.608419895172119\n",
      "(64, 33)\n",
      "step 23600, loss is 4.77020263671875\n",
      "(64, 33)\n",
      "step 23601, loss is 4.624878883361816\n",
      "(64, 33)\n",
      "step 23602, loss is 4.810204029083252\n",
      "(64, 33)\n",
      "step 23603, loss is 4.816977500915527\n",
      "(64, 33)\n",
      "step 23604, loss is 4.803580284118652\n",
      "(64, 33)\n",
      "step 23605, loss is 4.887760639190674\n",
      "(64, 33)\n",
      "step 23606, loss is 4.880292892456055\n",
      "(64, 33)\n",
      "step 23607, loss is 4.786323547363281\n",
      "(64, 33)\n",
      "step 23608, loss is 4.843423843383789\n",
      "(64, 33)\n",
      "step 23609, loss is 4.713306903839111\n",
      "(64, 33)\n",
      "step 23610, loss is 4.911226272583008\n",
      "(64, 33)\n",
      "step 23611, loss is 4.810207843780518\n",
      "(64, 33)\n",
      "step 23612, loss is 4.690291881561279\n",
      "(64, 33)\n",
      "step 23613, loss is 4.820516586303711\n",
      "(64, 33)\n",
      "step 23614, loss is 4.945210933685303\n",
      "(64, 33)\n",
      "step 23615, loss is 4.913751125335693\n",
      "(64, 33)\n",
      "step 23616, loss is 4.759564399719238\n",
      "(64, 33)\n",
      "step 23617, loss is 4.699910640716553\n",
      "(64, 33)\n",
      "step 23618, loss is 4.730795383453369\n",
      "(64, 33)\n",
      "step 23619, loss is 4.881534576416016\n",
      "(64, 33)\n",
      "step 23620, loss is 4.75154447555542\n",
      "(64, 33)\n",
      "step 23621, loss is 4.680142879486084\n",
      "(64, 33)\n",
      "step 23622, loss is 4.714768886566162\n",
      "(64, 33)\n",
      "step 23623, loss is 4.81336784362793\n",
      "(64, 33)\n",
      "step 23624, loss is 5.068735122680664\n",
      "(64, 33)\n",
      "step 23625, loss is 4.744781494140625\n",
      "(64, 33)\n",
      "step 23626, loss is 4.842413902282715\n",
      "(64, 33)\n",
      "step 23627, loss is 4.671175956726074\n",
      "(64, 33)\n",
      "step 23628, loss is 4.764249324798584\n",
      "(64, 33)\n",
      "step 23629, loss is 4.717901706695557\n",
      "(64, 33)\n",
      "step 23630, loss is 4.649319171905518\n",
      "(64, 33)\n",
      "step 23631, loss is 4.714607238769531\n",
      "(64, 33)\n",
      "step 23632, loss is 4.782685279846191\n",
      "(64, 33)\n",
      "step 23633, loss is 4.807333946228027\n",
      "(64, 33)\n",
      "step 23634, loss is 4.882139205932617\n",
      "(64, 33)\n",
      "step 23635, loss is 4.729796886444092\n",
      "(64, 33)\n",
      "step 23636, loss is 4.762566089630127\n",
      "(64, 33)\n",
      "step 23637, loss is 4.746608257293701\n",
      "(64, 33)\n",
      "step 23638, loss is 4.7086501121521\n",
      "(64, 33)\n",
      "step 23639, loss is 4.699078559875488\n",
      "(64, 33)\n",
      "step 23640, loss is 4.694038391113281\n",
      "(64, 33)\n",
      "step 23641, loss is 4.672325134277344\n",
      "(64, 33)\n",
      "step 23642, loss is 4.880390644073486\n",
      "(64, 33)\n",
      "step 23643, loss is 4.827366352081299\n",
      "(64, 33)\n",
      "step 23644, loss is 4.637521266937256\n",
      "(64, 33)\n",
      "step 23645, loss is 4.878746032714844\n",
      "(64, 33)\n",
      "step 23646, loss is 4.684415817260742\n",
      "(64, 33)\n",
      "step 23647, loss is 4.697257995605469\n",
      "(64, 33)\n",
      "step 23648, loss is 4.805354118347168\n",
      "(64, 33)\n",
      "step 23649, loss is 4.728351593017578\n",
      "(64, 33)\n",
      "step 23650, loss is 4.752673625946045\n",
      "(64, 33)\n",
      "step 23651, loss is 4.845884799957275\n",
      "(64, 33)\n",
      "step 23652, loss is 4.830989837646484\n",
      "(64, 33)\n",
      "step 23653, loss is 4.86476993560791\n",
      "(64, 33)\n",
      "step 23654, loss is 4.764002323150635\n",
      "(64, 33)\n",
      "step 23655, loss is 4.618545055389404\n",
      "(64, 33)\n",
      "step 23656, loss is 4.885433673858643\n",
      "(64, 33)\n",
      "step 23657, loss is 4.744592189788818\n",
      "(64, 33)\n",
      "step 23658, loss is 4.799111366271973\n",
      "(64, 33)\n",
      "step 23659, loss is 4.669275283813477\n",
      "(64, 33)\n",
      "step 23660, loss is 4.874275207519531\n",
      "(64, 33)\n",
      "step 23661, loss is 4.762352466583252\n",
      "(64, 33)\n",
      "step 23662, loss is 4.842901706695557\n",
      "(64, 33)\n",
      "step 23663, loss is 4.819676399230957\n",
      "(64, 33)\n",
      "step 23664, loss is 4.9101243019104\n",
      "(64, 33)\n",
      "step 23665, loss is 4.658370018005371\n",
      "(64, 33)\n",
      "step 23666, loss is 4.812386989593506\n",
      "(64, 33)\n",
      "step 23667, loss is 4.803069114685059\n",
      "(64, 33)\n",
      "step 23668, loss is 4.667423248291016\n",
      "(64, 33)\n",
      "step 23669, loss is 4.812922477722168\n",
      "(64, 33)\n",
      "step 23670, loss is 5.005417823791504\n",
      "(64, 33)\n",
      "step 23671, loss is 4.810019016265869\n",
      "(64, 33)\n",
      "step 23672, loss is 4.682955741882324\n",
      "(64, 33)\n",
      "step 23673, loss is 4.751167297363281\n",
      "(64, 33)\n",
      "step 23674, loss is 4.783688068389893\n",
      "(64, 33)\n",
      "step 23675, loss is 4.732700347900391\n",
      "(64, 33)\n",
      "step 23676, loss is 4.638821125030518\n",
      "(64, 33)\n",
      "step 23677, loss is 4.849735260009766\n",
      "(64, 33)\n",
      "step 23678, loss is 4.6248016357421875\n",
      "(64, 33)\n",
      "step 23679, loss is 4.885438442230225\n",
      "(64, 33)\n",
      "step 23680, loss is 4.825826168060303\n",
      "(64, 33)\n",
      "step 23681, loss is 4.762542724609375\n",
      "(64, 33)\n",
      "step 23682, loss is 4.777953624725342\n",
      "(64, 33)\n",
      "step 23683, loss is 4.802997589111328\n",
      "(64, 33)\n",
      "step 23684, loss is 4.7131242752075195\n",
      "(64, 33)\n",
      "step 23685, loss is 4.717018127441406\n",
      "(64, 33)\n",
      "step 23686, loss is 4.779823303222656\n",
      "(64, 33)\n",
      "step 23687, loss is 4.9576568603515625\n",
      "(64, 33)\n",
      "step 23688, loss is 4.895623683929443\n",
      "(64, 33)\n",
      "step 23689, loss is 4.5887370109558105\n",
      "(64, 33)\n",
      "step 23690, loss is 4.859957695007324\n",
      "(64, 33)\n",
      "step 23691, loss is 4.934481620788574\n",
      "(64, 33)\n",
      "step 23692, loss is 4.692197799682617\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23693, loss is 4.735346794128418\n",
      "(64, 33)\n",
      "step 23694, loss is 4.860713958740234\n",
      "(64, 33)\n",
      "step 23695, loss is 4.870731353759766\n",
      "(64, 33)\n",
      "step 23696, loss is 4.893267631530762\n",
      "(64, 33)\n",
      "step 23697, loss is 5.0055623054504395\n",
      "(64, 33)\n",
      "step 23698, loss is 4.923399925231934\n",
      "(64, 33)\n",
      "step 23699, loss is 4.83760929107666\n",
      "(64, 33)\n",
      "step 23700, loss is 4.758053779602051\n",
      "(64, 33)\n",
      "step 23701, loss is 4.790648937225342\n",
      "(64, 33)\n",
      "step 23702, loss is 4.780722141265869\n",
      "(64, 33)\n",
      "step 23703, loss is 4.747964382171631\n",
      "(64, 33)\n",
      "step 23704, loss is 4.911850452423096\n",
      "(64, 33)\n",
      "step 23705, loss is 4.791354179382324\n",
      "(64, 33)\n",
      "step 23706, loss is 4.79673433303833\n",
      "(64, 33)\n",
      "step 23707, loss is 4.894313812255859\n",
      "(64, 33)\n",
      "step 23708, loss is 4.63952112197876\n",
      "(64, 33)\n",
      "step 23709, loss is 4.839903831481934\n",
      "(64, 33)\n",
      "step 23710, loss is 4.763646602630615\n",
      "(64, 33)\n",
      "step 23711, loss is 4.7400336265563965\n",
      "(64, 33)\n",
      "step 23712, loss is 4.727093696594238\n",
      "(64, 33)\n",
      "step 23713, loss is 4.674431800842285\n",
      "(64, 33)\n",
      "step 23714, loss is 4.771744251251221\n",
      "(64, 33)\n",
      "step 23715, loss is 4.734930038452148\n",
      "(64, 33)\n",
      "step 23716, loss is 4.8884806632995605\n",
      "(64, 33)\n",
      "step 23717, loss is 4.922164440155029\n",
      "(64, 33)\n",
      "step 23718, loss is 4.72037935256958\n",
      "(64, 33)\n",
      "step 23719, loss is 4.776103496551514\n",
      "(64, 33)\n",
      "step 23720, loss is 4.989202499389648\n",
      "(64, 33)\n",
      "step 23721, loss is 4.761883735656738\n",
      "(64, 33)\n",
      "step 23722, loss is 4.798460006713867\n",
      "(64, 33)\n",
      "step 23723, loss is 4.630190849304199\n",
      "(64, 33)\n",
      "step 23724, loss is 4.658179759979248\n",
      "(64, 33)\n",
      "step 23725, loss is 4.698451042175293\n",
      "(64, 33)\n",
      "step 23726, loss is 4.7273359298706055\n",
      "(64, 33)\n",
      "step 23727, loss is 4.909788131713867\n",
      "(64, 33)\n",
      "step 23728, loss is 4.7397685050964355\n",
      "(64, 33)\n",
      "step 23729, loss is 4.893047332763672\n",
      "(64, 33)\n",
      "step 23730, loss is 4.634095668792725\n",
      "(64, 33)\n",
      "step 23731, loss is 4.918996334075928\n",
      "(64, 33)\n",
      "step 23732, loss is 4.902975082397461\n",
      "(64, 33)\n",
      "step 23733, loss is 4.9526567459106445\n",
      "(64, 33)\n",
      "step 23734, loss is 4.713156700134277\n",
      "(64, 33)\n",
      "step 23735, loss is 4.848713397979736\n",
      "(64, 33)\n",
      "step 23736, loss is 4.82569694519043\n",
      "(64, 33)\n",
      "step 23737, loss is 4.862037658691406\n",
      "(64, 33)\n",
      "step 23738, loss is 4.7765421867370605\n",
      "(64, 33)\n",
      "step 23739, loss is 4.745309352874756\n",
      "(64, 33)\n",
      "step 23740, loss is 4.7727227210998535\n",
      "(64, 33)\n",
      "step 23741, loss is 4.74601936340332\n",
      "(64, 33)\n",
      "step 23742, loss is 4.820996284484863\n",
      "(64, 33)\n",
      "step 23743, loss is 4.766931533813477\n",
      "(64, 33)\n",
      "step 23744, loss is 4.782776832580566\n",
      "(64, 33)\n",
      "step 23745, loss is 4.804004192352295\n",
      "(64, 33)\n",
      "step 23746, loss is 4.725896835327148\n",
      "(64, 33)\n",
      "step 23747, loss is 4.724803924560547\n",
      "(64, 33)\n",
      "step 23748, loss is 4.631368160247803\n",
      "(64, 33)\n",
      "step 23749, loss is 4.81331729888916\n",
      "(64, 33)\n",
      "step 23750, loss is 4.8002495765686035\n",
      "(64, 33)\n",
      "step 23751, loss is 4.773101806640625\n",
      "(64, 33)\n",
      "step 23752, loss is 4.619744300842285\n",
      "(64, 33)\n",
      "step 23753, loss is 4.64159631729126\n",
      "(64, 33)\n",
      "step 23754, loss is 4.724074840545654\n",
      "(64, 33)\n",
      "step 23755, loss is 4.868520259857178\n",
      "(64, 33)\n",
      "step 23756, loss is 4.8196187019348145\n",
      "(64, 33)\n",
      "step 23757, loss is 4.584775924682617\n",
      "(64, 33)\n",
      "step 23758, loss is 4.763488292694092\n",
      "(64, 33)\n",
      "step 23759, loss is 4.927314281463623\n",
      "(64, 33)\n",
      "step 23760, loss is 4.681341648101807\n",
      "(64, 33)\n",
      "step 23761, loss is 5.098880290985107\n",
      "(64, 33)\n",
      "step 23762, loss is 4.713785648345947\n",
      "(64, 33)\n",
      "step 23763, loss is 4.632805347442627\n",
      "(64, 33)\n",
      "step 23764, loss is 4.833573818206787\n",
      "(64, 33)\n",
      "step 23765, loss is 4.596330642700195\n",
      "(64, 33)\n",
      "step 23766, loss is 5.053381443023682\n",
      "(64, 33)\n",
      "step 23767, loss is 4.773970603942871\n",
      "(64, 33)\n",
      "step 23768, loss is 4.795018672943115\n",
      "(64, 33)\n",
      "step 23769, loss is 4.974262714385986\n",
      "(64, 33)\n",
      "step 23770, loss is 4.981431484222412\n",
      "(64, 33)\n",
      "step 23771, loss is 4.70454740524292\n",
      "(64, 33)\n",
      "step 23772, loss is 4.7898688316345215\n",
      "(64, 33)\n",
      "step 23773, loss is 4.826411247253418\n",
      "(64, 33)\n",
      "step 23774, loss is 4.506220817565918\n",
      "(64, 33)\n",
      "step 23775, loss is 4.960785865783691\n",
      "(64, 33)\n",
      "step 23776, loss is 4.816140174865723\n",
      "(64, 33)\n",
      "step 23777, loss is 5.017557621002197\n",
      "(64, 33)\n",
      "step 23778, loss is 4.837876796722412\n",
      "(64, 33)\n",
      "step 23779, loss is 4.676244735717773\n",
      "(64, 33)\n",
      "step 23780, loss is 4.757282257080078\n",
      "(64, 33)\n",
      "step 23781, loss is 4.74481201171875\n",
      "(64, 33)\n",
      "step 23782, loss is 4.816936016082764\n",
      "(64, 33)\n",
      "step 23783, loss is 4.720657825469971\n",
      "(64, 33)\n",
      "step 23784, loss is 4.9486565589904785\n",
      "(64, 33)\n",
      "step 23785, loss is 4.56463623046875\n",
      "(64, 33)\n",
      "step 23786, loss is 4.862756252288818\n",
      "(64, 33)\n",
      "step 23787, loss is 4.854613780975342\n",
      "(64, 33)\n",
      "step 23788, loss is 4.738454818725586\n",
      "(64, 33)\n",
      "step 23789, loss is 4.612099647521973\n",
      "(64, 33)\n",
      "step 23790, loss is 4.909307479858398\n",
      "(64, 33)\n",
      "step 23791, loss is 4.765133857727051\n",
      "(64, 33)\n",
      "step 23792, loss is 4.520563125610352\n",
      "(64, 33)\n",
      "step 23793, loss is 4.603109836578369\n",
      "(64, 33)\n",
      "step 23794, loss is 4.665489196777344\n",
      "(64, 33)\n",
      "step 23795, loss is 4.735382556915283\n",
      "(64, 33)\n",
      "step 23796, loss is 4.765434741973877\n",
      "(64, 33)\n",
      "step 23797, loss is 4.683350086212158\n",
      "(64, 33)\n",
      "step 23798, loss is 4.869500160217285\n",
      "(64, 33)\n",
      "step 23799, loss is 4.763651371002197\n",
      "(64, 33)\n",
      "step 23800, loss is 4.865413188934326\n",
      "(64, 33)\n",
      "step 23801, loss is 4.805305480957031\n",
      "(64, 33)\n",
      "step 23802, loss is 4.913478851318359\n",
      "(64, 33)\n",
      "step 23803, loss is 4.840336322784424\n",
      "(64, 33)\n",
      "step 23804, loss is 4.7003912925720215\n",
      "(64, 33)\n",
      "step 23805, loss is 4.692154407501221\n",
      "(64, 33)\n",
      "step 23806, loss is 4.608509063720703\n",
      "(64, 33)\n",
      "step 23807, loss is 4.782005310058594\n",
      "(64, 33)\n",
      "step 23808, loss is 4.690499782562256\n",
      "(64, 33)\n",
      "step 23809, loss is 4.70616340637207\n",
      "(64, 33)\n",
      "step 23810, loss is 4.720571517944336\n",
      "(64, 33)\n",
      "step 23811, loss is 5.047171115875244\n",
      "(64, 33)\n",
      "step 23812, loss is 4.797787666320801\n",
      "(64, 33)\n",
      "step 23813, loss is 4.8482818603515625\n",
      "(64, 33)\n",
      "step 23814, loss is 4.764967918395996\n",
      "(64, 33)\n",
      "step 23815, loss is 4.616764545440674\n",
      "(64, 33)\n",
      "step 23816, loss is 4.822137355804443\n",
      "(64, 33)\n",
      "step 23817, loss is 4.843191146850586\n",
      "(64, 33)\n",
      "step 23818, loss is 4.615280628204346\n",
      "(64, 33)\n",
      "step 23819, loss is 4.808167934417725\n",
      "(64, 33)\n",
      "step 23820, loss is 4.789694309234619\n",
      "(64, 33)\n",
      "step 23821, loss is 4.799169540405273\n",
      "(64, 33)\n",
      "step 23822, loss is 4.771067142486572\n",
      "(64, 33)\n",
      "step 23823, loss is 4.842226982116699\n",
      "(64, 33)\n",
      "step 23824, loss is 4.7617716789245605\n",
      "(64, 33)\n",
      "step 23825, loss is 4.774299621582031\n",
      "(64, 33)\n",
      "step 23826, loss is 5.041847229003906\n",
      "(64, 33)\n",
      "step 23827, loss is 4.731632709503174\n",
      "(64, 33)\n",
      "step 23828, loss is 4.808831214904785\n",
      "(64, 33)\n",
      "step 23829, loss is 4.741697311401367\n",
      "(64, 33)\n",
      "step 23830, loss is 4.6031718254089355\n",
      "(64, 33)\n",
      "step 23831, loss is 4.81667947769165\n",
      "(64, 33)\n",
      "step 23832, loss is 4.77254581451416\n",
      "(64, 33)\n",
      "step 23833, loss is 4.650075435638428\n",
      "(64, 33)\n",
      "step 23834, loss is 4.844184875488281\n",
      "(64, 33)\n",
      "step 23835, loss is 4.609788417816162\n",
      "(64, 33)\n",
      "step 23836, loss is 4.833380222320557\n",
      "(64, 33)\n",
      "step 23837, loss is 4.610924243927002\n",
      "(64, 33)\n",
      "step 23838, loss is 4.934028625488281\n",
      "(64, 33)\n",
      "step 23839, loss is 4.752571105957031\n",
      "(64, 33)\n",
      "step 23840, loss is 4.857991695404053\n",
      "(64, 33)\n",
      "step 23841, loss is 4.640129566192627\n",
      "(64, 33)\n",
      "step 23842, loss is 4.849918365478516\n",
      "(64, 33)\n",
      "step 23843, loss is 4.883861064910889\n",
      "(64, 33)\n",
      "step 23844, loss is 4.92080020904541\n",
      "(64, 33)\n",
      "step 23845, loss is 4.789748668670654\n",
      "(64, 33)\n",
      "step 23846, loss is 4.8471455574035645\n",
      "(64, 33)\n",
      "step 23847, loss is 4.700438022613525\n",
      "(64, 33)\n",
      "step 23848, loss is 4.808719158172607\n",
      "(64, 33)\n",
      "step 23849, loss is 4.881533145904541\n",
      "(64, 33)\n",
      "step 23850, loss is 4.841781139373779\n",
      "(64, 33)\n",
      "step 23851, loss is 4.905569076538086\n",
      "(64, 33)\n",
      "step 23852, loss is 4.657317638397217\n",
      "(64, 33)\n",
      "step 23853, loss is 4.884297847747803\n",
      "(64, 33)\n",
      "step 23854, loss is 4.76255989074707\n",
      "(64, 33)\n",
      "step 23855, loss is 4.676074504852295\n",
      "(64, 33)\n",
      "step 23856, loss is 4.7768707275390625\n",
      "(64, 33)\n",
      "step 23857, loss is 4.589642524719238\n",
      "(64, 33)\n",
      "step 23858, loss is 4.6625657081604\n",
      "(64, 33)\n",
      "step 23859, loss is 4.725093364715576\n",
      "(64, 33)\n",
      "step 23860, loss is 4.7385735511779785\n",
      "(64, 33)\n",
      "step 23861, loss is 4.917221546173096\n",
      "(64, 33)\n",
      "step 23862, loss is 4.749087810516357\n",
      "(64, 33)\n",
      "step 23863, loss is 4.720070838928223\n",
      "(64, 33)\n",
      "step 23864, loss is 4.65490198135376\n",
      "(64, 33)\n",
      "step 23865, loss is 4.719569683074951\n",
      "(64, 33)\n",
      "step 23866, loss is 4.909217834472656\n",
      "(64, 33)\n",
      "step 23867, loss is 4.691365718841553\n",
      "(64, 33)\n",
      "step 23868, loss is 4.708009719848633\n",
      "(64, 33)\n",
      "step 23869, loss is 4.6874165534973145\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23870, loss is 4.592895030975342\n",
      "(64, 33)\n",
      "step 23871, loss is 4.899073600769043\n",
      "(64, 33)\n",
      "step 23872, loss is 4.7736663818359375\n",
      "(64, 33)\n",
      "step 23873, loss is 5.0407304763793945\n",
      "(64, 33)\n",
      "step 23874, loss is 4.746696472167969\n",
      "(64, 33)\n",
      "step 23875, loss is 4.905841827392578\n",
      "(64, 33)\n",
      "step 23876, loss is 4.723555088043213\n",
      "(64, 33)\n",
      "step 23877, loss is 4.712148189544678\n",
      "(64, 33)\n",
      "step 23878, loss is 4.673867702484131\n",
      "(64, 33)\n",
      "step 23879, loss is 4.713301658630371\n",
      "(64, 33)\n",
      "step 23880, loss is 4.904149532318115\n",
      "(64, 33)\n",
      "step 23881, loss is 4.833226680755615\n",
      "(64, 33)\n",
      "step 23882, loss is 4.61487340927124\n",
      "(64, 33)\n",
      "step 23883, loss is 4.75430965423584\n",
      "(64, 33)\n",
      "step 23884, loss is 4.656379699707031\n",
      "(64, 33)\n",
      "step 23885, loss is 4.7876811027526855\n",
      "(64, 33)\n",
      "step 23886, loss is 4.847540855407715\n",
      "(64, 33)\n",
      "step 23887, loss is 4.790518283843994\n",
      "(64, 33)\n",
      "step 23888, loss is 4.8055100440979\n",
      "(64, 33)\n",
      "step 23889, loss is 4.854745864868164\n",
      "(64, 33)\n",
      "step 23890, loss is 4.707448482513428\n",
      "(64, 33)\n",
      "step 23891, loss is 4.867179870605469\n",
      "(64, 33)\n",
      "step 23892, loss is 4.701310634613037\n",
      "(64, 33)\n",
      "step 23893, loss is 4.758433818817139\n",
      "(64, 33)\n",
      "step 23894, loss is 4.974045753479004\n",
      "(64, 33)\n",
      "step 23895, loss is 4.653444290161133\n",
      "(64, 33)\n",
      "step 23896, loss is 4.7664570808410645\n",
      "(64, 33)\n",
      "step 23897, loss is 4.590129375457764\n",
      "(64, 33)\n",
      "step 23898, loss is 4.746540546417236\n",
      "(64, 33)\n",
      "step 23899, loss is 4.9827094078063965\n",
      "(64, 33)\n",
      "step 23900, loss is 4.901330947875977\n",
      "(64, 33)\n",
      "step 23901, loss is 4.621094703674316\n",
      "(64, 33)\n",
      "step 23902, loss is 4.677478313446045\n",
      "(64, 33)\n",
      "step 23903, loss is 4.8657660484313965\n",
      "(64, 33)\n",
      "step 23904, loss is 4.815569877624512\n",
      "(64, 33)\n",
      "step 23905, loss is 4.473873615264893\n",
      "(64, 33)\n",
      "step 23906, loss is 4.827127456665039\n",
      "(64, 33)\n",
      "step 23907, loss is 4.749321460723877\n",
      "(64, 33)\n",
      "step 23908, loss is 4.622584819793701\n",
      "(64, 33)\n",
      "step 23909, loss is 4.726138114929199\n",
      "(64, 33)\n",
      "step 23910, loss is 4.709362506866455\n",
      "(64, 33)\n",
      "step 23911, loss is 4.830800533294678\n",
      "(64, 33)\n",
      "step 23912, loss is 4.9453558921813965\n",
      "(64, 33)\n",
      "step 23913, loss is 4.7340779304504395\n",
      "(64, 33)\n",
      "step 23914, loss is 4.622918605804443\n",
      "(64, 33)\n",
      "step 23915, loss is 4.77154541015625\n",
      "(64, 33)\n",
      "step 23916, loss is 4.793271541595459\n",
      "(64, 33)\n",
      "step 23917, loss is 4.89345121383667\n",
      "(64, 33)\n",
      "step 23918, loss is 4.730513095855713\n",
      "(64, 33)\n",
      "step 23919, loss is 4.745270729064941\n",
      "(64, 33)\n",
      "step 23920, loss is 4.702907085418701\n",
      "(64, 33)\n",
      "step 23921, loss is 4.862831115722656\n",
      "(64, 33)\n",
      "step 23922, loss is 4.69437837600708\n",
      "(64, 33)\n",
      "step 23923, loss is 4.761856555938721\n",
      "(64, 33)\n",
      "step 23924, loss is 4.839979648590088\n",
      "(64, 33)\n",
      "step 23925, loss is 4.7503886222839355\n",
      "(64, 33)\n",
      "step 23926, loss is 4.760061264038086\n",
      "(64, 33)\n",
      "step 23927, loss is 4.784353256225586\n",
      "(64, 33)\n",
      "step 23928, loss is 4.833770751953125\n",
      "(64, 33)\n",
      "step 23929, loss is 4.645001411437988\n",
      "(64, 33)\n",
      "step 23930, loss is 4.852055549621582\n",
      "(64, 33)\n",
      "step 23931, loss is 4.781466007232666\n",
      "(64, 33)\n",
      "step 23932, loss is 4.966053485870361\n",
      "(64, 33)\n",
      "step 23933, loss is 4.664445400238037\n",
      "(64, 33)\n",
      "step 23934, loss is 4.943240165710449\n",
      "(64, 33)\n",
      "step 23935, loss is 4.721729755401611\n",
      "(64, 33)\n",
      "step 23936, loss is 4.888973712921143\n",
      "(64, 33)\n",
      "step 23937, loss is 4.7284836769104\n",
      "(64, 33)\n",
      "step 23938, loss is 4.736503601074219\n",
      "(64, 33)\n",
      "step 23939, loss is 4.678168773651123\n",
      "(64, 33)\n",
      "step 23940, loss is 4.743880748748779\n",
      "(64, 33)\n",
      "step 23941, loss is 4.613704204559326\n",
      "(64, 33)\n",
      "step 23942, loss is 4.594295501708984\n",
      "(64, 33)\n",
      "step 23943, loss is 4.9537529945373535\n",
      "(64, 33)\n",
      "step 23944, loss is 4.784079551696777\n",
      "(64, 33)\n",
      "step 23945, loss is 4.728816509246826\n",
      "(64, 33)\n",
      "step 23946, loss is 4.908512115478516\n",
      "(64, 33)\n",
      "step 23947, loss is 4.7476325035095215\n",
      "(64, 33)\n",
      "step 23948, loss is 4.760031223297119\n",
      "(64, 33)\n",
      "step 23949, loss is 4.665503025054932\n",
      "(64, 33)\n",
      "step 23950, loss is 4.72745418548584\n",
      "(64, 33)\n",
      "step 23951, loss is 4.705080032348633\n",
      "(64, 33)\n",
      "step 23952, loss is 4.800786972045898\n",
      "(64, 33)\n",
      "step 23953, loss is 4.789257049560547\n",
      "(64, 33)\n",
      "step 23954, loss is 4.8880181312561035\n",
      "(64, 33)\n",
      "step 23955, loss is 4.631887435913086\n",
      "(64, 33)\n",
      "step 23956, loss is 4.695556163787842\n",
      "(64, 33)\n",
      "step 23957, loss is 4.76651668548584\n",
      "(64, 33)\n",
      "step 23958, loss is 4.645090103149414\n",
      "(64, 33)\n",
      "step 23959, loss is 4.870238780975342\n",
      "(64, 33)\n",
      "step 23960, loss is 4.8216023445129395\n",
      "(64, 33)\n",
      "step 23961, loss is 4.733782768249512\n",
      "(64, 33)\n",
      "step 23962, loss is 4.866339683532715\n",
      "(64, 33)\n",
      "step 23963, loss is 4.713696002960205\n",
      "(64, 33)\n",
      "step 23964, loss is 4.738777160644531\n",
      "(64, 33)\n",
      "step 23965, loss is 4.692380428314209\n",
      "(64, 33)\n",
      "step 23966, loss is 4.7417683601379395\n",
      "(64, 33)\n",
      "step 23967, loss is 4.7786865234375\n",
      "(64, 33)\n",
      "step 23968, loss is 4.634320259094238\n",
      "(64, 33)\n",
      "step 23969, loss is 4.756319999694824\n",
      "(64, 33)\n",
      "step 23970, loss is 4.689140796661377\n",
      "(64, 33)\n",
      "step 23971, loss is 4.7162957191467285\n",
      "(64, 33)\n",
      "step 23972, loss is 4.75460147857666\n",
      "(64, 33)\n",
      "step 23973, loss is 4.780720233917236\n",
      "(64, 33)\n",
      "step 23974, loss is 4.806874752044678\n",
      "(64, 33)\n",
      "step 23975, loss is 4.794113636016846\n",
      "(64, 33)\n",
      "step 23976, loss is 4.727395534515381\n",
      "(64, 33)\n",
      "step 23977, loss is 4.853240013122559\n",
      "(64, 33)\n",
      "step 23978, loss is 4.921402454376221\n",
      "(64, 33)\n",
      "step 23979, loss is 4.640179634094238\n",
      "(64, 33)\n",
      "step 23980, loss is 4.839909553527832\n",
      "(64, 33)\n",
      "step 23981, loss is 4.5466766357421875\n",
      "(64, 33)\n",
      "step 23982, loss is 4.959311008453369\n",
      "(64, 33)\n",
      "step 23983, loss is 4.719078063964844\n",
      "(64, 33)\n",
      "step 23984, loss is 4.663822174072266\n",
      "(64, 33)\n",
      "step 23985, loss is 4.78066349029541\n",
      "(64, 33)\n",
      "step 23986, loss is 4.75514554977417\n",
      "(64, 33)\n",
      "step 23987, loss is 4.771750450134277\n",
      "(64, 33)\n",
      "step 23988, loss is 4.939092636108398\n",
      "(64, 33)\n",
      "step 23989, loss is 4.734589576721191\n",
      "(64, 33)\n",
      "step 23990, loss is 4.817909240722656\n",
      "(64, 33)\n",
      "step 23991, loss is 4.856089115142822\n",
      "(64, 33)\n",
      "step 23992, loss is 4.700984001159668\n",
      "(64, 33)\n",
      "step 23993, loss is 4.738699436187744\n",
      "(64, 33)\n",
      "step 23994, loss is 4.719323635101318\n",
      "(64, 33)\n",
      "step 23995, loss is 4.869205474853516\n",
      "(64, 33)\n",
      "step 23996, loss is 4.701779365539551\n",
      "(64, 33)\n",
      "step 23997, loss is 4.491195201873779\n",
      "(64, 33)\n",
      "step 23998, loss is 4.80760383605957\n",
      "(64, 33)\n",
      "step 23999, loss is 4.775224685668945\n",
      "(64, 33)\n",
      "step 24000, loss is 4.965471267700195\n",
      "(64, 33)\n",
      "step 24001, loss is 4.8572516441345215\n",
      "(64, 33)\n",
      "step 24002, loss is 4.796623229980469\n",
      "(64, 33)\n",
      "step 24003, loss is 4.7588276863098145\n",
      "(64, 33)\n",
      "step 24004, loss is 4.742520332336426\n",
      "(64, 33)\n",
      "step 24005, loss is 4.679714202880859\n",
      "(64, 33)\n",
      "step 24006, loss is 4.779799938201904\n",
      "(64, 33)\n",
      "step 24007, loss is 4.752786636352539\n",
      "(64, 33)\n",
      "step 24008, loss is 4.903385162353516\n",
      "(64, 33)\n",
      "step 24009, loss is 4.742848873138428\n",
      "(64, 33)\n",
      "step 24010, loss is 4.753396987915039\n",
      "(64, 33)\n",
      "step 24011, loss is 4.66666841506958\n",
      "(64, 33)\n",
      "step 24012, loss is 4.8284382820129395\n",
      "(64, 33)\n",
      "step 24013, loss is 4.631445407867432\n",
      "(64, 33)\n",
      "step 24014, loss is 4.868323802947998\n",
      "(64, 33)\n",
      "step 24015, loss is 4.760575294494629\n",
      "(64, 33)\n",
      "step 24016, loss is 4.709446430206299\n",
      "(64, 33)\n",
      "step 24017, loss is 5.011653423309326\n",
      "(64, 33)\n",
      "step 24018, loss is 4.7475199699401855\n",
      "(64, 33)\n",
      "step 24019, loss is 4.9673285484313965\n",
      "(64, 33)\n",
      "step 24020, loss is 4.83248233795166\n",
      "(64, 33)\n",
      "step 24021, loss is 4.946449279785156\n",
      "(64, 33)\n",
      "step 24022, loss is 4.618760108947754\n",
      "(64, 33)\n",
      "step 24023, loss is 4.917272090911865\n",
      "(64, 33)\n",
      "step 24024, loss is 4.634878635406494\n",
      "(64, 33)\n",
      "step 24025, loss is 4.679256916046143\n",
      "(64, 33)\n",
      "step 24026, loss is 4.739552974700928\n",
      "(64, 33)\n",
      "step 24027, loss is 4.692017555236816\n",
      "(64, 33)\n",
      "step 24028, loss is 5.0444512367248535\n",
      "(64, 33)\n",
      "step 24029, loss is 4.978757381439209\n",
      "(64, 33)\n",
      "step 24030, loss is 4.895920276641846\n",
      "(64, 33)\n",
      "step 24031, loss is 4.764276504516602\n",
      "(64, 33)\n",
      "step 24032, loss is 4.840813636779785\n",
      "(64, 33)\n",
      "step 24033, loss is 4.8223395347595215\n",
      "(64, 33)\n",
      "step 24034, loss is 4.629379749298096\n",
      "(64, 33)\n",
      "step 24035, loss is 4.6108269691467285\n",
      "(64, 33)\n",
      "step 24036, loss is 4.468191146850586\n",
      "(64, 33)\n",
      "step 24037, loss is 4.8953680992126465\n",
      "(64, 33)\n",
      "step 24038, loss is 4.799404621124268\n",
      "(64, 33)\n",
      "step 24039, loss is 4.761784553527832\n",
      "(64, 33)\n",
      "step 24040, loss is 4.877639293670654\n",
      "(64, 33)\n",
      "step 24041, loss is 4.842940807342529\n",
      "(64, 33)\n",
      "step 24042, loss is 4.715907096862793\n",
      "(64, 33)\n",
      "step 24043, loss is 4.630681037902832\n",
      "(64, 33)\n",
      "step 24044, loss is 4.72393798828125\n",
      "(64, 33)\n",
      "step 24045, loss is 4.640957832336426\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24046, loss is 4.8547587394714355\n",
      "(64, 33)\n",
      "step 24047, loss is 4.894916534423828\n",
      "(64, 33)\n",
      "step 24048, loss is 4.64661169052124\n",
      "(64, 33)\n",
      "step 24049, loss is 4.760962009429932\n",
      "(64, 33)\n",
      "step 24050, loss is 4.7542548179626465\n",
      "(64, 33)\n",
      "step 24051, loss is 4.743337154388428\n",
      "(64, 33)\n",
      "step 24052, loss is 4.774196624755859\n",
      "(64, 33)\n",
      "step 24053, loss is 4.931025505065918\n",
      "(64, 33)\n",
      "step 24054, loss is 4.7552313804626465\n",
      "(64, 33)\n",
      "step 24055, loss is 4.803152561187744\n",
      "(64, 33)\n",
      "step 24056, loss is 4.765280246734619\n",
      "(64, 33)\n",
      "step 24057, loss is 4.692087650299072\n",
      "(64, 33)\n",
      "step 24058, loss is 4.882028102874756\n",
      "(64, 33)\n",
      "step 24059, loss is 4.854276657104492\n",
      "(64, 33)\n",
      "step 24060, loss is 4.702864170074463\n",
      "(64, 33)\n",
      "step 24061, loss is 4.720334053039551\n",
      "(64, 33)\n",
      "step 24062, loss is 4.89314079284668\n",
      "(64, 33)\n",
      "step 24063, loss is 4.6951680183410645\n",
      "(64, 33)\n",
      "step 24064, loss is 4.837240695953369\n",
      "(64, 33)\n",
      "step 24065, loss is 4.7935638427734375\n",
      "(64, 33)\n",
      "step 24066, loss is 4.741143226623535\n",
      "(64, 33)\n",
      "step 24067, loss is 4.744316101074219\n",
      "(64, 33)\n",
      "step 24068, loss is 4.678315162658691\n",
      "(64, 33)\n",
      "step 24069, loss is 4.687699317932129\n",
      "(64, 33)\n",
      "step 24070, loss is 4.961167812347412\n",
      "(64, 33)\n",
      "step 24071, loss is 4.859417915344238\n",
      "(64, 33)\n",
      "step 24072, loss is 4.786677837371826\n",
      "(64, 33)\n",
      "step 24073, loss is 4.921035289764404\n",
      "(64, 33)\n",
      "step 24074, loss is 4.919068336486816\n",
      "(64, 33)\n",
      "step 24075, loss is 4.778194427490234\n",
      "(64, 33)\n",
      "step 24076, loss is 4.603395462036133\n",
      "(64, 33)\n",
      "step 24077, loss is 4.612617492675781\n",
      "(64, 33)\n",
      "step 24078, loss is 4.78895902633667\n",
      "(64, 33)\n",
      "step 24079, loss is 4.831782341003418\n",
      "(64, 33)\n",
      "step 24080, loss is 4.8324055671691895\n",
      "(64, 33)\n",
      "step 24081, loss is 4.520060062408447\n",
      "(64, 33)\n",
      "step 24082, loss is 4.943572998046875\n",
      "(64, 33)\n",
      "step 24083, loss is 4.83837890625\n",
      "(64, 33)\n",
      "step 24084, loss is 4.766349792480469\n",
      "(64, 33)\n",
      "step 24085, loss is 4.9480509757995605\n",
      "(64, 33)\n",
      "step 24086, loss is 4.763415336608887\n",
      "(64, 33)\n",
      "step 24087, loss is 4.9777092933654785\n",
      "(64, 33)\n",
      "step 24088, loss is 4.831804275512695\n",
      "(64, 33)\n",
      "step 24089, loss is 4.797042369842529\n",
      "(64, 33)\n",
      "step 24090, loss is 4.780981063842773\n",
      "(64, 33)\n",
      "step 24091, loss is 4.784468173980713\n",
      "(64, 33)\n",
      "step 24092, loss is 4.856387138366699\n",
      "(64, 33)\n",
      "step 24093, loss is 4.812614440917969\n",
      "(64, 33)\n",
      "step 24094, loss is 5.020608901977539\n",
      "(64, 33)\n",
      "step 24095, loss is 4.79424524307251\n",
      "(64, 33)\n",
      "step 24096, loss is 4.713724136352539\n",
      "(64, 33)\n",
      "step 24097, loss is 4.79583215713501\n",
      "(64, 33)\n",
      "step 24098, loss is 4.848541736602783\n",
      "(64, 33)\n",
      "step 24099, loss is 4.786082744598389\n",
      "(64, 33)\n",
      "step 24100, loss is 4.795129776000977\n",
      "(64, 33)\n",
      "step 24101, loss is 4.931496620178223\n",
      "(64, 33)\n",
      "step 24102, loss is 4.716348171234131\n",
      "(64, 33)\n",
      "step 24103, loss is 4.685190200805664\n",
      "(64, 33)\n",
      "step 24104, loss is 4.810268878936768\n",
      "(64, 33)\n",
      "step 24105, loss is 4.663756847381592\n",
      "(64, 33)\n",
      "step 24106, loss is 4.905680179595947\n",
      "(64, 33)\n",
      "step 24107, loss is 4.7573933601379395\n",
      "(64, 33)\n",
      "step 24108, loss is 4.745216369628906\n",
      "(64, 33)\n",
      "step 24109, loss is 4.7404327392578125\n",
      "(64, 33)\n",
      "step 24110, loss is 4.656519412994385\n",
      "(64, 33)\n",
      "step 24111, loss is 4.872385501861572\n",
      "(64, 33)\n",
      "step 24112, loss is 4.832007884979248\n",
      "(64, 33)\n",
      "step 24113, loss is 4.9066667556762695\n",
      "(64, 33)\n",
      "step 24114, loss is 4.593782424926758\n",
      "(64, 33)\n",
      "step 24115, loss is 4.823948860168457\n",
      "(64, 33)\n",
      "step 24116, loss is 4.814230442047119\n",
      "(64, 33)\n",
      "step 24117, loss is 4.605310440063477\n",
      "(64, 33)\n",
      "step 24118, loss is 4.599108695983887\n",
      "(64, 33)\n",
      "step 24119, loss is 4.7209930419921875\n",
      "(64, 33)\n",
      "step 24120, loss is 4.717087268829346\n",
      "(64, 33)\n",
      "step 24121, loss is 4.675556659698486\n",
      "(64, 33)\n",
      "step 24122, loss is 4.783946990966797\n",
      "(64, 33)\n",
      "step 24123, loss is 4.739902019500732\n",
      "(64, 33)\n",
      "step 24124, loss is 4.705463886260986\n",
      "(64, 33)\n",
      "step 24125, loss is 4.833600997924805\n",
      "(64, 33)\n",
      "step 24126, loss is 4.813865661621094\n",
      "(64, 33)\n",
      "step 24127, loss is 5.041794300079346\n",
      "(64, 33)\n",
      "step 24128, loss is 4.874034404754639\n",
      "(64, 33)\n",
      "step 24129, loss is 4.854127883911133\n",
      "(64, 33)\n",
      "step 24130, loss is 4.699555397033691\n",
      "(64, 33)\n",
      "step 24131, loss is 4.736939907073975\n",
      "(64, 33)\n",
      "step 24132, loss is 4.825337886810303\n",
      "(64, 33)\n",
      "step 24133, loss is 4.916922569274902\n",
      "(64, 33)\n",
      "step 24134, loss is 4.914707183837891\n",
      "(64, 33)\n",
      "step 24135, loss is 4.858435153961182\n",
      "(64, 33)\n",
      "step 24136, loss is 4.844232082366943\n",
      "(64, 33)\n",
      "step 24137, loss is 4.8049211502075195\n",
      "(64, 33)\n",
      "step 24138, loss is 4.766902446746826\n",
      "(64, 33)\n",
      "step 24139, loss is 4.845964431762695\n",
      "(64, 33)\n",
      "step 24140, loss is 4.735719680786133\n",
      "(64, 33)\n",
      "step 24141, loss is 4.762179374694824\n",
      "(64, 33)\n",
      "step 24142, loss is 4.684077262878418\n",
      "(64, 33)\n",
      "step 24143, loss is 4.929499626159668\n",
      "(64, 33)\n",
      "step 24144, loss is 4.762016296386719\n",
      "(64, 33)\n",
      "step 24145, loss is 4.858822345733643\n",
      "(64, 33)\n",
      "step 24146, loss is 4.703958034515381\n",
      "(64, 33)\n",
      "step 24147, loss is 4.659098148345947\n",
      "(64, 33)\n",
      "step 24148, loss is 4.801562309265137\n",
      "(64, 33)\n",
      "step 24149, loss is 4.677271842956543\n",
      "(64, 33)\n",
      "step 24150, loss is 4.857187747955322\n",
      "(64, 33)\n",
      "step 24151, loss is 4.678661823272705\n",
      "(64, 33)\n",
      "step 24152, loss is 4.730687618255615\n",
      "(64, 33)\n",
      "step 24153, loss is 4.802847862243652\n",
      "(64, 33)\n",
      "step 24154, loss is 4.7909722328186035\n",
      "(64, 33)\n",
      "step 24155, loss is 4.676784515380859\n",
      "(64, 33)\n",
      "step 24156, loss is 4.941150188446045\n",
      "(64, 33)\n",
      "step 24157, loss is 4.763704299926758\n",
      "(64, 33)\n",
      "step 24158, loss is 4.9119415283203125\n",
      "(64, 33)\n",
      "step 24159, loss is 4.670699596405029\n",
      "(64, 33)\n",
      "step 24160, loss is 4.834636688232422\n",
      "(64, 33)\n",
      "step 24161, loss is 4.774894714355469\n",
      "(64, 33)\n",
      "step 24162, loss is 4.733604431152344\n",
      "(64, 33)\n",
      "step 24163, loss is 5.092288970947266\n",
      "(64, 33)\n",
      "step 24164, loss is 4.4700140953063965\n",
      "(64, 33)\n",
      "step 24165, loss is 4.742176055908203\n",
      "(64, 33)\n",
      "step 24166, loss is 4.656707763671875\n",
      "(64, 33)\n",
      "step 24167, loss is 4.872419357299805\n",
      "(64, 33)\n",
      "step 24168, loss is 4.673457145690918\n",
      "(64, 33)\n",
      "step 24169, loss is 4.84127950668335\n",
      "(64, 33)\n",
      "step 24170, loss is 4.674413681030273\n",
      "(64, 33)\n",
      "step 24171, loss is 4.758631706237793\n",
      "(64, 33)\n",
      "step 24172, loss is 4.731937885284424\n",
      "(64, 33)\n",
      "step 24173, loss is 4.738637447357178\n",
      "(64, 33)\n",
      "step 24174, loss is 4.686934947967529\n",
      "(64, 33)\n",
      "step 24175, loss is 4.679280757904053\n",
      "(64, 33)\n",
      "step 24176, loss is 4.952148914337158\n",
      "(64, 33)\n",
      "step 24177, loss is 4.829164028167725\n",
      "(64, 33)\n",
      "step 24178, loss is 4.911398410797119\n",
      "(64, 33)\n",
      "step 24179, loss is 4.716636657714844\n",
      "(64, 33)\n",
      "step 24180, loss is 4.836626052856445\n",
      "(64, 33)\n",
      "step 24181, loss is 4.893881797790527\n",
      "(64, 33)\n",
      "step 24182, loss is 4.83436918258667\n",
      "(64, 33)\n",
      "step 24183, loss is 4.913285732269287\n",
      "(64, 33)\n",
      "step 24184, loss is 4.745835781097412\n",
      "(64, 33)\n",
      "step 24185, loss is 4.980829238891602\n",
      "(64, 33)\n",
      "step 24186, loss is 4.751987457275391\n",
      "(64, 33)\n",
      "step 24187, loss is 4.900734901428223\n",
      "(64, 33)\n",
      "step 24188, loss is 4.838661193847656\n",
      "(64, 33)\n",
      "step 24189, loss is 4.712741374969482\n",
      "(64, 33)\n",
      "step 24190, loss is 4.997958183288574\n",
      "(64, 33)\n",
      "step 24191, loss is 4.78413724899292\n",
      "(64, 33)\n",
      "step 24192, loss is 4.86883544921875\n",
      "(64, 33)\n",
      "step 24193, loss is 4.751998424530029\n",
      "(64, 33)\n",
      "step 24194, loss is 4.800382614135742\n",
      "(64, 33)\n",
      "step 24195, loss is 4.835530757904053\n",
      "(64, 33)\n",
      "step 24196, loss is 4.825047492980957\n",
      "(64, 33)\n",
      "step 24197, loss is 4.819757461547852\n",
      "(64, 33)\n",
      "step 24198, loss is 4.759834289550781\n",
      "(64, 33)\n",
      "step 24199, loss is 4.681373596191406\n",
      "(64, 33)\n",
      "step 24200, loss is 4.606260299682617\n",
      "(64, 33)\n",
      "step 24201, loss is 4.792708396911621\n",
      "(64, 33)\n",
      "step 24202, loss is 5.03833532333374\n",
      "(64, 33)\n",
      "step 24203, loss is 4.7050933837890625\n",
      "(64, 33)\n",
      "step 24204, loss is 4.758781433105469\n",
      "(64, 33)\n",
      "step 24205, loss is 4.802406311035156\n",
      "(64, 33)\n",
      "step 24206, loss is 4.680375576019287\n",
      "(64, 33)\n",
      "step 24207, loss is 4.723799705505371\n",
      "(64, 33)\n",
      "step 24208, loss is 4.878922939300537\n",
      "(64, 33)\n",
      "step 24209, loss is 4.866486072540283\n",
      "(64, 33)\n",
      "step 24210, loss is 4.685798645019531\n",
      "(64, 33)\n",
      "step 24211, loss is 4.615408897399902\n",
      "(64, 33)\n",
      "step 24212, loss is 4.736840724945068\n",
      "(64, 33)\n",
      "step 24213, loss is 4.886441230773926\n",
      "(64, 33)\n",
      "step 24214, loss is 4.678585052490234\n",
      "(64, 33)\n",
      "step 24215, loss is 4.83262300491333\n",
      "(64, 33)\n",
      "step 24216, loss is 4.813220977783203\n",
      "(64, 33)\n",
      "step 24217, loss is 4.706816673278809\n",
      "(64, 33)\n",
      "step 24218, loss is 4.727745532989502\n",
      "(64, 33)\n",
      "step 24219, loss is 4.885864734649658\n",
      "(64, 33)\n",
      "step 24220, loss is 4.728413105010986\n",
      "(64, 33)\n",
      "step 24221, loss is 4.694941520690918\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24222, loss is 4.8560590744018555\n",
      "(64, 33)\n",
      "step 24223, loss is 4.846010684967041\n",
      "(64, 33)\n",
      "step 24224, loss is 4.741728782653809\n",
      "(64, 33)\n",
      "step 24225, loss is 4.839863300323486\n",
      "(64, 33)\n",
      "step 24226, loss is 4.7519025802612305\n",
      "(64, 33)\n",
      "step 24227, loss is 4.80916166305542\n",
      "(64, 33)\n",
      "step 24228, loss is 4.71396541595459\n",
      "(64, 33)\n",
      "step 24229, loss is 5.0115509033203125\n",
      "(64, 33)\n",
      "step 24230, loss is 4.764055252075195\n",
      "(64, 33)\n",
      "step 24231, loss is 4.774488925933838\n",
      "(64, 33)\n",
      "step 24232, loss is 4.620567321777344\n",
      "(64, 33)\n",
      "step 24233, loss is 4.927379131317139\n",
      "(64, 33)\n",
      "step 24234, loss is 4.849503993988037\n",
      "(64, 33)\n",
      "step 24235, loss is 4.817928314208984\n",
      "(64, 33)\n",
      "step 24236, loss is 4.702471733093262\n",
      "(64, 33)\n",
      "step 24237, loss is 4.827764511108398\n",
      "(64, 33)\n",
      "step 24238, loss is 4.959481716156006\n",
      "(64, 33)\n",
      "step 24239, loss is 4.850579261779785\n",
      "(64, 33)\n",
      "step 24240, loss is 4.834859371185303\n",
      "(64, 33)\n",
      "step 24241, loss is 4.79314661026001\n",
      "(64, 33)\n",
      "step 24242, loss is 4.842774391174316\n",
      "(64, 33)\n",
      "step 24243, loss is 4.79741096496582\n",
      "(64, 33)\n",
      "step 24244, loss is 4.7834978103637695\n",
      "(64, 33)\n",
      "step 24245, loss is 4.654990196228027\n",
      "(64, 33)\n",
      "step 24246, loss is 4.65695333480835\n",
      "(64, 33)\n",
      "step 24247, loss is 4.737588882446289\n",
      "(64, 33)\n",
      "step 24248, loss is 4.824610233306885\n",
      "(64, 33)\n",
      "step 24249, loss is 4.990146636962891\n",
      "(64, 33)\n",
      "step 24250, loss is 4.815433502197266\n",
      "(64, 33)\n",
      "step 24251, loss is 4.976782321929932\n",
      "(64, 33)\n",
      "step 24252, loss is 4.788373947143555\n",
      "(64, 33)\n",
      "step 24253, loss is 4.863908767700195\n",
      "(64, 33)\n",
      "step 24254, loss is 4.720494270324707\n",
      "(64, 33)\n",
      "step 24255, loss is 4.806762218475342\n",
      "(64, 33)\n",
      "step 24256, loss is 4.693597316741943\n",
      "(64, 33)\n",
      "step 24257, loss is 4.625138282775879\n",
      "(64, 33)\n",
      "step 24258, loss is 4.748538494110107\n",
      "(64, 33)\n",
      "step 24259, loss is 4.785400867462158\n",
      "(64, 33)\n",
      "step 24260, loss is 4.698735237121582\n",
      "(64, 33)\n",
      "step 24261, loss is 4.741579055786133\n",
      "(64, 33)\n",
      "step 24262, loss is 4.764262676239014\n",
      "(64, 33)\n",
      "step 24263, loss is 4.7307634353637695\n",
      "(64, 33)\n",
      "step 24264, loss is 4.894864559173584\n",
      "(64, 33)\n",
      "step 24265, loss is 4.809459209442139\n",
      "(64, 33)\n",
      "step 24266, loss is 4.859544277191162\n",
      "(64, 33)\n",
      "step 24267, loss is 4.792701721191406\n",
      "(64, 33)\n",
      "step 24268, loss is 4.741361141204834\n",
      "(64, 33)\n",
      "step 24269, loss is 4.758901119232178\n",
      "(64, 33)\n",
      "step 24270, loss is 4.941648960113525\n",
      "(64, 33)\n",
      "step 24271, loss is 4.9183526039123535\n",
      "(64, 33)\n",
      "step 24272, loss is 4.692458152770996\n",
      "(64, 33)\n",
      "step 24273, loss is 4.886962890625\n",
      "(64, 33)\n",
      "step 24274, loss is 4.749364376068115\n",
      "(64, 33)\n",
      "step 24275, loss is 4.69714879989624\n",
      "(64, 33)\n",
      "step 24276, loss is 4.686458110809326\n",
      "(64, 33)\n",
      "step 24277, loss is 4.643099308013916\n",
      "(64, 33)\n",
      "step 24278, loss is 4.828603267669678\n",
      "(64, 33)\n",
      "step 24279, loss is 4.901604652404785\n",
      "(64, 33)\n",
      "step 24280, loss is 4.568304538726807\n",
      "(64, 33)\n",
      "step 24281, loss is 4.68266487121582\n",
      "(64, 33)\n",
      "step 24282, loss is 4.886377811431885\n",
      "(64, 33)\n",
      "step 24283, loss is 4.800491809844971\n",
      "(64, 33)\n",
      "step 24284, loss is 4.908682346343994\n",
      "(64, 33)\n",
      "step 24285, loss is 4.836621284484863\n",
      "(64, 33)\n",
      "step 24286, loss is 4.697869777679443\n",
      "(64, 33)\n",
      "step 24287, loss is 5.012012481689453\n",
      "(64, 33)\n",
      "step 24288, loss is 4.889869213104248\n",
      "(64, 33)\n",
      "step 24289, loss is 4.743007659912109\n",
      "(64, 33)\n",
      "step 24290, loss is 4.683187007904053\n",
      "(64, 33)\n",
      "step 24291, loss is 4.793095111846924\n",
      "(64, 33)\n",
      "step 24292, loss is 4.661745548248291\n",
      "(64, 33)\n",
      "step 24293, loss is 4.8759894371032715\n",
      "(64, 33)\n",
      "step 24294, loss is 4.62161111831665\n",
      "(64, 33)\n",
      "step 24295, loss is 4.788485527038574\n",
      "(64, 33)\n",
      "step 24296, loss is 4.8238911628723145\n",
      "(64, 33)\n",
      "step 24297, loss is 4.818320274353027\n",
      "(64, 33)\n",
      "step 24298, loss is 4.74677848815918\n",
      "(64, 33)\n",
      "step 24299, loss is 4.706729412078857\n",
      "(64, 33)\n",
      "step 24300, loss is 4.757739543914795\n",
      "(64, 33)\n",
      "step 24301, loss is 4.79905891418457\n",
      "(64, 33)\n",
      "step 24302, loss is 4.796370506286621\n",
      "(64, 33)\n",
      "step 24303, loss is 4.721846580505371\n",
      "(64, 33)\n",
      "step 24304, loss is 4.890226364135742\n",
      "(64, 33)\n",
      "step 24305, loss is 4.697874546051025\n",
      "(64, 33)\n",
      "step 24306, loss is 4.803300857543945\n",
      "(64, 33)\n",
      "step 24307, loss is 4.777418613433838\n",
      "(64, 33)\n",
      "step 24308, loss is 4.866020202636719\n",
      "(64, 33)\n",
      "step 24309, loss is 4.800717353820801\n",
      "(64, 33)\n",
      "step 24310, loss is 4.799654006958008\n",
      "(64, 33)\n",
      "step 24311, loss is 4.924854755401611\n",
      "(64, 33)\n",
      "step 24312, loss is 4.744054794311523\n",
      "(64, 33)\n",
      "step 24313, loss is 4.789809703826904\n",
      "(64, 33)\n",
      "step 24314, loss is 4.645050525665283\n",
      "(64, 33)\n",
      "step 24315, loss is 4.818291187286377\n",
      "(64, 33)\n",
      "step 24316, loss is 4.655337333679199\n",
      "(64, 33)\n",
      "step 24317, loss is 4.612327575683594\n",
      "(64, 33)\n",
      "step 24318, loss is 4.71123743057251\n",
      "(64, 33)\n",
      "step 24319, loss is 4.814711093902588\n",
      "(64, 33)\n",
      "step 24320, loss is 4.7189202308654785\n",
      "(64, 33)\n",
      "step 24321, loss is 4.8581719398498535\n",
      "(64, 33)\n",
      "step 24322, loss is 4.876938343048096\n",
      "(64, 33)\n",
      "step 24323, loss is 4.755862712860107\n",
      "(64, 33)\n",
      "step 24324, loss is 4.695284366607666\n",
      "(64, 33)\n",
      "step 24325, loss is 4.836915969848633\n",
      "(64, 33)\n",
      "step 24326, loss is 4.7755446434021\n",
      "(64, 33)\n",
      "step 24327, loss is 4.7612175941467285\n",
      "(64, 33)\n",
      "step 24328, loss is 4.682472229003906\n",
      "(64, 33)\n",
      "step 24329, loss is 4.781409740447998\n",
      "(64, 33)\n",
      "step 24330, loss is 4.744232177734375\n",
      "(64, 33)\n",
      "step 24331, loss is 4.847553253173828\n",
      "(64, 33)\n",
      "step 24332, loss is 4.788125038146973\n",
      "(64, 33)\n",
      "step 24333, loss is 4.873897552490234\n",
      "(64, 33)\n",
      "step 24334, loss is 4.844513416290283\n",
      "(64, 33)\n",
      "step 24335, loss is 4.873944282531738\n",
      "(64, 33)\n",
      "step 24336, loss is 4.868278503417969\n",
      "(64, 33)\n",
      "step 24337, loss is 4.840043544769287\n",
      "(64, 33)\n",
      "step 24338, loss is 4.654638767242432\n",
      "(64, 33)\n",
      "step 24339, loss is 4.8380446434021\n",
      "(64, 33)\n",
      "step 24340, loss is 4.722147464752197\n",
      "(64, 33)\n",
      "step 24341, loss is 4.934889793395996\n",
      "(64, 33)\n",
      "step 24342, loss is 4.719979286193848\n",
      "(64, 33)\n",
      "step 24343, loss is 4.784882068634033\n",
      "(64, 33)\n",
      "step 24344, loss is 4.703606605529785\n",
      "(64, 33)\n",
      "step 24345, loss is 4.703567028045654\n",
      "(64, 33)\n",
      "step 24346, loss is 4.896625995635986\n",
      "(64, 33)\n",
      "step 24347, loss is 4.744768142700195\n",
      "(64, 33)\n",
      "step 24348, loss is 4.717814922332764\n",
      "(64, 33)\n",
      "step 24349, loss is 4.704730987548828\n",
      "(64, 33)\n",
      "step 24350, loss is 4.866838455200195\n",
      "(64, 33)\n",
      "step 24351, loss is 4.710750102996826\n",
      "(64, 33)\n",
      "step 24352, loss is 4.843706130981445\n",
      "(64, 33)\n",
      "step 24353, loss is 4.8292765617370605\n",
      "(64, 33)\n",
      "step 24354, loss is 4.776507377624512\n",
      "(64, 33)\n",
      "step 24355, loss is 4.749714374542236\n",
      "(64, 33)\n",
      "step 24356, loss is 4.76447868347168\n",
      "(64, 33)\n",
      "step 24357, loss is 4.930673599243164\n",
      "(64, 33)\n",
      "step 24358, loss is 4.920356750488281\n",
      "(64, 33)\n",
      "step 24359, loss is 4.865268707275391\n",
      "(64, 33)\n",
      "step 24360, loss is 4.917388916015625\n",
      "(64, 33)\n",
      "step 24361, loss is 4.757803440093994\n",
      "(64, 33)\n",
      "step 24362, loss is 4.803705215454102\n",
      "(64, 33)\n",
      "step 24363, loss is 4.751498699188232\n",
      "(64, 33)\n",
      "step 24364, loss is 4.908613681793213\n",
      "(64, 33)\n",
      "step 24365, loss is 4.826866626739502\n",
      "(64, 33)\n",
      "step 24366, loss is 4.673407077789307\n",
      "(64, 33)\n",
      "step 24367, loss is 4.690583229064941\n",
      "(64, 33)\n",
      "step 24368, loss is 4.699899673461914\n",
      "(64, 33)\n",
      "step 24369, loss is 4.754756450653076\n",
      "(64, 33)\n",
      "step 24370, loss is 4.783121585845947\n",
      "(64, 33)\n",
      "step 24371, loss is 4.767680644989014\n",
      "(64, 33)\n",
      "step 24372, loss is 4.89349365234375\n",
      "(64, 33)\n",
      "step 24373, loss is 4.632135391235352\n",
      "(64, 33)\n",
      "step 24374, loss is 4.751535892486572\n",
      "(64, 33)\n",
      "step 24375, loss is 4.848848819732666\n",
      "(64, 33)\n",
      "step 24376, loss is 4.894920349121094\n",
      "(64, 33)\n",
      "step 24377, loss is 4.6984148025512695\n",
      "(64, 33)\n",
      "step 24378, loss is 4.65749454498291\n",
      "(64, 33)\n",
      "step 24379, loss is 4.776693820953369\n",
      "(64, 33)\n",
      "step 24380, loss is 4.90084171295166\n",
      "(64, 33)\n",
      "step 24381, loss is 4.887406349182129\n",
      "(64, 33)\n",
      "step 24382, loss is 4.713934421539307\n",
      "(64, 33)\n",
      "step 24383, loss is 4.770852088928223\n",
      "(64, 33)\n",
      "step 24384, loss is 4.745667934417725\n",
      "(64, 33)\n",
      "step 24385, loss is 4.782005786895752\n",
      "(64, 33)\n",
      "step 24386, loss is 4.749087333679199\n",
      "(64, 33)\n",
      "step 24387, loss is 4.81964111328125\n",
      "(64, 33)\n",
      "step 24388, loss is 4.582862377166748\n",
      "(64, 33)\n",
      "step 24389, loss is 4.612083435058594\n",
      "(64, 33)\n",
      "step 24390, loss is 4.617555618286133\n",
      "(64, 33)\n",
      "step 24391, loss is 4.837770938873291\n",
      "(64, 33)\n",
      "step 24392, loss is 4.823977470397949\n",
      "(64, 33)\n",
      "step 24393, loss is 4.939447402954102\n",
      "(64, 33)\n",
      "step 24394, loss is 4.701719284057617\n",
      "(64, 33)\n",
      "step 24395, loss is 4.868340969085693\n",
      "(64, 33)\n",
      "step 24396, loss is 4.934843063354492\n",
      "(64, 33)\n",
      "step 24397, loss is 4.753425598144531\n",
      "(64, 33)\n",
      "step 24398, loss is 4.617107391357422\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24399, loss is 4.867387294769287\n",
      "(64, 33)\n",
      "step 24400, loss is 4.704845428466797\n",
      "(64, 33)\n",
      "step 24401, loss is 4.801036357879639\n",
      "(64, 33)\n",
      "step 24402, loss is 4.8433356285095215\n",
      "(64, 33)\n",
      "step 24403, loss is 4.639719486236572\n",
      "(64, 33)\n",
      "step 24404, loss is 4.842950820922852\n",
      "(64, 33)\n",
      "step 24405, loss is 4.805349349975586\n",
      "(64, 33)\n",
      "step 24406, loss is 4.878026962280273\n",
      "(64, 33)\n",
      "step 24407, loss is 4.528125762939453\n",
      "(64, 33)\n",
      "step 24408, loss is 4.783690929412842\n",
      "(64, 33)\n",
      "step 24409, loss is 4.963007926940918\n",
      "(64, 33)\n",
      "step 24410, loss is 5.029770374298096\n",
      "(64, 33)\n",
      "step 24411, loss is 4.687067031860352\n",
      "(64, 33)\n",
      "step 24412, loss is 4.615148544311523\n",
      "(64, 33)\n",
      "step 24413, loss is 4.544070720672607\n",
      "(64, 33)\n",
      "step 24414, loss is 4.774528980255127\n",
      "(64, 33)\n",
      "step 24415, loss is 4.721931457519531\n",
      "(64, 33)\n",
      "step 24416, loss is 4.870304584503174\n",
      "(64, 33)\n",
      "step 24417, loss is 4.848210334777832\n",
      "(64, 33)\n",
      "step 24418, loss is 4.71669864654541\n",
      "(64, 33)\n",
      "step 24419, loss is 4.728260517120361\n",
      "(64, 33)\n",
      "step 24420, loss is 4.593398571014404\n",
      "(64, 33)\n",
      "step 24421, loss is 4.825705051422119\n",
      "(64, 33)\n",
      "step 24422, loss is 4.904051780700684\n",
      "(64, 33)\n",
      "step 24423, loss is 4.858196258544922\n",
      "(64, 33)\n",
      "step 24424, loss is 4.935003757476807\n",
      "(64, 33)\n",
      "step 24425, loss is 4.613219738006592\n",
      "(64, 33)\n",
      "step 24426, loss is 4.700497627258301\n",
      "(64, 33)\n",
      "step 24427, loss is 4.8097405433654785\n",
      "(64, 33)\n",
      "step 24428, loss is 4.716705799102783\n",
      "(64, 33)\n",
      "step 24429, loss is 4.792472839355469\n",
      "(64, 33)\n",
      "step 24430, loss is 4.801242351531982\n",
      "(64, 33)\n",
      "step 24431, loss is 4.7666120529174805\n",
      "(64, 33)\n",
      "step 24432, loss is 4.903793811798096\n",
      "(64, 33)\n",
      "step 24433, loss is 4.725753307342529\n",
      "(64, 33)\n",
      "step 24434, loss is 4.914821624755859\n",
      "(64, 33)\n",
      "step 24435, loss is 4.75374698638916\n",
      "(64, 33)\n",
      "step 24436, loss is 4.86269474029541\n",
      "(64, 33)\n",
      "step 24437, loss is 4.766286849975586\n",
      "(64, 33)\n",
      "step 24438, loss is 5.009316921234131\n",
      "(64, 33)\n",
      "step 24439, loss is 4.706286907196045\n",
      "(64, 33)\n",
      "step 24440, loss is 5.017563343048096\n",
      "(64, 33)\n",
      "step 24441, loss is 4.719554901123047\n",
      "(64, 33)\n",
      "step 24442, loss is 4.700187683105469\n",
      "(64, 33)\n",
      "step 24443, loss is 4.807281494140625\n",
      "(64, 33)\n",
      "step 24444, loss is 4.624077320098877\n",
      "(64, 33)\n",
      "step 24445, loss is 5.021850109100342\n",
      "(64, 33)\n",
      "step 24446, loss is 4.873464107513428\n",
      "(64, 33)\n",
      "step 24447, loss is 4.716725826263428\n",
      "(64, 33)\n",
      "step 24448, loss is 4.818909645080566\n",
      "(64, 33)\n",
      "step 24449, loss is 4.7910685539245605\n",
      "(64, 33)\n",
      "step 24450, loss is 4.74630880355835\n",
      "(64, 33)\n",
      "step 24451, loss is 4.770310878753662\n",
      "(64, 33)\n",
      "step 24452, loss is 4.774574279785156\n",
      "(64, 33)\n",
      "step 24453, loss is 4.694540023803711\n",
      "(64, 33)\n",
      "step 24454, loss is 4.80367374420166\n",
      "(64, 33)\n",
      "step 24455, loss is 4.880699157714844\n",
      "(64, 33)\n",
      "step 24456, loss is 4.739962577819824\n",
      "(64, 33)\n",
      "step 24457, loss is 4.959779739379883\n",
      "(64, 33)\n",
      "step 24458, loss is 4.709506511688232\n",
      "(64, 33)\n",
      "step 24459, loss is 4.863579750061035\n",
      "(64, 33)\n",
      "step 24460, loss is 4.741209506988525\n",
      "(64, 33)\n",
      "step 24461, loss is 4.499070167541504\n",
      "(64, 33)\n",
      "step 24462, loss is 4.78383731842041\n",
      "(64, 33)\n",
      "step 24463, loss is 4.769069671630859\n",
      "(64, 33)\n",
      "step 24464, loss is 4.725965976715088\n",
      "(64, 33)\n",
      "step 24465, loss is 4.748738765716553\n",
      "(64, 33)\n",
      "step 24466, loss is 4.882194995880127\n",
      "(64, 33)\n",
      "step 24467, loss is 4.873497009277344\n",
      "(64, 33)\n",
      "step 24468, loss is 4.700814247131348\n",
      "(64, 33)\n",
      "step 24469, loss is 4.820957183837891\n",
      "(64, 33)\n",
      "step 24470, loss is 4.755269527435303\n",
      "(64, 33)\n",
      "step 24471, loss is 4.928622245788574\n",
      "(64, 33)\n",
      "step 24472, loss is 4.779886722564697\n",
      "(64, 33)\n",
      "step 24473, loss is 4.86090087890625\n",
      "(64, 33)\n",
      "step 24474, loss is 4.6645121574401855\n",
      "(64, 33)\n",
      "step 24475, loss is 4.69241189956665\n",
      "(64, 33)\n",
      "step 24476, loss is 4.682742118835449\n",
      "(64, 33)\n",
      "step 24477, loss is 4.8546576499938965\n",
      "(64, 33)\n",
      "step 24478, loss is 4.753781795501709\n",
      "(64, 33)\n",
      "step 24479, loss is 4.839687347412109\n",
      "(64, 33)\n",
      "step 24480, loss is 4.6548004150390625\n",
      "(64, 33)\n",
      "step 24481, loss is 4.772709369659424\n",
      "(64, 33)\n",
      "step 24482, loss is 4.868539333343506\n",
      "(64, 33)\n",
      "step 24483, loss is 4.717092037200928\n",
      "(64, 33)\n",
      "step 24484, loss is 4.771273136138916\n",
      "(64, 33)\n",
      "step 24485, loss is 4.744192123413086\n",
      "(64, 33)\n",
      "step 24486, loss is 4.896364688873291\n",
      "(64, 33)\n",
      "step 24487, loss is 4.721073627471924\n",
      "(64, 33)\n",
      "step 24488, loss is 4.755566596984863\n",
      "(64, 33)\n",
      "step 24489, loss is 4.935389518737793\n",
      "(64, 33)\n",
      "step 24490, loss is 4.845780849456787\n",
      "(64, 33)\n",
      "step 24491, loss is 4.709996700286865\n",
      "(64, 33)\n",
      "step 24492, loss is 4.819819450378418\n",
      "(64, 33)\n",
      "step 24493, loss is 4.867597579956055\n",
      "(64, 33)\n",
      "step 24494, loss is 4.707709312438965\n",
      "(64, 33)\n",
      "step 24495, loss is 4.711822509765625\n",
      "(64, 33)\n",
      "step 24496, loss is 4.973178863525391\n",
      "(64, 33)\n",
      "step 24497, loss is 4.886622428894043\n",
      "(64, 33)\n",
      "step 24498, loss is 4.838849067687988\n",
      "(64, 33)\n",
      "step 24499, loss is 4.938076019287109\n",
      "(64, 33)\n",
      "step 24500, loss is 4.9506025314331055\n",
      "(64, 33)\n",
      "step 24501, loss is 4.752311706542969\n",
      "(64, 33)\n",
      "step 24502, loss is 4.591780662536621\n",
      "(64, 33)\n",
      "step 24503, loss is 4.83593225479126\n",
      "(64, 33)\n",
      "step 24504, loss is 4.670750617980957\n",
      "(64, 33)\n",
      "step 24505, loss is 4.931125640869141\n",
      "(64, 33)\n",
      "step 24506, loss is 4.913344860076904\n",
      "(64, 33)\n",
      "step 24507, loss is 4.935818672180176\n",
      "(64, 33)\n",
      "step 24508, loss is 4.861083507537842\n",
      "(64, 33)\n",
      "step 24509, loss is 4.713611125946045\n",
      "(64, 33)\n",
      "step 24510, loss is 4.7336602210998535\n",
      "(64, 33)\n",
      "step 24511, loss is 4.875175476074219\n",
      "(64, 33)\n",
      "step 24512, loss is 4.812267303466797\n",
      "(64, 33)\n",
      "step 24513, loss is 4.776833534240723\n",
      "(64, 33)\n",
      "step 24514, loss is 4.811788558959961\n",
      "(64, 33)\n",
      "step 24515, loss is 4.639222621917725\n",
      "(64, 33)\n",
      "step 24516, loss is 4.784082412719727\n",
      "(64, 33)\n",
      "step 24517, loss is 4.834651947021484\n",
      "(64, 33)\n",
      "step 24518, loss is 4.621025085449219\n",
      "(64, 33)\n",
      "step 24519, loss is 4.8047051429748535\n",
      "(64, 33)\n",
      "step 24520, loss is 4.651589870452881\n",
      "(64, 33)\n",
      "step 24521, loss is 4.731563091278076\n",
      "(64, 33)\n",
      "step 24522, loss is 4.644169330596924\n",
      "(64, 33)\n",
      "step 24523, loss is 4.759944438934326\n",
      "(64, 33)\n",
      "step 24524, loss is 4.9207353591918945\n",
      "(64, 33)\n",
      "step 24525, loss is 4.773897171020508\n",
      "(64, 33)\n",
      "step 24526, loss is 4.805903434753418\n",
      "(64, 33)\n",
      "step 24527, loss is 4.723580837249756\n",
      "(64, 33)\n",
      "step 24528, loss is 4.613705635070801\n",
      "(64, 33)\n",
      "step 24529, loss is 4.750169277191162\n",
      "(64, 33)\n",
      "step 24530, loss is 4.782725811004639\n",
      "(64, 33)\n",
      "step 24531, loss is 4.8856048583984375\n",
      "(64, 33)\n",
      "step 24532, loss is 4.7242655754089355\n",
      "(64, 33)\n",
      "step 24533, loss is 4.734416961669922\n",
      "(64, 33)\n",
      "step 24534, loss is 4.866074562072754\n",
      "(64, 33)\n",
      "step 24535, loss is 4.720281600952148\n",
      "(64, 33)\n",
      "step 24536, loss is 4.802700996398926\n",
      "(64, 33)\n",
      "step 24537, loss is 4.7130889892578125\n",
      "(64, 33)\n",
      "step 24538, loss is 4.730552673339844\n",
      "(64, 33)\n",
      "step 24539, loss is 4.9240570068359375\n",
      "(64, 33)\n",
      "step 24540, loss is 4.622648239135742\n",
      "(64, 33)\n",
      "step 24541, loss is 4.683627128601074\n",
      "(64, 33)\n",
      "step 24542, loss is 4.761915683746338\n",
      "(64, 33)\n",
      "step 24543, loss is 4.81045389175415\n",
      "(64, 33)\n",
      "step 24544, loss is 4.855990409851074\n",
      "(64, 33)\n",
      "step 24545, loss is 4.82354211807251\n",
      "(64, 33)\n",
      "step 24546, loss is 4.841692924499512\n",
      "(64, 33)\n",
      "step 24547, loss is 4.616682529449463\n",
      "(64, 33)\n",
      "step 24548, loss is 4.82125997543335\n",
      "(64, 33)\n",
      "step 24549, loss is 4.900176525115967\n",
      "(64, 33)\n",
      "step 24550, loss is 4.946125507354736\n",
      "(64, 33)\n",
      "step 24551, loss is 4.802311420440674\n",
      "(64, 33)\n",
      "step 24552, loss is 4.742353439331055\n",
      "(64, 33)\n",
      "step 24553, loss is 4.550687313079834\n",
      "(64, 33)\n",
      "step 24554, loss is 4.678226947784424\n",
      "(64, 33)\n",
      "step 24555, loss is 4.707888126373291\n",
      "(64, 33)\n",
      "step 24556, loss is 4.835921764373779\n",
      "(64, 33)\n",
      "step 24557, loss is 4.918356418609619\n",
      "(64, 33)\n",
      "step 24558, loss is 4.703887939453125\n",
      "(64, 33)\n",
      "step 24559, loss is 4.820117950439453\n",
      "(64, 33)\n",
      "step 24560, loss is 4.705392360687256\n",
      "(64, 33)\n",
      "step 24561, loss is 4.720414638519287\n",
      "(64, 33)\n",
      "step 24562, loss is 4.8461995124816895\n",
      "(64, 33)\n",
      "step 24563, loss is 4.997543811798096\n",
      "(64, 33)\n",
      "step 24564, loss is 4.683434009552002\n",
      "(64, 33)\n",
      "step 24565, loss is 4.745901584625244\n",
      "(64, 33)\n",
      "step 24566, loss is 4.7958173751831055\n",
      "(64, 33)\n",
      "step 24567, loss is 4.955424785614014\n",
      "(64, 33)\n",
      "step 24568, loss is 4.718900203704834\n",
      "(64, 33)\n",
      "step 24569, loss is 4.701366901397705\n",
      "(64, 33)\n",
      "step 24570, loss is 4.79628324508667\n",
      "(64, 33)\n",
      "step 24571, loss is 4.96997594833374\n",
      "(64, 33)\n",
      "step 24572, loss is 4.776533126831055\n",
      "(64, 33)\n",
      "step 24573, loss is 4.8689961433410645\n",
      "(64, 33)\n",
      "step 24574, loss is 4.9152750968933105\n",
      "(64, 33)\n",
      "step 24575, loss is 4.892700672149658\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24576, loss is 4.862936496734619\n",
      "(64, 33)\n",
      "step 24577, loss is 4.741325378417969\n",
      "(64, 33)\n",
      "step 24578, loss is 4.58469820022583\n",
      "(64, 33)\n",
      "step 24579, loss is 4.684781074523926\n",
      "(64, 33)\n",
      "step 24580, loss is 4.7116289138793945\n",
      "(64, 33)\n",
      "step 24581, loss is 4.851884365081787\n",
      "(64, 33)\n",
      "step 24582, loss is 4.651195049285889\n",
      "(64, 33)\n",
      "step 24583, loss is 4.720471382141113\n",
      "(64, 33)\n",
      "step 24584, loss is 4.7945356369018555\n",
      "(64, 33)\n",
      "step 24585, loss is 5.035961627960205\n",
      "(64, 33)\n",
      "step 24586, loss is 4.89207649230957\n",
      "(64, 33)\n",
      "step 24587, loss is 4.869908332824707\n",
      "(64, 33)\n",
      "step 24588, loss is 4.783034324645996\n",
      "(64, 33)\n",
      "step 24589, loss is 4.794567108154297\n",
      "(64, 33)\n",
      "step 24590, loss is 4.814250946044922\n",
      "(64, 33)\n",
      "step 24591, loss is 4.9793853759765625\n",
      "(64, 33)\n",
      "step 24592, loss is 4.825139045715332\n",
      "(64, 33)\n",
      "step 24593, loss is 4.820854187011719\n",
      "(64, 33)\n",
      "step 24594, loss is 4.966869354248047\n",
      "(64, 33)\n",
      "step 24595, loss is 4.791532516479492\n",
      "(64, 33)\n",
      "step 24596, loss is 4.79295015335083\n",
      "(64, 33)\n",
      "step 24597, loss is 4.839140892028809\n",
      "(64, 33)\n",
      "step 24598, loss is 4.918953895568848\n",
      "(64, 33)\n",
      "step 24599, loss is 4.9299468994140625\n",
      "(64, 33)\n",
      "step 24600, loss is 4.732638359069824\n",
      "(64, 33)\n",
      "step 24601, loss is 4.8308634757995605\n",
      "(64, 33)\n",
      "step 24602, loss is 4.756898880004883\n",
      "(64, 33)\n",
      "step 24603, loss is 4.817963600158691\n",
      "(64, 33)\n",
      "step 24604, loss is 4.9052581787109375\n",
      "(64, 33)\n",
      "step 24605, loss is 4.761997699737549\n",
      "(64, 33)\n",
      "step 24606, loss is 4.9006829261779785\n",
      "(64, 33)\n",
      "step 24607, loss is 4.807790279388428\n",
      "(64, 33)\n",
      "step 24608, loss is 4.823127269744873\n",
      "(64, 33)\n",
      "step 24609, loss is 4.926270961761475\n",
      "(64, 33)\n",
      "step 24610, loss is 4.855654239654541\n",
      "(64, 33)\n",
      "step 24611, loss is 4.921767711639404\n",
      "(64, 33)\n",
      "step 24612, loss is 4.8244948387146\n",
      "(64, 33)\n",
      "step 24613, loss is 4.886794567108154\n",
      "(64, 33)\n",
      "step 24614, loss is 4.817277908325195\n",
      "(64, 33)\n",
      "step 24615, loss is 4.7916460037231445\n",
      "(64, 33)\n",
      "step 24616, loss is 4.624682903289795\n",
      "(64, 33)\n",
      "step 24617, loss is 4.928513050079346\n",
      "(64, 33)\n",
      "step 24618, loss is 4.921692371368408\n",
      "(64, 33)\n",
      "step 24619, loss is 4.7724127769470215\n",
      "(64, 33)\n",
      "step 24620, loss is 4.835360527038574\n",
      "(64, 33)\n",
      "step 24621, loss is 4.773712158203125\n",
      "(64, 33)\n",
      "step 24622, loss is 4.849315166473389\n",
      "(64, 33)\n",
      "step 24623, loss is 4.727633953094482\n",
      "(64, 33)\n",
      "step 24624, loss is 5.031539440155029\n",
      "(64, 33)\n",
      "step 24625, loss is 4.8246541023254395\n",
      "(64, 33)\n",
      "step 24626, loss is 4.759387493133545\n",
      "(64, 33)\n",
      "step 24627, loss is 4.836162090301514\n",
      "(64, 33)\n",
      "step 24628, loss is 4.954225063323975\n",
      "(64, 33)\n",
      "step 24629, loss is 4.647232532501221\n",
      "(64, 33)\n",
      "step 24630, loss is 4.845835208892822\n",
      "(64, 33)\n",
      "step 24631, loss is 4.743793964385986\n",
      "(64, 33)\n",
      "step 24632, loss is 4.7968597412109375\n",
      "(64, 33)\n",
      "step 24633, loss is 5.011323928833008\n",
      "(64, 33)\n",
      "step 24634, loss is 4.8329644203186035\n",
      "(64, 33)\n",
      "step 24635, loss is 4.608368873596191\n",
      "(64, 33)\n",
      "step 24636, loss is 4.767276763916016\n",
      "(64, 33)\n",
      "step 24637, loss is 4.988272190093994\n",
      "(64, 33)\n",
      "step 24638, loss is 4.7612104415893555\n",
      "(64, 33)\n",
      "step 24639, loss is 4.738956451416016\n",
      "(64, 33)\n",
      "step 24640, loss is 4.692905426025391\n",
      "(64, 33)\n",
      "step 24641, loss is 4.799455165863037\n",
      "(64, 33)\n",
      "step 24642, loss is 4.8679327964782715\n",
      "(64, 33)\n",
      "step 24643, loss is 4.82576847076416\n",
      "(64, 33)\n",
      "step 24644, loss is 4.6669697761535645\n",
      "(64, 33)\n",
      "step 24645, loss is 4.785665512084961\n",
      "(64, 33)\n",
      "step 24646, loss is 4.960729598999023\n",
      "(64, 33)\n",
      "step 24647, loss is 4.7601542472839355\n",
      "(64, 33)\n",
      "step 24648, loss is 4.867908954620361\n",
      "(64, 33)\n",
      "step 24649, loss is 4.727585315704346\n",
      "(64, 33)\n",
      "step 24650, loss is 4.78029727935791\n",
      "(64, 33)\n",
      "step 24651, loss is 4.72871732711792\n",
      "(64, 33)\n",
      "step 24652, loss is 4.711793422698975\n",
      "(64, 33)\n",
      "step 24653, loss is 4.7938232421875\n",
      "(64, 33)\n",
      "step 24654, loss is 4.450737476348877\n",
      "(64, 33)\n",
      "step 24655, loss is 4.812531471252441\n",
      "(64, 33)\n",
      "step 24656, loss is 4.607151985168457\n",
      "(64, 33)\n",
      "step 24657, loss is 4.897060871124268\n",
      "(64, 33)\n",
      "step 24658, loss is 4.780138969421387\n",
      "(64, 33)\n",
      "step 24659, loss is 5.101359844207764\n",
      "(64, 33)\n",
      "step 24660, loss is 4.761806011199951\n",
      "(64, 33)\n",
      "step 24661, loss is 4.904067516326904\n",
      "(64, 33)\n",
      "step 24662, loss is 4.813158988952637\n",
      "(64, 33)\n",
      "step 24663, loss is 4.751960754394531\n",
      "(64, 33)\n",
      "step 24664, loss is 4.859350204467773\n",
      "(64, 33)\n",
      "step 24665, loss is 4.836289882659912\n",
      "(64, 33)\n",
      "step 24666, loss is 4.87816858291626\n",
      "(64, 33)\n",
      "step 24667, loss is 4.5162506103515625\n",
      "(64, 33)\n",
      "step 24668, loss is 4.802708625793457\n",
      "(64, 33)\n",
      "step 24669, loss is 4.871156692504883\n",
      "(64, 33)\n",
      "step 24670, loss is 4.658043384552002\n",
      "(64, 33)\n",
      "step 24671, loss is 4.7951741218566895\n",
      "(64, 33)\n",
      "step 24672, loss is 4.8832621574401855\n",
      "(64, 33)\n",
      "step 24673, loss is 4.778635501861572\n",
      "(64, 33)\n",
      "step 24674, loss is 4.85190486907959\n",
      "(64, 33)\n",
      "step 24675, loss is 4.794684410095215\n",
      "(64, 33)\n",
      "step 24676, loss is 4.78696346282959\n",
      "(64, 33)\n",
      "step 24677, loss is 4.9283766746521\n",
      "(64, 33)\n",
      "step 24678, loss is 4.891757965087891\n",
      "(64, 33)\n",
      "step 24679, loss is 4.870514869689941\n",
      "(64, 33)\n",
      "step 24680, loss is 4.856175899505615\n",
      "(64, 33)\n",
      "step 24681, loss is 4.772017955780029\n",
      "(64, 33)\n",
      "step 24682, loss is 4.829219341278076\n",
      "(64, 33)\n",
      "step 24683, loss is 4.914668083190918\n",
      "(64, 33)\n",
      "step 24684, loss is 4.6281657218933105\n",
      "(64, 33)\n",
      "step 24685, loss is 4.878106117248535\n",
      "(64, 33)\n",
      "step 24686, loss is 4.899074077606201\n",
      "(64, 33)\n",
      "step 24687, loss is 4.8828816413879395\n",
      "(64, 33)\n",
      "step 24688, loss is 4.9980950355529785\n",
      "(64, 33)\n",
      "step 24689, loss is 4.623314380645752\n",
      "(64, 33)\n",
      "step 24690, loss is 4.746376037597656\n",
      "(64, 33)\n",
      "step 24691, loss is 4.72715425491333\n",
      "(64, 33)\n",
      "step 24692, loss is 4.7206950187683105\n",
      "(64, 33)\n",
      "step 24693, loss is 4.801200866699219\n",
      "(64, 33)\n",
      "step 24694, loss is 4.987985134124756\n",
      "(64, 33)\n",
      "step 24695, loss is 4.654621601104736\n",
      "(64, 33)\n",
      "step 24696, loss is 4.800381183624268\n",
      "(64, 33)\n",
      "step 24697, loss is 4.7555251121521\n",
      "(64, 33)\n",
      "step 24698, loss is 4.694399833679199\n",
      "(64, 33)\n",
      "step 24699, loss is 4.711296081542969\n",
      "(64, 33)\n",
      "step 24700, loss is 4.819080829620361\n",
      "(64, 33)\n",
      "step 24701, loss is 4.980835914611816\n",
      "(64, 33)\n",
      "step 24702, loss is 4.802309036254883\n",
      "(64, 33)\n",
      "step 24703, loss is 4.645214557647705\n",
      "(64, 33)\n",
      "step 24704, loss is 4.8584394454956055\n",
      "(64, 33)\n",
      "step 24705, loss is 4.564729690551758\n",
      "(64, 33)\n",
      "step 24706, loss is 4.834075927734375\n",
      "(64, 33)\n",
      "step 24707, loss is 4.738668441772461\n",
      "(64, 33)\n",
      "step 24708, loss is 4.740290641784668\n",
      "(64, 33)\n",
      "step 24709, loss is 4.549976825714111\n",
      "(64, 33)\n",
      "step 24710, loss is 4.72467041015625\n",
      "(64, 33)\n",
      "step 24711, loss is 4.780552864074707\n",
      "(64, 33)\n",
      "step 24712, loss is 4.828524589538574\n",
      "(64, 33)\n",
      "step 24713, loss is 4.626824855804443\n",
      "(64, 33)\n",
      "step 24714, loss is 4.838027477264404\n",
      "(64, 33)\n",
      "step 24715, loss is 4.792435169219971\n",
      "(64, 33)\n",
      "step 24716, loss is 4.717599868774414\n",
      "(64, 33)\n",
      "step 24717, loss is 4.819053649902344\n",
      "(64, 33)\n",
      "step 24718, loss is 4.829683780670166\n",
      "(64, 33)\n",
      "step 24719, loss is 4.641929626464844\n",
      "(64, 33)\n",
      "step 24720, loss is 4.7625813484191895\n",
      "(64, 33)\n",
      "step 24721, loss is 4.6941680908203125\n",
      "(64, 33)\n",
      "step 24722, loss is 4.893181800842285\n",
      "(64, 33)\n",
      "step 24723, loss is 4.789310455322266\n",
      "(64, 33)\n",
      "step 24724, loss is 4.868924617767334\n",
      "(64, 33)\n",
      "step 24725, loss is 4.725694179534912\n",
      "(64, 33)\n",
      "step 24726, loss is 4.617195129394531\n",
      "(64, 33)\n",
      "step 24727, loss is 4.9388346672058105\n",
      "(64, 33)\n",
      "step 24728, loss is 4.8642449378967285\n",
      "(64, 33)\n",
      "step 24729, loss is 4.640223503112793\n",
      "(64, 33)\n",
      "step 24730, loss is 4.714174270629883\n",
      "(64, 33)\n",
      "step 24731, loss is 4.8843770027160645\n",
      "(64, 33)\n",
      "step 24732, loss is 4.896514892578125\n",
      "(64, 33)\n",
      "step 24733, loss is 4.841006755828857\n",
      "(64, 33)\n",
      "step 24734, loss is 4.787216663360596\n",
      "(64, 33)\n",
      "step 24735, loss is 4.742465019226074\n",
      "(64, 33)\n",
      "step 24736, loss is 4.635376930236816\n",
      "(64, 33)\n",
      "step 24737, loss is 4.880433082580566\n",
      "(64, 33)\n",
      "step 24738, loss is 4.84481143951416\n",
      "(64, 33)\n",
      "step 24739, loss is 4.843963623046875\n",
      "(64, 33)\n",
      "step 24740, loss is 4.962709426879883\n",
      "(64, 33)\n",
      "step 24741, loss is 4.716453552246094\n",
      "(64, 33)\n",
      "step 24742, loss is 4.788479328155518\n",
      "(64, 33)\n",
      "step 24743, loss is 4.798459529876709\n",
      "(64, 33)\n",
      "step 24744, loss is 4.694893836975098\n",
      "(64, 33)\n",
      "step 24745, loss is 4.775777339935303\n",
      "(64, 33)\n",
      "step 24746, loss is 4.7437005043029785\n",
      "(64, 33)\n",
      "step 24747, loss is 4.681640625\n",
      "(64, 33)\n",
      "step 24748, loss is 4.723957538604736\n",
      "(64, 33)\n",
      "step 24749, loss is 4.973551273345947\n",
      "(64, 33)\n",
      "step 24750, loss is 4.770045280456543\n",
      "(64, 33)\n",
      "step 24751, loss is 4.72174072265625\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24752, loss is 4.754310607910156\n",
      "(64, 33)\n",
      "step 24753, loss is 4.85407829284668\n",
      "(64, 33)\n",
      "step 24754, loss is 4.841483116149902\n",
      "(64, 33)\n",
      "step 24755, loss is 4.90106201171875\n",
      "(64, 33)\n",
      "step 24756, loss is 4.613697052001953\n",
      "(64, 33)\n",
      "step 24757, loss is 4.701157569885254\n",
      "(64, 33)\n",
      "step 24758, loss is 4.969146728515625\n",
      "(64, 33)\n",
      "step 24759, loss is 4.824394226074219\n",
      "(64, 33)\n",
      "step 24760, loss is 4.848530292510986\n",
      "(64, 33)\n",
      "step 24761, loss is 4.963413715362549\n",
      "(64, 33)\n",
      "step 24762, loss is 4.683530807495117\n",
      "(64, 33)\n",
      "step 24763, loss is 4.708124160766602\n",
      "(64, 33)\n",
      "step 24764, loss is 4.987486362457275\n",
      "(64, 33)\n",
      "step 24765, loss is 4.997652053833008\n",
      "(64, 33)\n",
      "step 24766, loss is 4.747254848480225\n",
      "(64, 33)\n",
      "step 24767, loss is 4.70882511138916\n",
      "(64, 33)\n",
      "step 24768, loss is 4.805711269378662\n",
      "(64, 33)\n",
      "step 24769, loss is 4.676097393035889\n",
      "(64, 33)\n",
      "step 24770, loss is 5.053874969482422\n",
      "(64, 33)\n",
      "step 24771, loss is 4.833456993103027\n",
      "(64, 33)\n",
      "step 24772, loss is 4.925580978393555\n",
      "(64, 33)\n",
      "step 24773, loss is 4.555506229400635\n",
      "(64, 33)\n",
      "step 24774, loss is 4.7807936668396\n",
      "(64, 33)\n",
      "step 24775, loss is 4.704768657684326\n",
      "(64, 33)\n",
      "step 24776, loss is 4.882884502410889\n",
      "(64, 33)\n",
      "step 24777, loss is 4.7249250411987305\n",
      "(64, 33)\n",
      "step 24778, loss is 4.673637390136719\n",
      "(64, 33)\n",
      "step 24779, loss is 4.640841484069824\n",
      "(64, 33)\n",
      "step 24780, loss is 4.8859639167785645\n",
      "(64, 33)\n",
      "step 24781, loss is 5.001953125\n",
      "(64, 33)\n",
      "step 24782, loss is 4.702376842498779\n",
      "(64, 33)\n",
      "step 24783, loss is 4.70615816116333\n",
      "(64, 33)\n",
      "step 24784, loss is 4.766928672790527\n",
      "(64, 33)\n",
      "step 24785, loss is 4.754425525665283\n",
      "(64, 33)\n",
      "step 24786, loss is 4.903805255889893\n",
      "(64, 33)\n",
      "step 24787, loss is 4.724329471588135\n",
      "(64, 33)\n",
      "step 24788, loss is 4.782260417938232\n",
      "(64, 33)\n",
      "step 24789, loss is 4.817893981933594\n",
      "(64, 33)\n",
      "step 24790, loss is 4.759854793548584\n",
      "(64, 33)\n",
      "step 24791, loss is 4.9956560134887695\n",
      "(64, 33)\n",
      "step 24792, loss is 4.663128852844238\n",
      "(64, 33)\n",
      "step 24793, loss is 4.861429214477539\n",
      "(64, 33)\n",
      "step 24794, loss is 4.655337810516357\n",
      "(64, 33)\n",
      "step 24795, loss is 4.77550745010376\n",
      "(64, 33)\n",
      "step 24796, loss is 4.736961364746094\n",
      "(64, 33)\n",
      "step 24797, loss is 4.7598185539245605\n",
      "(64, 33)\n",
      "step 24798, loss is 4.751513957977295\n",
      "(64, 33)\n",
      "step 24799, loss is 4.732748985290527\n",
      "(64, 33)\n",
      "step 24800, loss is 4.878176212310791\n",
      "(64, 33)\n",
      "step 24801, loss is 4.599480628967285\n",
      "(64, 33)\n",
      "step 24802, loss is 4.879377365112305\n",
      "(64, 33)\n",
      "step 24803, loss is 4.611170291900635\n",
      "(64, 33)\n",
      "step 24804, loss is 4.582497596740723\n",
      "(64, 33)\n",
      "step 24805, loss is 4.830300331115723\n",
      "(64, 33)\n",
      "step 24806, loss is 4.74162483215332\n",
      "(64, 33)\n",
      "step 24807, loss is 4.790348529815674\n",
      "(64, 33)\n",
      "step 24808, loss is 4.703132152557373\n",
      "(64, 33)\n",
      "step 24809, loss is 4.885031223297119\n",
      "(64, 33)\n",
      "step 24810, loss is 4.826668739318848\n",
      "(64, 33)\n",
      "step 24811, loss is 4.8909382820129395\n",
      "(64, 33)\n",
      "step 24812, loss is 4.760462284088135\n",
      "(64, 33)\n",
      "step 24813, loss is 4.580530166625977\n",
      "(64, 33)\n",
      "step 24814, loss is 4.589848041534424\n",
      "(64, 33)\n",
      "step 24815, loss is 4.948320388793945\n",
      "(64, 33)\n",
      "step 24816, loss is 4.870840549468994\n",
      "(64, 33)\n",
      "step 24817, loss is 4.900919437408447\n",
      "(64, 33)\n",
      "step 24818, loss is 4.747379779815674\n",
      "(64, 33)\n",
      "step 24819, loss is 4.655787467956543\n",
      "(64, 33)\n",
      "step 24820, loss is 4.6705756187438965\n",
      "(64, 33)\n",
      "step 24821, loss is 4.74160623550415\n",
      "(64, 33)\n",
      "step 24822, loss is 4.781002998352051\n",
      "(64, 33)\n",
      "step 24823, loss is 4.748519420623779\n",
      "(64, 33)\n",
      "step 24824, loss is 4.885838985443115\n",
      "(64, 33)\n",
      "step 24825, loss is 4.886985778808594\n",
      "(64, 33)\n",
      "step 24826, loss is 4.571403503417969\n",
      "(64, 33)\n",
      "step 24827, loss is 4.841779708862305\n",
      "(64, 33)\n",
      "step 24828, loss is 4.874751567840576\n",
      "(64, 33)\n",
      "step 24829, loss is 4.594295024871826\n",
      "(64, 33)\n",
      "step 24830, loss is 4.958894729614258\n",
      "(64, 33)\n",
      "step 24831, loss is 4.744971752166748\n",
      "(64, 33)\n",
      "step 24832, loss is 4.690277099609375\n",
      "(64, 33)\n",
      "step 24833, loss is 4.877701282501221\n",
      "(64, 33)\n",
      "step 24834, loss is 5.028107643127441\n",
      "(64, 33)\n",
      "step 24835, loss is 4.754101276397705\n",
      "(64, 33)\n",
      "step 24836, loss is 4.8905229568481445\n",
      "(64, 33)\n",
      "step 24837, loss is 4.819258213043213\n",
      "(64, 33)\n",
      "step 24838, loss is 4.846945285797119\n",
      "(64, 33)\n",
      "step 24839, loss is 4.769639492034912\n",
      "(64, 33)\n",
      "step 24840, loss is 4.82102632522583\n",
      "(64, 33)\n",
      "step 24841, loss is 4.696569919586182\n",
      "(64, 33)\n",
      "step 24842, loss is 4.677783489227295\n",
      "(64, 33)\n",
      "step 24843, loss is 4.833372116088867\n",
      "(64, 33)\n",
      "step 24844, loss is 4.982966899871826\n",
      "(64, 33)\n",
      "step 24845, loss is 4.678427219390869\n",
      "(64, 33)\n",
      "step 24846, loss is 4.578126430511475\n",
      "(64, 33)\n",
      "step 24847, loss is 4.7558674812316895\n",
      "(64, 33)\n",
      "step 24848, loss is 4.673264980316162\n",
      "(64, 33)\n",
      "step 24849, loss is 4.760111331939697\n",
      "(64, 33)\n",
      "step 24850, loss is 4.859833240509033\n",
      "(64, 33)\n",
      "step 24851, loss is 4.869014739990234\n",
      "(64, 33)\n",
      "step 24852, loss is 4.896055221557617\n",
      "(64, 33)\n",
      "step 24853, loss is 4.670188903808594\n",
      "(64, 33)\n",
      "step 24854, loss is 4.840242862701416\n",
      "(64, 33)\n",
      "step 24855, loss is 4.672316074371338\n",
      "(64, 33)\n",
      "step 24856, loss is 4.738064289093018\n",
      "(64, 33)\n",
      "step 24857, loss is 4.912428855895996\n",
      "(64, 33)\n",
      "step 24858, loss is 4.730605125427246\n",
      "(64, 33)\n",
      "step 24859, loss is 4.866438388824463\n",
      "(64, 33)\n",
      "step 24860, loss is 4.889599800109863\n",
      "(64, 33)\n",
      "step 24861, loss is 4.740780830383301\n",
      "(64, 33)\n",
      "step 24862, loss is 4.77146577835083\n",
      "(64, 33)\n",
      "step 24863, loss is 4.799036979675293\n",
      "(64, 33)\n",
      "step 24864, loss is 4.683255195617676\n",
      "(64, 33)\n",
      "step 24865, loss is 4.688267230987549\n",
      "(64, 33)\n",
      "step 24866, loss is 4.892945289611816\n",
      "(64, 33)\n",
      "step 24867, loss is 4.714269161224365\n",
      "(64, 33)\n",
      "step 24868, loss is 4.896200180053711\n",
      "(64, 33)\n",
      "step 24869, loss is 4.778998374938965\n",
      "(64, 33)\n",
      "step 24870, loss is 4.654596328735352\n",
      "(64, 33)\n",
      "step 24871, loss is 4.720992565155029\n",
      "(64, 33)\n",
      "step 24872, loss is 4.830562591552734\n",
      "(64, 33)\n",
      "step 24873, loss is 4.744241237640381\n",
      "(64, 33)\n",
      "step 24874, loss is 4.561668872833252\n",
      "(64, 33)\n",
      "step 24875, loss is 4.72304630279541\n",
      "(64, 33)\n",
      "step 24876, loss is 4.889860153198242\n",
      "(64, 33)\n",
      "step 24877, loss is 4.638376712799072\n",
      "(64, 33)\n",
      "step 24878, loss is 4.765901565551758\n",
      "(64, 33)\n",
      "step 24879, loss is 4.604308128356934\n",
      "(64, 33)\n",
      "step 24880, loss is 4.736159801483154\n",
      "(64, 33)\n",
      "step 24881, loss is 4.764649391174316\n",
      "(64, 33)\n",
      "step 24882, loss is 4.8196868896484375\n",
      "(64, 33)\n",
      "step 24883, loss is 4.981929779052734\n",
      "(64, 33)\n",
      "step 24884, loss is 4.855947494506836\n",
      "(64, 33)\n",
      "step 24885, loss is 4.817723751068115\n",
      "(64, 33)\n",
      "step 24886, loss is 4.810428619384766\n",
      "(64, 33)\n",
      "step 24887, loss is 4.672820091247559\n",
      "(64, 33)\n",
      "step 24888, loss is 4.585281848907471\n",
      "(64, 33)\n",
      "step 24889, loss is 4.841423988342285\n",
      "(64, 33)\n",
      "step 24890, loss is 4.626224517822266\n",
      "(64, 33)\n",
      "step 24891, loss is 4.756982803344727\n",
      "(64, 33)\n",
      "step 24892, loss is 4.76325798034668\n",
      "(64, 33)\n",
      "step 24893, loss is 4.639603137969971\n",
      "(64, 33)\n",
      "step 24894, loss is 4.663654804229736\n",
      "(64, 33)\n",
      "step 24895, loss is 4.771296501159668\n",
      "(64, 33)\n",
      "step 24896, loss is 4.83346700668335\n",
      "(64, 33)\n",
      "step 24897, loss is 4.885255813598633\n",
      "(64, 33)\n",
      "step 24898, loss is 4.826791286468506\n",
      "(64, 33)\n",
      "step 24899, loss is 4.621964931488037\n",
      "(64, 33)\n",
      "step 24900, loss is 4.700221061706543\n",
      "(64, 33)\n",
      "step 24901, loss is 4.644423961639404\n",
      "(64, 33)\n",
      "step 24902, loss is 4.893037796020508\n",
      "(64, 33)\n",
      "step 24903, loss is 4.808792591094971\n",
      "(64, 33)\n",
      "step 24904, loss is 4.718509674072266\n",
      "(64, 33)\n",
      "step 24905, loss is 4.794861793518066\n",
      "(64, 33)\n",
      "step 24906, loss is 4.752985954284668\n",
      "(64, 33)\n",
      "step 24907, loss is 4.674699306488037\n",
      "(64, 33)\n",
      "step 24908, loss is 4.780755996704102\n",
      "(64, 33)\n",
      "step 24909, loss is 4.697774410247803\n",
      "(64, 33)\n",
      "step 24910, loss is 4.643613815307617\n",
      "(64, 33)\n",
      "step 24911, loss is 4.817739963531494\n",
      "(64, 33)\n",
      "step 24912, loss is 4.756507396697998\n",
      "(64, 33)\n",
      "step 24913, loss is 4.859416484832764\n",
      "(64, 33)\n",
      "step 24914, loss is 4.901895523071289\n",
      "(64, 33)\n",
      "step 24915, loss is 4.683948040008545\n",
      "(64, 33)\n",
      "step 24916, loss is 4.8209357261657715\n",
      "(64, 33)\n",
      "step 24917, loss is 4.797628879547119\n",
      "(64, 33)\n",
      "step 24918, loss is 4.910100936889648\n",
      "(64, 33)\n",
      "step 24919, loss is 4.850543975830078\n",
      "(64, 33)\n",
      "step 24920, loss is 4.807210445404053\n",
      "(64, 33)\n",
      "step 24921, loss is 4.715511322021484\n",
      "(64, 33)\n",
      "step 24922, loss is 4.979824066162109\n",
      "(64, 33)\n",
      "step 24923, loss is 4.701045513153076\n",
      "(64, 33)\n",
      "step 24924, loss is 4.835305690765381\n",
      "(64, 33)\n",
      "step 24925, loss is 4.779333114624023\n",
      "(64, 33)\n",
      "step 24926, loss is 4.862607955932617\n",
      "(64, 33)\n",
      "step 24927, loss is 4.711066722869873\n",
      "(64, 33)\n",
      "step 24928, loss is 4.801754474639893\n",
      "(64, 33)\n",
      "step 24929, loss is 4.76027250289917\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24930, loss is 4.844513893127441\n",
      "(64, 33)\n",
      "step 24931, loss is 4.751008033752441\n",
      "(64, 33)\n",
      "step 24932, loss is 4.677920341491699\n",
      "(64, 33)\n",
      "step 24933, loss is 4.58897590637207\n",
      "(64, 33)\n",
      "step 24934, loss is 4.7380266189575195\n",
      "(64, 33)\n",
      "step 24935, loss is 4.6501641273498535\n",
      "(64, 33)\n",
      "step 24936, loss is 4.746908664703369\n",
      "(64, 33)\n",
      "step 24937, loss is 4.730152130126953\n",
      "(64, 33)\n",
      "step 24938, loss is 4.783575534820557\n",
      "(64, 33)\n",
      "step 24939, loss is 4.911821365356445\n",
      "(64, 33)\n",
      "step 24940, loss is 4.977240085601807\n",
      "(64, 33)\n",
      "step 24941, loss is 4.5870795249938965\n",
      "(64, 33)\n",
      "step 24942, loss is 4.777878761291504\n",
      "(64, 33)\n",
      "step 24943, loss is 4.645585536956787\n",
      "(64, 33)\n",
      "step 24944, loss is 4.685419082641602\n",
      "(64, 33)\n",
      "step 24945, loss is 4.921793460845947\n",
      "(64, 33)\n",
      "step 24946, loss is 4.5097174644470215\n",
      "(64, 33)\n",
      "step 24947, loss is 4.6276984214782715\n",
      "(64, 33)\n",
      "step 24948, loss is 4.884632110595703\n",
      "(64, 33)\n",
      "step 24949, loss is 4.582953453063965\n",
      "(64, 33)\n",
      "step 24950, loss is 4.715694427490234\n",
      "(64, 33)\n",
      "step 24951, loss is 4.708796977996826\n",
      "(64, 33)\n",
      "step 24952, loss is 4.6586995124816895\n",
      "(64, 33)\n",
      "step 24953, loss is 4.8026227951049805\n",
      "(64, 33)\n",
      "step 24954, loss is 4.671679496765137\n",
      "(64, 33)\n",
      "step 24955, loss is 4.627897262573242\n",
      "(64, 33)\n",
      "step 24956, loss is 4.86604118347168\n",
      "(64, 33)\n",
      "step 24957, loss is 4.76525354385376\n",
      "(64, 33)\n",
      "step 24958, loss is 4.886427402496338\n",
      "(64, 33)\n",
      "step 24959, loss is 4.902202606201172\n",
      "(64, 33)\n",
      "step 24960, loss is 4.812206268310547\n",
      "(64, 33)\n",
      "step 24961, loss is 4.752003192901611\n",
      "(64, 33)\n",
      "step 24962, loss is 4.8570475578308105\n",
      "(64, 33)\n",
      "step 24963, loss is 4.9268879890441895\n",
      "(64, 33)\n",
      "step 24964, loss is 4.673580646514893\n",
      "(64, 33)\n",
      "step 24965, loss is 4.64283561706543\n",
      "(64, 33)\n",
      "step 24966, loss is 4.763455867767334\n",
      "(64, 33)\n",
      "step 24967, loss is 4.911165237426758\n",
      "(64, 33)\n",
      "step 24968, loss is 4.702322483062744\n",
      "(64, 33)\n",
      "step 24969, loss is 4.743490219116211\n",
      "(64, 33)\n",
      "step 24970, loss is 4.803077697753906\n",
      "(64, 33)\n",
      "step 24971, loss is 4.8256378173828125\n",
      "(64, 33)\n",
      "step 24972, loss is 4.843523025512695\n",
      "(64, 33)\n",
      "step 24973, loss is 4.6844048500061035\n",
      "(64, 33)\n",
      "step 24974, loss is 4.740964889526367\n",
      "(64, 33)\n",
      "step 24975, loss is 4.852199077606201\n",
      "(64, 33)\n",
      "step 24976, loss is 4.874199867248535\n",
      "(64, 33)\n",
      "step 24977, loss is 4.592491149902344\n",
      "(64, 33)\n",
      "step 24978, loss is 4.829948425292969\n",
      "(64, 33)\n",
      "step 24979, loss is 4.919186592102051\n",
      "(64, 33)\n",
      "step 24980, loss is 4.859004974365234\n",
      "(64, 33)\n",
      "step 24981, loss is 4.787045478820801\n",
      "(64, 33)\n",
      "step 24982, loss is 4.7711501121521\n",
      "(64, 33)\n",
      "step 24983, loss is 4.7748212814331055\n",
      "(64, 33)\n",
      "step 24984, loss is 4.713789939880371\n",
      "(64, 33)\n",
      "step 24985, loss is 4.739204406738281\n",
      "(64, 33)\n",
      "step 24986, loss is 4.84066104888916\n",
      "(64, 33)\n",
      "step 24987, loss is 4.837796688079834\n",
      "(64, 33)\n",
      "step 24988, loss is 4.5709428787231445\n",
      "(64, 33)\n",
      "step 24989, loss is 4.679323673248291\n",
      "(64, 33)\n",
      "step 24990, loss is 4.910610198974609\n",
      "(64, 33)\n",
      "step 24991, loss is 4.8431315422058105\n",
      "(64, 33)\n",
      "step 24992, loss is 4.592604160308838\n",
      "(64, 33)\n",
      "step 24993, loss is 4.855676174163818\n",
      "(64, 33)\n",
      "step 24994, loss is 4.841240882873535\n",
      "(64, 33)\n",
      "step 24995, loss is 4.6809539794921875\n",
      "(64, 33)\n",
      "step 24996, loss is 4.875535011291504\n",
      "(64, 33)\n",
      "step 24997, loss is 4.694413661956787\n",
      "(64, 33)\n",
      "step 24998, loss is 4.663093090057373\n",
      "(64, 33)\n",
      "step 24999, loss is 4.655061721801758\n",
      "(64, 33)\n",
      "step 25000, loss is 4.82526159286499\n",
      "(64, 33)\n",
      "step 25001, loss is 4.682021141052246\n",
      "(64, 33)\n",
      "step 25002, loss is 4.812441349029541\n",
      "(64, 33)\n",
      "step 25003, loss is 4.7475080490112305\n",
      "(64, 33)\n",
      "step 25004, loss is 4.860306262969971\n",
      "(64, 33)\n",
      "step 25005, loss is 4.8099284172058105\n",
      "(64, 33)\n",
      "step 25006, loss is 4.812928199768066\n",
      "(64, 33)\n",
      "step 25007, loss is 4.8090033531188965\n",
      "(64, 33)\n",
      "step 25008, loss is 4.638206481933594\n",
      "(64, 33)\n",
      "step 25009, loss is 4.729250431060791\n",
      "(64, 33)\n",
      "step 25010, loss is 4.67545747756958\n",
      "(64, 33)\n",
      "step 25011, loss is 4.800276279449463\n",
      "(64, 33)\n",
      "step 25012, loss is 4.818480491638184\n",
      "(64, 33)\n",
      "step 25013, loss is 4.8838582038879395\n",
      "(64, 33)\n",
      "step 25014, loss is 4.794121742248535\n",
      "(64, 33)\n",
      "step 25015, loss is 4.797358989715576\n",
      "(64, 33)\n",
      "step 25016, loss is 4.7474565505981445\n",
      "(64, 33)\n",
      "step 25017, loss is 4.865706920623779\n",
      "(64, 33)\n",
      "step 25018, loss is 4.785375118255615\n",
      "(64, 33)\n",
      "step 25019, loss is 4.682430267333984\n",
      "(64, 33)\n",
      "step 25020, loss is 4.63447380065918\n",
      "(64, 33)\n",
      "step 25021, loss is 4.8614888191223145\n",
      "(64, 33)\n",
      "step 25022, loss is 4.8634257316589355\n",
      "(64, 33)\n",
      "step 25023, loss is 4.7739739418029785\n",
      "(64, 33)\n",
      "step 25024, loss is 4.687716960906982\n",
      "(64, 33)\n",
      "step 25025, loss is 4.789834499359131\n",
      "(64, 33)\n",
      "step 25026, loss is 4.75949239730835\n",
      "(64, 33)\n",
      "step 25027, loss is 4.58425235748291\n",
      "(64, 33)\n",
      "step 25028, loss is 4.884666919708252\n",
      "(64, 33)\n",
      "step 25029, loss is 4.754950523376465\n",
      "(64, 33)\n",
      "step 25030, loss is 4.7918267250061035\n",
      "(64, 33)\n",
      "step 25031, loss is 4.756807327270508\n",
      "(64, 33)\n",
      "step 25032, loss is 4.681370258331299\n",
      "(64, 33)\n",
      "step 25033, loss is 4.927888870239258\n",
      "(64, 33)\n",
      "step 25034, loss is 4.891029357910156\n",
      "(64, 33)\n",
      "step 25035, loss is 4.7161455154418945\n",
      "(64, 33)\n",
      "step 25036, loss is 4.725176811218262\n",
      "(64, 33)\n",
      "step 25037, loss is 4.852898597717285\n",
      "(64, 33)\n",
      "step 25038, loss is 4.718069553375244\n",
      "(64, 33)\n",
      "step 25039, loss is 4.795974254608154\n",
      "(64, 33)\n",
      "step 25040, loss is 4.949768543243408\n",
      "(64, 33)\n",
      "step 25041, loss is 4.787930488586426\n",
      "(64, 33)\n",
      "step 25042, loss is 4.7264790534973145\n",
      "(64, 33)\n",
      "step 25043, loss is 4.798355579376221\n",
      "(64, 33)\n",
      "step 25044, loss is 4.773519992828369\n",
      "(64, 33)\n",
      "step 25045, loss is 4.764312267303467\n",
      "(64, 33)\n",
      "step 25046, loss is 4.895567417144775\n",
      "(64, 33)\n",
      "step 25047, loss is 4.842369079589844\n",
      "(64, 33)\n",
      "step 25048, loss is 4.664544105529785\n",
      "(64, 33)\n",
      "step 25049, loss is 4.577548503875732\n",
      "(64, 33)\n",
      "step 25050, loss is 4.854093551635742\n",
      "(64, 33)\n",
      "step 25051, loss is 4.911542892456055\n",
      "(64, 33)\n",
      "step 25052, loss is 4.820666313171387\n",
      "(64, 33)\n",
      "step 25053, loss is 4.917113780975342\n",
      "(64, 33)\n",
      "step 25054, loss is 4.785148620605469\n",
      "(64, 33)\n",
      "step 25055, loss is 4.637355804443359\n",
      "(64, 33)\n",
      "step 25056, loss is 4.812967300415039\n",
      "(64, 33)\n",
      "step 25057, loss is 4.876399517059326\n",
      "(64, 33)\n",
      "step 25058, loss is 4.833490371704102\n",
      "(64, 33)\n",
      "step 25059, loss is 4.732316493988037\n",
      "(64, 33)\n",
      "step 25060, loss is 4.784021854400635\n",
      "(64, 33)\n",
      "step 25061, loss is 4.760946273803711\n",
      "(64, 33)\n",
      "step 25062, loss is 4.757628917694092\n",
      "(64, 33)\n",
      "step 25063, loss is 4.849367141723633\n",
      "(64, 33)\n",
      "step 25064, loss is 4.881252765655518\n",
      "(64, 33)\n",
      "step 25065, loss is 4.711702823638916\n",
      "(64, 33)\n",
      "step 25066, loss is 4.785534381866455\n",
      "(64, 33)\n",
      "step 25067, loss is 4.697510242462158\n",
      "(64, 33)\n",
      "step 25068, loss is 4.655237197875977\n",
      "(64, 33)\n",
      "step 25069, loss is 4.7948126792907715\n",
      "(64, 33)\n",
      "step 25070, loss is 4.6823015213012695\n",
      "(64, 33)\n",
      "step 25071, loss is 4.7165207862854\n",
      "(64, 33)\n",
      "step 25072, loss is 4.7155375480651855\n",
      "(64, 33)\n",
      "step 25073, loss is 4.847239971160889\n",
      "(64, 33)\n",
      "step 25074, loss is 4.989435195922852\n",
      "(64, 33)\n",
      "step 25075, loss is 4.8533244132995605\n",
      "(64, 33)\n",
      "step 25076, loss is 4.897507190704346\n",
      "(64, 33)\n",
      "step 25077, loss is 4.848406791687012\n",
      "(64, 33)\n",
      "step 25078, loss is 4.618588447570801\n",
      "(64, 33)\n",
      "step 25079, loss is 4.870720863342285\n",
      "(64, 33)\n",
      "step 25080, loss is 4.721032619476318\n",
      "(64, 33)\n",
      "step 25081, loss is 4.701708793640137\n",
      "(64, 33)\n",
      "step 25082, loss is 4.906836986541748\n",
      "(64, 33)\n",
      "step 25083, loss is 4.773348808288574\n",
      "(64, 33)\n",
      "step 25084, loss is 4.785606384277344\n",
      "(64, 33)\n",
      "step 25085, loss is 4.869808197021484\n",
      "(64, 33)\n",
      "step 25086, loss is 4.727172374725342\n",
      "(64, 33)\n",
      "step 25087, loss is 4.700694561004639\n",
      "(64, 33)\n",
      "step 25088, loss is 4.796273708343506\n",
      "(64, 33)\n",
      "step 25089, loss is 4.751733303070068\n",
      "(64, 33)\n",
      "step 25090, loss is 4.73438024520874\n",
      "(64, 33)\n",
      "step 25091, loss is 4.887794494628906\n",
      "(64, 33)\n",
      "step 25092, loss is 4.931101322174072\n",
      "(64, 33)\n",
      "step 25093, loss is 4.788512229919434\n",
      "(64, 33)\n",
      "step 25094, loss is 4.836291313171387\n",
      "(64, 33)\n",
      "step 25095, loss is 4.478518962860107\n",
      "(64, 33)\n",
      "step 25096, loss is 4.71903133392334\n",
      "(64, 33)\n",
      "step 25097, loss is 4.666018009185791\n",
      "(64, 33)\n",
      "step 25098, loss is 4.724041938781738\n",
      "(64, 33)\n",
      "step 25099, loss is 4.9049272537231445\n",
      "(64, 33)\n",
      "step 25100, loss is 4.874878883361816\n",
      "(64, 33)\n",
      "step 25101, loss is 4.9475555419921875\n",
      "(64, 33)\n",
      "step 25102, loss is 4.810266494750977\n",
      "(64, 33)\n",
      "step 25103, loss is 4.787532329559326\n",
      "(64, 33)\n",
      "step 25104, loss is 4.789105415344238\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25105, loss is 4.897873878479004\n",
      "(64, 33)\n",
      "step 25106, loss is 4.672904968261719\n",
      "(64, 33)\n",
      "step 25107, loss is 4.809483528137207\n",
      "(64, 33)\n",
      "step 25108, loss is 4.907111167907715\n",
      "(64, 33)\n",
      "step 25109, loss is 4.857627868652344\n",
      "(64, 33)\n",
      "step 25110, loss is 4.624081611633301\n",
      "(64, 33)\n",
      "step 25111, loss is 4.71474552154541\n",
      "(64, 33)\n",
      "step 25112, loss is 4.835141181945801\n",
      "(64, 33)\n",
      "step 25113, loss is 4.7009758949279785\n",
      "(64, 33)\n",
      "step 25114, loss is 4.749991416931152\n",
      "(64, 33)\n",
      "step 25115, loss is 4.661025524139404\n",
      "(64, 33)\n",
      "step 25116, loss is 4.787285327911377\n",
      "(64, 33)\n",
      "step 25117, loss is 4.6559953689575195\n",
      "(64, 33)\n",
      "step 25118, loss is 4.7846360206604\n",
      "(64, 33)\n",
      "step 25119, loss is 4.6532511711120605\n",
      "(64, 33)\n",
      "step 25120, loss is 4.729707717895508\n",
      "(64, 33)\n",
      "step 25121, loss is 4.668127536773682\n",
      "(64, 33)\n",
      "step 25122, loss is 4.809651851654053\n",
      "(64, 33)\n",
      "step 25123, loss is 4.891590595245361\n",
      "(64, 33)\n",
      "step 25124, loss is 4.788210868835449\n",
      "(64, 33)\n",
      "step 25125, loss is 4.694328784942627\n",
      "(64, 33)\n",
      "step 25126, loss is 4.659921169281006\n",
      "(64, 33)\n",
      "step 25127, loss is 4.852314472198486\n",
      "(64, 33)\n",
      "step 25128, loss is 4.712406635284424\n",
      "(64, 33)\n",
      "step 25129, loss is 4.755455493927002\n",
      "(64, 33)\n",
      "step 25130, loss is 4.608396530151367\n",
      "(64, 33)\n",
      "step 25131, loss is 4.909488677978516\n",
      "(64, 33)\n",
      "step 25132, loss is 4.924223899841309\n",
      "(64, 33)\n",
      "step 25133, loss is 4.948362827301025\n",
      "(64, 33)\n",
      "step 25134, loss is 4.730518341064453\n",
      "(64, 33)\n",
      "step 25135, loss is 4.894440650939941\n",
      "(64, 33)\n",
      "step 25136, loss is 4.699141502380371\n",
      "(64, 33)\n",
      "step 25137, loss is 4.800213813781738\n",
      "(64, 33)\n",
      "step 25138, loss is 4.969320297241211\n",
      "(64, 33)\n",
      "step 25139, loss is 4.7054667472839355\n",
      "(64, 33)\n",
      "step 25140, loss is 4.723728179931641\n",
      "(64, 33)\n",
      "step 25141, loss is 4.8759236335754395\n",
      "(64, 33)\n",
      "step 25142, loss is 4.701636791229248\n",
      "(64, 33)\n",
      "step 25143, loss is 4.8327436447143555\n",
      "(64, 33)\n",
      "step 25144, loss is 4.664334297180176\n",
      "(64, 33)\n",
      "step 25145, loss is 4.732327461242676\n",
      "(64, 33)\n",
      "step 25146, loss is 4.877114295959473\n",
      "(64, 33)\n",
      "step 25147, loss is 4.959042072296143\n",
      "(64, 33)\n",
      "step 25148, loss is 4.875333309173584\n",
      "(64, 33)\n",
      "step 25149, loss is 4.902121067047119\n",
      "(64, 33)\n",
      "step 25150, loss is 4.852348327636719\n",
      "(64, 33)\n",
      "step 25151, loss is 4.828834056854248\n",
      "(64, 33)\n",
      "step 25152, loss is 4.930696964263916\n",
      "(64, 33)\n",
      "step 25153, loss is 4.796517848968506\n",
      "(64, 33)\n",
      "step 25154, loss is 4.7392096519470215\n",
      "(64, 33)\n",
      "step 25155, loss is 4.734879970550537\n",
      "(64, 33)\n",
      "step 25156, loss is 4.7613325119018555\n",
      "(64, 33)\n",
      "step 25157, loss is 4.87643575668335\n",
      "(64, 33)\n",
      "step 25158, loss is 4.785709381103516\n",
      "(64, 33)\n",
      "step 25159, loss is 4.85191535949707\n",
      "(64, 33)\n",
      "step 25160, loss is 4.792923927307129\n",
      "(64, 33)\n",
      "step 25161, loss is 4.799089431762695\n",
      "(64, 33)\n",
      "step 25162, loss is 4.854003429412842\n",
      "(64, 33)\n",
      "step 25163, loss is 4.89150333404541\n",
      "(64, 33)\n",
      "step 25164, loss is 4.646431922912598\n",
      "(64, 33)\n",
      "step 25165, loss is 4.833142280578613\n",
      "(64, 33)\n",
      "step 25166, loss is 4.720818996429443\n",
      "(64, 33)\n",
      "step 25167, loss is 4.877599716186523\n",
      "(64, 33)\n",
      "step 25168, loss is 4.713279724121094\n",
      "(64, 33)\n",
      "step 25169, loss is 4.779199600219727\n",
      "(64, 33)\n",
      "step 25170, loss is 4.671392440795898\n",
      "(64, 33)\n",
      "step 25171, loss is 4.79850435256958\n",
      "(64, 33)\n",
      "step 25172, loss is 4.628971576690674\n",
      "(64, 33)\n",
      "step 25173, loss is 4.897993564605713\n",
      "(64, 33)\n",
      "step 25174, loss is 4.7127275466918945\n",
      "(64, 33)\n",
      "step 25175, loss is 4.626310348510742\n",
      "(64, 33)\n",
      "step 25176, loss is 4.872279167175293\n",
      "(64, 33)\n",
      "step 25177, loss is 4.55213737487793\n",
      "(64, 33)\n",
      "step 25178, loss is 4.624894142150879\n",
      "(64, 33)\n",
      "step 25179, loss is 4.700605869293213\n",
      "(64, 33)\n",
      "step 25180, loss is 4.839095115661621\n",
      "(64, 33)\n",
      "step 25181, loss is 4.737956523895264\n",
      "(64, 33)\n",
      "step 25182, loss is 4.716037273406982\n",
      "(64, 33)\n",
      "step 25183, loss is 4.544739723205566\n",
      "(64, 33)\n",
      "step 25184, loss is 4.687177658081055\n",
      "(64, 33)\n",
      "step 25185, loss is 4.846493721008301\n",
      "(64, 33)\n",
      "step 25186, loss is 4.705168724060059\n",
      "(64, 33)\n",
      "step 25187, loss is 4.784100532531738\n",
      "(64, 33)\n",
      "step 25188, loss is 4.66502571105957\n",
      "(64, 33)\n",
      "step 25189, loss is 4.6609907150268555\n",
      "(64, 33)\n",
      "step 25190, loss is 4.956883430480957\n",
      "(64, 33)\n",
      "step 25191, loss is 4.7954020500183105\n",
      "(64, 33)\n",
      "step 25192, loss is 4.8408942222595215\n",
      "(64, 33)\n",
      "step 25193, loss is 4.728251934051514\n",
      "(64, 33)\n",
      "step 25194, loss is 4.7347869873046875\n",
      "(64, 33)\n",
      "step 25195, loss is 4.827401638031006\n",
      "(64, 33)\n",
      "step 25196, loss is 4.844049453735352\n",
      "(64, 33)\n",
      "step 25197, loss is 4.824300289154053\n",
      "(64, 33)\n",
      "step 25198, loss is 4.753754138946533\n",
      "(64, 33)\n",
      "step 25199, loss is 4.752872467041016\n",
      "(64, 33)\n",
      "step 25200, loss is 4.760848045349121\n",
      "(64, 33)\n",
      "step 25201, loss is 4.842248439788818\n",
      "(64, 33)\n",
      "step 25202, loss is 4.873556137084961\n",
      "(64, 33)\n",
      "step 25203, loss is 4.8164215087890625\n",
      "(64, 33)\n",
      "step 25204, loss is 4.733008861541748\n",
      "(64, 33)\n",
      "step 25205, loss is 4.838222503662109\n",
      "(64, 33)\n",
      "step 25206, loss is 4.930843353271484\n",
      "(64, 33)\n",
      "step 25207, loss is 4.987890243530273\n",
      "(64, 33)\n",
      "step 25208, loss is 4.811392307281494\n",
      "(64, 33)\n",
      "step 25209, loss is 4.8378777503967285\n",
      "(64, 33)\n",
      "step 25210, loss is 4.7289814949035645\n",
      "(64, 33)\n",
      "step 25211, loss is 4.723933219909668\n",
      "(64, 33)\n",
      "step 25212, loss is 4.777263164520264\n",
      "(64, 33)\n",
      "step 25213, loss is 4.794148921966553\n",
      "(64, 33)\n",
      "step 25214, loss is 4.9394850730896\n",
      "(64, 33)\n",
      "step 25215, loss is 4.679856300354004\n",
      "(64, 33)\n",
      "step 25216, loss is 4.94760274887085\n",
      "(64, 33)\n",
      "step 25217, loss is 4.670956611633301\n",
      "(64, 33)\n",
      "step 25218, loss is 4.8700785636901855\n",
      "(64, 33)\n",
      "step 25219, loss is 4.639699935913086\n",
      "(64, 33)\n",
      "step 25220, loss is 4.695960998535156\n",
      "(64, 33)\n",
      "step 25221, loss is 4.765852451324463\n",
      "(64, 33)\n",
      "step 25222, loss is 4.763861656188965\n",
      "(64, 33)\n",
      "step 25223, loss is 4.917708873748779\n",
      "(64, 33)\n",
      "step 25224, loss is 4.856816291809082\n",
      "(64, 33)\n",
      "step 25225, loss is 4.929192066192627\n",
      "(64, 33)\n",
      "step 25226, loss is 4.70839262008667\n",
      "(64, 33)\n",
      "step 25227, loss is 4.8750691413879395\n",
      "(64, 33)\n",
      "step 25228, loss is 4.6916961669921875\n",
      "(64, 33)\n",
      "step 25229, loss is 4.720734596252441\n",
      "(64, 33)\n",
      "step 25230, loss is 4.775934219360352\n",
      "(64, 33)\n",
      "step 25231, loss is 4.794401168823242\n",
      "(64, 33)\n",
      "step 25232, loss is 4.9031829833984375\n",
      "(64, 33)\n",
      "step 25233, loss is 4.880778789520264\n",
      "(64, 33)\n",
      "step 25234, loss is 4.631439208984375\n",
      "(64, 33)\n",
      "step 25235, loss is 4.911984443664551\n",
      "(64, 33)\n",
      "step 25236, loss is 4.813011169433594\n",
      "(64, 33)\n",
      "step 25237, loss is 4.777851581573486\n",
      "(64, 33)\n",
      "step 25238, loss is 4.712131023406982\n",
      "(64, 33)\n",
      "step 25239, loss is 4.913640975952148\n",
      "(64, 33)\n",
      "step 25240, loss is 4.906340599060059\n",
      "(64, 33)\n",
      "step 25241, loss is 4.951150417327881\n",
      "(64, 33)\n",
      "step 25242, loss is 4.759098529815674\n",
      "(64, 33)\n",
      "step 25243, loss is 4.6607890129089355\n",
      "(64, 33)\n",
      "step 25244, loss is 4.708738803863525\n",
      "(64, 33)\n",
      "step 25245, loss is 4.815068244934082\n",
      "(64, 33)\n",
      "step 25246, loss is 4.98322057723999\n",
      "(64, 33)\n",
      "step 25247, loss is 4.62217378616333\n",
      "(64, 33)\n",
      "step 25248, loss is 4.822410583496094\n",
      "(64, 33)\n",
      "step 25249, loss is 4.845503330230713\n",
      "(64, 33)\n",
      "step 25250, loss is 4.8542256355285645\n",
      "(64, 33)\n",
      "step 25251, loss is 4.6399922370910645\n",
      "(64, 33)\n",
      "step 25252, loss is 4.936861515045166\n",
      "(64, 33)\n",
      "step 25253, loss is 5.011326313018799\n",
      "(64, 33)\n",
      "step 25254, loss is 4.934990406036377\n",
      "(64, 33)\n",
      "step 25255, loss is 4.920066833496094\n",
      "(64, 33)\n",
      "step 25256, loss is 4.797574043273926\n",
      "(64, 33)\n",
      "step 25257, loss is 4.891587257385254\n",
      "(64, 33)\n",
      "step 25258, loss is 4.8716230392456055\n",
      "(64, 33)\n",
      "step 25259, loss is 4.731076717376709\n",
      "(64, 33)\n",
      "step 25260, loss is 4.426673889160156\n",
      "(64, 33)\n",
      "step 25261, loss is 4.759011268615723\n",
      "(64, 33)\n",
      "step 25262, loss is 4.784404754638672\n",
      "(64, 33)\n",
      "step 25263, loss is 4.8320794105529785\n",
      "(64, 33)\n",
      "step 25264, loss is 4.590637683868408\n",
      "(64, 33)\n",
      "step 25265, loss is 4.712060928344727\n",
      "(64, 33)\n",
      "step 25266, loss is 4.7808966636657715\n",
      "(64, 33)\n",
      "step 25267, loss is 4.967281341552734\n",
      "(64, 33)\n",
      "step 25268, loss is 4.764101028442383\n",
      "(64, 33)\n",
      "step 25269, loss is 4.790434837341309\n",
      "(64, 33)\n",
      "step 25270, loss is 4.724337577819824\n",
      "(64, 33)\n",
      "step 25271, loss is 4.717716693878174\n",
      "(64, 33)\n",
      "step 25272, loss is 4.780089855194092\n",
      "(64, 33)\n",
      "step 25273, loss is 4.731307029724121\n",
      "(64, 33)\n",
      "step 25274, loss is 4.6675872802734375\n",
      "(64, 33)\n",
      "step 25275, loss is 4.812714099884033\n",
      "(64, 33)\n",
      "step 25276, loss is 4.682505130767822\n",
      "(64, 33)\n",
      "step 25277, loss is 4.773259162902832\n",
      "(64, 33)\n",
      "step 25278, loss is 4.6407904624938965\n",
      "(64, 33)\n",
      "step 25279, loss is 4.807627201080322\n",
      "(64, 33)\n",
      "step 25280, loss is 4.564426422119141\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25281, loss is 4.801368236541748\n",
      "(64, 33)\n",
      "step 25282, loss is 4.6973185539245605\n",
      "(64, 33)\n",
      "step 25283, loss is 4.953808784484863\n",
      "(64, 33)\n",
      "step 25284, loss is 4.648078918457031\n",
      "(64, 33)\n",
      "step 25285, loss is 4.688178062438965\n",
      "(64, 33)\n",
      "step 25286, loss is 4.8578643798828125\n",
      "(64, 33)\n",
      "step 25287, loss is 4.8653082847595215\n",
      "(64, 33)\n",
      "step 25288, loss is 4.6394877433776855\n",
      "(64, 33)\n",
      "step 25289, loss is 4.659337043762207\n",
      "(64, 33)\n",
      "step 25290, loss is 4.822381019592285\n",
      "(64, 33)\n",
      "step 25291, loss is 4.833195686340332\n",
      "(64, 33)\n",
      "step 25292, loss is 4.75083589553833\n",
      "(64, 33)\n",
      "step 25293, loss is 4.858298301696777\n",
      "(64, 33)\n",
      "step 25294, loss is 4.852593421936035\n",
      "(64, 33)\n",
      "step 25295, loss is 4.814469814300537\n",
      "(64, 33)\n",
      "step 25296, loss is 4.714864253997803\n",
      "(64, 33)\n",
      "step 25297, loss is 4.815922260284424\n",
      "(64, 33)\n",
      "step 25298, loss is 4.8372015953063965\n",
      "(64, 33)\n",
      "step 25299, loss is 4.775716304779053\n",
      "(64, 33)\n",
      "step 25300, loss is 4.779212951660156\n",
      "(64, 33)\n",
      "step 25301, loss is 4.75061559677124\n",
      "(64, 33)\n",
      "step 25302, loss is 4.872591972351074\n",
      "(64, 33)\n",
      "step 25303, loss is 4.8230366706848145\n",
      "(64, 33)\n",
      "step 25304, loss is 4.630304336547852\n",
      "(64, 33)\n",
      "step 25305, loss is 4.745859146118164\n",
      "(64, 33)\n",
      "step 25306, loss is 5.0474467277526855\n",
      "(64, 33)\n",
      "step 25307, loss is 4.783665657043457\n",
      "(64, 33)\n",
      "step 25308, loss is 4.783066272735596\n",
      "(64, 33)\n",
      "step 25309, loss is 4.767429351806641\n",
      "(64, 33)\n",
      "step 25310, loss is 4.699760913848877\n",
      "(64, 33)\n",
      "step 25311, loss is 4.831655025482178\n",
      "(64, 33)\n",
      "step 25312, loss is 4.7816338539123535\n",
      "(64, 33)\n",
      "step 25313, loss is 4.702883720397949\n",
      "(64, 33)\n",
      "step 25314, loss is 4.669907569885254\n",
      "(64, 33)\n",
      "step 25315, loss is 4.818411827087402\n",
      "(64, 33)\n",
      "step 25316, loss is 4.690939903259277\n",
      "(64, 33)\n",
      "step 25317, loss is 4.755531311035156\n",
      "(64, 33)\n",
      "step 25318, loss is 4.750516891479492\n",
      "(64, 33)\n",
      "step 25319, loss is 4.644021511077881\n",
      "(64, 33)\n",
      "step 25320, loss is 4.745944976806641\n",
      "(64, 33)\n",
      "step 25321, loss is 4.809449672698975\n",
      "(64, 33)\n",
      "step 25322, loss is 4.842085838317871\n",
      "(64, 33)\n",
      "step 25323, loss is 4.808320045471191\n",
      "(64, 33)\n",
      "step 25324, loss is 4.667876720428467\n",
      "(64, 33)\n",
      "step 25325, loss is 4.779070854187012\n",
      "(64, 33)\n",
      "step 25326, loss is 4.787032604217529\n",
      "(64, 33)\n",
      "step 25327, loss is 4.81758975982666\n",
      "(64, 33)\n",
      "step 25328, loss is 4.664158821105957\n",
      "(64, 33)\n",
      "step 25329, loss is 4.85433292388916\n",
      "(64, 33)\n",
      "step 25330, loss is 4.732858180999756\n",
      "(64, 33)\n",
      "step 25331, loss is 4.810637474060059\n",
      "(64, 33)\n",
      "step 25332, loss is 4.9491801261901855\n",
      "(64, 33)\n",
      "step 25333, loss is 4.689138412475586\n",
      "(64, 33)\n",
      "step 25334, loss is 4.874794960021973\n",
      "(64, 33)\n",
      "step 25335, loss is 4.790562152862549\n",
      "(64, 33)\n",
      "step 25336, loss is 4.554230213165283\n",
      "(64, 33)\n",
      "step 25337, loss is 4.748728275299072\n",
      "(64, 33)\n",
      "step 25338, loss is 4.903476238250732\n",
      "(64, 33)\n",
      "step 25339, loss is 4.716312408447266\n",
      "(64, 33)\n",
      "step 25340, loss is 4.676387310028076\n",
      "(64, 33)\n",
      "step 25341, loss is 4.823360443115234\n",
      "(64, 33)\n",
      "step 25342, loss is 4.795374393463135\n",
      "(64, 33)\n",
      "step 25343, loss is 4.901978969573975\n",
      "(64, 33)\n",
      "step 25344, loss is 4.619075775146484\n",
      "(64, 33)\n",
      "step 25345, loss is 4.859362602233887\n",
      "(64, 33)\n",
      "step 25346, loss is 4.70816707611084\n",
      "(64, 33)\n",
      "step 25347, loss is 4.772493839263916\n",
      "(64, 33)\n",
      "step 25348, loss is 4.85507869720459\n",
      "(64, 33)\n",
      "step 25349, loss is 4.763828277587891\n",
      "(64, 33)\n",
      "step 25350, loss is 4.788092613220215\n",
      "(64, 33)\n",
      "step 25351, loss is 4.769876003265381\n",
      "(64, 33)\n",
      "step 25352, loss is 4.565803050994873\n",
      "(64, 33)\n",
      "step 25353, loss is 4.70536470413208\n",
      "(64, 33)\n",
      "step 25354, loss is 4.852275848388672\n",
      "(64, 33)\n",
      "step 25355, loss is 4.845446586608887\n",
      "(64, 33)\n",
      "step 25356, loss is 4.713827133178711\n",
      "(64, 33)\n",
      "step 25357, loss is 4.723899841308594\n",
      "(64, 33)\n",
      "step 25358, loss is 4.810744762420654\n",
      "(64, 33)\n",
      "step 25359, loss is 4.836544990539551\n",
      "(64, 33)\n",
      "step 25360, loss is 4.777286529541016\n",
      "(64, 33)\n",
      "step 25361, loss is 4.661553382873535\n",
      "(64, 33)\n",
      "step 25362, loss is 4.749777793884277\n",
      "(64, 33)\n",
      "step 25363, loss is 4.847011566162109\n",
      "(64, 33)\n",
      "step 25364, loss is 4.714884281158447\n",
      "(64, 33)\n",
      "step 25365, loss is 4.8265275955200195\n",
      "(64, 33)\n",
      "step 25366, loss is 4.600664138793945\n",
      "(64, 33)\n",
      "step 25367, loss is 4.7628254890441895\n",
      "(64, 33)\n",
      "step 25368, loss is 4.789055347442627\n",
      "(64, 33)\n",
      "step 25369, loss is 4.894055366516113\n",
      "(64, 33)\n",
      "step 25370, loss is 4.725256443023682\n",
      "(64, 33)\n",
      "step 25371, loss is 4.817055702209473\n",
      "(64, 33)\n",
      "step 25372, loss is 4.907464981079102\n",
      "(64, 33)\n",
      "step 25373, loss is 4.9682512283325195\n",
      "(64, 33)\n",
      "step 25374, loss is 4.8716230392456055\n",
      "(64, 33)\n",
      "step 25375, loss is 4.779290676116943\n",
      "(64, 33)\n",
      "step 25376, loss is 4.770286560058594\n",
      "(64, 33)\n",
      "step 25377, loss is 4.899791717529297\n",
      "(64, 33)\n",
      "step 25378, loss is 4.7866339683532715\n",
      "(64, 33)\n",
      "step 25379, loss is 4.595880031585693\n",
      "(64, 33)\n",
      "step 25380, loss is 4.8492255210876465\n",
      "(64, 33)\n",
      "step 25381, loss is 4.888121128082275\n",
      "(64, 33)\n",
      "step 25382, loss is 4.567801475524902\n",
      "(64, 33)\n",
      "step 25383, loss is 4.964313983917236\n",
      "(64, 33)\n",
      "step 25384, loss is 4.78223991394043\n",
      "(64, 33)\n",
      "step 25385, loss is 5.039948463439941\n",
      "(64, 33)\n",
      "step 25386, loss is 4.6094970703125\n",
      "(64, 33)\n",
      "step 25387, loss is 4.761306285858154\n",
      "(64, 33)\n",
      "step 25388, loss is 4.809298038482666\n",
      "(64, 33)\n",
      "step 25389, loss is 4.791460990905762\n",
      "(64, 33)\n",
      "step 25390, loss is 4.7923359870910645\n",
      "(64, 33)\n",
      "step 25391, loss is 4.856614112854004\n",
      "(64, 33)\n",
      "step 25392, loss is 4.768274784088135\n",
      "(64, 33)\n",
      "step 25393, loss is 4.966242790222168\n",
      "(64, 33)\n",
      "step 25394, loss is 4.7459001541137695\n",
      "(64, 33)\n",
      "step 25395, loss is 4.768324375152588\n",
      "(64, 33)\n",
      "step 25396, loss is 4.710675239562988\n",
      "(64, 33)\n",
      "step 25397, loss is 4.726471900939941\n",
      "(64, 33)\n",
      "step 25398, loss is 4.928771018981934\n",
      "(64, 33)\n",
      "step 25399, loss is 4.796718597412109\n",
      "(64, 33)\n",
      "step 25400, loss is 4.91810417175293\n",
      "(64, 33)\n",
      "step 25401, loss is 4.631785869598389\n",
      "(64, 33)\n",
      "step 25402, loss is 4.824582099914551\n",
      "(64, 33)\n",
      "step 25403, loss is 4.761807918548584\n",
      "(64, 33)\n",
      "step 25404, loss is 4.717767238616943\n",
      "(64, 33)\n",
      "step 25405, loss is 4.880659103393555\n",
      "(64, 33)\n",
      "step 25406, loss is 4.823901653289795\n",
      "(64, 33)\n",
      "step 25407, loss is 5.037187099456787\n",
      "(64, 33)\n",
      "step 25408, loss is 4.869798183441162\n",
      "(64, 33)\n",
      "step 25409, loss is 4.924710750579834\n",
      "(64, 33)\n",
      "step 25410, loss is 4.7250895500183105\n",
      "(64, 33)\n",
      "step 25411, loss is 4.701043605804443\n",
      "(64, 33)\n",
      "step 25412, loss is 4.775265693664551\n",
      "(64, 33)\n",
      "step 25413, loss is 4.824499607086182\n",
      "(64, 33)\n",
      "step 25414, loss is 4.762234210968018\n",
      "(64, 33)\n",
      "step 25415, loss is 4.792194843292236\n",
      "(64, 33)\n",
      "step 25416, loss is 4.845420837402344\n",
      "(64, 33)\n",
      "step 25417, loss is 4.578051567077637\n",
      "(64, 33)\n",
      "step 25418, loss is 4.764039993286133\n",
      "(64, 33)\n",
      "step 25419, loss is 4.915058612823486\n",
      "(64, 33)\n",
      "step 25420, loss is 4.707171440124512\n",
      "(64, 33)\n",
      "step 25421, loss is 4.797316074371338\n",
      "(64, 33)\n",
      "step 25422, loss is 4.737163066864014\n",
      "(64, 33)\n",
      "step 25423, loss is 4.960737705230713\n",
      "(64, 33)\n",
      "step 25424, loss is 4.784755229949951\n",
      "(64, 33)\n",
      "step 25425, loss is 4.722889423370361\n",
      "(64, 33)\n",
      "step 25426, loss is 4.901443004608154\n",
      "(64, 33)\n",
      "step 25427, loss is 4.808026313781738\n",
      "(64, 33)\n",
      "step 25428, loss is 4.73926305770874\n",
      "(64, 33)\n",
      "step 25429, loss is 4.823430061340332\n",
      "(64, 33)\n",
      "step 25430, loss is 4.848205089569092\n",
      "(64, 33)\n",
      "step 25431, loss is 4.8697190284729\n",
      "(64, 33)\n",
      "step 25432, loss is 4.915682792663574\n",
      "(64, 33)\n",
      "step 25433, loss is 4.867433547973633\n",
      "(64, 33)\n",
      "step 25434, loss is 4.859711170196533\n",
      "(64, 33)\n",
      "step 25435, loss is 4.763199329376221\n",
      "(64, 33)\n",
      "step 25436, loss is 4.736915588378906\n",
      "(64, 33)\n",
      "step 25437, loss is 4.76550817489624\n",
      "(64, 33)\n",
      "step 25438, loss is 4.681895732879639\n",
      "(64, 33)\n",
      "step 25439, loss is 4.678688049316406\n",
      "(64, 33)\n",
      "step 25440, loss is 4.703023433685303\n",
      "(64, 33)\n",
      "step 25441, loss is 4.919076442718506\n",
      "(64, 33)\n",
      "step 25442, loss is 4.683265686035156\n",
      "(64, 33)\n",
      "step 25443, loss is 4.747404098510742\n",
      "(64, 33)\n",
      "step 25444, loss is 4.9268927574157715\n",
      "(64, 33)\n",
      "step 25445, loss is 4.72147798538208\n",
      "(64, 33)\n",
      "step 25446, loss is 4.839085578918457\n",
      "(64, 33)\n",
      "step 25447, loss is 4.806653022766113\n",
      "(64, 33)\n",
      "step 25448, loss is 5.027979850769043\n",
      "(64, 33)\n",
      "step 25449, loss is 4.718829154968262\n",
      "(64, 33)\n",
      "step 25450, loss is 4.722171306610107\n",
      "(64, 33)\n",
      "step 25451, loss is 4.655990123748779\n",
      "(64, 33)\n",
      "step 25452, loss is 4.645636081695557\n",
      "(64, 33)\n",
      "step 25453, loss is 4.78477144241333\n",
      "(64, 33)\n",
      "step 25454, loss is 4.748037338256836\n",
      "(64, 33)\n",
      "step 25455, loss is 4.70095157623291\n",
      "(64, 33)\n",
      "step 25456, loss is 4.742600440979004\n",
      "(64, 33)\n",
      "step 25457, loss is 4.783440589904785\n",
      "(64, 33)\n",
      "step 25458, loss is 4.799595355987549\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25459, loss is 4.867039203643799\n",
      "(64, 33)\n",
      "step 25460, loss is 4.796666145324707\n",
      "(64, 33)\n",
      "step 25461, loss is 4.670421123504639\n",
      "(64, 33)\n",
      "step 25462, loss is 4.775791168212891\n",
      "(64, 33)\n",
      "step 25463, loss is 4.785980224609375\n",
      "(64, 33)\n",
      "step 25464, loss is 4.693525314331055\n",
      "(64, 33)\n",
      "step 25465, loss is 4.9330854415893555\n",
      "(64, 33)\n",
      "step 25466, loss is 4.910247802734375\n",
      "(64, 33)\n",
      "step 25467, loss is 4.721717834472656\n",
      "(64, 33)\n",
      "step 25468, loss is 4.5892462730407715\n",
      "(64, 33)\n",
      "step 25469, loss is 4.943578720092773\n",
      "(64, 33)\n",
      "step 25470, loss is 4.803884029388428\n",
      "(64, 33)\n",
      "step 25471, loss is 4.8910956382751465\n",
      "(64, 33)\n",
      "step 25472, loss is 4.690374851226807\n",
      "(64, 33)\n",
      "step 25473, loss is 4.853273391723633\n",
      "(64, 33)\n",
      "step 25474, loss is 4.803988456726074\n",
      "(64, 33)\n",
      "step 25475, loss is 4.6951422691345215\n",
      "(64, 33)\n",
      "step 25476, loss is 4.5813140869140625\n",
      "(64, 33)\n",
      "step 25477, loss is 4.875370502471924\n",
      "(64, 33)\n",
      "step 25478, loss is 4.686665058135986\n",
      "(64, 33)\n",
      "step 25479, loss is 4.995973587036133\n",
      "(64, 33)\n",
      "step 25480, loss is 4.816615104675293\n",
      "(64, 33)\n",
      "step 25481, loss is 4.8273820877075195\n",
      "(64, 33)\n",
      "step 25482, loss is 4.72271728515625\n",
      "(64, 33)\n",
      "step 25483, loss is 4.8610429763793945\n",
      "(64, 33)\n",
      "step 25484, loss is 4.7548933029174805\n",
      "(64, 33)\n",
      "step 25485, loss is 4.740015506744385\n",
      "(64, 33)\n",
      "step 25486, loss is 5.015621185302734\n",
      "(64, 33)\n",
      "step 25487, loss is 4.650704383850098\n",
      "(64, 33)\n",
      "step 25488, loss is 4.788311004638672\n",
      "(64, 33)\n",
      "step 25489, loss is 4.740278244018555\n",
      "(64, 33)\n",
      "step 25490, loss is 4.723049163818359\n",
      "(64, 33)\n",
      "step 25491, loss is 4.596090316772461\n",
      "(64, 33)\n",
      "step 25492, loss is 4.692416667938232\n",
      "(64, 33)\n",
      "step 25493, loss is 4.943305969238281\n",
      "(64, 33)\n",
      "step 25494, loss is 4.706735134124756\n",
      "(64, 33)\n",
      "step 25495, loss is 4.7063374519348145\n",
      "(64, 33)\n",
      "step 25496, loss is 4.76490592956543\n",
      "(64, 33)\n",
      "step 25497, loss is 4.320679664611816\n",
      "(64, 33)\n",
      "step 25498, loss is 4.600741863250732\n",
      "(64, 33)\n",
      "step 25499, loss is 4.628174304962158\n",
      "(64, 33)\n",
      "step 25500, loss is 4.885693550109863\n",
      "(64, 33)\n",
      "step 25501, loss is 4.582429885864258\n",
      "(64, 33)\n",
      "step 25502, loss is 4.800045013427734\n",
      "(64, 33)\n",
      "step 25503, loss is 4.715155601501465\n",
      "(64, 33)\n",
      "step 25504, loss is 4.686031341552734\n",
      "(64, 33)\n",
      "step 25505, loss is 4.811366558074951\n",
      "(64, 33)\n",
      "step 25506, loss is 4.7724761962890625\n",
      "(64, 33)\n",
      "step 25507, loss is 4.792022228240967\n",
      "(64, 33)\n",
      "step 25508, loss is 4.832240581512451\n",
      "(64, 33)\n",
      "step 25509, loss is 4.926806926727295\n",
      "(64, 33)\n",
      "step 25510, loss is 4.813100814819336\n",
      "(64, 33)\n",
      "step 25511, loss is 4.8710246086120605\n",
      "(64, 33)\n",
      "step 25512, loss is 4.625061988830566\n",
      "(64, 33)\n",
      "step 25513, loss is 4.736718654632568\n",
      "(64, 33)\n",
      "step 25514, loss is 4.848634243011475\n",
      "(64, 33)\n",
      "step 25515, loss is 4.978076457977295\n",
      "(64, 33)\n",
      "step 25516, loss is 4.880671501159668\n",
      "(64, 33)\n",
      "step 25517, loss is 4.663991451263428\n",
      "(64, 33)\n",
      "step 25518, loss is 4.737273216247559\n",
      "(64, 33)\n",
      "step 25519, loss is 4.798952579498291\n",
      "(64, 33)\n",
      "step 25520, loss is 4.843591690063477\n",
      "(64, 33)\n",
      "step 25521, loss is 4.85438871383667\n",
      "(64, 33)\n",
      "step 25522, loss is 4.506272315979004\n",
      "(64, 33)\n",
      "step 25523, loss is 4.810001373291016\n",
      "(64, 33)\n",
      "step 25524, loss is 4.6578497886657715\n",
      "(64, 33)\n",
      "step 25525, loss is 4.6551899909973145\n",
      "(64, 33)\n",
      "step 25526, loss is 4.917941093444824\n",
      "(64, 33)\n",
      "step 25527, loss is 4.902406215667725\n",
      "(64, 33)\n",
      "step 25528, loss is 4.865366458892822\n",
      "(64, 33)\n",
      "step 25529, loss is 4.788300514221191\n",
      "(64, 33)\n",
      "step 25530, loss is 4.695728778839111\n",
      "(64, 33)\n",
      "step 25531, loss is 4.643123626708984\n",
      "(64, 33)\n",
      "step 25532, loss is 4.787897109985352\n",
      "(64, 33)\n",
      "step 25533, loss is 4.879960536956787\n",
      "(64, 33)\n",
      "step 25534, loss is 4.838903427124023\n",
      "(64, 33)\n",
      "step 25535, loss is 4.858551979064941\n",
      "(64, 33)\n",
      "step 25536, loss is 4.954159736633301\n",
      "(64, 33)\n",
      "step 25537, loss is 4.910823345184326\n",
      "(64, 33)\n",
      "step 25538, loss is 4.77493143081665\n",
      "(64, 33)\n",
      "step 25539, loss is 4.750921249389648\n",
      "(64, 33)\n",
      "step 25540, loss is 4.664103031158447\n",
      "(64, 33)\n",
      "step 25541, loss is 4.796511173248291\n",
      "(64, 33)\n",
      "step 25542, loss is 4.574983596801758\n",
      "(64, 33)\n",
      "step 25543, loss is 4.829728126525879\n",
      "(64, 33)\n",
      "step 25544, loss is 4.854642868041992\n",
      "(64, 33)\n",
      "step 25545, loss is 4.755690097808838\n",
      "(64, 33)\n",
      "step 25546, loss is 4.876602649688721\n",
      "(64, 33)\n",
      "step 25547, loss is 4.695664882659912\n",
      "(64, 33)\n",
      "step 25548, loss is 4.705659866333008\n",
      "(64, 33)\n",
      "step 25549, loss is 4.777851104736328\n",
      "(64, 33)\n",
      "step 25550, loss is 4.681792736053467\n",
      "(64, 33)\n",
      "step 25551, loss is 4.672836780548096\n",
      "(64, 33)\n",
      "step 25552, loss is 4.7599029541015625\n",
      "(64, 33)\n",
      "step 25553, loss is 4.633920669555664\n",
      "(64, 33)\n",
      "step 25554, loss is 4.676538944244385\n",
      "(64, 33)\n",
      "step 25555, loss is 4.643180847167969\n",
      "(64, 33)\n",
      "step 25556, loss is 4.76173734664917\n",
      "(64, 33)\n",
      "step 25557, loss is 5.078403472900391\n",
      "(64, 33)\n",
      "step 25558, loss is 5.056849002838135\n",
      "(64, 33)\n",
      "step 25559, loss is 4.654466152191162\n",
      "(64, 33)\n",
      "step 25560, loss is 4.575977325439453\n",
      "(64, 33)\n",
      "step 25561, loss is 4.865521430969238\n",
      "(64, 33)\n",
      "step 25562, loss is 4.720133304595947\n",
      "(64, 33)\n",
      "step 25563, loss is 4.998099327087402\n",
      "(64, 33)\n",
      "step 25564, loss is 4.601241111755371\n",
      "(64, 33)\n",
      "step 25565, loss is 4.799922943115234\n",
      "(64, 33)\n",
      "step 25566, loss is 4.750009059906006\n",
      "(64, 33)\n",
      "step 25567, loss is 4.775881767272949\n",
      "(64, 33)\n",
      "step 25568, loss is 4.797664642333984\n",
      "(64, 33)\n",
      "step 25569, loss is 4.927523612976074\n",
      "(64, 33)\n",
      "step 25570, loss is 4.6914191246032715\n",
      "(64, 33)\n",
      "step 25571, loss is 4.556665420532227\n",
      "(64, 33)\n",
      "step 25572, loss is 4.710676193237305\n",
      "(64, 33)\n",
      "step 25573, loss is 4.764777183532715\n",
      "(64, 33)\n",
      "step 25574, loss is 4.900288105010986\n",
      "(64, 33)\n",
      "step 25575, loss is 4.443632125854492\n",
      "(64, 33)\n",
      "step 25576, loss is 4.6477274894714355\n",
      "(64, 33)\n",
      "step 25577, loss is 4.884683609008789\n",
      "(64, 33)\n",
      "step 25578, loss is 4.749392509460449\n",
      "(64, 33)\n",
      "step 25579, loss is 4.696030616760254\n",
      "(64, 33)\n",
      "step 25580, loss is 4.769654273986816\n",
      "(64, 33)\n",
      "step 25581, loss is 4.773386001586914\n",
      "(64, 33)\n",
      "step 25582, loss is 4.529636383056641\n",
      "(64, 33)\n",
      "step 25583, loss is 4.792908191680908\n",
      "(64, 33)\n",
      "step 25584, loss is 4.895138263702393\n",
      "(64, 33)\n",
      "step 25585, loss is 4.8047895431518555\n",
      "(64, 33)\n",
      "step 25586, loss is 4.836488723754883\n",
      "(64, 33)\n",
      "step 25587, loss is 4.689663410186768\n",
      "(64, 33)\n",
      "step 25588, loss is 4.6951680183410645\n",
      "(64, 33)\n",
      "step 25589, loss is 4.715285778045654\n",
      "(64, 33)\n",
      "step 25590, loss is 4.771396636962891\n",
      "(64, 33)\n",
      "step 25591, loss is 4.750524520874023\n",
      "(64, 33)\n",
      "step 25592, loss is 4.881218433380127\n",
      "(64, 33)\n",
      "step 25593, loss is 4.759015083312988\n",
      "(64, 33)\n",
      "step 25594, loss is 4.718017578125\n",
      "(64, 33)\n",
      "step 25595, loss is 4.820688247680664\n",
      "(64, 33)\n",
      "step 25596, loss is 4.795261859893799\n",
      "(64, 33)\n",
      "step 25597, loss is 4.6366167068481445\n",
      "(64, 33)\n",
      "step 25598, loss is 4.664921283721924\n",
      "(64, 33)\n",
      "step 25599, loss is 4.840846538543701\n",
      "(64, 33)\n",
      "step 25600, loss is 4.68928861618042\n",
      "(64, 33)\n",
      "step 25601, loss is 4.713686466217041\n",
      "(64, 33)\n",
      "step 25602, loss is 4.652557849884033\n",
      "(64, 33)\n",
      "step 25603, loss is 4.7574591636657715\n",
      "(64, 33)\n",
      "step 25604, loss is 4.746554374694824\n",
      "(64, 33)\n",
      "step 25605, loss is 4.651231288909912\n",
      "(64, 33)\n",
      "step 25606, loss is 4.82595157623291\n",
      "(64, 33)\n",
      "step 25607, loss is 4.7608513832092285\n",
      "(64, 33)\n",
      "step 25608, loss is 4.769940376281738\n",
      "(64, 33)\n",
      "step 25609, loss is 4.828371524810791\n",
      "(64, 33)\n",
      "step 25610, loss is 4.756148815155029\n",
      "(64, 33)\n",
      "step 25611, loss is 4.6384124755859375\n",
      "(64, 33)\n",
      "step 25612, loss is 4.799687385559082\n",
      "(64, 33)\n",
      "step 25613, loss is 4.864257335662842\n",
      "(64, 33)\n",
      "step 25614, loss is 4.792045593261719\n",
      "(64, 33)\n",
      "step 25615, loss is 4.6353230476379395\n",
      "(64, 33)\n",
      "step 25616, loss is 4.718632221221924\n",
      "(64, 33)\n",
      "step 25617, loss is 4.682064533233643\n",
      "(64, 33)\n",
      "step 25618, loss is 4.62411642074585\n",
      "(64, 33)\n",
      "step 25619, loss is 4.787947177886963\n",
      "(64, 33)\n",
      "step 25620, loss is 4.727256774902344\n",
      "(64, 33)\n",
      "step 25621, loss is 4.8163161277771\n",
      "(64, 33)\n",
      "step 25622, loss is 4.6559271812438965\n",
      "(64, 33)\n",
      "step 25623, loss is 4.737699031829834\n",
      "(64, 33)\n",
      "step 25624, loss is 4.941652297973633\n",
      "(64, 33)\n",
      "step 25625, loss is 4.738095760345459\n",
      "(64, 33)\n",
      "step 25626, loss is 4.607287406921387\n",
      "(64, 33)\n",
      "step 25627, loss is 4.733865737915039\n",
      "(64, 33)\n",
      "step 25628, loss is 4.9133453369140625\n",
      "(64, 33)\n",
      "step 25629, loss is 4.602993965148926\n",
      "(64, 33)\n",
      "step 25630, loss is 4.796630382537842\n",
      "(64, 33)\n",
      "step 25631, loss is 4.664366722106934\n",
      "(64, 33)\n",
      "step 25632, loss is 4.973062992095947\n",
      "(64, 33)\n",
      "step 25633, loss is 4.833907604217529\n",
      "(64, 33)\n",
      "step 25634, loss is 4.591345310211182\n",
      "(64, 33)\n",
      "step 25635, loss is 4.852685928344727\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25636, loss is 4.63102912902832\n",
      "(64, 33)\n",
      "step 25637, loss is 4.623122692108154\n",
      "(64, 33)\n",
      "step 25638, loss is 4.954172611236572\n",
      "(64, 33)\n",
      "step 25639, loss is 4.778961658477783\n",
      "(64, 33)\n",
      "step 25640, loss is 4.7034831047058105\n",
      "(64, 33)\n",
      "step 25641, loss is 4.764262676239014\n",
      "(64, 33)\n",
      "step 25642, loss is 5.030012607574463\n",
      "(64, 33)\n",
      "step 25643, loss is 4.652810573577881\n",
      "(64, 33)\n",
      "step 25644, loss is 4.532022953033447\n",
      "(64, 33)\n",
      "step 25645, loss is 4.928783893585205\n",
      "(64, 33)\n",
      "step 25646, loss is 4.815829753875732\n",
      "(64, 33)\n",
      "step 25647, loss is 4.653555393218994\n",
      "(64, 33)\n",
      "step 25648, loss is 4.809753894805908\n",
      "(64, 33)\n",
      "step 25649, loss is 4.7281670570373535\n",
      "(64, 33)\n",
      "step 25650, loss is 4.572242736816406\n",
      "(64, 33)\n",
      "step 25651, loss is 4.68247127532959\n",
      "(64, 33)\n",
      "step 25652, loss is 4.62382698059082\n",
      "(64, 33)\n",
      "step 25653, loss is 4.87421989440918\n",
      "(64, 33)\n",
      "step 25654, loss is 4.872395038604736\n",
      "(64, 33)\n",
      "step 25655, loss is 4.654857635498047\n",
      "(64, 33)\n",
      "step 25656, loss is 4.883983612060547\n",
      "(64, 33)\n",
      "step 25657, loss is 4.807639122009277\n",
      "(64, 33)\n",
      "step 25658, loss is 4.759048938751221\n",
      "(64, 33)\n",
      "step 25659, loss is 4.822299480438232\n",
      "(64, 33)\n",
      "step 25660, loss is 4.869750022888184\n",
      "(64, 33)\n",
      "step 25661, loss is 4.86193323135376\n",
      "(64, 33)\n",
      "step 25662, loss is 4.820513725280762\n",
      "(64, 33)\n",
      "step 25663, loss is 4.963368892669678\n",
      "(64, 33)\n",
      "step 25664, loss is 4.802955150604248\n",
      "(64, 33)\n",
      "step 25665, loss is 4.821202754974365\n",
      "(64, 33)\n",
      "step 25666, loss is 4.729029655456543\n",
      "(64, 33)\n",
      "step 25667, loss is 4.813784122467041\n",
      "(64, 33)\n",
      "step 25668, loss is 4.62847900390625\n",
      "(64, 33)\n",
      "step 25669, loss is 4.629694938659668\n",
      "(64, 33)\n",
      "step 25670, loss is 4.858831405639648\n",
      "(64, 33)\n",
      "step 25671, loss is 4.700714111328125\n",
      "(64, 33)\n",
      "step 25672, loss is 5.0887861251831055\n",
      "(64, 33)\n",
      "step 25673, loss is 4.688636302947998\n",
      "(64, 33)\n",
      "step 25674, loss is 4.682394027709961\n",
      "(64, 33)\n",
      "step 25675, loss is 4.786319255828857\n",
      "(64, 33)\n",
      "step 25676, loss is 4.5844221115112305\n",
      "(64, 33)\n",
      "step 25677, loss is 4.592212677001953\n",
      "(64, 33)\n",
      "step 25678, loss is 4.793685436248779\n",
      "(64, 33)\n",
      "step 25679, loss is 4.850892066955566\n",
      "(64, 33)\n",
      "step 25680, loss is 4.696975231170654\n",
      "(64, 33)\n",
      "step 25681, loss is 4.949940204620361\n",
      "(64, 33)\n",
      "step 25682, loss is 4.699588775634766\n",
      "(64, 33)\n",
      "step 25683, loss is 4.8362345695495605\n",
      "(64, 33)\n",
      "step 25684, loss is 4.906994342803955\n",
      "(64, 33)\n",
      "step 25685, loss is 4.779623985290527\n",
      "(64, 33)\n",
      "step 25686, loss is 4.8234477043151855\n",
      "(64, 33)\n",
      "step 25687, loss is 4.665332794189453\n",
      "(64, 33)\n",
      "step 25688, loss is 4.802436828613281\n",
      "(64, 33)\n",
      "step 25689, loss is 4.792866230010986\n",
      "(64, 33)\n",
      "step 25690, loss is 4.779575347900391\n",
      "(64, 33)\n",
      "step 25691, loss is 4.780831336975098\n",
      "(64, 33)\n",
      "step 25692, loss is 4.896997451782227\n",
      "(64, 33)\n",
      "step 25693, loss is 4.557925224304199\n",
      "(64, 33)\n",
      "step 25694, loss is 4.723479747772217\n",
      "(64, 33)\n",
      "step 25695, loss is 4.768365383148193\n",
      "(64, 33)\n",
      "step 25696, loss is 4.869354248046875\n",
      "(64, 33)\n",
      "step 25697, loss is 4.735472679138184\n",
      "(64, 33)\n",
      "step 25698, loss is 4.546843528747559\n",
      "(64, 33)\n",
      "step 25699, loss is 4.722177982330322\n",
      "(64, 33)\n",
      "step 25700, loss is 4.765289306640625\n",
      "(64, 33)\n",
      "step 25701, loss is 4.883241653442383\n",
      "(64, 33)\n",
      "step 25702, loss is 4.5289154052734375\n",
      "(64, 33)\n",
      "step 25703, loss is 4.810095310211182\n",
      "(64, 33)\n",
      "step 25704, loss is 4.767955303192139\n",
      "(64, 33)\n",
      "step 25705, loss is 4.924980163574219\n",
      "(64, 33)\n",
      "step 25706, loss is 4.820696830749512\n",
      "(64, 33)\n",
      "step 25707, loss is 4.682753086090088\n",
      "(64, 33)\n",
      "step 25708, loss is 4.688998222351074\n",
      "(64, 33)\n",
      "step 25709, loss is 4.686639785766602\n",
      "(64, 33)\n",
      "step 25710, loss is 4.554422378540039\n",
      "(64, 33)\n",
      "step 25711, loss is 4.889736652374268\n",
      "(64, 33)\n",
      "step 25712, loss is 4.901790618896484\n",
      "(64, 33)\n",
      "step 25713, loss is 4.701433181762695\n",
      "(64, 33)\n",
      "step 25714, loss is 4.729912757873535\n",
      "(64, 33)\n",
      "step 25715, loss is 4.631874084472656\n",
      "(64, 33)\n",
      "step 25716, loss is 4.837233066558838\n",
      "(64, 33)\n",
      "step 25717, loss is 4.9323906898498535\n",
      "(64, 33)\n",
      "step 25718, loss is 4.858595848083496\n",
      "(64, 33)\n",
      "step 25719, loss is 4.750957012176514\n",
      "(64, 33)\n",
      "step 25720, loss is 4.878598213195801\n",
      "(64, 33)\n",
      "step 25721, loss is 4.6750407218933105\n",
      "(64, 33)\n",
      "step 25722, loss is 4.906825065612793\n",
      "(64, 33)\n",
      "step 25723, loss is 4.943090915679932\n",
      "(64, 33)\n",
      "step 25724, loss is 4.788591384887695\n",
      "(64, 33)\n",
      "step 25725, loss is 4.963993549346924\n",
      "(64, 33)\n",
      "step 25726, loss is 4.8272857666015625\n",
      "(64, 33)\n",
      "step 25727, loss is 4.9456706047058105\n",
      "(64, 33)\n",
      "step 25728, loss is 4.765605926513672\n",
      "(64, 33)\n",
      "step 25729, loss is 4.827990531921387\n",
      "(64, 33)\n",
      "step 25730, loss is 4.849891185760498\n",
      "(64, 33)\n",
      "step 25731, loss is 4.6175150871276855\n",
      "(64, 33)\n",
      "step 25732, loss is 4.877536296844482\n",
      "(64, 33)\n",
      "step 25733, loss is 4.885860919952393\n",
      "(64, 33)\n",
      "step 25734, loss is 4.859099864959717\n",
      "(64, 33)\n",
      "step 25735, loss is 4.759428024291992\n",
      "(64, 33)\n",
      "step 25736, loss is 4.667901992797852\n",
      "(64, 33)\n",
      "step 25737, loss is 4.686005592346191\n",
      "(64, 33)\n",
      "step 25738, loss is 4.6642632484436035\n",
      "(64, 33)\n",
      "step 25739, loss is 4.776546478271484\n",
      "(64, 33)\n",
      "step 25740, loss is 4.618232250213623\n",
      "(64, 33)\n",
      "step 25741, loss is 4.76945161819458\n",
      "(64, 33)\n",
      "step 25742, loss is 4.880224704742432\n",
      "(64, 33)\n",
      "step 25743, loss is 4.9537882804870605\n",
      "(64, 33)\n",
      "step 25744, loss is 4.961014270782471\n",
      "(64, 33)\n",
      "step 25745, loss is 4.8351359367370605\n",
      "(64, 33)\n",
      "step 25746, loss is 4.699865818023682\n",
      "(64, 33)\n",
      "step 25747, loss is 4.6651763916015625\n",
      "(64, 33)\n",
      "step 25748, loss is 4.755452632904053\n",
      "(64, 33)\n",
      "step 25749, loss is 4.72952127456665\n",
      "(64, 33)\n",
      "step 25750, loss is 5.010770797729492\n",
      "(64, 33)\n",
      "step 25751, loss is 4.69835901260376\n",
      "(64, 33)\n",
      "step 25752, loss is 4.957221984863281\n",
      "(64, 33)\n",
      "step 25753, loss is 4.779601097106934\n",
      "(64, 33)\n",
      "step 25754, loss is 4.700431823730469\n",
      "(64, 33)\n",
      "step 25755, loss is 4.77870512008667\n",
      "(64, 33)\n",
      "step 25756, loss is 4.735184192657471\n",
      "(64, 33)\n",
      "step 25757, loss is 4.838489055633545\n",
      "(64, 33)\n",
      "step 25758, loss is 4.6978325843811035\n",
      "(64, 33)\n",
      "step 25759, loss is 4.712440490722656\n",
      "(64, 33)\n",
      "step 25760, loss is 4.667453765869141\n",
      "(64, 33)\n",
      "step 25761, loss is 4.890727996826172\n",
      "(64, 33)\n",
      "step 25762, loss is 4.932826042175293\n",
      "(64, 33)\n",
      "step 25763, loss is 4.710093021392822\n",
      "(64, 33)\n",
      "step 25764, loss is 4.853495121002197\n",
      "(64, 33)\n",
      "step 25765, loss is 4.773400783538818\n",
      "(64, 33)\n",
      "step 25766, loss is 4.700447082519531\n",
      "(64, 33)\n",
      "step 25767, loss is 4.818380832672119\n",
      "(64, 33)\n",
      "step 25768, loss is 4.873239517211914\n",
      "(64, 33)\n",
      "step 25769, loss is 4.955837249755859\n",
      "(64, 33)\n",
      "step 25770, loss is 4.648488998413086\n",
      "(64, 33)\n",
      "step 25771, loss is 4.744045257568359\n",
      "(64, 33)\n",
      "step 25772, loss is 4.693904399871826\n",
      "(64, 33)\n",
      "step 25773, loss is 4.496960163116455\n",
      "(64, 33)\n",
      "step 25774, loss is 4.764042377471924\n",
      "(64, 33)\n",
      "step 25775, loss is 4.777190685272217\n",
      "(64, 33)\n",
      "step 25776, loss is 4.867927551269531\n",
      "(64, 33)\n",
      "step 25777, loss is 4.743386268615723\n",
      "(64, 33)\n",
      "step 25778, loss is 4.8017988204956055\n",
      "(64, 33)\n",
      "step 25779, loss is 5.0659260749816895\n",
      "(64, 33)\n",
      "step 25780, loss is 5.0065388679504395\n",
      "(64, 33)\n",
      "step 25781, loss is 4.666488170623779\n",
      "(64, 33)\n",
      "step 25782, loss is 4.869651794433594\n",
      "(64, 33)\n",
      "step 25783, loss is 4.662397861480713\n",
      "(64, 33)\n",
      "step 25784, loss is 4.7813215255737305\n",
      "(64, 33)\n",
      "step 25785, loss is 4.809974670410156\n",
      "(64, 33)\n",
      "step 25786, loss is 4.82668924331665\n",
      "(64, 33)\n",
      "step 25787, loss is 4.711758613586426\n",
      "(64, 33)\n",
      "step 25788, loss is 4.856452941894531\n",
      "(64, 33)\n",
      "step 25789, loss is 4.752110481262207\n",
      "(64, 33)\n",
      "step 25790, loss is 4.720493316650391\n",
      "(64, 33)\n",
      "step 25791, loss is 4.852016925811768\n",
      "(64, 33)\n",
      "step 25792, loss is 4.727478981018066\n",
      "(64, 33)\n",
      "step 25793, loss is 4.889300346374512\n",
      "(64, 33)\n",
      "step 25794, loss is 4.890981674194336\n",
      "(64, 33)\n",
      "step 25795, loss is 4.763559341430664\n",
      "(64, 33)\n",
      "step 25796, loss is 4.755988121032715\n",
      "(64, 33)\n",
      "step 25797, loss is 4.743249893188477\n",
      "(64, 33)\n",
      "step 25798, loss is 4.744781017303467\n",
      "(64, 33)\n",
      "step 25799, loss is 4.853917121887207\n",
      "(64, 33)\n",
      "step 25800, loss is 4.727232933044434\n",
      "(64, 33)\n",
      "step 25801, loss is 4.839599609375\n",
      "(64, 33)\n",
      "step 25802, loss is 4.768776893615723\n",
      "(64, 33)\n",
      "step 25803, loss is 4.676257133483887\n",
      "(64, 33)\n",
      "step 25804, loss is 4.547730922698975\n",
      "(64, 33)\n",
      "step 25805, loss is 4.765565872192383\n",
      "(64, 33)\n",
      "step 25806, loss is 4.810210704803467\n",
      "(64, 33)\n",
      "step 25807, loss is 4.8773112297058105\n",
      "(64, 33)\n",
      "step 25808, loss is 4.753124713897705\n",
      "(64, 33)\n",
      "step 25809, loss is 4.603636264801025\n",
      "(64, 33)\n",
      "step 25810, loss is 4.756198883056641\n",
      "(64, 33)\n",
      "step 25811, loss is 4.815107822418213\n",
      "(64, 33)\n",
      "step 25812, loss is 4.741992950439453\n",
      "(64, 33)\n",
      "step 25813, loss is 4.880990028381348\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25814, loss is 4.804504871368408\n",
      "(64, 33)\n",
      "step 25815, loss is 4.792655944824219\n",
      "(64, 33)\n",
      "step 25816, loss is 4.846389293670654\n",
      "(64, 33)\n",
      "step 25817, loss is 4.980565547943115\n",
      "(64, 33)\n",
      "step 25818, loss is 4.876687526702881\n",
      "(64, 33)\n",
      "step 25819, loss is 5.013962745666504\n",
      "(64, 33)\n",
      "step 25820, loss is 4.737403869628906\n",
      "(64, 33)\n",
      "step 25821, loss is 4.814393043518066\n",
      "(64, 33)\n",
      "step 25822, loss is 4.762092113494873\n",
      "(64, 33)\n",
      "step 25823, loss is 4.682094573974609\n",
      "(64, 33)\n",
      "step 25824, loss is 4.78609561920166\n",
      "(64, 33)\n",
      "step 25825, loss is 4.754933834075928\n",
      "(64, 33)\n",
      "step 25826, loss is 4.95278787612915\n",
      "(64, 33)\n",
      "step 25827, loss is 4.64306640625\n",
      "(64, 33)\n",
      "step 25828, loss is 4.645415782928467\n",
      "(64, 33)\n",
      "step 25829, loss is 4.84391450881958\n",
      "(64, 33)\n",
      "step 25830, loss is 4.849014759063721\n",
      "(64, 33)\n",
      "step 25831, loss is 4.700699329376221\n",
      "(64, 33)\n",
      "step 25832, loss is 4.884785175323486\n",
      "(64, 33)\n",
      "step 25833, loss is 4.7641520500183105\n",
      "(64, 33)\n",
      "step 25834, loss is 4.887197017669678\n",
      "(64, 33)\n",
      "step 25835, loss is 4.926875114440918\n",
      "(64, 33)\n",
      "step 25836, loss is 4.721487522125244\n",
      "(64, 33)\n",
      "step 25837, loss is 4.96389102935791\n",
      "(64, 33)\n",
      "step 25838, loss is 4.70428466796875\n",
      "(64, 33)\n",
      "step 25839, loss is 4.840826988220215\n",
      "(64, 33)\n",
      "step 25840, loss is 4.73721981048584\n",
      "(64, 33)\n",
      "step 25841, loss is 4.713996887207031\n",
      "(64, 33)\n",
      "step 25842, loss is 4.825343132019043\n",
      "(64, 33)\n",
      "step 25843, loss is 4.874216079711914\n",
      "(64, 33)\n",
      "step 25844, loss is 4.829848289489746\n",
      "(64, 33)\n",
      "step 25845, loss is 4.791539192199707\n",
      "(64, 33)\n",
      "step 25846, loss is 4.712155342102051\n",
      "(64, 33)\n",
      "step 25847, loss is 4.710072994232178\n",
      "(64, 33)\n",
      "step 25848, loss is 4.994014739990234\n",
      "(64, 33)\n",
      "step 25849, loss is 4.800300598144531\n",
      "(64, 33)\n",
      "step 25850, loss is 4.8700666427612305\n",
      "(64, 33)\n",
      "step 25851, loss is 4.732659339904785\n",
      "(64, 33)\n",
      "step 25852, loss is 4.888639450073242\n",
      "(64, 33)\n",
      "step 25853, loss is 4.793012619018555\n",
      "(64, 33)\n",
      "step 25854, loss is 4.906953811645508\n",
      "(64, 33)\n",
      "step 25855, loss is 4.991385459899902\n",
      "(64, 33)\n",
      "step 25856, loss is 4.9199957847595215\n",
      "(64, 33)\n",
      "step 25857, loss is 4.747152805328369\n",
      "(64, 33)\n",
      "step 25858, loss is 5.020908832550049\n",
      "(64, 33)\n",
      "step 25859, loss is 4.64383602142334\n",
      "(64, 33)\n",
      "step 25860, loss is 4.6895670890808105\n",
      "(64, 33)\n",
      "step 25861, loss is 4.6056060791015625\n",
      "(64, 33)\n",
      "step 25862, loss is 4.990641117095947\n",
      "(64, 33)\n",
      "step 25863, loss is 4.863849639892578\n",
      "(64, 33)\n",
      "step 25864, loss is 4.7138214111328125\n",
      "(64, 33)\n",
      "step 25865, loss is 4.872323989868164\n",
      "(64, 33)\n",
      "step 25866, loss is 4.846071243286133\n",
      "(64, 33)\n",
      "step 25867, loss is 4.9731950759887695\n",
      "(64, 33)\n",
      "step 25868, loss is 4.648228645324707\n",
      "(64, 33)\n",
      "step 25869, loss is 4.926555156707764\n",
      "(64, 33)\n",
      "step 25870, loss is 4.8618245124816895\n",
      "(64, 33)\n",
      "step 25871, loss is 4.972715377807617\n",
      "(64, 33)\n",
      "step 25872, loss is 4.798912525177002\n",
      "(64, 33)\n",
      "step 25873, loss is 4.625093936920166\n",
      "(64, 33)\n",
      "step 25874, loss is 4.9371867179870605\n",
      "(64, 33)\n",
      "step 25875, loss is 4.86638069152832\n",
      "(64, 33)\n",
      "step 25876, loss is 4.741832256317139\n",
      "(64, 33)\n",
      "step 25877, loss is 4.790877342224121\n",
      "(64, 33)\n",
      "step 25878, loss is 4.785727500915527\n",
      "(64, 33)\n",
      "step 25879, loss is 4.607714653015137\n",
      "(64, 33)\n",
      "step 25880, loss is 4.954349994659424\n",
      "(64, 33)\n",
      "step 25881, loss is 4.728669166564941\n",
      "(64, 33)\n",
      "step 25882, loss is 4.679471015930176\n",
      "(64, 33)\n",
      "step 25883, loss is 4.824522018432617\n",
      "(64, 33)\n",
      "step 25884, loss is 4.663281440734863\n",
      "(64, 33)\n",
      "step 25885, loss is 4.698492527008057\n",
      "(64, 33)\n",
      "step 25886, loss is 4.862663269042969\n",
      "(64, 33)\n",
      "step 25887, loss is 4.700313091278076\n",
      "(64, 33)\n",
      "step 25888, loss is 4.774238586425781\n",
      "(64, 33)\n",
      "step 25889, loss is 5.0362935066223145\n",
      "(64, 33)\n",
      "step 25890, loss is 4.7021965980529785\n",
      "(64, 33)\n",
      "step 25891, loss is 4.7772650718688965\n",
      "(64, 33)\n",
      "step 25892, loss is 4.730270862579346\n",
      "(64, 33)\n",
      "step 25893, loss is 4.623513698577881\n",
      "(64, 33)\n",
      "step 25894, loss is 4.829061508178711\n",
      "(64, 33)\n",
      "step 25895, loss is 4.777278900146484\n",
      "(64, 33)\n",
      "step 25896, loss is 4.911872863769531\n",
      "(64, 33)\n",
      "step 25897, loss is 4.804897785186768\n",
      "(64, 33)\n",
      "step 25898, loss is 4.626424789428711\n",
      "(64, 33)\n",
      "step 25899, loss is 4.7925543785095215\n",
      "(64, 33)\n",
      "step 25900, loss is 4.623435974121094\n",
      "(64, 33)\n",
      "step 25901, loss is 4.7851691246032715\n",
      "(64, 33)\n",
      "step 25902, loss is 4.818941593170166\n",
      "(64, 33)\n",
      "step 25903, loss is 4.710258483886719\n",
      "(64, 33)\n",
      "step 25904, loss is 4.994669437408447\n",
      "(64, 33)\n",
      "step 25905, loss is 4.761591911315918\n",
      "(64, 33)\n",
      "step 25906, loss is 4.739030361175537\n",
      "(64, 33)\n",
      "step 25907, loss is 4.709821701049805\n",
      "(64, 33)\n",
      "step 25908, loss is 4.601993083953857\n",
      "(64, 33)\n",
      "step 25909, loss is 4.627193927764893\n",
      "(64, 33)\n",
      "step 25910, loss is 4.812381744384766\n",
      "(64, 33)\n",
      "step 25911, loss is 4.930377960205078\n",
      "(64, 33)\n",
      "step 25912, loss is 4.8957133293151855\n",
      "(64, 33)\n",
      "step 25913, loss is 4.6633687019348145\n",
      "(64, 33)\n",
      "step 25914, loss is 4.962778091430664\n",
      "(64, 33)\n",
      "step 25915, loss is 5.012524604797363\n",
      "(64, 33)\n",
      "step 25916, loss is 4.878203392028809\n",
      "(64, 33)\n",
      "step 25917, loss is 4.825595855712891\n",
      "(64, 33)\n",
      "step 25918, loss is 4.828339576721191\n",
      "(64, 33)\n",
      "step 25919, loss is 4.74709415435791\n",
      "(64, 33)\n",
      "step 25920, loss is 4.569443702697754\n",
      "(64, 33)\n",
      "step 25921, loss is 4.776374816894531\n",
      "(64, 33)\n",
      "step 25922, loss is 4.757112503051758\n",
      "(64, 33)\n",
      "step 25923, loss is 4.824270725250244\n",
      "(64, 33)\n",
      "step 25924, loss is 4.905311584472656\n",
      "(64, 33)\n",
      "step 25925, loss is 4.863439083099365\n",
      "(64, 33)\n",
      "step 25926, loss is 4.852721691131592\n",
      "(64, 33)\n",
      "step 25927, loss is 4.688775062561035\n",
      "(64, 33)\n",
      "step 25928, loss is 4.715041637420654\n",
      "(64, 33)\n",
      "step 25929, loss is 4.650113105773926\n",
      "(64, 33)\n",
      "step 25930, loss is 4.8089070320129395\n",
      "(64, 33)\n",
      "step 25931, loss is 4.766669750213623\n",
      "(64, 33)\n",
      "step 25932, loss is 4.564661979675293\n",
      "(64, 33)\n",
      "step 25933, loss is 4.8520636558532715\n",
      "(64, 33)\n",
      "step 25934, loss is 4.849611282348633\n",
      "(64, 33)\n",
      "step 25935, loss is 4.7324395179748535\n",
      "(64, 33)\n",
      "step 25936, loss is 4.870142459869385\n",
      "(64, 33)\n",
      "step 25937, loss is 4.7337212562561035\n",
      "(64, 33)\n",
      "step 25938, loss is 4.854400634765625\n",
      "(64, 33)\n",
      "step 25939, loss is 4.927097320556641\n",
      "(64, 33)\n",
      "step 25940, loss is 4.713879585266113\n",
      "(64, 33)\n",
      "step 25941, loss is 4.933424949645996\n",
      "(64, 33)\n",
      "step 25942, loss is 4.613751411437988\n",
      "(64, 33)\n",
      "step 25943, loss is 4.863458156585693\n",
      "(64, 33)\n",
      "step 25944, loss is 4.648069381713867\n",
      "(64, 33)\n",
      "step 25945, loss is 4.783505439758301\n",
      "(64, 33)\n",
      "step 25946, loss is 4.76787805557251\n",
      "(64, 33)\n",
      "step 25947, loss is 4.6821465492248535\n",
      "(64, 33)\n",
      "step 25948, loss is 4.797606468200684\n",
      "(64, 33)\n",
      "step 25949, loss is 4.9026923179626465\n",
      "(64, 33)\n",
      "step 25950, loss is 4.705211639404297\n",
      "(64, 33)\n",
      "step 25951, loss is 4.645942211151123\n",
      "(64, 33)\n",
      "step 25952, loss is 4.571540355682373\n",
      "(64, 33)\n",
      "step 25953, loss is 4.798389911651611\n",
      "(64, 33)\n",
      "step 25954, loss is 4.812686920166016\n",
      "(64, 33)\n",
      "step 25955, loss is 4.851846218109131\n",
      "(64, 33)\n",
      "step 25956, loss is 4.737945556640625\n",
      "(64, 33)\n",
      "step 25957, loss is 4.795993328094482\n",
      "(64, 33)\n",
      "step 25958, loss is 4.724607944488525\n",
      "(64, 33)\n",
      "step 25959, loss is 4.553287029266357\n",
      "(64, 33)\n",
      "step 25960, loss is 4.701127052307129\n",
      "(64, 33)\n",
      "step 25961, loss is 4.8031721115112305\n",
      "(64, 33)\n",
      "step 25962, loss is 4.92891263961792\n",
      "(64, 33)\n",
      "step 25963, loss is 4.744760990142822\n",
      "(64, 33)\n",
      "step 25964, loss is 4.705236911773682\n",
      "(64, 33)\n",
      "step 25965, loss is 4.879427909851074\n",
      "(64, 33)\n",
      "step 25966, loss is 4.695223808288574\n",
      "(64, 33)\n",
      "step 25967, loss is 4.689149856567383\n",
      "(64, 33)\n",
      "step 25968, loss is 4.732865333557129\n",
      "(64, 33)\n",
      "step 25969, loss is 4.737863063812256\n",
      "(64, 33)\n",
      "step 25970, loss is 4.733768463134766\n",
      "(64, 33)\n",
      "step 25971, loss is 4.90745735168457\n",
      "(64, 33)\n",
      "step 25972, loss is 4.876739025115967\n",
      "(64, 33)\n",
      "step 25973, loss is 4.8105034828186035\n",
      "(64, 33)\n",
      "step 25974, loss is 4.863444805145264\n",
      "(64, 33)\n",
      "step 25975, loss is 4.916903495788574\n",
      "(64, 33)\n",
      "step 25976, loss is 4.840470314025879\n",
      "(64, 33)\n",
      "step 25977, loss is 4.863503932952881\n",
      "(64, 33)\n",
      "step 25978, loss is 5.012618064880371\n",
      "(64, 33)\n",
      "step 25979, loss is 4.805752754211426\n",
      "(64, 33)\n",
      "step 25980, loss is 4.809624671936035\n",
      "(64, 33)\n",
      "step 25981, loss is 4.810486316680908\n",
      "(64, 33)\n",
      "step 25982, loss is 4.764003276824951\n",
      "(64, 33)\n",
      "step 25983, loss is 4.978696823120117\n",
      "(64, 33)\n",
      "step 25984, loss is 4.951488018035889\n",
      "(64, 33)\n",
      "step 25985, loss is 5.005253314971924\n",
      "(64, 33)\n",
      "step 25986, loss is 4.904938220977783\n",
      "(64, 33)\n",
      "step 25987, loss is 4.900894641876221\n",
      "(64, 33)\n",
      "step 25988, loss is 4.855437278747559\n",
      "(64, 33)\n",
      "step 25989, loss is 4.886685848236084\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25990, loss is 4.776363849639893\n",
      "(64, 33)\n",
      "step 25991, loss is 4.892402172088623\n",
      "(64, 33)\n",
      "step 25992, loss is 4.824484825134277\n",
      "(64, 33)\n",
      "step 25993, loss is 4.838698863983154\n",
      "(64, 33)\n",
      "step 25994, loss is 4.851367473602295\n",
      "(64, 33)\n",
      "step 25995, loss is 4.790027618408203\n",
      "(64, 33)\n",
      "step 25996, loss is 4.599122524261475\n",
      "(64, 33)\n",
      "step 25997, loss is 4.799887180328369\n",
      "(64, 33)\n",
      "step 25998, loss is 4.766119956970215\n",
      "(64, 33)\n",
      "step 25999, loss is 4.692392349243164\n",
      "(64, 33)\n",
      "step 26000, loss is 4.807892799377441\n",
      "(64, 33)\n",
      "step 26001, loss is 4.72407341003418\n",
      "(64, 33)\n",
      "step 26002, loss is 4.879748344421387\n",
      "(64, 33)\n",
      "step 26003, loss is 4.635727405548096\n",
      "(64, 33)\n",
      "step 26004, loss is 4.693199634552002\n",
      "(64, 33)\n",
      "step 26005, loss is 4.623824596405029\n",
      "(64, 33)\n",
      "step 26006, loss is 4.864428997039795\n",
      "(64, 33)\n",
      "step 26007, loss is 4.624276638031006\n",
      "(64, 33)\n",
      "step 26008, loss is 4.743793487548828\n",
      "(64, 33)\n",
      "step 26009, loss is 4.954560279846191\n",
      "(64, 33)\n",
      "step 26010, loss is 4.80519437789917\n",
      "(64, 33)\n",
      "step 26011, loss is 4.835870742797852\n",
      "(64, 33)\n",
      "step 26012, loss is 4.664009094238281\n",
      "(64, 33)\n",
      "step 26013, loss is 4.94015645980835\n",
      "(64, 33)\n",
      "step 26014, loss is 4.8053717613220215\n",
      "(64, 33)\n",
      "step 26015, loss is 4.824263095855713\n",
      "(64, 33)\n",
      "step 26016, loss is 4.8258056640625\n",
      "(64, 33)\n",
      "step 26017, loss is 4.896169185638428\n",
      "(64, 33)\n",
      "step 26018, loss is 4.855620384216309\n",
      "(64, 33)\n",
      "step 26019, loss is 4.77576208114624\n",
      "(64, 33)\n",
      "step 26020, loss is 4.946052551269531\n",
      "(64, 33)\n",
      "step 26021, loss is 4.657871246337891\n",
      "(64, 33)\n",
      "step 26022, loss is 4.812073707580566\n",
      "(64, 33)\n",
      "step 26023, loss is 4.736623287200928\n",
      "(64, 33)\n",
      "step 26024, loss is 4.726318836212158\n",
      "(64, 33)\n",
      "step 26025, loss is 4.759403228759766\n",
      "(64, 33)\n",
      "step 26026, loss is 4.80748176574707\n",
      "(64, 33)\n",
      "step 26027, loss is 4.906594276428223\n",
      "(64, 33)\n",
      "step 26028, loss is 4.867740631103516\n",
      "(64, 33)\n",
      "step 26029, loss is 4.930359363555908\n",
      "(64, 33)\n",
      "step 26030, loss is 4.784049987792969\n",
      "(64, 33)\n",
      "step 26031, loss is 4.792701721191406\n",
      "(64, 33)\n",
      "step 26032, loss is 4.94734525680542\n",
      "(64, 33)\n",
      "step 26033, loss is 4.906355381011963\n",
      "(64, 33)\n",
      "step 26034, loss is 4.602626800537109\n",
      "(64, 33)\n",
      "step 26035, loss is 4.825461387634277\n",
      "(64, 33)\n",
      "step 26036, loss is 4.721894264221191\n",
      "(64, 33)\n",
      "step 26037, loss is 4.914844036102295\n",
      "(64, 33)\n",
      "step 26038, loss is 4.592713356018066\n",
      "(64, 33)\n",
      "step 26039, loss is 4.902412414550781\n",
      "(64, 33)\n",
      "step 26040, loss is 4.689734935760498\n",
      "(64, 33)\n",
      "step 26041, loss is 4.896990776062012\n",
      "(64, 33)\n",
      "step 26042, loss is 4.857492446899414\n",
      "(64, 33)\n",
      "step 26043, loss is 4.6277852058410645\n",
      "(64, 33)\n",
      "step 26044, loss is 4.685309410095215\n",
      "(64, 33)\n",
      "step 26045, loss is 4.81217622756958\n",
      "(64, 33)\n",
      "step 26046, loss is 4.8677568435668945\n",
      "(64, 33)\n",
      "step 26047, loss is 4.730591297149658\n",
      "(64, 33)\n",
      "step 26048, loss is 4.812954902648926\n",
      "(64, 33)\n",
      "step 26049, loss is 4.732141017913818\n",
      "(64, 33)\n",
      "step 26050, loss is 4.536121845245361\n",
      "(64, 33)\n",
      "step 26051, loss is 4.860397815704346\n",
      "(64, 33)\n",
      "step 26052, loss is 4.895493030548096\n",
      "(64, 33)\n",
      "step 26053, loss is 4.6450419425964355\n",
      "(64, 33)\n",
      "step 26054, loss is 4.786944389343262\n",
      "(64, 33)\n",
      "step 26055, loss is 4.8103437423706055\n",
      "(64, 33)\n",
      "step 26056, loss is 4.81727409362793\n",
      "(64, 33)\n",
      "step 26057, loss is 4.720860004425049\n",
      "(64, 33)\n",
      "step 26058, loss is 4.953171253204346\n",
      "(64, 33)\n",
      "step 26059, loss is 4.630331039428711\n",
      "(64, 33)\n",
      "step 26060, loss is 4.864632606506348\n",
      "(64, 33)\n",
      "step 26061, loss is 4.733687877655029\n",
      "(64, 33)\n",
      "step 26062, loss is 4.849369525909424\n",
      "(64, 33)\n",
      "step 26063, loss is 4.774256706237793\n",
      "(64, 33)\n",
      "step 26064, loss is 4.825807094573975\n",
      "(64, 33)\n",
      "step 26065, loss is 4.707162380218506\n",
      "(64, 33)\n",
      "step 26066, loss is 4.748968124389648\n",
      "(64, 33)\n",
      "step 26067, loss is 4.938446044921875\n",
      "(64, 33)\n",
      "step 26068, loss is 4.952415943145752\n",
      "(64, 33)\n",
      "step 26069, loss is 4.7449188232421875\n",
      "(64, 33)\n",
      "step 26070, loss is 4.950104236602783\n",
      "(64, 33)\n",
      "step 26071, loss is 4.682838439941406\n",
      "(64, 33)\n",
      "step 26072, loss is 4.775592803955078\n",
      "(64, 33)\n",
      "step 26073, loss is 4.875787258148193\n",
      "(64, 33)\n",
      "step 26074, loss is 4.796142101287842\n",
      "(64, 33)\n",
      "step 26075, loss is 4.881629467010498\n",
      "(64, 33)\n",
      "step 26076, loss is 4.846377849578857\n",
      "(64, 33)\n",
      "step 26077, loss is 4.843406677246094\n",
      "(64, 33)\n",
      "step 26078, loss is 4.791258811950684\n",
      "(64, 33)\n",
      "step 26079, loss is 4.8237385749816895\n",
      "(64, 33)\n",
      "step 26080, loss is 4.641508102416992\n",
      "(64, 33)\n",
      "step 26081, loss is 4.920843124389648\n",
      "(64, 33)\n",
      "step 26082, loss is 4.873017311096191\n",
      "(64, 33)\n",
      "step 26083, loss is 4.812727451324463\n",
      "(64, 33)\n",
      "step 26084, loss is 4.713813781738281\n",
      "(64, 33)\n",
      "step 26085, loss is 4.757951259613037\n",
      "(64, 33)\n",
      "step 26086, loss is 4.83997917175293\n",
      "(64, 33)\n",
      "step 26087, loss is 4.692988872528076\n",
      "(64, 33)\n",
      "step 26088, loss is 4.887023448944092\n",
      "(64, 33)\n",
      "step 26089, loss is 4.736569404602051\n",
      "(64, 33)\n",
      "step 26090, loss is 4.925196170806885\n",
      "(64, 33)\n",
      "step 26091, loss is 4.884223461151123\n",
      "(64, 33)\n",
      "step 26092, loss is 4.713199138641357\n",
      "(64, 33)\n",
      "step 26093, loss is 4.7982497215271\n",
      "(64, 33)\n",
      "step 26094, loss is 4.754374980926514\n",
      "(64, 33)\n",
      "step 26095, loss is 4.776371479034424\n",
      "(64, 33)\n",
      "step 26096, loss is 4.5403666496276855\n",
      "(64, 33)\n",
      "step 26097, loss is 4.685438632965088\n",
      "(64, 33)\n",
      "step 26098, loss is 4.873915672302246\n",
      "(64, 33)\n",
      "step 26099, loss is 4.786802291870117\n",
      "(64, 33)\n",
      "step 26100, loss is 4.713094234466553\n",
      "(64, 33)\n",
      "step 26101, loss is 4.7015886306762695\n",
      "(64, 33)\n",
      "step 26102, loss is 4.691890716552734\n",
      "(64, 33)\n",
      "step 26103, loss is 4.739677906036377\n",
      "(64, 33)\n",
      "step 26104, loss is 4.883306503295898\n",
      "(64, 33)\n",
      "step 26105, loss is 4.90560245513916\n",
      "(64, 33)\n",
      "step 26106, loss is 4.772182941436768\n",
      "(64, 33)\n",
      "step 26107, loss is 4.786354064941406\n",
      "(64, 33)\n",
      "step 26108, loss is 4.608715057373047\n",
      "(64, 33)\n",
      "step 26109, loss is 4.717020511627197\n",
      "(64, 33)\n",
      "step 26110, loss is 4.80088996887207\n",
      "(64, 33)\n",
      "step 26111, loss is 4.798508167266846\n",
      "(64, 33)\n",
      "step 26112, loss is 4.727933883666992\n",
      "(64, 33)\n",
      "step 26113, loss is 4.906558990478516\n",
      "(64, 33)\n",
      "step 26114, loss is 4.849282264709473\n",
      "(64, 33)\n",
      "step 26115, loss is 4.818950653076172\n",
      "(64, 33)\n",
      "step 26116, loss is 4.859588623046875\n",
      "(64, 33)\n",
      "step 26117, loss is 4.604434013366699\n",
      "(64, 33)\n",
      "step 26118, loss is 4.727768898010254\n",
      "(64, 33)\n",
      "step 26119, loss is 4.596645355224609\n",
      "(64, 33)\n",
      "step 26120, loss is 4.9015326499938965\n",
      "(64, 33)\n",
      "step 26121, loss is 4.889921188354492\n",
      "(64, 33)\n",
      "step 26122, loss is 4.84037971496582\n",
      "(64, 33)\n",
      "step 26123, loss is 4.740464687347412\n",
      "(64, 33)\n",
      "step 26124, loss is 4.790637969970703\n",
      "(64, 33)\n",
      "step 26125, loss is 4.796298980712891\n",
      "(64, 33)\n",
      "step 26126, loss is 4.792919158935547\n",
      "(64, 33)\n",
      "step 26127, loss is 4.757717609405518\n",
      "(64, 33)\n",
      "step 26128, loss is 4.6415534019470215\n",
      "(64, 33)\n",
      "step 26129, loss is 4.763249397277832\n",
      "(64, 33)\n",
      "step 26130, loss is 4.729537487030029\n",
      "(64, 33)\n",
      "step 26131, loss is 4.775421142578125\n",
      "(64, 33)\n",
      "step 26132, loss is 4.735793113708496\n",
      "(64, 33)\n",
      "step 26133, loss is 4.629673004150391\n",
      "(64, 33)\n",
      "step 26134, loss is 4.753684997558594\n",
      "(64, 33)\n",
      "step 26135, loss is 4.686638832092285\n",
      "(64, 33)\n",
      "step 26136, loss is 4.890110969543457\n",
      "(64, 33)\n",
      "step 26137, loss is 4.826596736907959\n",
      "(64, 33)\n",
      "step 26138, loss is 4.820590496063232\n",
      "(64, 33)\n",
      "step 26139, loss is 4.6979594230651855\n",
      "(64, 33)\n",
      "step 26140, loss is 4.711297035217285\n",
      "(64, 33)\n",
      "step 26141, loss is 4.646209716796875\n",
      "(64, 33)\n",
      "step 26142, loss is 4.759369850158691\n",
      "(64, 33)\n",
      "step 26143, loss is 4.838253021240234\n",
      "(64, 33)\n",
      "step 26144, loss is 4.664359092712402\n",
      "(64, 33)\n",
      "step 26145, loss is 4.753227233886719\n",
      "(64, 33)\n",
      "step 26146, loss is 4.753109931945801\n",
      "(64, 33)\n",
      "step 26147, loss is 4.758498668670654\n",
      "(64, 33)\n",
      "step 26148, loss is 4.664021015167236\n",
      "(64, 33)\n",
      "step 26149, loss is 4.629734039306641\n",
      "(64, 33)\n",
      "step 26150, loss is 4.988612174987793\n",
      "(64, 33)\n",
      "step 26151, loss is 4.7644219398498535\n",
      "(64, 33)\n",
      "step 26152, loss is 4.669186592102051\n",
      "(64, 33)\n",
      "step 26153, loss is 4.646432876586914\n",
      "(64, 33)\n",
      "step 26154, loss is 4.688097953796387\n",
      "(64, 33)\n",
      "step 26155, loss is 4.701085567474365\n",
      "(64, 33)\n",
      "step 26156, loss is 4.737667083740234\n",
      "(64, 33)\n",
      "step 26157, loss is 4.787286281585693\n",
      "(64, 33)\n",
      "step 26158, loss is 4.801568508148193\n",
      "(64, 33)\n",
      "step 26159, loss is 4.806600093841553\n",
      "(64, 33)\n",
      "step 26160, loss is 4.715575218200684\n",
      "(64, 33)\n",
      "step 26161, loss is 4.694389343261719\n",
      "(64, 33)\n",
      "step 26162, loss is 4.914140701293945\n",
      "(64, 33)\n",
      "step 26163, loss is 4.739023208618164\n",
      "(64, 33)\n",
      "step 26164, loss is 4.856419563293457\n",
      "(64, 33)\n",
      "step 26165, loss is 4.739500045776367\n",
      "(64, 33)\n",
      "step 26166, loss is 4.763309955596924\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26167, loss is 4.660951614379883\n",
      "(64, 33)\n",
      "step 26168, loss is 4.717735290527344\n",
      "(64, 33)\n",
      "step 26169, loss is 4.77288818359375\n",
      "(64, 33)\n",
      "step 26170, loss is 4.736821174621582\n",
      "(64, 33)\n",
      "step 26171, loss is 4.880360126495361\n",
      "(64, 33)\n",
      "step 26172, loss is 4.6789021492004395\n",
      "(64, 33)\n",
      "step 26173, loss is 4.63770866394043\n",
      "(64, 33)\n",
      "step 26174, loss is 4.858770370483398\n",
      "(64, 33)\n",
      "step 26175, loss is 4.802012920379639\n",
      "(64, 33)\n",
      "step 26176, loss is 4.648975849151611\n",
      "(64, 33)\n",
      "step 26177, loss is 4.717892169952393\n",
      "(64, 33)\n",
      "step 26178, loss is 4.701545715332031\n",
      "(64, 33)\n",
      "step 26179, loss is 4.742054462432861\n",
      "(64, 33)\n",
      "step 26180, loss is 4.809663772583008\n",
      "(64, 33)\n",
      "step 26181, loss is 4.661182880401611\n",
      "(64, 33)\n",
      "step 26182, loss is 4.715450763702393\n",
      "(64, 33)\n",
      "step 26183, loss is 4.733647346496582\n",
      "(64, 33)\n",
      "step 26184, loss is 4.7789530754089355\n",
      "(64, 33)\n",
      "step 26185, loss is 4.963868618011475\n",
      "(64, 33)\n",
      "step 26186, loss is 4.735047340393066\n",
      "(64, 33)\n",
      "step 26187, loss is 4.873015403747559\n",
      "(64, 33)\n",
      "step 26188, loss is 4.696658611297607\n",
      "(64, 33)\n",
      "step 26189, loss is 4.975353717803955\n",
      "(64, 33)\n",
      "step 26190, loss is 4.827896595001221\n",
      "(64, 33)\n",
      "step 26191, loss is 4.62784481048584\n",
      "(64, 33)\n",
      "step 26192, loss is 4.8369340896606445\n",
      "(64, 33)\n",
      "step 26193, loss is 4.775371074676514\n",
      "(64, 33)\n",
      "step 26194, loss is 4.862966537475586\n",
      "(64, 33)\n",
      "step 26195, loss is 4.765956878662109\n",
      "(64, 33)\n",
      "step 26196, loss is 4.5179643630981445\n",
      "(64, 33)\n",
      "step 26197, loss is 4.693406581878662\n",
      "(64, 33)\n",
      "step 26198, loss is 4.708003520965576\n",
      "(64, 33)\n",
      "step 26199, loss is 4.76018762588501\n",
      "(64, 33)\n",
      "step 26200, loss is 4.626930236816406\n",
      "(64, 33)\n",
      "step 26201, loss is 4.810800075531006\n",
      "(64, 33)\n",
      "step 26202, loss is 4.520796298980713\n",
      "(64, 33)\n",
      "step 26203, loss is 4.793765544891357\n",
      "(64, 33)\n",
      "step 26204, loss is 4.750333309173584\n",
      "(64, 33)\n",
      "step 26205, loss is 4.713057041168213\n",
      "(64, 33)\n",
      "step 26206, loss is 4.743969440460205\n",
      "(64, 33)\n",
      "step 26207, loss is 4.726735591888428\n",
      "(64, 33)\n",
      "step 26208, loss is 4.895564556121826\n",
      "(64, 33)\n",
      "step 26209, loss is 4.75603723526001\n",
      "(64, 33)\n",
      "step 26210, loss is 4.6176605224609375\n",
      "(64, 33)\n",
      "step 26211, loss is 4.741783618927002\n",
      "(64, 33)\n",
      "step 26212, loss is 4.911894798278809\n",
      "(64, 33)\n",
      "step 26213, loss is 4.764891147613525\n",
      "(64, 33)\n",
      "step 26214, loss is 4.8314738273620605\n",
      "(64, 33)\n",
      "step 26215, loss is 4.745458602905273\n",
      "(64, 33)\n",
      "step 26216, loss is 4.802569389343262\n",
      "(64, 33)\n",
      "step 26217, loss is 4.789816379547119\n",
      "(64, 33)\n",
      "step 26218, loss is 4.7279133796691895\n",
      "(64, 33)\n",
      "step 26219, loss is 4.839974880218506\n",
      "(64, 33)\n",
      "step 26220, loss is 4.865865707397461\n",
      "(64, 33)\n",
      "step 26221, loss is 4.9774980545043945\n",
      "(64, 33)\n",
      "step 26222, loss is 4.81900691986084\n",
      "(64, 33)\n",
      "step 26223, loss is 4.677570819854736\n",
      "(64, 33)\n",
      "step 26224, loss is 4.781459331512451\n",
      "(64, 33)\n",
      "step 26225, loss is 4.7837138175964355\n",
      "(64, 33)\n",
      "step 26226, loss is 4.74655294418335\n",
      "(64, 33)\n",
      "step 26227, loss is 4.8391947746276855\n",
      "(64, 33)\n",
      "step 26228, loss is 4.692701816558838\n",
      "(64, 33)\n",
      "step 26229, loss is 4.671925067901611\n",
      "(64, 33)\n",
      "step 26230, loss is 4.812946796417236\n",
      "(64, 33)\n",
      "step 26231, loss is 4.900662899017334\n",
      "(64, 33)\n",
      "step 26232, loss is 4.798316478729248\n",
      "(64, 33)\n",
      "step 26233, loss is 4.886178970336914\n",
      "(64, 33)\n",
      "step 26234, loss is 4.786594867706299\n",
      "(64, 33)\n",
      "step 26235, loss is 4.667535781860352\n",
      "(64, 33)\n",
      "step 26236, loss is 4.838496208190918\n",
      "(64, 33)\n",
      "step 26237, loss is 4.7327775955200195\n",
      "(64, 33)\n",
      "step 26238, loss is 4.930537223815918\n",
      "(64, 33)\n",
      "step 26239, loss is 4.664656162261963\n",
      "(64, 33)\n",
      "step 26240, loss is 4.820207118988037\n",
      "(64, 33)\n",
      "step 26241, loss is 4.771093368530273\n",
      "(64, 33)\n",
      "step 26242, loss is 4.951832294464111\n",
      "(64, 33)\n",
      "step 26243, loss is 4.866185188293457\n",
      "(64, 33)\n",
      "step 26244, loss is 4.5976972579956055\n",
      "(64, 33)\n",
      "step 26245, loss is 4.82810640335083\n",
      "(64, 33)\n",
      "step 26246, loss is 4.730898857116699\n",
      "(64, 33)\n",
      "step 26247, loss is 4.893871307373047\n",
      "(64, 33)\n",
      "step 26248, loss is 4.7160139083862305\n",
      "(64, 33)\n",
      "step 26249, loss is 4.910921096801758\n",
      "(64, 33)\n",
      "step 26250, loss is 4.842827796936035\n",
      "(64, 33)\n",
      "step 26251, loss is 4.889591693878174\n",
      "(64, 33)\n",
      "step 26252, loss is 4.8007636070251465\n",
      "(64, 33)\n",
      "step 26253, loss is 4.774138450622559\n",
      "(64, 33)\n",
      "step 26254, loss is 4.801667213439941\n",
      "(64, 33)\n",
      "step 26255, loss is 4.857705116271973\n",
      "(64, 33)\n",
      "step 26256, loss is 4.912724494934082\n",
      "(64, 33)\n",
      "step 26257, loss is 4.773809909820557\n",
      "(64, 33)\n",
      "step 26258, loss is 4.632029056549072\n",
      "(64, 33)\n",
      "step 26259, loss is 4.809945583343506\n",
      "(64, 33)\n",
      "step 26260, loss is 4.82253360748291\n",
      "(64, 33)\n",
      "step 26261, loss is 4.899695873260498\n",
      "(64, 33)\n",
      "step 26262, loss is 4.70210075378418\n",
      "(64, 33)\n",
      "step 26263, loss is 4.651856899261475\n",
      "(64, 33)\n",
      "step 26264, loss is 4.856186389923096\n",
      "(64, 33)\n",
      "step 26265, loss is 4.692694664001465\n",
      "(64, 33)\n",
      "step 26266, loss is 4.908819675445557\n",
      "(64, 33)\n",
      "step 26267, loss is 4.827995300292969\n",
      "(64, 33)\n",
      "step 26268, loss is 4.770216464996338\n",
      "(64, 33)\n",
      "step 26269, loss is 4.7488837242126465\n",
      "(64, 33)\n",
      "step 26270, loss is 4.853456020355225\n",
      "(64, 33)\n",
      "step 26271, loss is 4.609630584716797\n",
      "(64, 33)\n",
      "step 26272, loss is 4.647835731506348\n",
      "(64, 33)\n",
      "step 26273, loss is 4.876922130584717\n",
      "(64, 33)\n",
      "step 26274, loss is 4.634435653686523\n",
      "(64, 33)\n",
      "step 26275, loss is 4.841287136077881\n",
      "(64, 33)\n",
      "step 26276, loss is 4.936188220977783\n",
      "(64, 33)\n",
      "step 26277, loss is 4.7126688957214355\n",
      "(64, 33)\n",
      "step 26278, loss is 4.703114032745361\n",
      "(64, 33)\n",
      "step 26279, loss is 4.908874034881592\n",
      "(64, 33)\n",
      "step 26280, loss is 4.799762725830078\n",
      "(64, 33)\n",
      "step 26281, loss is 4.961063385009766\n",
      "(64, 33)\n",
      "step 26282, loss is 4.700700759887695\n",
      "(64, 33)\n",
      "step 26283, loss is 4.7923479080200195\n",
      "(64, 33)\n",
      "step 26284, loss is 4.751439571380615\n",
      "(64, 33)\n",
      "step 26285, loss is 4.690858840942383\n",
      "(64, 33)\n",
      "step 26286, loss is 4.853266716003418\n",
      "(64, 33)\n",
      "step 26287, loss is 4.805455207824707\n",
      "(64, 33)\n",
      "step 26288, loss is 4.84541130065918\n",
      "(64, 33)\n",
      "step 26289, loss is 4.897644519805908\n",
      "(64, 33)\n",
      "step 26290, loss is 4.647861480712891\n",
      "(64, 33)\n",
      "step 26291, loss is 4.7976250648498535\n",
      "(64, 33)\n",
      "step 26292, loss is 4.766790390014648\n",
      "(64, 33)\n",
      "step 26293, loss is 4.8991570472717285\n",
      "(64, 33)\n",
      "step 26294, loss is 4.849796772003174\n",
      "(64, 33)\n",
      "step 26295, loss is 4.7985405921936035\n",
      "(64, 33)\n",
      "step 26296, loss is 4.7989501953125\n",
      "(64, 33)\n",
      "step 26297, loss is 4.842735290527344\n",
      "(64, 33)\n",
      "step 26298, loss is 4.836662769317627\n",
      "(64, 33)\n",
      "step 26299, loss is 4.693824768066406\n",
      "(64, 33)\n",
      "step 26300, loss is 4.836550712585449\n",
      "(64, 33)\n",
      "step 26301, loss is 4.805384635925293\n",
      "(64, 33)\n",
      "step 26302, loss is 4.799185276031494\n",
      "(64, 33)\n",
      "step 26303, loss is 4.885468006134033\n",
      "(64, 33)\n",
      "step 26304, loss is 4.838744163513184\n",
      "(64, 33)\n",
      "step 26305, loss is 4.653833389282227\n",
      "(64, 33)\n",
      "step 26306, loss is 4.9617414474487305\n",
      "(64, 33)\n",
      "step 26307, loss is 4.982420921325684\n",
      "(64, 33)\n",
      "step 26308, loss is 4.623771667480469\n",
      "(64, 33)\n",
      "step 26309, loss is 4.779429912567139\n",
      "(64, 33)\n",
      "step 26310, loss is 4.930576324462891\n",
      "(64, 33)\n",
      "step 26311, loss is 4.661596775054932\n",
      "(64, 33)\n",
      "step 26312, loss is 4.899893283843994\n",
      "(64, 33)\n",
      "step 26313, loss is 4.947855472564697\n",
      "(64, 33)\n",
      "step 26314, loss is 4.821227073669434\n",
      "(64, 33)\n",
      "step 26315, loss is 4.900356292724609\n",
      "(64, 33)\n",
      "step 26316, loss is 4.7389397621154785\n",
      "(64, 33)\n",
      "step 26317, loss is 4.715945720672607\n",
      "(64, 33)\n",
      "step 26318, loss is 4.5746073722839355\n",
      "(64, 33)\n",
      "step 26319, loss is 4.628932952880859\n",
      "(64, 33)\n",
      "step 26320, loss is 4.940597057342529\n",
      "(64, 33)\n",
      "step 26321, loss is 4.797455787658691\n",
      "(64, 33)\n",
      "step 26322, loss is 4.745871543884277\n",
      "(64, 33)\n",
      "step 26323, loss is 4.776100158691406\n",
      "(64, 33)\n",
      "step 26324, loss is 4.945994853973389\n",
      "(64, 33)\n",
      "step 26325, loss is 4.94135046005249\n",
      "(64, 33)\n",
      "step 26326, loss is 4.889124870300293\n",
      "(64, 33)\n",
      "step 26327, loss is 4.767446041107178\n",
      "(64, 33)\n",
      "step 26328, loss is 4.818840980529785\n",
      "(64, 33)\n",
      "step 26329, loss is 4.793418884277344\n",
      "(64, 33)\n",
      "step 26330, loss is 4.839976787567139\n",
      "(64, 33)\n",
      "step 26331, loss is 4.783780097961426\n",
      "(64, 33)\n",
      "step 26332, loss is 4.861295700073242\n",
      "(64, 33)\n",
      "step 26333, loss is 4.589657783508301\n",
      "(64, 33)\n",
      "step 26334, loss is 4.738892555236816\n",
      "(64, 33)\n",
      "step 26335, loss is 4.771284103393555\n",
      "(64, 33)\n",
      "step 26336, loss is 4.852391242980957\n",
      "(64, 33)\n",
      "step 26337, loss is 4.623836994171143\n",
      "(64, 33)\n",
      "step 26338, loss is 4.8703083992004395\n",
      "(64, 33)\n",
      "step 26339, loss is 4.757026672363281\n",
      "(64, 33)\n",
      "step 26340, loss is 4.8810577392578125\n",
      "(64, 33)\n",
      "step 26341, loss is 4.836276054382324\n",
      "(64, 33)\n",
      "step 26342, loss is 4.648471355438232\n",
      "(64, 33)\n",
      "step 26343, loss is 4.624454498291016\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26344, loss is 4.766475200653076\n",
      "(64, 33)\n",
      "step 26345, loss is 4.653663158416748\n",
      "(64, 33)\n",
      "step 26346, loss is 4.821219444274902\n",
      "(64, 33)\n",
      "step 26347, loss is 4.730812072753906\n",
      "(64, 33)\n",
      "step 26348, loss is 4.741232872009277\n",
      "(64, 33)\n",
      "step 26349, loss is 4.871800899505615\n",
      "(64, 33)\n",
      "step 26350, loss is 4.889887809753418\n",
      "(64, 33)\n",
      "step 26351, loss is 4.962873935699463\n",
      "(64, 33)\n",
      "step 26352, loss is 4.81800651550293\n",
      "(64, 33)\n",
      "step 26353, loss is 5.012705326080322\n",
      "(64, 33)\n",
      "step 26354, loss is 4.803659439086914\n",
      "(64, 33)\n",
      "step 26355, loss is 4.913259029388428\n",
      "(64, 33)\n",
      "step 26356, loss is 5.018584251403809\n",
      "(64, 33)\n",
      "step 26357, loss is 4.88029670715332\n",
      "(64, 33)\n",
      "step 26358, loss is 4.696134567260742\n",
      "(64, 33)\n",
      "step 26359, loss is 4.673646926879883\n",
      "(64, 33)\n",
      "step 26360, loss is 4.581503391265869\n",
      "(64, 33)\n",
      "step 26361, loss is 4.816236972808838\n",
      "(64, 33)\n",
      "step 26362, loss is 4.718046188354492\n",
      "(64, 33)\n",
      "step 26363, loss is 4.9811930656433105\n",
      "(64, 33)\n",
      "step 26364, loss is 4.781267166137695\n",
      "(64, 33)\n",
      "step 26365, loss is 4.4666948318481445\n",
      "(64, 33)\n",
      "step 26366, loss is 4.739821910858154\n",
      "(64, 33)\n",
      "step 26367, loss is 4.537855625152588\n",
      "(64, 33)\n",
      "step 26368, loss is 4.703599452972412\n",
      "(64, 33)\n",
      "step 26369, loss is 4.664833068847656\n",
      "(64, 33)\n",
      "step 26370, loss is 4.810240745544434\n",
      "(64, 33)\n",
      "step 26371, loss is 4.804630756378174\n",
      "(64, 33)\n",
      "step 26372, loss is 4.631043910980225\n",
      "(64, 33)\n",
      "step 26373, loss is 4.9180378913879395\n",
      "(64, 33)\n",
      "step 26374, loss is 4.830687999725342\n",
      "(64, 33)\n",
      "step 26375, loss is 4.693458557128906\n",
      "(64, 33)\n",
      "step 26376, loss is 4.786977291107178\n",
      "(64, 33)\n",
      "step 26377, loss is 4.847261905670166\n",
      "(64, 33)\n",
      "step 26378, loss is 4.889218807220459\n",
      "(64, 33)\n",
      "step 26379, loss is 4.761157035827637\n",
      "(64, 33)\n",
      "step 26380, loss is 4.728244781494141\n",
      "(64, 33)\n",
      "step 26381, loss is 4.9221978187561035\n",
      "(64, 33)\n",
      "step 26382, loss is 4.783867835998535\n",
      "(64, 33)\n",
      "step 26383, loss is 4.853862762451172\n",
      "(64, 33)\n",
      "step 26384, loss is 4.837602138519287\n",
      "(64, 33)\n",
      "step 26385, loss is 4.754151821136475\n",
      "(64, 33)\n",
      "step 26386, loss is 4.922577857971191\n",
      "(64, 33)\n",
      "step 26387, loss is 5.019573211669922\n",
      "(64, 33)\n",
      "step 26388, loss is 4.729018688201904\n",
      "(64, 33)\n",
      "step 26389, loss is 4.667226314544678\n",
      "(64, 33)\n",
      "step 26390, loss is 4.783693313598633\n",
      "(64, 33)\n",
      "step 26391, loss is 4.724324703216553\n",
      "(64, 33)\n",
      "step 26392, loss is 4.663084983825684\n",
      "(64, 33)\n",
      "step 26393, loss is 4.625251770019531\n",
      "(64, 33)\n",
      "step 26394, loss is 4.54667854309082\n",
      "(64, 33)\n",
      "step 26395, loss is 4.638713359832764\n",
      "(64, 33)\n",
      "step 26396, loss is 4.76739501953125\n",
      "(64, 33)\n",
      "step 26397, loss is 4.618469715118408\n",
      "(64, 33)\n",
      "step 26398, loss is 4.701548099517822\n",
      "(64, 33)\n",
      "step 26399, loss is 4.665060520172119\n",
      "(64, 33)\n",
      "step 26400, loss is 4.814223766326904\n",
      "(64, 33)\n",
      "step 26401, loss is 4.7627482414245605\n",
      "(64, 33)\n",
      "step 26402, loss is 4.652058124542236\n",
      "(64, 33)\n",
      "step 26403, loss is 4.904647350311279\n",
      "(64, 33)\n",
      "step 26404, loss is 4.799500465393066\n",
      "(64, 33)\n",
      "step 26405, loss is 4.959818363189697\n",
      "(64, 33)\n",
      "step 26406, loss is 4.839881896972656\n",
      "(64, 33)\n",
      "step 26407, loss is 4.865832805633545\n",
      "(64, 33)\n",
      "step 26408, loss is 4.794274806976318\n",
      "(64, 33)\n",
      "step 26409, loss is 4.833803653717041\n",
      "(64, 33)\n",
      "step 26410, loss is 4.76793098449707\n",
      "(64, 33)\n",
      "step 26411, loss is 4.718682765960693\n",
      "(64, 33)\n",
      "step 26412, loss is 4.708815574645996\n",
      "(64, 33)\n",
      "step 26413, loss is 4.603243827819824\n",
      "(64, 33)\n",
      "step 26414, loss is 4.494627952575684\n",
      "(64, 33)\n",
      "step 26415, loss is 4.792451858520508\n",
      "(64, 33)\n",
      "step 26416, loss is 4.658149719238281\n",
      "(64, 33)\n",
      "step 26417, loss is 4.814377307891846\n",
      "(64, 33)\n",
      "step 26418, loss is 4.868070602416992\n",
      "(64, 33)\n",
      "step 26419, loss is 4.753422737121582\n",
      "(64, 33)\n",
      "step 26420, loss is 4.547995567321777\n",
      "(64, 33)\n",
      "step 26421, loss is 4.702005386352539\n",
      "(64, 33)\n",
      "step 26422, loss is 4.799534797668457\n",
      "(64, 33)\n",
      "step 26423, loss is 4.654892921447754\n",
      "(64, 33)\n",
      "step 26424, loss is 4.795862197875977\n",
      "(64, 33)\n",
      "step 26425, loss is 4.664824962615967\n",
      "(64, 33)\n",
      "step 26426, loss is 4.829226016998291\n",
      "(64, 33)\n",
      "step 26427, loss is 4.928066253662109\n",
      "(64, 33)\n",
      "step 26428, loss is 4.843705654144287\n",
      "(64, 33)\n",
      "step 26429, loss is 4.849086761474609\n",
      "(64, 33)\n",
      "step 26430, loss is 4.715580463409424\n",
      "(64, 33)\n",
      "step 26431, loss is 4.64155912399292\n",
      "(64, 33)\n",
      "step 26432, loss is 4.801264762878418\n",
      "(64, 33)\n",
      "step 26433, loss is 4.639092445373535\n",
      "(64, 33)\n",
      "step 26434, loss is 4.809264183044434\n",
      "(64, 33)\n",
      "step 26435, loss is 4.637959957122803\n",
      "(64, 33)\n",
      "step 26436, loss is 4.8362226486206055\n",
      "(64, 33)\n",
      "step 26437, loss is 4.677140712738037\n",
      "(64, 33)\n",
      "step 26438, loss is 4.802868843078613\n",
      "(64, 33)\n",
      "step 26439, loss is 4.868532657623291\n",
      "(64, 33)\n",
      "step 26440, loss is 4.748780727386475\n",
      "(64, 33)\n",
      "step 26441, loss is 4.721864700317383\n",
      "(64, 33)\n",
      "step 26442, loss is 4.732217311859131\n",
      "(64, 33)\n",
      "step 26443, loss is 4.832671165466309\n",
      "(64, 33)\n",
      "step 26444, loss is 4.84775447845459\n",
      "(64, 33)\n",
      "step 26445, loss is 4.768657684326172\n",
      "(64, 33)\n",
      "step 26446, loss is 4.653200626373291\n",
      "(64, 33)\n",
      "step 26447, loss is 4.483351230621338\n",
      "(64, 33)\n",
      "step 26448, loss is 5.01003360748291\n",
      "(64, 33)\n",
      "step 26449, loss is 4.859931468963623\n",
      "(64, 33)\n",
      "step 26450, loss is 4.610135078430176\n",
      "(64, 33)\n",
      "step 26451, loss is 4.852302551269531\n",
      "(64, 33)\n",
      "step 26452, loss is 4.650384426116943\n",
      "(64, 33)\n",
      "step 26453, loss is 4.837423801422119\n",
      "(64, 33)\n",
      "step 26454, loss is 4.724608421325684\n",
      "(64, 33)\n",
      "step 26455, loss is 4.7730231285095215\n",
      "(64, 33)\n",
      "step 26456, loss is 4.70767068862915\n",
      "(64, 33)\n",
      "step 26457, loss is 4.8670549392700195\n",
      "(64, 33)\n",
      "step 26458, loss is 4.804008483886719\n",
      "(64, 33)\n",
      "step 26459, loss is 4.8861212730407715\n",
      "(64, 33)\n",
      "step 26460, loss is 4.728630065917969\n",
      "(64, 33)\n",
      "step 26461, loss is 4.795438289642334\n",
      "(64, 33)\n",
      "step 26462, loss is 4.683632850646973\n",
      "(64, 33)\n",
      "step 26463, loss is 4.710201263427734\n",
      "(64, 33)\n",
      "step 26464, loss is 4.834666728973389\n",
      "(64, 33)\n",
      "step 26465, loss is 4.795499324798584\n",
      "(64, 33)\n",
      "step 26466, loss is 4.680352210998535\n",
      "(64, 33)\n",
      "step 26467, loss is 4.838018894195557\n",
      "(64, 33)\n",
      "step 26468, loss is 4.595320224761963\n",
      "(64, 33)\n",
      "step 26469, loss is 4.553279399871826\n",
      "(64, 33)\n",
      "step 26470, loss is 4.78666353225708\n",
      "(64, 33)\n",
      "step 26471, loss is 4.768461227416992\n",
      "(64, 33)\n",
      "step 26472, loss is 4.799351692199707\n",
      "(64, 33)\n",
      "step 26473, loss is 4.604829788208008\n",
      "(64, 33)\n",
      "step 26474, loss is 4.770732402801514\n",
      "(64, 33)\n",
      "step 26475, loss is 4.690489768981934\n",
      "(64, 33)\n",
      "step 26476, loss is 4.569416522979736\n",
      "(64, 33)\n",
      "step 26477, loss is 4.655303478240967\n",
      "(64, 33)\n",
      "step 26478, loss is 4.801559925079346\n",
      "(64, 33)\n",
      "step 26479, loss is 4.786092758178711\n",
      "(64, 33)\n",
      "step 26480, loss is 4.755650043487549\n",
      "(64, 33)\n",
      "step 26481, loss is 4.657571792602539\n",
      "(64, 33)\n",
      "step 26482, loss is 4.694065570831299\n",
      "(64, 33)\n",
      "step 26483, loss is 4.85947847366333\n",
      "(64, 33)\n",
      "step 26484, loss is 4.761526584625244\n",
      "(64, 33)\n",
      "step 26485, loss is 4.742807388305664\n",
      "(64, 33)\n",
      "step 26486, loss is 4.678584575653076\n",
      "(64, 33)\n",
      "step 26487, loss is 4.8098883628845215\n",
      "(64, 33)\n",
      "step 26488, loss is 4.71248197555542\n",
      "(64, 33)\n",
      "step 26489, loss is 4.688851833343506\n",
      "(64, 33)\n",
      "step 26490, loss is 4.728569030761719\n",
      "(64, 33)\n",
      "step 26491, loss is 4.678189754486084\n",
      "(64, 33)\n",
      "step 26492, loss is 4.87252950668335\n",
      "(64, 33)\n",
      "step 26493, loss is 4.645691871643066\n",
      "(64, 33)\n",
      "step 26494, loss is 4.832671165466309\n",
      "(64, 33)\n",
      "step 26495, loss is 4.8065314292907715\n",
      "(64, 33)\n",
      "step 26496, loss is 4.790024757385254\n",
      "(64, 33)\n",
      "step 26497, loss is 4.787147045135498\n",
      "(64, 33)\n",
      "step 26498, loss is 4.687355995178223\n",
      "(64, 33)\n",
      "step 26499, loss is 4.8427205085754395\n",
      "(64, 33)\n",
      "step 26500, loss is 4.830867767333984\n",
      "(64, 33)\n",
      "step 26501, loss is 4.775768280029297\n",
      "(64, 33)\n",
      "step 26502, loss is 4.7569661140441895\n",
      "(64, 33)\n",
      "step 26503, loss is 4.9583001136779785\n",
      "(64, 33)\n",
      "step 26504, loss is 4.834995746612549\n",
      "(64, 33)\n",
      "step 26505, loss is 4.756723880767822\n",
      "(64, 33)\n",
      "step 26506, loss is 4.73534631729126\n",
      "(64, 33)\n",
      "step 26507, loss is 4.764772415161133\n",
      "(64, 33)\n",
      "step 26508, loss is 4.639134407043457\n",
      "(64, 33)\n",
      "step 26509, loss is 4.885900020599365\n",
      "(64, 33)\n",
      "step 26510, loss is 4.6369452476501465\n",
      "(64, 33)\n",
      "step 26511, loss is 4.828444480895996\n",
      "(64, 33)\n",
      "step 26512, loss is 4.845386028289795\n",
      "(64, 33)\n",
      "step 26513, loss is 4.948125839233398\n",
      "(64, 33)\n",
      "step 26514, loss is 4.739694595336914\n",
      "(64, 33)\n",
      "step 26515, loss is 4.733035564422607\n",
      "(64, 33)\n",
      "step 26516, loss is 4.717573165893555\n",
      "(64, 33)\n",
      "step 26517, loss is 4.936149597167969\n",
      "(64, 33)\n",
      "step 26518, loss is 4.748891353607178\n",
      "(64, 33)\n",
      "step 26519, loss is 4.828792572021484\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26520, loss is 4.805304527282715\n",
      "(64, 33)\n",
      "step 26521, loss is 4.903841018676758\n",
      "(64, 33)\n",
      "step 26522, loss is 4.791415691375732\n",
      "(64, 33)\n",
      "step 26523, loss is 4.817542552947998\n",
      "(64, 33)\n",
      "step 26524, loss is 4.720325469970703\n",
      "(64, 33)\n",
      "step 26525, loss is 4.81205415725708\n",
      "(64, 33)\n",
      "step 26526, loss is 4.800261497497559\n",
      "(64, 33)\n",
      "step 26527, loss is 4.830841064453125\n",
      "(64, 33)\n",
      "step 26528, loss is 4.859498977661133\n",
      "(64, 33)\n",
      "step 26529, loss is 4.751664161682129\n",
      "(64, 33)\n",
      "step 26530, loss is 4.654596328735352\n",
      "(64, 33)\n",
      "step 26531, loss is 4.832650184631348\n",
      "(64, 33)\n",
      "step 26532, loss is 4.623805046081543\n",
      "(64, 33)\n",
      "step 26533, loss is 4.901132106781006\n",
      "(64, 33)\n",
      "step 26534, loss is 4.779863357543945\n",
      "(64, 33)\n",
      "step 26535, loss is 4.959653377532959\n",
      "(64, 33)\n",
      "step 26536, loss is 4.8927178382873535\n",
      "(64, 33)\n",
      "step 26537, loss is 4.906651973724365\n",
      "(64, 33)\n",
      "step 26538, loss is 4.833365440368652\n",
      "(64, 33)\n",
      "step 26539, loss is 4.6778178215026855\n",
      "(64, 33)\n",
      "step 26540, loss is 4.703710079193115\n",
      "(64, 33)\n",
      "step 26541, loss is 4.910198211669922\n",
      "(64, 33)\n",
      "step 26542, loss is 4.799586296081543\n",
      "(64, 33)\n",
      "step 26543, loss is 4.749375820159912\n",
      "(64, 33)\n",
      "step 26544, loss is 4.5477824211120605\n",
      "(64, 33)\n",
      "step 26545, loss is 4.865786552429199\n",
      "(64, 33)\n",
      "step 26546, loss is 4.616081237792969\n",
      "(64, 33)\n",
      "step 26547, loss is 4.491669178009033\n",
      "(64, 33)\n",
      "step 26548, loss is 4.7627177238464355\n",
      "(64, 33)\n",
      "step 26549, loss is 4.773837089538574\n",
      "(64, 33)\n",
      "step 26550, loss is 4.798336505889893\n",
      "(64, 33)\n",
      "step 26551, loss is 4.808394908905029\n",
      "(64, 33)\n",
      "step 26552, loss is 4.912228584289551\n",
      "(64, 33)\n",
      "step 26553, loss is 4.964100360870361\n",
      "(64, 33)\n",
      "step 26554, loss is 4.85813045501709\n",
      "(64, 33)\n",
      "step 26555, loss is 4.752568244934082\n",
      "(64, 33)\n",
      "step 26556, loss is 4.77139949798584\n",
      "(64, 33)\n",
      "step 26557, loss is 4.682867527008057\n",
      "(64, 33)\n",
      "step 26558, loss is 4.711695194244385\n",
      "(64, 33)\n",
      "step 26559, loss is 4.877996444702148\n",
      "(64, 33)\n",
      "step 26560, loss is 4.926790237426758\n",
      "(64, 33)\n",
      "step 26561, loss is 4.6182050704956055\n",
      "(64, 33)\n",
      "step 26562, loss is 4.6284918785095215\n",
      "(64, 33)\n",
      "step 26563, loss is 4.827006816864014\n",
      "(64, 33)\n",
      "step 26564, loss is 4.819380283355713\n",
      "(64, 33)\n",
      "step 26565, loss is 4.927165508270264\n",
      "(64, 33)\n",
      "step 26566, loss is 4.742116451263428\n",
      "(64, 33)\n",
      "step 26567, loss is 4.771187782287598\n",
      "(64, 33)\n",
      "step 26568, loss is 4.722576141357422\n",
      "(64, 33)\n",
      "step 26569, loss is 4.5887250900268555\n",
      "(64, 33)\n",
      "step 26570, loss is 4.897172451019287\n",
      "(64, 33)\n",
      "step 26571, loss is 4.651033401489258\n",
      "(64, 33)\n",
      "step 26572, loss is 4.8194122314453125\n",
      "(64, 33)\n",
      "step 26573, loss is 4.79452657699585\n",
      "(64, 33)\n",
      "step 26574, loss is 4.827212810516357\n",
      "(64, 33)\n",
      "step 26575, loss is 4.649008274078369\n",
      "(64, 33)\n",
      "step 26576, loss is 4.759207725524902\n",
      "(64, 33)\n",
      "step 26577, loss is 4.712342262268066\n",
      "(64, 33)\n",
      "step 26578, loss is 4.6724467277526855\n",
      "(64, 33)\n",
      "step 26579, loss is 4.7799153327941895\n",
      "(64, 33)\n",
      "step 26580, loss is 4.787471294403076\n",
      "(64, 33)\n",
      "step 26581, loss is 4.907176494598389\n",
      "(64, 33)\n",
      "step 26582, loss is 4.801529407501221\n",
      "(64, 33)\n",
      "step 26583, loss is 4.882043838500977\n",
      "(64, 33)\n",
      "step 26584, loss is 4.435032844543457\n",
      "(64, 33)\n",
      "step 26585, loss is 4.796191692352295\n",
      "(64, 33)\n",
      "step 26586, loss is 4.719620704650879\n",
      "(64, 33)\n",
      "step 26587, loss is 4.730127334594727\n",
      "(64, 33)\n",
      "step 26588, loss is 4.683802127838135\n",
      "(64, 33)\n",
      "step 26589, loss is 4.677647590637207\n",
      "(64, 33)\n",
      "step 26590, loss is 4.6975297927856445\n",
      "(64, 33)\n",
      "step 26591, loss is 4.812368869781494\n",
      "(64, 33)\n",
      "step 26592, loss is 4.860724925994873\n",
      "(64, 33)\n",
      "step 26593, loss is 4.767175197601318\n",
      "(64, 33)\n",
      "step 26594, loss is 4.76866340637207\n",
      "(64, 33)\n",
      "step 26595, loss is 4.672880172729492\n",
      "(64, 33)\n",
      "step 26596, loss is 4.749281883239746\n",
      "(64, 33)\n",
      "step 26597, loss is 5.023872375488281\n",
      "(64, 33)\n",
      "step 26598, loss is 4.712654113769531\n",
      "(64, 33)\n",
      "step 26599, loss is 4.863402366638184\n",
      "(64, 33)\n",
      "step 26600, loss is 4.871859550476074\n",
      "(64, 33)\n",
      "step 26601, loss is 4.677226543426514\n",
      "(64, 33)\n",
      "step 26602, loss is 4.902259826660156\n",
      "(64, 33)\n",
      "step 26603, loss is 4.792273044586182\n",
      "(64, 33)\n",
      "step 26604, loss is 4.918287754058838\n",
      "(64, 33)\n",
      "step 26605, loss is 4.821905136108398\n",
      "(64, 33)\n",
      "step 26606, loss is 4.768003463745117\n",
      "(64, 33)\n",
      "step 26607, loss is 4.8036041259765625\n",
      "(64, 33)\n",
      "step 26608, loss is 4.738224983215332\n",
      "(64, 33)\n",
      "step 26609, loss is 4.755409240722656\n",
      "(64, 33)\n",
      "step 26610, loss is 4.714774131774902\n",
      "(64, 33)\n",
      "step 26611, loss is 4.75142765045166\n",
      "(64, 33)\n",
      "step 26612, loss is 4.776577472686768\n",
      "(64, 33)\n",
      "step 26613, loss is 4.894432067871094\n",
      "(64, 33)\n",
      "step 26614, loss is 4.799739360809326\n",
      "(64, 33)\n",
      "step 26615, loss is 4.614320755004883\n",
      "(64, 33)\n",
      "step 26616, loss is 4.798501014709473\n",
      "(64, 33)\n",
      "step 26617, loss is 4.833495140075684\n",
      "(64, 33)\n",
      "step 26618, loss is 4.710673809051514\n",
      "(64, 33)\n",
      "step 26619, loss is 4.706026554107666\n",
      "(64, 33)\n",
      "step 26620, loss is 4.62597131729126\n",
      "(64, 33)\n",
      "step 26621, loss is 4.720153331756592\n",
      "(64, 33)\n",
      "step 26622, loss is 4.70418643951416\n",
      "(64, 33)\n",
      "step 26623, loss is 4.869863986968994\n",
      "(64, 33)\n",
      "step 26624, loss is 4.670373916625977\n",
      "(64, 33)\n",
      "step 26625, loss is 4.610633373260498\n",
      "(64, 33)\n",
      "step 26626, loss is 4.765932083129883\n",
      "(64, 33)\n",
      "step 26627, loss is 4.869539260864258\n",
      "(64, 33)\n",
      "step 26628, loss is 4.822028160095215\n",
      "(64, 33)\n",
      "step 26629, loss is 4.693916320800781\n",
      "(64, 33)\n",
      "step 26630, loss is 4.852652072906494\n",
      "(64, 33)\n",
      "step 26631, loss is 4.793891429901123\n",
      "(64, 33)\n",
      "step 26632, loss is 4.572551727294922\n",
      "(64, 33)\n",
      "step 26633, loss is 4.704285144805908\n",
      "(64, 33)\n",
      "step 26634, loss is 4.909006118774414\n",
      "(64, 33)\n",
      "step 26635, loss is 4.917922496795654\n",
      "(64, 33)\n",
      "step 26636, loss is 4.861176490783691\n",
      "(64, 33)\n",
      "step 26637, loss is 4.702003479003906\n",
      "(64, 33)\n",
      "step 26638, loss is 4.949383735656738\n",
      "(64, 33)\n",
      "step 26639, loss is 4.757012367248535\n",
      "(64, 33)\n",
      "step 26640, loss is 4.926238059997559\n",
      "(64, 33)\n",
      "step 26641, loss is 4.835727691650391\n",
      "(64, 33)\n",
      "step 26642, loss is 4.708807468414307\n",
      "(64, 33)\n",
      "step 26643, loss is 4.808382511138916\n",
      "(64, 33)\n",
      "step 26644, loss is 4.737375259399414\n",
      "(64, 33)\n",
      "step 26645, loss is 4.841628551483154\n",
      "(64, 33)\n",
      "step 26646, loss is 4.895754814147949\n",
      "(64, 33)\n",
      "step 26647, loss is 4.748889446258545\n",
      "(64, 33)\n",
      "step 26648, loss is 4.784881114959717\n",
      "(64, 33)\n",
      "step 26649, loss is 4.804876804351807\n",
      "(64, 33)\n",
      "step 26650, loss is 4.826824188232422\n",
      "(64, 33)\n",
      "step 26651, loss is 4.883250713348389\n",
      "(64, 33)\n",
      "step 26652, loss is 4.818539619445801\n",
      "(64, 33)\n",
      "step 26653, loss is 4.7799072265625\n",
      "(64, 33)\n",
      "step 26654, loss is 4.7730393409729\n",
      "(64, 33)\n",
      "step 26655, loss is 4.916872978210449\n",
      "(64, 33)\n",
      "step 26656, loss is 4.803820610046387\n",
      "(64, 33)\n",
      "step 26657, loss is 4.711236476898193\n",
      "(64, 33)\n",
      "step 26658, loss is 4.8391337394714355\n",
      "(64, 33)\n",
      "step 26659, loss is 4.823728561401367\n",
      "(64, 33)\n",
      "step 26660, loss is 4.714639663696289\n",
      "(64, 33)\n",
      "step 26661, loss is 4.762574672698975\n",
      "(64, 33)\n",
      "step 26662, loss is 4.729011535644531\n",
      "(64, 33)\n",
      "step 26663, loss is 4.901364326477051\n",
      "(64, 33)\n",
      "step 26664, loss is 4.674997806549072\n",
      "(64, 33)\n",
      "step 26665, loss is 4.784707546234131\n",
      "(64, 33)\n",
      "step 26666, loss is 4.68080997467041\n",
      "(64, 33)\n",
      "step 26667, loss is 4.549445629119873\n",
      "(64, 33)\n",
      "step 26668, loss is 4.850266933441162\n",
      "(64, 33)\n",
      "step 26669, loss is 4.618262767791748\n",
      "(64, 33)\n",
      "step 26670, loss is 4.777107238769531\n",
      "(64, 33)\n",
      "step 26671, loss is 4.613492488861084\n",
      "(64, 33)\n",
      "step 26672, loss is 4.95892858505249\n",
      "(64, 33)\n",
      "step 26673, loss is 4.705174446105957\n",
      "(64, 33)\n",
      "step 26674, loss is 4.886425018310547\n",
      "(64, 33)\n",
      "step 26675, loss is 4.62465238571167\n",
      "(64, 33)\n",
      "step 26676, loss is 4.776196479797363\n",
      "(64, 33)\n",
      "step 26677, loss is 4.7453999519348145\n",
      "(64, 33)\n",
      "step 26678, loss is 4.737958908081055\n",
      "(64, 33)\n",
      "step 26679, loss is 4.687742233276367\n",
      "(64, 33)\n",
      "step 26680, loss is 4.793565273284912\n",
      "(64, 33)\n",
      "step 26681, loss is 4.868299961090088\n",
      "(64, 33)\n",
      "step 26682, loss is 4.6259942054748535\n",
      "(64, 33)\n",
      "step 26683, loss is 4.9123101234436035\n",
      "(64, 33)\n",
      "step 26684, loss is 4.711784362792969\n",
      "(64, 33)\n",
      "step 26685, loss is 4.719438076019287\n",
      "(64, 33)\n",
      "step 26686, loss is 4.942007064819336\n",
      "(64, 33)\n",
      "step 26687, loss is 4.822687149047852\n",
      "(64, 33)\n",
      "step 26688, loss is 4.753180027008057\n",
      "(64, 33)\n",
      "step 26689, loss is 4.612931251525879\n",
      "(64, 33)\n",
      "step 26690, loss is 4.861700057983398\n",
      "(64, 33)\n",
      "step 26691, loss is 4.828353404998779\n",
      "(64, 33)\n",
      "step 26692, loss is 4.8236613273620605\n",
      "(64, 33)\n",
      "step 26693, loss is 4.727798938751221\n",
      "(64, 33)\n",
      "step 26694, loss is 4.7147650718688965\n",
      "(64, 33)\n",
      "step 26695, loss is 4.632226467132568\n",
      "(64, 33)\n",
      "step 26696, loss is 4.762187957763672\n",
      "(64, 33)\n",
      "step 26697, loss is 4.792782306671143\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26698, loss is 4.931084632873535\n",
      "(64, 33)\n",
      "step 26699, loss is 4.76254415512085\n",
      "(64, 33)\n",
      "step 26700, loss is 4.816129684448242\n",
      "(64, 33)\n",
      "step 26701, loss is 4.661620616912842\n",
      "(64, 33)\n",
      "step 26702, loss is 4.851458549499512\n",
      "(64, 33)\n",
      "step 26703, loss is 4.7532267570495605\n",
      "(64, 33)\n",
      "step 26704, loss is 4.875927448272705\n",
      "(64, 33)\n",
      "step 26705, loss is 4.827721118927002\n",
      "(64, 33)\n",
      "step 26706, loss is 4.742668151855469\n",
      "(64, 33)\n",
      "step 26707, loss is 4.777424335479736\n",
      "(64, 33)\n",
      "step 26708, loss is 4.492450714111328\n",
      "(64, 33)\n",
      "step 26709, loss is 4.870847702026367\n",
      "(64, 33)\n",
      "step 26710, loss is 4.759730339050293\n",
      "(64, 33)\n",
      "step 26711, loss is 4.871329307556152\n",
      "(64, 33)\n",
      "step 26712, loss is 4.622091770172119\n",
      "(64, 33)\n",
      "step 26713, loss is 4.900824546813965\n",
      "(64, 33)\n",
      "step 26714, loss is 4.701113700866699\n",
      "(64, 33)\n",
      "step 26715, loss is 4.903881072998047\n",
      "(64, 33)\n",
      "step 26716, loss is 4.658639907836914\n",
      "(64, 33)\n",
      "step 26717, loss is 4.83697509765625\n",
      "(64, 33)\n",
      "step 26718, loss is 4.555874347686768\n",
      "(64, 33)\n",
      "step 26719, loss is 4.714199066162109\n",
      "(64, 33)\n",
      "step 26720, loss is 4.7943196296691895\n",
      "(64, 33)\n",
      "step 26721, loss is 4.842862129211426\n",
      "(64, 33)\n",
      "step 26722, loss is 4.780784606933594\n",
      "(64, 33)\n",
      "step 26723, loss is 4.809563636779785\n",
      "(64, 33)\n",
      "step 26724, loss is 4.802715301513672\n",
      "(64, 33)\n",
      "step 26725, loss is 4.755960941314697\n",
      "(64, 33)\n",
      "step 26726, loss is 4.783541679382324\n",
      "(64, 33)\n",
      "step 26727, loss is 4.56669807434082\n",
      "(64, 33)\n",
      "step 26728, loss is 4.776244640350342\n",
      "(64, 33)\n",
      "step 26729, loss is 4.817036151885986\n",
      "(64, 33)\n",
      "step 26730, loss is 4.738999366760254\n",
      "(64, 33)\n",
      "step 26731, loss is 4.850395202636719\n",
      "(64, 33)\n",
      "step 26732, loss is 4.691652297973633\n",
      "(64, 33)\n",
      "step 26733, loss is 4.621989727020264\n",
      "(64, 33)\n",
      "step 26734, loss is 4.72782564163208\n",
      "(64, 33)\n",
      "step 26735, loss is 5.07336950302124\n",
      "(64, 33)\n",
      "step 26736, loss is 4.801211357116699\n",
      "(64, 33)\n",
      "step 26737, loss is 4.787101745605469\n",
      "(64, 33)\n",
      "step 26738, loss is 4.5074992179870605\n",
      "(64, 33)\n",
      "step 26739, loss is 5.030882358551025\n",
      "(64, 33)\n",
      "step 26740, loss is 4.701342582702637\n",
      "(64, 33)\n",
      "step 26741, loss is 4.935860633850098\n",
      "(64, 33)\n",
      "step 26742, loss is 4.762625694274902\n",
      "(64, 33)\n",
      "step 26743, loss is 4.974228382110596\n",
      "(64, 33)\n",
      "step 26744, loss is 4.81046724319458\n",
      "(64, 33)\n",
      "step 26745, loss is 4.760548114776611\n",
      "(64, 33)\n",
      "step 26746, loss is 4.832630634307861\n",
      "(64, 33)\n",
      "step 26747, loss is 4.5144219398498535\n",
      "(64, 33)\n",
      "step 26748, loss is 4.645522594451904\n",
      "(64, 33)\n",
      "step 26749, loss is 4.763784885406494\n",
      "(64, 33)\n",
      "step 26750, loss is 4.763214111328125\n",
      "(64, 33)\n",
      "step 26751, loss is 4.680940628051758\n",
      "(64, 33)\n",
      "step 26752, loss is 4.707756996154785\n",
      "(64, 33)\n",
      "step 26753, loss is 4.834006309509277\n",
      "(64, 33)\n",
      "step 26754, loss is 4.7384819984436035\n",
      "(64, 33)\n",
      "step 26755, loss is 4.7552924156188965\n",
      "(64, 33)\n",
      "step 26756, loss is 4.667052268981934\n",
      "(64, 33)\n",
      "step 26757, loss is 4.774604797363281\n",
      "(64, 33)\n",
      "step 26758, loss is 4.751387596130371\n",
      "(64, 33)\n",
      "step 26759, loss is 4.693296909332275\n",
      "(64, 33)\n",
      "step 26760, loss is 4.581118583679199\n",
      "(64, 33)\n",
      "step 26761, loss is 4.860889911651611\n",
      "(64, 33)\n",
      "step 26762, loss is 4.872002601623535\n",
      "(64, 33)\n",
      "step 26763, loss is 4.766043663024902\n",
      "(64, 33)\n",
      "step 26764, loss is 4.812746524810791\n",
      "(64, 33)\n",
      "step 26765, loss is 4.745047092437744\n",
      "(64, 33)\n",
      "step 26766, loss is 4.7750630378723145\n",
      "(64, 33)\n",
      "step 26767, loss is 4.778201580047607\n",
      "(64, 33)\n",
      "step 26768, loss is 4.834836959838867\n",
      "(64, 33)\n",
      "step 26769, loss is 4.854074478149414\n",
      "(64, 33)\n",
      "step 26770, loss is 4.779014587402344\n",
      "(64, 33)\n",
      "step 26771, loss is 4.9448652267456055\n",
      "(64, 33)\n",
      "step 26772, loss is 4.811792850494385\n",
      "(64, 33)\n",
      "step 26773, loss is 4.570505619049072\n",
      "(64, 33)\n",
      "step 26774, loss is 4.817058563232422\n",
      "(64, 33)\n",
      "step 26775, loss is 4.9136457443237305\n",
      "(64, 33)\n",
      "step 26776, loss is 4.703248023986816\n",
      "(64, 33)\n",
      "step 26777, loss is 4.813546657562256\n",
      "(64, 33)\n",
      "step 26778, loss is 4.7859673500061035\n",
      "(64, 33)\n",
      "step 26779, loss is 4.801071643829346\n",
      "(64, 33)\n",
      "step 26780, loss is 4.566830635070801\n",
      "(64, 33)\n",
      "step 26781, loss is 4.857143402099609\n",
      "(64, 33)\n",
      "step 26782, loss is 4.798043727874756\n",
      "(64, 33)\n",
      "step 26783, loss is 4.770833969116211\n",
      "(64, 33)\n",
      "step 26784, loss is 4.687056064605713\n",
      "(64, 33)\n",
      "step 26785, loss is 4.7374067306518555\n",
      "(64, 33)\n",
      "step 26786, loss is 4.6956987380981445\n",
      "(64, 33)\n",
      "step 26787, loss is 4.8194260597229\n",
      "(64, 33)\n",
      "step 26788, loss is 4.6257219314575195\n",
      "(64, 33)\n",
      "step 26789, loss is 4.97118616104126\n",
      "(64, 33)\n",
      "step 26790, loss is 4.71448278427124\n",
      "(64, 33)\n",
      "step 26791, loss is 4.925446033477783\n",
      "(64, 33)\n",
      "step 26792, loss is 4.8569722175598145\n",
      "(64, 33)\n",
      "step 26793, loss is 4.783882141113281\n",
      "(64, 33)\n",
      "step 26794, loss is 4.789518356323242\n",
      "(64, 33)\n",
      "step 26795, loss is 4.871692657470703\n",
      "(64, 33)\n",
      "step 26796, loss is 4.622215270996094\n",
      "(64, 33)\n",
      "step 26797, loss is 4.843010425567627\n",
      "(64, 33)\n",
      "step 26798, loss is 4.803169250488281\n",
      "(64, 33)\n",
      "step 26799, loss is 4.978978157043457\n",
      "(64, 33)\n",
      "step 26800, loss is 4.683819770812988\n",
      "(64, 33)\n",
      "step 26801, loss is 4.791426181793213\n",
      "(64, 33)\n",
      "step 26802, loss is 4.788554668426514\n",
      "(64, 33)\n",
      "step 26803, loss is 4.919193744659424\n",
      "(64, 33)\n",
      "step 26804, loss is 4.836323261260986\n",
      "(64, 33)\n",
      "step 26805, loss is 4.700544357299805\n",
      "(64, 33)\n",
      "step 26806, loss is 4.737734794616699\n",
      "(64, 33)\n",
      "step 26807, loss is 4.8074798583984375\n",
      "(64, 33)\n",
      "step 26808, loss is 4.706164836883545\n",
      "(64, 33)\n",
      "step 26809, loss is 4.791686058044434\n",
      "(64, 33)\n",
      "step 26810, loss is 4.846611022949219\n",
      "(64, 33)\n",
      "step 26811, loss is 4.759803771972656\n",
      "(64, 33)\n",
      "step 26812, loss is 4.676535606384277\n",
      "(64, 33)\n",
      "step 26813, loss is 4.939425945281982\n",
      "(64, 33)\n",
      "step 26814, loss is 5.009807586669922\n",
      "(64, 33)\n",
      "step 26815, loss is 4.642322063446045\n",
      "(64, 33)\n",
      "step 26816, loss is 4.813880920410156\n",
      "(64, 33)\n",
      "step 26817, loss is 4.674619674682617\n",
      "(64, 33)\n",
      "step 26818, loss is 4.821855068206787\n",
      "(64, 33)\n",
      "step 26819, loss is 4.9228515625\n",
      "(64, 33)\n",
      "step 26820, loss is 4.810573101043701\n",
      "(64, 33)\n",
      "step 26821, loss is 4.719313144683838\n",
      "(64, 33)\n",
      "step 26822, loss is 4.583219528198242\n",
      "(64, 33)\n",
      "step 26823, loss is 4.658454895019531\n",
      "(64, 33)\n",
      "step 26824, loss is 4.770771503448486\n",
      "(64, 33)\n",
      "step 26825, loss is 4.654426574707031\n",
      "(64, 33)\n",
      "step 26826, loss is 4.906067848205566\n",
      "(64, 33)\n",
      "step 26827, loss is 4.753307819366455\n",
      "(64, 33)\n",
      "step 26828, loss is 4.7939534187316895\n",
      "(64, 33)\n",
      "step 26829, loss is 4.771121025085449\n",
      "(64, 33)\n",
      "step 26830, loss is 4.789974689483643\n",
      "(64, 33)\n",
      "step 26831, loss is 4.8406572341918945\n",
      "(64, 33)\n",
      "step 26832, loss is 4.708521366119385\n",
      "(64, 33)\n",
      "step 26833, loss is 4.765237808227539\n",
      "(64, 33)\n",
      "step 26834, loss is 4.877530574798584\n",
      "(64, 33)\n",
      "step 26835, loss is 4.8214874267578125\n",
      "(64, 33)\n",
      "step 26836, loss is 4.673248291015625\n",
      "(64, 33)\n",
      "step 26837, loss is 4.696199893951416\n",
      "(64, 33)\n",
      "step 26838, loss is 4.764491558074951\n",
      "(64, 33)\n",
      "step 26839, loss is 4.749078273773193\n",
      "(64, 33)\n",
      "step 26840, loss is 4.668152809143066\n",
      "(64, 33)\n",
      "step 26841, loss is 4.799501419067383\n",
      "(64, 33)\n",
      "step 26842, loss is 4.84127140045166\n",
      "(64, 33)\n",
      "step 26843, loss is 4.759687900543213\n",
      "(64, 33)\n",
      "step 26844, loss is 4.871891021728516\n",
      "(64, 33)\n",
      "step 26845, loss is 4.6985673904418945\n",
      "(64, 33)\n",
      "step 26846, loss is 4.470973014831543\n",
      "(64, 33)\n",
      "step 26847, loss is 4.7964959144592285\n",
      "(64, 33)\n",
      "step 26848, loss is 4.817531108856201\n",
      "(64, 33)\n",
      "step 26849, loss is 4.777599334716797\n",
      "(64, 33)\n",
      "step 26850, loss is 4.704886436462402\n",
      "(64, 33)\n",
      "step 26851, loss is 4.775038719177246\n",
      "(64, 33)\n",
      "step 26852, loss is 4.689721584320068\n",
      "(64, 33)\n",
      "step 26853, loss is 4.67155122756958\n",
      "(64, 33)\n",
      "step 26854, loss is 4.893993854522705\n",
      "(64, 33)\n",
      "step 26855, loss is 4.753674507141113\n",
      "(64, 33)\n",
      "step 26856, loss is 4.770857334136963\n",
      "(64, 33)\n",
      "step 26857, loss is 4.699019908905029\n",
      "(64, 33)\n",
      "step 26858, loss is 4.811099052429199\n",
      "(64, 33)\n",
      "step 26859, loss is 4.858274936676025\n",
      "(64, 33)\n",
      "step 26860, loss is 4.77774715423584\n",
      "(64, 33)\n",
      "step 26861, loss is 4.607911586761475\n",
      "(64, 33)\n",
      "step 26862, loss is 4.802866458892822\n",
      "(64, 33)\n",
      "step 26863, loss is 4.856611251831055\n",
      "(64, 33)\n",
      "step 26864, loss is 4.907292366027832\n",
      "(64, 33)\n",
      "step 26865, loss is 4.802903175354004\n",
      "(64, 33)\n",
      "step 26866, loss is 4.804651260375977\n",
      "(64, 33)\n",
      "step 26867, loss is 4.7159104347229\n",
      "(64, 33)\n",
      "step 26868, loss is 4.855887413024902\n",
      "(64, 33)\n",
      "step 26869, loss is 4.800599575042725\n",
      "(64, 33)\n",
      "step 26870, loss is 4.901116847991943\n",
      "(64, 33)\n",
      "step 26871, loss is 4.776179313659668\n",
      "(64, 33)\n",
      "step 26872, loss is 4.732604026794434\n",
      "(64, 33)\n",
      "step 26873, loss is 4.7079315185546875\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26874, loss is 4.774933815002441\n",
      "(64, 33)\n",
      "step 26875, loss is 4.793067455291748\n",
      "(64, 33)\n",
      "step 26876, loss is 4.734245300292969\n",
      "(64, 33)\n",
      "step 26877, loss is 4.616448402404785\n",
      "(64, 33)\n",
      "step 26878, loss is 4.641785144805908\n",
      "(64, 33)\n",
      "step 26879, loss is 4.6986308097839355\n",
      "(64, 33)\n",
      "step 26880, loss is 4.754427909851074\n",
      "(64, 33)\n",
      "step 26881, loss is 4.578206539154053\n",
      "(64, 33)\n",
      "step 26882, loss is 4.86857271194458\n",
      "(64, 33)\n",
      "step 26883, loss is 4.846861839294434\n",
      "(64, 33)\n",
      "step 26884, loss is 5.016733646392822\n",
      "(64, 33)\n",
      "step 26885, loss is 4.786561489105225\n",
      "(64, 33)\n",
      "step 26886, loss is 4.7751593589782715\n",
      "(64, 33)\n",
      "step 26887, loss is 4.815277099609375\n",
      "(64, 33)\n",
      "step 26888, loss is 4.8504958152771\n",
      "(64, 33)\n",
      "step 26889, loss is 4.995965480804443\n",
      "(64, 33)\n",
      "step 26890, loss is 4.667541980743408\n",
      "(64, 33)\n",
      "step 26891, loss is 4.768013954162598\n",
      "(64, 33)\n",
      "step 26892, loss is 4.775219440460205\n",
      "(64, 33)\n",
      "step 26893, loss is 4.848000526428223\n",
      "(64, 33)\n",
      "step 26894, loss is 4.7594709396362305\n",
      "(64, 33)\n",
      "step 26895, loss is 4.719686031341553\n",
      "(64, 33)\n",
      "step 26896, loss is 4.732163906097412\n",
      "(64, 33)\n",
      "step 26897, loss is 5.049415588378906\n",
      "(64, 33)\n",
      "step 26898, loss is 4.697671890258789\n",
      "(64, 33)\n",
      "step 26899, loss is 4.784201622009277\n",
      "(64, 33)\n",
      "step 26900, loss is 4.786251544952393\n",
      "(64, 33)\n",
      "step 26901, loss is 4.901705265045166\n",
      "(64, 33)\n",
      "step 26902, loss is 4.7307562828063965\n",
      "(64, 33)\n",
      "step 26903, loss is 4.605789661407471\n",
      "(64, 33)\n",
      "step 26904, loss is 4.8547492027282715\n",
      "(64, 33)\n",
      "step 26905, loss is 4.747974872589111\n",
      "(64, 33)\n",
      "step 26906, loss is 4.780080318450928\n",
      "(64, 33)\n",
      "step 26907, loss is 5.074246406555176\n",
      "(64, 33)\n",
      "step 26908, loss is 4.696768283843994\n",
      "(64, 33)\n",
      "step 26909, loss is 4.716614246368408\n",
      "(64, 33)\n",
      "step 26910, loss is 4.749536514282227\n",
      "(64, 33)\n",
      "step 26911, loss is 4.767682075500488\n",
      "(64, 33)\n",
      "step 26912, loss is 4.9601545333862305\n",
      "(64, 33)\n",
      "step 26913, loss is 4.7283830642700195\n",
      "(64, 33)\n",
      "step 26914, loss is 4.7218451499938965\n",
      "(64, 33)\n",
      "step 26915, loss is 4.672750473022461\n",
      "(64, 33)\n",
      "step 26916, loss is 4.694451332092285\n",
      "(64, 33)\n",
      "step 26917, loss is 4.564465045928955\n",
      "(64, 33)\n",
      "step 26918, loss is 4.855285167694092\n",
      "(64, 33)\n",
      "step 26919, loss is 4.698724269866943\n",
      "(64, 33)\n",
      "step 26920, loss is 4.798128128051758\n",
      "(64, 33)\n",
      "step 26921, loss is 4.803532600402832\n",
      "(64, 33)\n",
      "step 26922, loss is 4.774777889251709\n",
      "(64, 33)\n",
      "step 26923, loss is 4.6730523109436035\n",
      "(64, 33)\n",
      "step 26924, loss is 4.7982282638549805\n",
      "(64, 33)\n",
      "step 26925, loss is 4.736066818237305\n",
      "(64, 33)\n",
      "step 26926, loss is 4.810239791870117\n",
      "(64, 33)\n",
      "step 26927, loss is 4.856511116027832\n",
      "(64, 33)\n",
      "step 26928, loss is 4.972949028015137\n",
      "(64, 33)\n",
      "step 26929, loss is 4.8428754806518555\n",
      "(64, 33)\n",
      "step 26930, loss is 4.876586437225342\n",
      "(64, 33)\n",
      "step 26931, loss is 4.784602165222168\n",
      "(64, 33)\n",
      "step 26932, loss is 4.72782039642334\n",
      "(64, 33)\n",
      "step 26933, loss is 4.560853004455566\n",
      "(64, 33)\n",
      "step 26934, loss is 4.793272495269775\n",
      "(64, 33)\n",
      "step 26935, loss is 4.793434143066406\n",
      "(64, 33)\n",
      "step 26936, loss is 4.61360502243042\n",
      "(64, 33)\n",
      "step 26937, loss is 4.714593887329102\n",
      "(64, 33)\n",
      "step 26938, loss is 4.901710033416748\n",
      "(64, 33)\n",
      "step 26939, loss is 4.867660045623779\n",
      "(64, 33)\n",
      "step 26940, loss is 4.709727764129639\n",
      "(64, 33)\n",
      "step 26941, loss is 4.625728130340576\n",
      "(64, 33)\n",
      "step 26942, loss is 4.749741077423096\n",
      "(64, 33)\n",
      "step 26943, loss is 4.671577453613281\n",
      "(64, 33)\n",
      "step 26944, loss is 4.7513298988342285\n",
      "(64, 33)\n",
      "step 26945, loss is 4.7739033699035645\n",
      "(64, 33)\n",
      "step 26946, loss is 4.8835768699646\n",
      "(64, 33)\n",
      "step 26947, loss is 4.709036350250244\n",
      "(64, 33)\n",
      "step 26948, loss is 4.847498893737793\n",
      "(64, 33)\n",
      "step 26949, loss is 4.739677429199219\n",
      "(64, 33)\n",
      "step 26950, loss is 4.846040725708008\n",
      "(64, 33)\n",
      "step 26951, loss is 4.834292888641357\n",
      "(64, 33)\n",
      "step 26952, loss is 4.72810697555542\n",
      "(64, 33)\n",
      "step 26953, loss is 4.831361770629883\n",
      "(64, 33)\n",
      "step 26954, loss is 4.913715362548828\n",
      "(64, 33)\n",
      "step 26955, loss is 4.842695713043213\n",
      "(64, 33)\n",
      "step 26956, loss is 4.737374782562256\n",
      "(64, 33)\n",
      "step 26957, loss is 4.853682041168213\n",
      "(64, 33)\n",
      "step 26958, loss is 4.85685396194458\n",
      "(64, 33)\n",
      "step 26959, loss is 4.680425643920898\n",
      "(64, 33)\n",
      "step 26960, loss is 4.920089244842529\n",
      "(64, 33)\n",
      "step 26961, loss is 4.932172775268555\n",
      "(64, 33)\n",
      "step 26962, loss is 4.827033519744873\n",
      "(64, 33)\n",
      "step 26963, loss is 4.750250816345215\n",
      "(64, 33)\n",
      "step 26964, loss is 4.78272819519043\n",
      "(64, 33)\n",
      "step 26965, loss is 4.620331764221191\n",
      "(64, 33)\n",
      "step 26966, loss is 4.874507427215576\n",
      "(64, 33)\n",
      "step 26967, loss is 4.7105793952941895\n",
      "(64, 33)\n",
      "step 26968, loss is 4.705378532409668\n",
      "(64, 33)\n",
      "step 26969, loss is 4.956291198730469\n",
      "(64, 33)\n",
      "step 26970, loss is 4.652867794036865\n",
      "(64, 33)\n",
      "step 26971, loss is 4.772210597991943\n",
      "(64, 33)\n",
      "step 26972, loss is 4.740210056304932\n",
      "(64, 33)\n",
      "step 26973, loss is 4.691518306732178\n",
      "(64, 33)\n",
      "step 26974, loss is 4.739440441131592\n",
      "(64, 33)\n",
      "step 26975, loss is 4.996882915496826\n",
      "(64, 33)\n",
      "step 26976, loss is 4.809828758239746\n",
      "(64, 33)\n",
      "step 26977, loss is 4.706023216247559\n",
      "(64, 33)\n",
      "step 26978, loss is 4.719010353088379\n",
      "(64, 33)\n",
      "step 26979, loss is 4.821406841278076\n",
      "(64, 33)\n",
      "step 26980, loss is 4.532554626464844\n",
      "(64, 33)\n",
      "step 26981, loss is 4.5734782218933105\n",
      "(64, 33)\n",
      "step 26982, loss is 4.985262870788574\n",
      "(64, 33)\n",
      "step 26983, loss is 4.715576171875\n",
      "(64, 33)\n",
      "step 26984, loss is 4.802318096160889\n",
      "(64, 33)\n",
      "step 26985, loss is 4.731998920440674\n",
      "(64, 33)\n",
      "step 26986, loss is 4.8689351081848145\n",
      "(64, 33)\n",
      "step 26987, loss is 4.485986709594727\n",
      "(64, 33)\n",
      "step 26988, loss is 4.794580459594727\n",
      "(64, 33)\n",
      "step 26989, loss is 5.021946907043457\n",
      "(64, 33)\n",
      "step 26990, loss is 4.704897880554199\n",
      "(64, 33)\n",
      "step 26991, loss is 5.02215576171875\n",
      "(64, 33)\n",
      "step 26992, loss is 4.775390625\n",
      "(64, 33)\n",
      "step 26993, loss is 4.806617736816406\n",
      "(64, 33)\n",
      "step 26994, loss is 4.661060810089111\n",
      "(64, 33)\n",
      "step 26995, loss is 4.606597900390625\n",
      "(64, 33)\n",
      "step 26996, loss is 4.905762672424316\n",
      "(64, 33)\n",
      "step 26997, loss is 4.860365867614746\n",
      "(64, 33)\n",
      "step 26998, loss is 4.737964153289795\n",
      "(64, 33)\n",
      "step 26999, loss is 4.855813503265381\n",
      "(64, 33)\n",
      "step 27000, loss is 4.724430084228516\n",
      "(64, 33)\n",
      "step 27001, loss is 4.7906928062438965\n",
      "(64, 33)\n",
      "step 27002, loss is 4.53003454208374\n",
      "(64, 33)\n",
      "step 27003, loss is 4.938399791717529\n",
      "(64, 33)\n",
      "step 27004, loss is 4.873134613037109\n",
      "(64, 33)\n",
      "step 27005, loss is 4.70662784576416\n",
      "(64, 33)\n",
      "step 27006, loss is 4.602651119232178\n",
      "(64, 33)\n",
      "step 27007, loss is 5.0590596199035645\n",
      "(64, 33)\n",
      "step 27008, loss is 4.552699089050293\n",
      "(64, 33)\n",
      "step 27009, loss is 4.80040168762207\n",
      "(64, 33)\n",
      "step 27010, loss is 4.714836120605469\n",
      "(64, 33)\n",
      "step 27011, loss is 4.82737922668457\n",
      "(64, 33)\n",
      "step 27012, loss is 4.853042125701904\n",
      "(64, 33)\n",
      "step 27013, loss is 4.8809051513671875\n",
      "(64, 33)\n",
      "step 27014, loss is 4.735414505004883\n",
      "(64, 33)\n",
      "step 27015, loss is 4.699512958526611\n",
      "(64, 33)\n",
      "step 27016, loss is 4.686222076416016\n",
      "(64, 33)\n",
      "step 27017, loss is 4.825472831726074\n",
      "(64, 33)\n",
      "step 27018, loss is 4.7278313636779785\n",
      "(64, 33)\n",
      "step 27019, loss is 4.8884453773498535\n",
      "(64, 33)\n",
      "step 27020, loss is 4.819370746612549\n",
      "(64, 33)\n",
      "step 27021, loss is 4.924002647399902\n",
      "(64, 33)\n",
      "step 27022, loss is 4.847476482391357\n",
      "(64, 33)\n",
      "step 27023, loss is 4.655346393585205\n",
      "(64, 33)\n",
      "step 27024, loss is 4.912529468536377\n",
      "(64, 33)\n",
      "step 27025, loss is 4.923538684844971\n",
      "(64, 33)\n",
      "step 27026, loss is 4.758301734924316\n",
      "(64, 33)\n",
      "step 27027, loss is 4.644345283508301\n",
      "(64, 33)\n",
      "step 27028, loss is 4.705463886260986\n",
      "(64, 33)\n",
      "step 27029, loss is 4.722742557525635\n",
      "(64, 33)\n",
      "step 27030, loss is 4.6978440284729\n",
      "(64, 33)\n",
      "step 27031, loss is 4.762092590332031\n",
      "(64, 33)\n",
      "step 27032, loss is 4.671440124511719\n",
      "(64, 33)\n",
      "step 27033, loss is 4.721773147583008\n",
      "(64, 33)\n",
      "step 27034, loss is 4.713094234466553\n",
      "(64, 33)\n",
      "step 27035, loss is 4.675753116607666\n",
      "(64, 33)\n",
      "step 27036, loss is 4.731645107269287\n",
      "(64, 33)\n",
      "step 27037, loss is 4.7757978439331055\n",
      "(64, 33)\n",
      "step 27038, loss is 4.873935699462891\n",
      "(64, 33)\n",
      "step 27039, loss is 4.615584373474121\n",
      "(64, 33)\n",
      "step 27040, loss is 4.654681205749512\n",
      "(64, 33)\n",
      "step 27041, loss is 4.952752113342285\n",
      "(64, 33)\n",
      "step 27042, loss is 4.958758354187012\n",
      "(64, 33)\n",
      "step 27043, loss is 4.843661308288574\n",
      "(64, 33)\n",
      "step 27044, loss is 4.832277774810791\n",
      "(64, 33)\n",
      "step 27045, loss is 4.700108528137207\n",
      "(64, 33)\n",
      "step 27046, loss is 4.817481517791748\n",
      "(64, 33)\n",
      "step 27047, loss is 4.704178810119629\n",
      "(64, 33)\n",
      "step 27048, loss is 4.795334815979004\n",
      "(64, 33)\n",
      "step 27049, loss is 4.744372367858887\n",
      "(64, 33)\n",
      "step 27050, loss is 4.905768871307373\n",
      "(64, 33)\n",
      "step 27051, loss is 4.616815567016602\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 27052, loss is 4.77303409576416\n",
      "(64, 33)\n",
      "step 27053, loss is 4.689932346343994\n",
      "(64, 33)\n",
      "step 27054, loss is 4.902603626251221\n",
      "(64, 33)\n",
      "step 27055, loss is 4.699716567993164\n",
      "(64, 33)\n",
      "step 27056, loss is 4.702193260192871\n",
      "(64, 33)\n",
      "step 27057, loss is 4.805811405181885\n",
      "(64, 33)\n",
      "step 27058, loss is 4.632664203643799\n",
      "(64, 33)\n",
      "step 27059, loss is 4.934233665466309\n",
      "(64, 33)\n",
      "step 27060, loss is 4.848049163818359\n",
      "(64, 33)\n",
      "step 27061, loss is 4.89048433303833\n",
      "(64, 33)\n",
      "step 27062, loss is 4.699729919433594\n",
      "(64, 33)\n",
      "step 27063, loss is 4.904486179351807\n",
      "(64, 33)\n",
      "step 27064, loss is 4.782804489135742\n",
      "(64, 33)\n",
      "step 27065, loss is 4.758327484130859\n",
      "(64, 33)\n",
      "step 27066, loss is 4.74965238571167\n",
      "(64, 33)\n",
      "step 27067, loss is 4.909068584442139\n",
      "(64, 33)\n",
      "step 27068, loss is 4.7308878898620605\n",
      "(64, 33)\n",
      "step 27069, loss is 4.701585292816162\n",
      "(64, 33)\n",
      "step 27070, loss is 4.914797306060791\n",
      "(64, 33)\n",
      "step 27071, loss is 4.794997692108154\n",
      "(64, 33)\n",
      "step 27072, loss is 4.798776626586914\n",
      "(64, 33)\n",
      "step 27073, loss is 4.824409484863281\n",
      "(64, 33)\n",
      "step 27074, loss is 4.728603839874268\n",
      "(64, 33)\n",
      "step 27075, loss is 4.857590198516846\n",
      "(64, 33)\n",
      "step 27076, loss is 4.997596263885498\n",
      "(64, 33)\n",
      "step 27077, loss is 4.740483283996582\n",
      "(64, 33)\n",
      "step 27078, loss is 4.811336994171143\n",
      "(64, 33)\n",
      "step 27079, loss is 4.674230098724365\n",
      "(64, 33)\n",
      "step 27080, loss is 4.842714786529541\n",
      "(64, 33)\n",
      "step 27081, loss is 4.997142791748047\n",
      "(64, 33)\n",
      "step 27082, loss is 4.661912441253662\n",
      "(64, 33)\n",
      "step 27083, loss is 4.784529209136963\n",
      "(64, 33)\n",
      "step 27084, loss is 4.71919584274292\n",
      "(64, 33)\n",
      "step 27085, loss is 4.854501247406006\n",
      "(64, 33)\n",
      "step 27086, loss is 5.033544540405273\n",
      "(64, 33)\n",
      "step 27087, loss is 4.654092788696289\n",
      "(64, 33)\n",
      "step 27088, loss is 4.951147079467773\n",
      "(64, 33)\n",
      "step 27089, loss is 4.904520511627197\n",
      "(64, 33)\n",
      "step 27090, loss is 4.838165283203125\n",
      "(64, 33)\n",
      "step 27091, loss is 4.839303970336914\n",
      "(64, 33)\n",
      "step 27092, loss is 4.685643196105957\n",
      "(64, 33)\n",
      "step 27093, loss is 4.7122979164123535\n",
      "(64, 33)\n",
      "step 27094, loss is 4.603912353515625\n",
      "(64, 33)\n",
      "step 27095, loss is 4.59220027923584\n",
      "(64, 33)\n",
      "step 27096, loss is 5.020840644836426\n",
      "(64, 33)\n",
      "step 27097, loss is 4.923377513885498\n",
      "(64, 33)\n",
      "step 27098, loss is 4.775618076324463\n",
      "(64, 33)\n",
      "step 27099, loss is 4.731550216674805\n",
      "(64, 33)\n",
      "step 27100, loss is 4.869917392730713\n",
      "(64, 33)\n",
      "step 27101, loss is 4.751497745513916\n",
      "(64, 33)\n",
      "step 27102, loss is 4.778079509735107\n",
      "(64, 33)\n",
      "step 27103, loss is 4.77921199798584\n",
      "(64, 33)\n",
      "step 27104, loss is 4.834918975830078\n",
      "(64, 33)\n",
      "step 27105, loss is 4.724152565002441\n",
      "(64, 33)\n",
      "step 27106, loss is 4.590608596801758\n",
      "(64, 33)\n",
      "step 27107, loss is 4.918145656585693\n",
      "(64, 33)\n",
      "step 27108, loss is 4.665292263031006\n",
      "(64, 33)\n",
      "step 27109, loss is 4.750521659851074\n",
      "(64, 33)\n",
      "step 27110, loss is 4.8104987144470215\n",
      "(64, 33)\n",
      "step 27111, loss is 4.754306316375732\n",
      "(64, 33)\n",
      "step 27112, loss is 4.768322944641113\n",
      "(64, 33)\n",
      "step 27113, loss is 4.760343551635742\n",
      "(64, 33)\n",
      "step 27114, loss is 4.996812343597412\n",
      "(64, 33)\n",
      "step 27115, loss is 4.739460468292236\n",
      "(64, 33)\n",
      "step 27116, loss is 4.751844882965088\n",
      "(64, 33)\n",
      "step 27117, loss is 4.656639575958252\n",
      "(64, 33)\n",
      "step 27118, loss is 4.734620571136475\n",
      "(64, 33)\n",
      "step 27119, loss is 4.772043228149414\n",
      "(64, 33)\n",
      "step 27120, loss is 4.714836597442627\n",
      "(64, 33)\n",
      "step 27121, loss is 4.704001426696777\n",
      "(64, 33)\n",
      "step 27122, loss is 4.831296443939209\n",
      "(64, 33)\n",
      "step 27123, loss is 4.843332290649414\n",
      "(64, 33)\n",
      "step 27124, loss is 4.649823188781738\n",
      "(64, 33)\n",
      "step 27125, loss is 4.841055870056152\n",
      "(64, 33)\n",
      "step 27126, loss is 4.576441287994385\n",
      "(64, 33)\n",
      "step 27127, loss is 4.778370380401611\n",
      "(64, 33)\n",
      "step 27128, loss is 4.836785793304443\n",
      "(64, 33)\n",
      "step 27129, loss is 4.837135314941406\n",
      "(64, 33)\n",
      "step 27130, loss is 4.711847305297852\n",
      "(64, 33)\n",
      "step 27131, loss is 4.8003926277160645\n",
      "(64, 33)\n",
      "step 27132, loss is 4.840931415557861\n",
      "(64, 33)\n",
      "step 27133, loss is 4.822751045227051\n",
      "(64, 33)\n",
      "step 27134, loss is 4.815611362457275\n",
      "(64, 33)\n",
      "step 27135, loss is 4.872631549835205\n",
      "(64, 33)\n",
      "step 27136, loss is 4.790895462036133\n",
      "(64, 33)\n",
      "step 27137, loss is 4.967419624328613\n",
      "(64, 33)\n",
      "step 27138, loss is 4.87153959274292\n",
      "(64, 33)\n",
      "step 27139, loss is 4.922821044921875\n",
      "(64, 33)\n",
      "step 27140, loss is 4.726630210876465\n",
      "(64, 33)\n",
      "step 27141, loss is 4.653908729553223\n",
      "(64, 33)\n",
      "step 27142, loss is 4.843765735626221\n",
      "(64, 33)\n",
      "step 27143, loss is 4.867612361907959\n",
      "(64, 33)\n",
      "step 27144, loss is 4.6415910720825195\n",
      "(64, 33)\n",
      "step 27145, loss is 4.688177585601807\n",
      "(64, 33)\n",
      "step 27146, loss is 4.705705642700195\n",
      "(64, 33)\n",
      "step 27147, loss is 4.894484996795654\n",
      "(64, 33)\n",
      "step 27148, loss is 4.954036235809326\n",
      "(64, 33)\n",
      "step 27149, loss is 4.77211332321167\n",
      "(64, 33)\n",
      "step 27150, loss is 4.745414733886719\n",
      "(64, 33)\n",
      "step 27151, loss is 4.7912774085998535\n",
      "(64, 33)\n",
      "step 27152, loss is 4.77783203125\n",
      "(64, 33)\n",
      "step 27153, loss is 4.853067874908447\n",
      "(64, 33)\n",
      "step 27154, loss is 4.72875452041626\n",
      "(64, 33)\n",
      "step 27155, loss is 4.895774841308594\n",
      "(64, 33)\n",
      "step 27156, loss is 4.844310283660889\n",
      "(64, 33)\n",
      "step 27157, loss is 4.580098628997803\n",
      "(64, 33)\n",
      "step 27158, loss is 4.89396333694458\n",
      "(64, 33)\n",
      "step 27159, loss is 4.6469526290893555\n",
      "(64, 33)\n",
      "step 27160, loss is 4.8636064529418945\n",
      "(64, 33)\n",
      "step 27161, loss is 4.886008262634277\n",
      "(64, 33)\n",
      "step 27162, loss is 4.827470779418945\n",
      "(64, 33)\n",
      "step 27163, loss is 4.760451316833496\n",
      "(64, 33)\n",
      "step 27164, loss is 4.758197784423828\n",
      "(64, 33)\n",
      "step 27165, loss is 4.774459362030029\n",
      "(64, 33)\n",
      "step 27166, loss is 4.8655924797058105\n",
      "(64, 33)\n",
      "step 27167, loss is 4.585874080657959\n",
      "(64, 33)\n",
      "step 27168, loss is 4.743226051330566\n",
      "(64, 33)\n",
      "step 27169, loss is 4.996585845947266\n",
      "(64, 33)\n",
      "step 27170, loss is 4.8380656242370605\n",
      "(64, 33)\n",
      "step 27171, loss is 4.844738483428955\n",
      "(64, 33)\n",
      "step 27172, loss is 4.757211685180664\n",
      "(64, 33)\n",
      "step 27173, loss is 4.6214375495910645\n",
      "(64, 33)\n",
      "step 27174, loss is 4.636777877807617\n",
      "(64, 33)\n",
      "step 27175, loss is 5.031976222991943\n",
      "(64, 33)\n",
      "step 27176, loss is 4.764837741851807\n",
      "(64, 33)\n",
      "step 27177, loss is 4.771795272827148\n",
      "(64, 33)\n",
      "step 27178, loss is 4.780600070953369\n",
      "(64, 33)\n",
      "step 27179, loss is 4.877493858337402\n",
      "(64, 33)\n",
      "step 27180, loss is 4.863062858581543\n",
      "(64, 33)\n",
      "step 27181, loss is 4.840808391571045\n",
      "(64, 33)\n",
      "step 27182, loss is 4.899852275848389\n",
      "(64, 33)\n",
      "step 27183, loss is 4.752949237823486\n",
      "(64, 33)\n",
      "step 27184, loss is 4.473556041717529\n",
      "(64, 33)\n",
      "step 27185, loss is 4.831943035125732\n",
      "(64, 33)\n",
      "step 27186, loss is 4.82935094833374\n",
      "(64, 33)\n",
      "step 27187, loss is 4.920905113220215\n",
      "(64, 33)\n",
      "step 27188, loss is 4.861050128936768\n",
      "(64, 33)\n",
      "step 27189, loss is 4.621842861175537\n",
      "(64, 33)\n",
      "step 27190, loss is 4.898556709289551\n",
      "(64, 33)\n",
      "step 27191, loss is 4.7016215324401855\n",
      "(64, 33)\n",
      "step 27192, loss is 4.726984977722168\n",
      "(64, 33)\n",
      "step 27193, loss is 4.812870979309082\n",
      "(64, 33)\n",
      "step 27194, loss is 4.73150634765625\n",
      "(64, 33)\n",
      "step 27195, loss is 4.69224739074707\n",
      "(64, 33)\n",
      "step 27196, loss is 4.661908149719238\n",
      "(64, 33)\n",
      "step 27197, loss is 4.971839427947998\n",
      "(64, 33)\n",
      "step 27198, loss is 4.814263820648193\n",
      "(64, 33)\n",
      "step 27199, loss is 4.824972152709961\n",
      "(64, 33)\n",
      "step 27200, loss is 4.864291191101074\n",
      "(64, 33)\n",
      "step 27201, loss is 4.76559591293335\n",
      "(64, 33)\n",
      "step 27202, loss is 4.788505554199219\n",
      "(64, 33)\n",
      "step 27203, loss is 4.916905879974365\n",
      "(64, 33)\n",
      "step 27204, loss is 4.805135726928711\n",
      "(64, 33)\n",
      "step 27205, loss is 4.856672763824463\n",
      "(64, 33)\n",
      "step 27206, loss is 4.778473854064941\n",
      "(64, 33)\n",
      "step 27207, loss is 4.766508102416992\n",
      "(64, 33)\n",
      "step 27208, loss is 4.68950891494751\n",
      "(64, 33)\n",
      "step 27209, loss is 4.739731311798096\n",
      "(64, 33)\n",
      "step 27210, loss is 4.725536823272705\n",
      "(64, 33)\n",
      "step 27211, loss is 4.726417541503906\n",
      "(64, 33)\n",
      "step 27212, loss is 4.8048834800720215\n",
      "(64, 33)\n",
      "step 27213, loss is 4.807766914367676\n",
      "(64, 33)\n",
      "step 27214, loss is 4.902041435241699\n",
      "(64, 33)\n",
      "step 27215, loss is 4.507933616638184\n",
      "(64, 33)\n",
      "step 27216, loss is 4.641088485717773\n",
      "(64, 33)\n",
      "step 27217, loss is 4.7880425453186035\n",
      "(64, 33)\n",
      "step 27218, loss is 4.756586074829102\n",
      "(64, 33)\n",
      "step 27219, loss is 4.906923294067383\n",
      "(64, 33)\n",
      "step 27220, loss is 4.679145812988281\n",
      "(64, 33)\n",
      "step 27221, loss is 4.951181411743164\n",
      "(64, 33)\n",
      "step 27222, loss is 4.67451810836792\n",
      "(64, 33)\n",
      "step 27223, loss is 4.602597236633301\n",
      "(64, 33)\n",
      "step 27224, loss is 4.756367206573486\n",
      "(64, 33)\n",
      "step 27225, loss is 4.766769886016846\n",
      "(64, 33)\n",
      "step 27226, loss is 4.719925403594971\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 27227, loss is 4.833897590637207\n",
      "(64, 33)\n",
      "step 27228, loss is 4.8309221267700195\n",
      "(64, 33)\n",
      "step 27229, loss is 4.973627090454102\n",
      "(64, 33)\n",
      "step 27230, loss is 4.780398368835449\n",
      "(64, 33)\n",
      "step 27231, loss is 4.779263019561768\n",
      "(64, 33)\n",
      "step 27232, loss is 4.73140811920166\n",
      "(64, 33)\n",
      "step 27233, loss is 4.713324069976807\n",
      "(64, 33)\n",
      "step 27234, loss is 4.6035871505737305\n",
      "(64, 33)\n",
      "step 27235, loss is 4.76531457901001\n",
      "(64, 33)\n",
      "step 27236, loss is 4.629348278045654\n",
      "(64, 33)\n",
      "step 27237, loss is 4.797447204589844\n",
      "(64, 33)\n",
      "step 27238, loss is 4.8060302734375\n",
      "(64, 33)\n",
      "step 27239, loss is 4.831159591674805\n",
      "(64, 33)\n",
      "step 27240, loss is 4.891955375671387\n",
      "(64, 33)\n",
      "step 27241, loss is 4.869411945343018\n",
      "(64, 33)\n",
      "step 27242, loss is 4.783601760864258\n",
      "(64, 33)\n",
      "step 27243, loss is 4.865126132965088\n",
      "(64, 33)\n",
      "step 27244, loss is 4.710964679718018\n",
      "(64, 33)\n",
      "step 27245, loss is 4.907130718231201\n",
      "(64, 33)\n",
      "step 27246, loss is 4.812921047210693\n",
      "(64, 33)\n",
      "step 27247, loss is 4.684502601623535\n",
      "(64, 33)\n",
      "step 27248, loss is 4.799037456512451\n",
      "(64, 33)\n",
      "step 27249, loss is 4.928498268127441\n",
      "(64, 33)\n",
      "step 27250, loss is 4.9164252281188965\n",
      "(64, 33)\n",
      "step 27251, loss is 4.764334201812744\n",
      "(64, 33)\n",
      "step 27252, loss is 4.6991801261901855\n",
      "(64, 33)\n",
      "step 27253, loss is 4.726140975952148\n",
      "(64, 33)\n",
      "step 27254, loss is 4.892073154449463\n",
      "(64, 33)\n",
      "step 27255, loss is 4.744256973266602\n",
      "(64, 33)\n",
      "step 27256, loss is 4.677964687347412\n",
      "(64, 33)\n",
      "step 27257, loss is 4.719153881072998\n",
      "(64, 33)\n",
      "step 27258, loss is 4.8073601722717285\n",
      "(64, 33)\n",
      "step 27259, loss is 5.064645767211914\n",
      "(64, 33)\n",
      "step 27260, loss is 4.746597766876221\n",
      "(64, 33)\n",
      "step 27261, loss is 4.825510025024414\n",
      "(64, 33)\n",
      "step 27262, loss is 4.6693806648254395\n",
      "(64, 33)\n",
      "step 27263, loss is 4.753947734832764\n",
      "(64, 33)\n",
      "step 27264, loss is 4.719329357147217\n",
      "(64, 33)\n",
      "step 27265, loss is 4.645240783691406\n",
      "(64, 33)\n",
      "step 27266, loss is 4.706511497497559\n",
      "(64, 33)\n",
      "step 27267, loss is 4.7703680992126465\n",
      "(64, 33)\n",
      "step 27268, loss is 4.794127464294434\n",
      "(64, 33)\n",
      "step 27269, loss is 4.87722635269165\n",
      "(64, 33)\n",
      "step 27270, loss is 4.729737758636475\n",
      "(64, 33)\n",
      "step 27271, loss is 4.767106056213379\n",
      "(64, 33)\n",
      "step 27272, loss is 4.755263328552246\n",
      "(64, 33)\n",
      "step 27273, loss is 4.7063422203063965\n",
      "(64, 33)\n",
      "step 27274, loss is 4.692803859710693\n",
      "(64, 33)\n",
      "step 27275, loss is 4.66609525680542\n",
      "(64, 33)\n",
      "step 27276, loss is 4.663073539733887\n",
      "(64, 33)\n",
      "step 27277, loss is 4.890157699584961\n",
      "(64, 33)\n",
      "step 27278, loss is 4.822072505950928\n",
      "(64, 33)\n",
      "step 27279, loss is 4.6237640380859375\n",
      "(64, 33)\n",
      "step 27280, loss is 4.865213871002197\n",
      "(64, 33)\n",
      "step 27281, loss is 4.685357570648193\n",
      "(64, 33)\n",
      "step 27282, loss is 4.686564922332764\n",
      "(64, 33)\n",
      "step 27283, loss is 4.801141262054443\n",
      "(64, 33)\n",
      "step 27284, loss is 4.732336521148682\n",
      "(64, 33)\n",
      "step 27285, loss is 4.740930080413818\n",
      "(64, 33)\n",
      "step 27286, loss is 4.828297138214111\n",
      "(64, 33)\n",
      "step 27287, loss is 4.827839374542236\n",
      "(64, 33)\n",
      "step 27288, loss is 4.868847846984863\n",
      "(64, 33)\n",
      "step 27289, loss is 4.746534824371338\n",
      "(64, 33)\n",
      "step 27290, loss is 4.624861240386963\n",
      "(64, 33)\n",
      "step 27291, loss is 4.874753952026367\n",
      "(64, 33)\n",
      "step 27292, loss is 4.73553466796875\n",
      "(64, 33)\n",
      "step 27293, loss is 4.766525745391846\n",
      "(64, 33)\n",
      "step 27294, loss is 4.6681036949157715\n",
      "(64, 33)\n",
      "step 27295, loss is 4.879007816314697\n",
      "(64, 33)\n",
      "step 27296, loss is 4.756942272186279\n",
      "(64, 33)\n",
      "step 27297, loss is 4.817901611328125\n",
      "(64, 33)\n",
      "step 27298, loss is 4.813921928405762\n",
      "(64, 33)\n",
      "step 27299, loss is 4.91228723526001\n",
      "(64, 33)\n",
      "step 27300, loss is 4.646816730499268\n",
      "(64, 33)\n",
      "step 27301, loss is 4.804561614990234\n",
      "(64, 33)\n",
      "step 27302, loss is 4.81491756439209\n",
      "(64, 33)\n",
      "step 27303, loss is 4.672741413116455\n",
      "(64, 33)\n",
      "step 27304, loss is 4.80352258682251\n",
      "(64, 33)\n",
      "step 27305, loss is 5.02199125289917\n",
      "(64, 33)\n",
      "step 27306, loss is 4.794741630554199\n",
      "(64, 33)\n",
      "step 27307, loss is 4.6883158683776855\n",
      "(64, 33)\n",
      "step 27308, loss is 4.747272968292236\n",
      "(64, 33)\n",
      "step 27309, loss is 4.792026042938232\n",
      "(64, 33)\n",
      "step 27310, loss is 4.72389554977417\n",
      "(64, 33)\n",
      "step 27311, loss is 4.632381916046143\n",
      "(64, 33)\n",
      "step 27312, loss is 4.861026287078857\n",
      "(64, 33)\n",
      "step 27313, loss is 4.6260857582092285\n",
      "(64, 33)\n",
      "step 27314, loss is 4.878255367279053\n",
      "(64, 33)\n",
      "step 27315, loss is 4.81874942779541\n",
      "(64, 33)\n",
      "step 27316, loss is 4.752968788146973\n",
      "(64, 33)\n",
      "step 27317, loss is 4.760044574737549\n",
      "(64, 33)\n",
      "step 27318, loss is 4.780889511108398\n",
      "(64, 33)\n",
      "step 27319, loss is 4.716316223144531\n",
      "(64, 33)\n",
      "step 27320, loss is 4.708395481109619\n",
      "(64, 33)\n",
      "step 27321, loss is 4.786308288574219\n",
      "(64, 33)\n",
      "step 27322, loss is 4.967735290527344\n",
      "(64, 33)\n",
      "step 27323, loss is 4.887367248535156\n",
      "(64, 33)\n",
      "step 27324, loss is 4.573835372924805\n",
      "(64, 33)\n",
      "step 27325, loss is 4.843576908111572\n",
      "(64, 33)\n",
      "step 27326, loss is 4.926018714904785\n",
      "(64, 33)\n",
      "step 27327, loss is 4.689456939697266\n",
      "(64, 33)\n",
      "step 27328, loss is 4.750322341918945\n",
      "(64, 33)\n",
      "step 27329, loss is 4.852912425994873\n",
      "(64, 33)\n",
      "step 27330, loss is 4.858585834503174\n",
      "(64, 33)\n",
      "step 27331, loss is 4.886195182800293\n",
      "(64, 33)\n",
      "step 27332, loss is 4.996986389160156\n",
      "(64, 33)\n",
      "step 27333, loss is 4.925506114959717\n",
      "(64, 33)\n",
      "step 27334, loss is 4.833999156951904\n",
      "(64, 33)\n",
      "step 27335, loss is 4.78021240234375\n",
      "(64, 33)\n",
      "step 27336, loss is 4.790432929992676\n",
      "(64, 33)\n",
      "step 27337, loss is 4.777483940124512\n",
      "(64, 33)\n",
      "step 27338, loss is 4.7532172203063965\n",
      "(64, 33)\n",
      "step 27339, loss is 4.914680004119873\n",
      "(64, 33)\n",
      "step 27340, loss is 4.800922393798828\n",
      "(64, 33)\n",
      "step 27341, loss is 4.79446268081665\n",
      "(64, 33)\n",
      "step 27342, loss is 4.894805908203125\n",
      "(64, 33)\n",
      "step 27343, loss is 4.633511066436768\n",
      "(64, 33)\n",
      "step 27344, loss is 4.839694976806641\n",
      "(64, 33)\n",
      "step 27345, loss is 4.768728256225586\n",
      "(64, 33)\n",
      "step 27346, loss is 4.7570481300354\n",
      "(64, 33)\n",
      "step 27347, loss is 4.743649005889893\n",
      "(64, 33)\n",
      "step 27348, loss is 4.663124084472656\n",
      "(64, 33)\n",
      "step 27349, loss is 4.751863956451416\n",
      "(64, 33)\n",
      "step 27350, loss is 4.734076976776123\n",
      "(64, 33)\n",
      "step 27351, loss is 4.881484508514404\n",
      "(64, 33)\n",
      "step 27352, loss is 4.925713062286377\n",
      "(64, 33)\n",
      "step 27353, loss is 4.713598251342773\n",
      "(64, 33)\n",
      "step 27354, loss is 4.756832122802734\n",
      "(64, 33)\n",
      "step 27355, loss is 4.986563205718994\n",
      "(64, 33)\n",
      "step 27356, loss is 4.758058547973633\n",
      "(64, 33)\n",
      "step 27357, loss is 4.803631782531738\n",
      "(64, 33)\n",
      "step 27358, loss is 4.634086608886719\n",
      "(64, 33)\n",
      "step 27359, loss is 4.646204948425293\n",
      "(64, 33)\n",
      "step 27360, loss is 4.690722942352295\n",
      "(64, 33)\n",
      "step 27361, loss is 4.736426830291748\n",
      "(64, 33)\n",
      "step 27362, loss is 4.887490272521973\n",
      "(64, 33)\n",
      "step 27363, loss is 4.730179786682129\n",
      "(64, 33)\n",
      "step 27364, loss is 4.88138484954834\n",
      "(64, 33)\n",
      "step 27365, loss is 4.626680850982666\n",
      "(64, 33)\n",
      "step 27366, loss is 4.913516044616699\n",
      "(64, 33)\n",
      "step 27367, loss is 4.898727893829346\n",
      "(64, 33)\n",
      "step 27368, loss is 4.938355445861816\n",
      "(64, 33)\n",
      "step 27369, loss is 4.695545673370361\n",
      "(64, 33)\n",
      "step 27370, loss is 4.852680683135986\n",
      "(64, 33)\n",
      "step 27371, loss is 4.845779895782471\n",
      "(64, 33)\n",
      "step 27372, loss is 4.869840145111084\n",
      "(64, 33)\n",
      "step 27373, loss is 4.779677867889404\n",
      "(64, 33)\n",
      "step 27374, loss is 4.752180099487305\n",
      "(64, 33)\n",
      "step 27375, loss is 4.76889705657959\n",
      "(64, 33)\n",
      "step 27376, loss is 4.745460510253906\n",
      "(64, 33)\n",
      "step 27377, loss is 4.82706356048584\n",
      "(64, 33)\n",
      "step 27378, loss is 4.761802673339844\n",
      "(64, 33)\n",
      "step 27379, loss is 4.7725348472595215\n",
      "(64, 33)\n",
      "step 27380, loss is 4.808903694152832\n",
      "(64, 33)\n",
      "step 27381, loss is 4.745794296264648\n",
      "(64, 33)\n",
      "step 27382, loss is 4.728015422821045\n",
      "(64, 33)\n",
      "step 27383, loss is 4.612011909484863\n",
      "(64, 33)\n",
      "step 27384, loss is 4.817278861999512\n",
      "(64, 33)\n",
      "step 27385, loss is 4.788547515869141\n",
      "(64, 33)\n",
      "step 27386, loss is 4.768825054168701\n",
      "(64, 33)\n",
      "step 27387, loss is 4.615890979766846\n",
      "(64, 33)\n",
      "step 27388, loss is 4.642723083496094\n",
      "(64, 33)\n",
      "step 27389, loss is 4.730852127075195\n",
      "(64, 33)\n",
      "step 27390, loss is 4.852876663208008\n",
      "(64, 33)\n",
      "step 27391, loss is 4.831864833831787\n",
      "(64, 33)\n",
      "step 27392, loss is 4.58989953994751\n",
      "(64, 33)\n",
      "step 27393, loss is 4.766652584075928\n",
      "(64, 33)\n",
      "step 27394, loss is 4.928064823150635\n",
      "(64, 33)\n",
      "step 27395, loss is 4.672814846038818\n",
      "(64, 33)\n",
      "step 27396, loss is 5.105735778808594\n",
      "(64, 33)\n",
      "step 27397, loss is 4.704326152801514\n",
      "(64, 33)\n",
      "step 27398, loss is 4.630971431732178\n",
      "(64, 33)\n",
      "step 27399, loss is 4.82819128036499\n",
      "(64, 33)\n",
      "step 27400, loss is 4.607011318206787\n",
      "(64, 33)\n",
      "step 27401, loss is 5.078033447265625\n",
      "(64, 33)\n",
      "step 27402, loss is 4.751046657562256\n",
      "(64, 33)\n",
      "step 27403, loss is 4.790521621704102\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 27404, loss is 4.965091705322266\n",
      "(64, 33)\n",
      "step 27405, loss is 4.967474937438965\n",
      "(64, 33)\n",
      "step 27406, loss is 4.720440864562988\n",
      "(64, 33)\n",
      "step 27407, loss is 4.774477958679199\n",
      "(64, 33)\n",
      "step 27408, loss is 4.821325778961182\n",
      "(64, 33)\n",
      "step 27409, loss is 4.510859966278076\n",
      "(64, 33)\n",
      "step 27410, loss is 4.973840236663818\n",
      "(64, 33)\n",
      "step 27411, loss is 4.810814380645752\n",
      "(64, 33)\n",
      "step 27412, loss is 5.01041316986084\n",
      "(64, 33)\n",
      "step 27413, loss is 4.819057464599609\n",
      "(64, 33)\n",
      "step 27414, loss is 4.67745304107666\n",
      "(64, 33)\n",
      "step 27415, loss is 4.732570648193359\n",
      "(64, 33)\n",
      "step 27416, loss is 4.736570358276367\n",
      "(64, 33)\n",
      "step 27417, loss is 4.803591251373291\n",
      "(64, 33)\n",
      "step 27418, loss is 4.705080986022949\n",
      "(64, 33)\n",
      "step 27419, loss is 4.944955348968506\n",
      "(64, 33)\n",
      "step 27420, loss is 4.5533447265625\n",
      "(64, 33)\n",
      "step 27421, loss is 4.850325107574463\n",
      "(64, 33)\n",
      "step 27422, loss is 4.877105236053467\n",
      "(64, 33)\n",
      "step 27423, loss is 4.741612434387207\n",
      "(64, 33)\n",
      "step 27424, loss is 4.599862098693848\n",
      "(64, 33)\n",
      "step 27425, loss is 4.914607048034668\n",
      "(64, 33)\n",
      "step 27426, loss is 4.761134624481201\n",
      "(64, 33)\n",
      "step 27427, loss is 4.527345180511475\n",
      "(64, 33)\n",
      "step 27428, loss is 4.612839698791504\n",
      "(64, 33)\n",
      "step 27429, loss is 4.6493144035339355\n",
      "(64, 33)\n",
      "step 27430, loss is 4.746709823608398\n",
      "(64, 33)\n",
      "step 27431, loss is 4.775018692016602\n",
      "(64, 33)\n",
      "step 27432, loss is 4.697240829467773\n",
      "(64, 33)\n",
      "step 27433, loss is 4.859190940856934\n",
      "(64, 33)\n",
      "step 27434, loss is 4.774925708770752\n",
      "(64, 33)\n",
      "step 27435, loss is 4.857892990112305\n",
      "(64, 33)\n",
      "step 27436, loss is 4.777021408081055\n",
      "(64, 33)\n",
      "step 27437, loss is 4.909658432006836\n",
      "(64, 33)\n",
      "step 27438, loss is 4.856595993041992\n",
      "(64, 33)\n",
      "step 27439, loss is 4.703980922698975\n",
      "(64, 33)\n",
      "step 27440, loss is 4.678451061248779\n",
      "(64, 33)\n",
      "step 27441, loss is 4.615784645080566\n",
      "(64, 33)\n",
      "step 27442, loss is 4.789237976074219\n",
      "(64, 33)\n",
      "step 27443, loss is 4.672394275665283\n",
      "(64, 33)\n",
      "step 27444, loss is 4.699696063995361\n",
      "(64, 33)\n",
      "step 27445, loss is 4.713203430175781\n",
      "(64, 33)\n",
      "step 27446, loss is 5.032299041748047\n",
      "(64, 33)\n",
      "step 27447, loss is 4.785377502441406\n",
      "(64, 33)\n",
      "step 27448, loss is 4.845649242401123\n",
      "(64, 33)\n",
      "step 27449, loss is 4.765867710113525\n",
      "(64, 33)\n",
      "step 27450, loss is 4.602898120880127\n",
      "(64, 33)\n",
      "step 27451, loss is 4.832211494445801\n",
      "(64, 33)\n",
      "step 27452, loss is 4.83921480178833\n",
      "(64, 33)\n",
      "step 27453, loss is 4.588207721710205\n",
      "(64, 33)\n",
      "step 27454, loss is 4.818645000457764\n",
      "(64, 33)\n",
      "step 27455, loss is 4.791392803192139\n",
      "(64, 33)\n",
      "step 27456, loss is 4.80216646194458\n",
      "(64, 33)\n",
      "step 27457, loss is 4.755747318267822\n",
      "(64, 33)\n",
      "step 27458, loss is 4.832729339599609\n",
      "(64, 33)\n",
      "step 27459, loss is 4.759801864624023\n",
      "(64, 33)\n",
      "step 27460, loss is 4.771871089935303\n",
      "(64, 33)\n",
      "step 27461, loss is 5.040012836456299\n",
      "(64, 33)\n",
      "step 27462, loss is 4.734897613525391\n",
      "(64, 33)\n",
      "step 27463, loss is 4.807229518890381\n",
      "(64, 33)\n",
      "step 27464, loss is 4.7215576171875\n",
      "(64, 33)\n",
      "step 27465, loss is 4.607738018035889\n",
      "(64, 33)\n",
      "step 27466, loss is 4.817253112792969\n",
      "(64, 33)\n",
      "step 27467, loss is 4.770740985870361\n",
      "(64, 33)\n",
      "step 27468, loss is 4.656484603881836\n",
      "(64, 33)\n",
      "step 27469, loss is 4.827474117279053\n",
      "(64, 33)\n",
      "step 27470, loss is 4.589055061340332\n",
      "(64, 33)\n",
      "step 27471, loss is 4.825637340545654\n",
      "(64, 33)\n",
      "step 27472, loss is 4.612337589263916\n",
      "(64, 33)\n",
      "step 27473, loss is 4.930327415466309\n",
      "(64, 33)\n",
      "step 27474, loss is 4.759791374206543\n",
      "(64, 33)\n",
      "step 27475, loss is 4.849356174468994\n",
      "(64, 33)\n",
      "step 27476, loss is 4.633749008178711\n",
      "(64, 33)\n",
      "step 27477, loss is 4.8436760902404785\n",
      "(64, 33)\n",
      "step 27478, loss is 4.8815083503723145\n",
      "(64, 33)\n",
      "step 27479, loss is 4.917876720428467\n",
      "(64, 33)\n",
      "step 27480, loss is 4.798854827880859\n",
      "(64, 33)\n",
      "step 27481, loss is 4.833291053771973\n",
      "(64, 33)\n",
      "step 27482, loss is 4.703376293182373\n",
      "(64, 33)\n",
      "step 27483, loss is 4.79061222076416\n",
      "(64, 33)\n",
      "step 27484, loss is 4.8889241218566895\n",
      "(64, 33)\n",
      "step 27485, loss is 4.814492702484131\n",
      "(64, 33)\n",
      "step 27486, loss is 4.891239643096924\n",
      "(64, 33)\n",
      "step 27487, loss is 4.663573265075684\n",
      "(64, 33)\n",
      "step 27488, loss is 4.90462589263916\n",
      "(64, 33)\n",
      "step 27489, loss is 4.7686567306518555\n",
      "(64, 33)\n",
      "step 27490, loss is 4.67286491394043\n",
      "(64, 33)\n",
      "step 27491, loss is 4.7740912437438965\n",
      "(64, 33)\n",
      "step 27492, loss is 4.579697132110596\n",
      "(64, 33)\n",
      "step 27493, loss is 4.66227388381958\n",
      "(64, 33)\n",
      "step 27494, loss is 4.725254535675049\n",
      "(64, 33)\n",
      "step 27495, loss is 4.728143692016602\n",
      "(64, 33)\n",
      "step 27496, loss is 4.911466598510742\n",
      "(64, 33)\n",
      "step 27497, loss is 4.728643417358398\n",
      "(64, 33)\n",
      "step 27498, loss is 4.711686611175537\n",
      "(64, 33)\n",
      "step 27499, loss is 4.6523356437683105\n",
      "(64, 33)\n",
      "step 27500, loss is 4.713291168212891\n",
      "(64, 33)\n",
      "step 27501, loss is 4.901499271392822\n",
      "(64, 33)\n",
      "step 27502, loss is 4.6846842765808105\n",
      "(64, 33)\n",
      "step 27503, loss is 4.712158203125\n",
      "(64, 33)\n",
      "step 27504, loss is 4.678786754608154\n",
      "(64, 33)\n",
      "step 27505, loss is 4.591375350952148\n",
      "(64, 33)\n",
      "step 27506, loss is 4.8861236572265625\n",
      "(64, 33)\n",
      "step 27507, loss is 4.7855000495910645\n",
      "(64, 33)\n",
      "step 27508, loss is 5.055763244628906\n",
      "(64, 33)\n",
      "step 27509, loss is 4.742378234863281\n",
      "(64, 33)\n",
      "step 27510, loss is 4.890847682952881\n",
      "(64, 33)\n",
      "step 27511, loss is 4.71142578125\n",
      "(64, 33)\n",
      "step 27512, loss is 4.712170124053955\n",
      "(64, 33)\n",
      "step 27513, loss is 4.667237758636475\n",
      "(64, 33)\n",
      "step 27514, loss is 4.729809284210205\n",
      "(64, 33)\n",
      "step 27515, loss is 4.911256790161133\n",
      "(64, 33)\n",
      "step 27516, loss is 4.825416564941406\n",
      "(64, 33)\n",
      "step 27517, loss is 4.624661445617676\n",
      "(64, 33)\n",
      "step 27518, loss is 4.745296478271484\n",
      "(64, 33)\n",
      "step 27519, loss is 4.657217979431152\n",
      "(64, 33)\n",
      "step 27520, loss is 4.7900567054748535\n",
      "(64, 33)\n",
      "step 27521, loss is 4.829370498657227\n",
      "(64, 33)\n",
      "step 27522, loss is 4.769473075866699\n",
      "(64, 33)\n",
      "step 27523, loss is 4.782985687255859\n",
      "(64, 33)\n",
      "step 27524, loss is 4.870593547821045\n",
      "(64, 33)\n",
      "step 27525, loss is 4.717220783233643\n",
      "(64, 33)\n",
      "step 27526, loss is 4.86686372756958\n",
      "(64, 33)\n",
      "step 27527, loss is 4.697285175323486\n",
      "(64, 33)\n",
      "step 27528, loss is 4.752992630004883\n",
      "(64, 33)\n",
      "step 27529, loss is 4.982450485229492\n",
      "(64, 33)\n",
      "step 27530, loss is 4.651050090789795\n",
      "(64, 33)\n",
      "step 27531, loss is 4.767246246337891\n",
      "(64, 33)\n",
      "step 27532, loss is 4.5788493156433105\n",
      "(64, 33)\n",
      "step 27533, loss is 4.76291561126709\n",
      "(64, 33)\n",
      "step 27534, loss is 4.988805770874023\n",
      "(64, 33)\n",
      "step 27535, loss is 4.902575969696045\n",
      "(64, 33)\n",
      "step 27536, loss is 4.608573913574219\n",
      "(64, 33)\n",
      "step 27537, loss is 4.682642936706543\n",
      "(64, 33)\n",
      "step 27538, loss is 4.8443603515625\n",
      "(64, 33)\n",
      "step 27539, loss is 4.823997497558594\n",
      "(64, 33)\n",
      "step 27540, loss is 4.468839168548584\n",
      "(64, 33)\n",
      "step 27541, loss is 4.825303077697754\n",
      "(64, 33)\n",
      "step 27542, loss is 4.727879524230957\n",
      "(64, 33)\n",
      "step 27543, loss is 4.627532958984375\n",
      "(64, 33)\n",
      "step 27544, loss is 4.713544845581055\n",
      "(64, 33)\n",
      "step 27545, loss is 4.708942890167236\n",
      "(64, 33)\n",
      "step 27546, loss is 4.828435897827148\n",
      "(64, 33)\n",
      "step 27547, loss is 4.946078777313232\n",
      "(64, 33)\n",
      "step 27548, loss is 4.735945701599121\n",
      "(64, 33)\n",
      "step 27549, loss is 4.631828308105469\n",
      "(64, 33)\n",
      "step 27550, loss is 4.762411594390869\n",
      "(64, 33)\n",
      "step 27551, loss is 4.79421329498291\n",
      "(64, 33)\n",
      "step 27552, loss is 4.892112731933594\n",
      "(64, 33)\n",
      "step 27553, loss is 4.724145412445068\n",
      "(64, 33)\n",
      "step 27554, loss is 4.7422051429748535\n",
      "(64, 33)\n",
      "step 27555, loss is 4.693819046020508\n",
      "(64, 33)\n",
      "step 27556, loss is 4.871725559234619\n",
      "(64, 33)\n",
      "step 27557, loss is 4.686661720275879\n",
      "(64, 33)\n",
      "step 27558, loss is 4.758482456207275\n",
      "(64, 33)\n",
      "step 27559, loss is 4.84645938873291\n",
      "(64, 33)\n",
      "step 27560, loss is 4.7453932762146\n",
      "(64, 33)\n",
      "step 27561, loss is 4.7528157234191895\n",
      "(64, 33)\n",
      "step 27562, loss is 4.791791915893555\n",
      "(64, 33)\n",
      "step 27563, loss is 4.823417663574219\n",
      "(64, 33)\n",
      "step 27564, loss is 4.652624607086182\n",
      "(64, 33)\n",
      "step 27565, loss is 4.85928201675415\n",
      "(64, 33)\n",
      "step 27566, loss is 4.779704570770264\n",
      "(64, 33)\n",
      "step 27567, loss is 4.966410160064697\n",
      "(64, 33)\n",
      "step 27568, loss is 4.668948173522949\n",
      "(64, 33)\n",
      "step 27569, loss is 4.935316562652588\n",
      "(64, 33)\n",
      "step 27570, loss is 4.7096452713012695\n",
      "(64, 33)\n",
      "step 27571, loss is 4.890676021575928\n",
      "(64, 33)\n",
      "step 27572, loss is 4.712212562561035\n",
      "(64, 33)\n",
      "step 27573, loss is 4.735405921936035\n",
      "(64, 33)\n",
      "step 27574, loss is 4.663963794708252\n",
      "(64, 33)\n",
      "step 27575, loss is 4.730551242828369\n",
      "(64, 33)\n",
      "step 27576, loss is 4.619624614715576\n",
      "(64, 33)\n",
      "step 27577, loss is 4.605345249176025\n",
      "(64, 33)\n",
      "step 27578, loss is 4.949487686157227\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 27579, loss is 4.776819229125977\n",
      "(64, 33)\n",
      "step 27580, loss is 4.7183966636657715\n",
      "(64, 33)\n",
      "step 27581, loss is 4.923695087432861\n",
      "(64, 33)\n",
      "step 27582, loss is 4.76644229888916\n",
      "(64, 33)\n",
      "step 27583, loss is 4.7525200843811035\n",
      "(64, 33)\n",
      "step 27584, loss is 4.651154041290283\n",
      "(64, 33)\n",
      "step 27585, loss is 4.719342231750488\n",
      "(64, 33)\n",
      "step 27586, loss is 4.703208923339844\n",
      "(64, 33)\n",
      "step 27587, loss is 4.803661823272705\n",
      "(64, 33)\n",
      "step 27588, loss is 4.772482872009277\n",
      "(64, 33)\n",
      "step 27589, loss is 4.891323089599609\n",
      "(64, 33)\n",
      "step 27590, loss is 4.629642009735107\n",
      "(64, 33)\n",
      "step 27591, loss is 4.694031715393066\n",
      "(64, 33)\n",
      "step 27592, loss is 4.7705864906311035\n",
      "(64, 33)\n",
      "step 27593, loss is 4.639326572418213\n",
      "(64, 33)\n",
      "step 27594, loss is 4.875675678253174\n",
      "(64, 33)\n",
      "step 27595, loss is 4.80520486831665\n",
      "(64, 33)\n",
      "step 27596, loss is 4.740007400512695\n",
      "(64, 33)\n",
      "step 27597, loss is 4.867611885070801\n",
      "(64, 33)\n",
      "step 27598, loss is 4.718408584594727\n",
      "(64, 33)\n",
      "step 27599, loss is 4.726428985595703\n",
      "(64, 33)\n",
      "step 27600, loss is 4.671034336090088\n",
      "(64, 33)\n",
      "step 27601, loss is 4.7324042320251465\n",
      "(64, 33)\n",
      "step 27602, loss is 4.791172027587891\n",
      "(64, 33)\n",
      "step 27603, loss is 4.6163201332092285\n",
      "(64, 33)\n",
      "step 27604, loss is 4.7552876472473145\n",
      "(64, 33)\n",
      "step 27605, loss is 4.716798305511475\n",
      "(64, 33)\n",
      "step 27606, loss is 4.712440490722656\n",
      "(64, 33)\n",
      "step 27607, loss is 4.750610828399658\n",
      "(64, 33)\n",
      "step 27608, loss is 4.766589641571045\n",
      "(64, 33)\n",
      "step 27609, loss is 4.794079780578613\n",
      "(64, 33)\n",
      "step 27610, loss is 4.772647857666016\n",
      "(64, 33)\n",
      "step 27611, loss is 4.712875843048096\n",
      "(64, 33)\n",
      "step 27612, loss is 4.85570764541626\n",
      "(64, 33)\n",
      "step 27613, loss is 4.920858860015869\n",
      "(64, 33)\n",
      "step 27614, loss is 4.652293682098389\n",
      "(64, 33)\n",
      "step 27615, loss is 4.84699010848999\n",
      "(64, 33)\n",
      "step 27616, loss is 4.527272701263428\n",
      "(64, 33)\n",
      "step 27617, loss is 4.95418119430542\n",
      "(64, 33)\n",
      "step 27618, loss is 4.7133684158325195\n",
      "(64, 33)\n",
      "step 27619, loss is 4.6668314933776855\n",
      "(64, 33)\n",
      "step 27620, loss is 4.793735027313232\n",
      "(64, 33)\n",
      "step 27621, loss is 4.760191917419434\n",
      "(64, 33)\n",
      "step 27622, loss is 4.752076625823975\n",
      "(64, 33)\n",
      "step 27623, loss is 4.944365501403809\n",
      "(64, 33)\n",
      "step 27624, loss is 4.724126815795898\n",
      "(64, 33)\n",
      "step 27625, loss is 4.813055515289307\n",
      "(64, 33)\n",
      "step 27626, loss is 4.854618549346924\n",
      "(64, 33)\n",
      "step 27627, loss is 4.687187194824219\n",
      "(64, 33)\n",
      "step 27628, loss is 4.735829830169678\n",
      "(64, 33)\n",
      "step 27629, loss is 4.722347736358643\n",
      "(64, 33)\n",
      "step 27630, loss is 4.8695149421691895\n",
      "(64, 33)\n",
      "step 27631, loss is 4.6976799964904785\n",
      "(64, 33)\n",
      "step 27632, loss is 4.495537757873535\n",
      "(64, 33)\n",
      "step 27633, loss is 4.798689365386963\n",
      "(64, 33)\n",
      "step 27634, loss is 4.767867565155029\n",
      "(64, 33)\n",
      "step 27635, loss is 4.947171688079834\n",
      "(64, 33)\n",
      "step 27636, loss is 4.863128662109375\n",
      "(64, 33)\n",
      "step 27637, loss is 4.795801162719727\n",
      "(64, 33)\n",
      "step 27638, loss is 4.747658729553223\n",
      "(64, 33)\n",
      "step 27639, loss is 4.755240440368652\n",
      "(64, 33)\n",
      "step 27640, loss is 4.68362283706665\n",
      "(64, 33)\n",
      "step 27641, loss is 4.768863677978516\n",
      "(64, 33)\n",
      "step 27642, loss is 4.74285364151001\n",
      "(64, 33)\n",
      "step 27643, loss is 4.885585784912109\n",
      "(64, 33)\n",
      "step 27644, loss is 4.7578911781311035\n",
      "(64, 33)\n",
      "step 27645, loss is 4.756675720214844\n",
      "(64, 33)\n",
      "step 27646, loss is 4.6601386070251465\n",
      "(64, 33)\n",
      "step 27647, loss is 4.841191291809082\n",
      "(64, 33)\n",
      "step 27648, loss is 4.610532760620117\n",
      "(64, 33)\n",
      "step 27649, loss is 4.867295742034912\n",
      "(64, 33)\n",
      "step 27650, loss is 4.750924587249756\n",
      "(64, 33)\n",
      "step 27651, loss is 4.699068546295166\n",
      "(64, 33)\n",
      "step 27652, loss is 5.005905628204346\n",
      "(64, 33)\n",
      "step 27653, loss is 4.750229835510254\n",
      "(64, 33)\n",
      "step 27654, loss is 4.9580230712890625\n",
      "(64, 33)\n",
      "step 27655, loss is 4.831418037414551\n",
      "(64, 33)\n",
      "step 27656, loss is 4.926079750061035\n",
      "(64, 33)\n",
      "step 27657, loss is 4.62414026260376\n",
      "(64, 33)\n",
      "step 27658, loss is 4.929636001586914\n",
      "(64, 33)\n",
      "step 27659, loss is 4.628086090087891\n",
      "(64, 33)\n",
      "step 27660, loss is 4.695747375488281\n",
      "(64, 33)\n",
      "step 27661, loss is 4.728918075561523\n",
      "(64, 33)\n",
      "step 27662, loss is 4.6888837814331055\n",
      "(64, 33)\n",
      "step 27663, loss is 5.050041198730469\n",
      "(64, 33)\n",
      "step 27664, loss is 4.9697747230529785\n",
      "(64, 33)\n",
      "step 27665, loss is 4.903447151184082\n",
      "(64, 33)\n",
      "step 27666, loss is 4.759095191955566\n",
      "(64, 33)\n",
      "step 27667, loss is 4.8375139236450195\n",
      "(64, 33)\n",
      "step 27668, loss is 4.836803913116455\n",
      "(64, 33)\n",
      "step 27669, loss is 4.612231731414795\n",
      "(64, 33)\n",
      "step 27670, loss is 4.611511707305908\n",
      "(64, 33)\n",
      "step 27671, loss is 4.460506916046143\n",
      "(64, 33)\n",
      "step 27672, loss is 4.885778427124023\n",
      "(64, 33)\n",
      "step 27673, loss is 4.781866550445557\n",
      "(64, 33)\n",
      "step 27674, loss is 4.765117168426514\n",
      "(64, 33)\n",
      "step 27675, loss is 4.8832502365112305\n",
      "(64, 33)\n",
      "step 27676, loss is 4.8528361320495605\n",
      "(64, 33)\n",
      "step 27677, loss is 4.702574729919434\n",
      "(64, 33)\n",
      "step 27678, loss is 4.616627216339111\n",
      "(64, 33)\n",
      "step 27679, loss is 4.718123912811279\n",
      "(64, 33)\n",
      "step 27680, loss is 4.638167858123779\n",
      "(64, 33)\n",
      "step 27681, loss is 4.87546968460083\n",
      "(64, 33)\n",
      "step 27682, loss is 4.888105869293213\n",
      "(64, 33)\n",
      "step 27683, loss is 4.6398444175720215\n",
      "(64, 33)\n",
      "step 27684, loss is 4.768296718597412\n",
      "(64, 33)\n",
      "step 27685, loss is 4.758407115936279\n",
      "(64, 33)\n",
      "step 27686, loss is 4.730045795440674\n",
      "(64, 33)\n",
      "step 27687, loss is 4.7839131355285645\n",
      "(64, 33)\n",
      "step 27688, loss is 4.929169654846191\n",
      "(64, 33)\n",
      "step 27689, loss is 4.761931896209717\n",
      "(64, 33)\n",
      "step 27690, loss is 4.807529926300049\n",
      "(64, 33)\n",
      "step 27691, loss is 4.777585983276367\n",
      "(64, 33)\n",
      "step 27692, loss is 4.687911510467529\n",
      "(64, 33)\n",
      "step 27693, loss is 4.877562522888184\n",
      "(64, 33)\n",
      "step 27694, loss is 4.85753870010376\n",
      "(64, 33)\n",
      "step 27695, loss is 4.695437908172607\n",
      "(64, 33)\n",
      "step 27696, loss is 4.712353706359863\n",
      "(64, 33)\n",
      "step 27697, loss is 4.872128486633301\n",
      "(64, 33)\n",
      "step 27698, loss is 4.689423084259033\n",
      "(64, 33)\n",
      "step 27699, loss is 4.8443284034729\n",
      "(64, 33)\n",
      "step 27700, loss is 4.789919376373291\n",
      "(64, 33)\n",
      "step 27701, loss is 4.742912292480469\n",
      "(64, 33)\n",
      "step 27702, loss is 4.75172758102417\n",
      "(64, 33)\n",
      "step 27703, loss is 4.667924880981445\n",
      "(64, 33)\n",
      "step 27704, loss is 4.686479091644287\n",
      "(64, 33)\n",
      "step 27705, loss is 4.945616722106934\n",
      "(64, 33)\n",
      "step 27706, loss is 4.835420608520508\n",
      "(64, 33)\n",
      "step 27707, loss is 4.810851097106934\n",
      "(64, 33)\n",
      "step 27708, loss is 4.924116611480713\n",
      "(64, 33)\n",
      "step 27709, loss is 4.904422760009766\n",
      "(64, 33)\n",
      "step 27710, loss is 4.778388977050781\n",
      "(64, 33)\n",
      "step 27711, loss is 4.59084415435791\n",
      "(64, 33)\n",
      "step 27712, loss is 4.618653774261475\n",
      "(64, 33)\n",
      "step 27713, loss is 4.796671390533447\n",
      "(64, 33)\n",
      "step 27714, loss is 4.828315734863281\n",
      "(64, 33)\n",
      "step 27715, loss is 4.8256306648254395\n",
      "(64, 33)\n",
      "step 27716, loss is 4.51507043838501\n",
      "(64, 33)\n",
      "step 27717, loss is 4.931483745574951\n",
      "(64, 33)\n",
      "step 27718, loss is 4.823103427886963\n",
      "(64, 33)\n",
      "step 27719, loss is 4.763740062713623\n",
      "(64, 33)\n",
      "step 27720, loss is 4.952941417694092\n",
      "(64, 33)\n",
      "step 27721, loss is 4.741929531097412\n",
      "(64, 33)\n",
      "step 27722, loss is 4.964014530181885\n",
      "(64, 33)\n",
      "step 27723, loss is 4.823757171630859\n",
      "(64, 33)\n",
      "step 27724, loss is 4.775582790374756\n",
      "(64, 33)\n",
      "step 27725, loss is 4.7820963859558105\n",
      "(64, 33)\n",
      "step 27726, loss is 4.787013053894043\n",
      "(64, 33)\n",
      "step 27727, loss is 4.843247413635254\n",
      "(64, 33)\n",
      "step 27728, loss is 4.813749313354492\n",
      "(64, 33)\n",
      "step 27729, loss is 5.011585235595703\n",
      "(64, 33)\n",
      "step 27730, loss is 4.788559436798096\n",
      "(64, 33)\n",
      "step 27731, loss is 4.698482990264893\n",
      "(64, 33)\n",
      "step 27732, loss is 4.795655727386475\n",
      "(64, 33)\n",
      "step 27733, loss is 4.833141803741455\n",
      "(64, 33)\n",
      "step 27734, loss is 4.775520324707031\n",
      "(64, 33)\n",
      "step 27735, loss is 4.808053493499756\n",
      "(64, 33)\n",
      "step 27736, loss is 4.9338555335998535\n",
      "(64, 33)\n",
      "step 27737, loss is 4.7273101806640625\n",
      "(64, 33)\n",
      "step 27738, loss is 4.674433708190918\n",
      "(64, 33)\n",
      "step 27739, loss is 4.808932781219482\n",
      "(64, 33)\n",
      "step 27740, loss is 4.663938522338867\n",
      "(64, 33)\n",
      "step 27741, loss is 4.9103193283081055\n",
      "(64, 33)\n",
      "step 27742, loss is 4.759214401245117\n",
      "(64, 33)\n",
      "step 27743, loss is 4.731200218200684\n",
      "(64, 33)\n",
      "step 27744, loss is 4.723696708679199\n",
      "(64, 33)\n",
      "step 27745, loss is 4.65718412399292\n",
      "(64, 33)\n",
      "step 27746, loss is 4.8723015785217285\n",
      "(64, 33)\n",
      "step 27747, loss is 4.834162712097168\n",
      "(64, 33)\n",
      "step 27748, loss is 4.905960559844971\n",
      "(64, 33)\n",
      "step 27749, loss is 4.597160339355469\n",
      "(64, 33)\n",
      "step 27750, loss is 4.826685905456543\n",
      "(64, 33)\n",
      "step 27751, loss is 4.802871227264404\n",
      "(64, 33)\n",
      "step 27752, loss is 4.607522964477539\n",
      "(64, 33)\n",
      "step 27753, loss is 4.599185466766357\n",
      "(64, 33)\n",
      "step 27754, loss is 4.722385406494141\n",
      "(64, 33)\n",
      "step 27755, loss is 4.7102484703063965\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 27756, loss is 4.6879143714904785\n",
      "(64, 33)\n",
      "step 27757, loss is 4.761900901794434\n",
      "(64, 33)\n",
      "step 27758, loss is 4.725013256072998\n",
      "(64, 33)\n",
      "step 27759, loss is 4.70786714553833\n",
      "(64, 33)\n",
      "step 27760, loss is 4.82061767578125\n",
      "(64, 33)\n",
      "step 27761, loss is 4.825002670288086\n",
      "(64, 33)\n",
      "step 27762, loss is 5.033350467681885\n",
      "(64, 33)\n",
      "step 27763, loss is 4.860750198364258\n",
      "(64, 33)\n",
      "step 27764, loss is 4.869176864624023\n",
      "(64, 33)\n",
      "step 27765, loss is 4.6976752281188965\n",
      "(64, 33)\n",
      "step 27766, loss is 4.736166477203369\n",
      "(64, 33)\n",
      "step 27767, loss is 4.823367118835449\n",
      "(64, 33)\n",
      "step 27768, loss is 4.902949333190918\n",
      "(64, 33)\n",
      "step 27769, loss is 4.900546550750732\n",
      "(64, 33)\n",
      "step 27770, loss is 4.866745471954346\n",
      "(64, 33)\n",
      "step 27771, loss is 4.850159168243408\n",
      "(64, 33)\n",
      "step 27772, loss is 4.79958438873291\n",
      "(64, 33)\n",
      "step 27773, loss is 4.752784252166748\n",
      "(64, 33)\n",
      "step 27774, loss is 4.854124069213867\n",
      "(64, 33)\n",
      "step 27775, loss is 4.726295471191406\n",
      "(64, 33)\n",
      "step 27776, loss is 4.770942687988281\n",
      "(64, 33)\n",
      "step 27777, loss is 4.686008453369141\n",
      "(64, 33)\n",
      "step 27778, loss is 4.925386428833008\n",
      "(64, 33)\n",
      "step 27779, loss is 4.757696151733398\n",
      "(64, 33)\n",
      "step 27780, loss is 4.857118129730225\n",
      "(64, 33)\n",
      "step 27781, loss is 4.69370174407959\n",
      "(64, 33)\n",
      "step 27782, loss is 4.656904697418213\n",
      "(64, 33)\n",
      "step 27783, loss is 4.797155380249023\n",
      "(64, 33)\n",
      "step 27784, loss is 4.673209190368652\n",
      "(64, 33)\n",
      "step 27785, loss is 4.859764099121094\n",
      "(64, 33)\n",
      "step 27786, loss is 4.691793441772461\n",
      "(64, 33)\n",
      "step 27787, loss is 4.7342095375061035\n",
      "(64, 33)\n",
      "step 27788, loss is 4.800622940063477\n",
      "(64, 33)\n",
      "step 27789, loss is 4.784093379974365\n",
      "(64, 33)\n",
      "step 27790, loss is 4.664350986480713\n",
      "(64, 33)\n",
      "step 27791, loss is 4.922431945800781\n",
      "(64, 33)\n",
      "step 27792, loss is 4.767955780029297\n",
      "(64, 33)\n",
      "step 27793, loss is 4.911613464355469\n",
      "(64, 33)\n",
      "step 27794, loss is 4.661676406860352\n",
      "(64, 33)\n",
      "step 27795, loss is 4.833676815032959\n",
      "(64, 33)\n",
      "step 27796, loss is 4.781778335571289\n",
      "(64, 33)\n",
      "step 27797, loss is 4.726527214050293\n",
      "(64, 33)\n",
      "step 27798, loss is 5.084432601928711\n",
      "(64, 33)\n",
      "step 27799, loss is 4.468922138214111\n",
      "(64, 33)\n",
      "step 27800, loss is 4.744078636169434\n",
      "(64, 33)\n",
      "step 27801, loss is 4.661011219024658\n",
      "(64, 33)\n",
      "step 27802, loss is 4.869706630706787\n",
      "(64, 33)\n",
      "step 27803, loss is 4.684114456176758\n",
      "(64, 33)\n",
      "step 27804, loss is 4.835438251495361\n",
      "(64, 33)\n",
      "step 27805, loss is 4.665469646453857\n",
      "(64, 33)\n",
      "step 27806, loss is 4.763396739959717\n",
      "(64, 33)\n",
      "step 27807, loss is 4.728006839752197\n",
      "(64, 33)\n",
      "step 27808, loss is 4.734108924865723\n",
      "(64, 33)\n",
      "step 27809, loss is 4.664691925048828\n",
      "(64, 33)\n",
      "step 27810, loss is 4.6709136962890625\n",
      "(64, 33)\n",
      "step 27811, loss is 4.930850028991699\n",
      "(64, 33)\n",
      "step 27812, loss is 4.839550971984863\n",
      "(64, 33)\n",
      "step 27813, loss is 4.922013759613037\n",
      "(64, 33)\n",
      "step 27814, loss is 4.7159423828125\n",
      "(64, 33)\n",
      "step 27815, loss is 4.850172519683838\n",
      "(64, 33)\n",
      "step 27816, loss is 4.900566577911377\n",
      "(64, 33)\n",
      "step 27817, loss is 4.833710193634033\n",
      "(64, 33)\n",
      "step 27818, loss is 4.896702766418457\n",
      "(64, 33)\n",
      "step 27819, loss is 4.750208377838135\n",
      "(64, 33)\n",
      "step 27820, loss is 4.991369247436523\n",
      "(64, 33)\n",
      "step 27821, loss is 4.763049602508545\n",
      "(64, 33)\n",
      "step 27822, loss is 4.911250591278076\n",
      "(64, 33)\n",
      "step 27823, loss is 4.851371765136719\n",
      "(64, 33)\n",
      "step 27824, loss is 4.7168731689453125\n",
      "(64, 33)\n",
      "step 27825, loss is 4.990697860717773\n",
      "(64, 33)\n",
      "step 27826, loss is 4.79766845703125\n",
      "(64, 33)\n",
      "step 27827, loss is 4.87521505355835\n",
      "(64, 33)\n",
      "step 27828, loss is 4.756265640258789\n",
      "(64, 33)\n",
      "step 27829, loss is 4.794145107269287\n",
      "(64, 33)\n",
      "step 27830, loss is 4.847071647644043\n",
      "(64, 33)\n",
      "step 27831, loss is 4.826772689819336\n",
      "(64, 33)\n",
      "step 27832, loss is 4.8303117752075195\n",
      "(64, 33)\n",
      "step 27833, loss is 4.759524345397949\n",
      "(64, 33)\n",
      "step 27834, loss is 4.68117618560791\n",
      "(64, 33)\n",
      "step 27835, loss is 4.60581636428833\n",
      "(64, 33)\n",
      "step 27836, loss is 4.782622814178467\n",
      "(64, 33)\n",
      "step 27837, loss is 5.039379119873047\n",
      "(64, 33)\n",
      "step 27838, loss is 4.711336135864258\n",
      "(64, 33)\n",
      "step 27839, loss is 4.741079330444336\n",
      "(64, 33)\n",
      "step 27840, loss is 4.812088966369629\n",
      "(64, 33)\n",
      "step 27841, loss is 4.670814037322998\n",
      "(64, 33)\n",
      "step 27842, loss is 4.708736419677734\n",
      "(64, 33)\n",
      "step 27843, loss is 4.892429351806641\n",
      "(64, 33)\n",
      "step 27844, loss is 4.863989353179932\n",
      "(64, 33)\n",
      "step 27845, loss is 4.676274299621582\n",
      "(64, 33)\n",
      "step 27846, loss is 4.62957763671875\n",
      "(64, 33)\n",
      "step 27847, loss is 4.732265949249268\n",
      "(64, 33)\n",
      "step 27848, loss is 4.8995490074157715\n",
      "(64, 33)\n",
      "step 27849, loss is 4.691679954528809\n",
      "(64, 33)\n",
      "step 27850, loss is 4.822481155395508\n",
      "(64, 33)\n",
      "step 27851, loss is 4.809088230133057\n",
      "(64, 33)\n",
      "step 27852, loss is 4.693628787994385\n",
      "(64, 33)\n",
      "step 27853, loss is 4.709053993225098\n",
      "(64, 33)\n",
      "step 27854, loss is 4.8824920654296875\n",
      "(64, 33)\n",
      "step 27855, loss is 4.721377849578857\n",
      "(64, 33)\n",
      "step 27856, loss is 4.692509174346924\n",
      "(64, 33)\n",
      "step 27857, loss is 4.851585388183594\n",
      "(64, 33)\n",
      "step 27858, loss is 4.86920166015625\n",
      "(64, 33)\n",
      "step 27859, loss is 4.73429012298584\n",
      "(64, 33)\n",
      "step 27860, loss is 4.836220741271973\n",
      "(64, 33)\n",
      "step 27861, loss is 4.7438554763793945\n",
      "(64, 33)\n",
      "step 27862, loss is 4.804682731628418\n",
      "(64, 33)\n",
      "step 27863, loss is 4.710087776184082\n",
      "(64, 33)\n",
      "step 27864, loss is 5.008748531341553\n",
      "(64, 33)\n",
      "step 27865, loss is 4.745911598205566\n",
      "(64, 33)\n",
      "step 27866, loss is 4.779415130615234\n",
      "(64, 33)\n",
      "step 27867, loss is 4.625120639801025\n",
      "(64, 33)\n",
      "step 27868, loss is 4.936131954193115\n",
      "(64, 33)\n",
      "step 27869, loss is 4.850040435791016\n",
      "(64, 33)\n",
      "step 27870, loss is 4.821125507354736\n",
      "(64, 33)\n",
      "step 27871, loss is 4.689587116241455\n",
      "(64, 33)\n",
      "step 27872, loss is 4.83731746673584\n",
      "(64, 33)\n",
      "step 27873, loss is 4.945858001708984\n",
      "(64, 33)\n",
      "step 27874, loss is 4.844306468963623\n",
      "(64, 33)\n",
      "step 27875, loss is 4.816863059997559\n",
      "(64, 33)\n",
      "step 27876, loss is 4.781562805175781\n",
      "(64, 33)\n",
      "step 27877, loss is 4.830376625061035\n",
      "(64, 33)\n",
      "step 27878, loss is 4.80441427230835\n",
      "(64, 33)\n",
      "step 27879, loss is 4.773404121398926\n",
      "(64, 33)\n",
      "step 27880, loss is 4.657426834106445\n",
      "(64, 33)\n",
      "step 27881, loss is 4.645915508270264\n",
      "(64, 33)\n",
      "step 27882, loss is 4.7285566329956055\n",
      "(64, 33)\n",
      "step 27883, loss is 4.80286169052124\n",
      "(64, 33)\n",
      "step 27884, loss is 4.987735748291016\n",
      "(64, 33)\n",
      "step 27885, loss is 4.825646877288818\n",
      "(64, 33)\n",
      "step 27886, loss is 4.977208614349365\n",
      "(64, 33)\n",
      "step 27887, loss is 4.7840142250061035\n",
      "(64, 33)\n",
      "step 27888, loss is 4.869215488433838\n",
      "(64, 33)\n",
      "step 27889, loss is 4.702464580535889\n",
      "(64, 33)\n",
      "step 27890, loss is 4.8077569007873535\n",
      "(64, 33)\n",
      "step 27891, loss is 4.684567451477051\n",
      "(64, 33)\n",
      "step 27892, loss is 4.628690719604492\n",
      "(64, 33)\n",
      "step 27893, loss is 4.746880531311035\n",
      "(64, 33)\n",
      "step 27894, loss is 4.774594306945801\n",
      "(64, 33)\n",
      "step 27895, loss is 4.680778980255127\n",
      "(64, 33)\n",
      "step 27896, loss is 4.727762222290039\n",
      "(64, 33)\n",
      "step 27897, loss is 4.7504496574401855\n",
      "(64, 33)\n",
      "step 27898, loss is 4.716168403625488\n",
      "(64, 33)\n",
      "step 27899, loss is 4.879855155944824\n",
      "(64, 33)\n",
      "step 27900, loss is 4.80900764465332\n",
      "(64, 33)\n",
      "step 27901, loss is 4.845264911651611\n",
      "(64, 33)\n",
      "step 27902, loss is 4.787532806396484\n",
      "(64, 33)\n",
      "step 27903, loss is 4.757843971252441\n",
      "(64, 33)\n",
      "step 27904, loss is 4.754454135894775\n",
      "(64, 33)\n",
      "step 27905, loss is 4.950888156890869\n",
      "(64, 33)\n",
      "step 27906, loss is 4.922547340393066\n",
      "(64, 33)\n",
      "step 27907, loss is 4.690840244293213\n",
      "(64, 33)\n",
      "step 27908, loss is 4.894211292266846\n",
      "(64, 33)\n",
      "step 27909, loss is 4.750225067138672\n",
      "(64, 33)\n",
      "step 27910, loss is 4.708096981048584\n",
      "(64, 33)\n",
      "step 27911, loss is 4.694256782531738\n",
      "(64, 33)\n",
      "step 27912, loss is 4.647749900817871\n",
      "(64, 33)\n",
      "step 27913, loss is 4.832319736480713\n",
      "(64, 33)\n",
      "step 27914, loss is 4.897338390350342\n",
      "(64, 33)\n",
      "step 27915, loss is 4.566154479980469\n",
      "(64, 33)\n",
      "step 27916, loss is 4.672335624694824\n",
      "(64, 33)\n",
      "step 27917, loss is 4.88251256942749\n",
      "(64, 33)\n",
      "step 27918, loss is 4.798226833343506\n",
      "(64, 33)\n",
      "step 27919, loss is 4.90533447265625\n",
      "(64, 33)\n",
      "step 27920, loss is 4.832025527954102\n",
      "(64, 33)\n",
      "step 27921, loss is 4.70029878616333\n",
      "(64, 33)\n",
      "step 27922, loss is 4.993439674377441\n",
      "(64, 33)\n",
      "step 27923, loss is 4.857395648956299\n",
      "(64, 33)\n",
      "step 27924, loss is 4.7420501708984375\n",
      "(64, 33)\n",
      "step 27925, loss is 4.652092456817627\n",
      "(64, 33)\n",
      "step 27926, loss is 4.774464130401611\n",
      "(64, 33)\n",
      "step 27927, loss is 4.64199686050415\n",
      "(64, 33)\n",
      "step 27928, loss is 4.870055198669434\n",
      "(64, 33)\n",
      "step 27929, loss is 4.615416049957275\n",
      "(64, 33)\n",
      "step 27930, loss is 4.791186332702637\n",
      "(64, 33)\n",
      "step 27931, loss is 4.8254313468933105\n",
      "(64, 33)\n",
      "step 27932, loss is 4.823878288269043\n",
      "(64, 33)\n",
      "step 27933, loss is 4.756587028503418\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 27934, loss is 4.707405090332031\n",
      "(64, 33)\n",
      "step 27935, loss is 4.758115768432617\n",
      "(64, 33)\n",
      "step 27936, loss is 4.809312343597412\n",
      "(64, 33)\n",
      "step 27937, loss is 4.791700839996338\n",
      "(64, 33)\n",
      "step 27938, loss is 4.731167316436768\n",
      "(64, 33)\n",
      "step 27939, loss is 4.8985595703125\n",
      "(64, 33)\n",
      "step 27940, loss is 4.705029010772705\n",
      "(64, 33)\n",
      "step 27941, loss is 4.823897361755371\n",
      "(64, 33)\n",
      "step 27942, loss is 4.7536845207214355\n",
      "(64, 33)\n",
      "step 27943, loss is 4.8523783683776855\n",
      "(64, 33)\n",
      "step 27944, loss is 4.790782451629639\n",
      "(64, 33)\n",
      "step 27945, loss is 4.792027473449707\n",
      "(64, 33)\n",
      "step 27946, loss is 4.929060459136963\n",
      "(64, 33)\n",
      "step 27947, loss is 4.73966121673584\n",
      "(64, 33)\n",
      "step 27948, loss is 4.793344974517822\n",
      "(64, 33)\n",
      "step 27949, loss is 4.6475138664245605\n",
      "(64, 33)\n",
      "step 27950, loss is 4.826040267944336\n",
      "(64, 33)\n",
      "step 27951, loss is 4.648116111755371\n",
      "(64, 33)\n",
      "step 27952, loss is 4.6086859703063965\n",
      "(64, 33)\n",
      "step 27953, loss is 4.6985015869140625\n",
      "(64, 33)\n",
      "step 27954, loss is 4.813718795776367\n",
      "(64, 33)\n",
      "step 27955, loss is 4.733050346374512\n",
      "(64, 33)\n",
      "step 27956, loss is 4.853463649749756\n",
      "(64, 33)\n",
      "step 27957, loss is 4.876908779144287\n",
      "(64, 33)\n",
      "step 27958, loss is 4.750606536865234\n",
      "(64, 33)\n",
      "step 27959, loss is 4.692168235778809\n",
      "(64, 33)\n",
      "step 27960, loss is 4.840831279754639\n",
      "(64, 33)\n",
      "step 27961, loss is 4.7696123123168945\n",
      "(64, 33)\n",
      "step 27962, loss is 4.750697612762451\n",
      "(64, 33)\n",
      "step 27963, loss is 4.690756797790527\n",
      "(64, 33)\n",
      "step 27964, loss is 4.758574485778809\n",
      "(64, 33)\n",
      "step 27965, loss is 4.743074893951416\n",
      "(64, 33)\n",
      "step 27966, loss is 4.862424850463867\n",
      "(64, 33)\n",
      "step 27967, loss is 4.795550346374512\n",
      "(64, 33)\n",
      "step 27968, loss is 4.883761882781982\n",
      "(64, 33)\n",
      "step 27969, loss is 4.834794998168945\n",
      "(64, 33)\n",
      "step 27970, loss is 4.876413822174072\n",
      "(64, 33)\n",
      "step 27971, loss is 4.867132186889648\n",
      "(64, 33)\n",
      "step 27972, loss is 4.829717636108398\n",
      "(64, 33)\n",
      "step 27973, loss is 4.662967681884766\n",
      "(64, 33)\n",
      "step 27974, loss is 4.839907646179199\n",
      "(64, 33)\n",
      "step 27975, loss is 4.714224815368652\n",
      "(64, 33)\n",
      "step 27976, loss is 4.94140100479126\n",
      "(64, 33)\n",
      "step 27977, loss is 4.726676940917969\n",
      "(64, 33)\n",
      "step 27978, loss is 4.777602672576904\n",
      "(64, 33)\n",
      "step 27979, loss is 4.687591552734375\n",
      "(64, 33)\n",
      "step 27980, loss is 4.7071380615234375\n",
      "(64, 33)\n",
      "step 27981, loss is 4.883530139923096\n",
      "(64, 33)\n",
      "step 27982, loss is 4.75317907333374\n",
      "(64, 33)\n",
      "step 27983, loss is 4.721183776855469\n",
      "(64, 33)\n",
      "step 27984, loss is 4.6867852210998535\n",
      "(64, 33)\n",
      "step 27985, loss is 4.865838527679443\n",
      "(64, 33)\n",
      "step 27986, loss is 4.708576202392578\n",
      "(64, 33)\n",
      "step 27987, loss is 4.851683616638184\n",
      "(64, 33)\n",
      "step 27988, loss is 4.830143928527832\n",
      "(64, 33)\n",
      "step 27989, loss is 4.76547908782959\n",
      "(64, 33)\n",
      "step 27990, loss is 4.753482818603516\n",
      "(64, 33)\n",
      "step 27991, loss is 4.762852191925049\n",
      "(64, 33)\n",
      "step 27992, loss is 4.915582656860352\n",
      "(64, 33)\n",
      "step 27993, loss is 4.92392635345459\n",
      "(64, 33)\n",
      "step 27994, loss is 4.8583598136901855\n",
      "(64, 33)\n",
      "step 27995, loss is 4.912438869476318\n",
      "(64, 33)\n",
      "step 27996, loss is 4.747396469116211\n",
      "(64, 33)\n",
      "step 27997, loss is 4.792088508605957\n",
      "(64, 33)\n",
      "step 27998, loss is 4.747666835784912\n",
      "(64, 33)\n",
      "step 27999, loss is 4.9068498611450195\n",
      "(64, 33)\n",
      "step 28000, loss is 4.818355083465576\n",
      "(64, 33)\n",
      "step 28001, loss is 4.662588596343994\n",
      "(64, 33)\n",
      "step 28002, loss is 4.686314105987549\n",
      "(64, 33)\n",
      "step 28003, loss is 4.7046122550964355\n",
      "(64, 33)\n",
      "step 28004, loss is 4.749236583709717\n",
      "(64, 33)\n",
      "step 28005, loss is 4.772226810455322\n",
      "(64, 33)\n",
      "step 28006, loss is 4.756698131561279\n",
      "(64, 33)\n",
      "step 28007, loss is 4.891265869140625\n",
      "(64, 33)\n",
      "step 28008, loss is 4.639918804168701\n",
      "(64, 33)\n",
      "step 28009, loss is 4.747525691986084\n",
      "(64, 33)\n",
      "step 28010, loss is 4.8327317237854\n",
      "(64, 33)\n",
      "step 28011, loss is 4.891868591308594\n",
      "(64, 33)\n",
      "step 28012, loss is 4.700379848480225\n",
      "(64, 33)\n",
      "step 28013, loss is 4.645482063293457\n",
      "(64, 33)\n",
      "step 28014, loss is 4.779277801513672\n",
      "(64, 33)\n",
      "step 28015, loss is 4.8776092529296875\n",
      "(64, 33)\n",
      "step 28016, loss is 4.904689311981201\n",
      "(64, 33)\n",
      "step 28017, loss is 4.70501708984375\n",
      "(64, 33)\n",
      "step 28018, loss is 4.754605770111084\n",
      "(64, 33)\n",
      "step 28019, loss is 4.7327189445495605\n",
      "(64, 33)\n",
      "step 28020, loss is 4.774468421936035\n",
      "(64, 33)\n",
      "step 28021, loss is 4.747822284698486\n",
      "(64, 33)\n",
      "step 28022, loss is 4.8171210289001465\n",
      "(64, 33)\n",
      "step 28023, loss is 4.585224628448486\n",
      "(64, 33)\n",
      "step 28024, loss is 4.602778434753418\n",
      "(64, 33)\n",
      "step 28025, loss is 4.619990825653076\n",
      "(64, 33)\n",
      "step 28026, loss is 4.827556610107422\n",
      "(64, 33)\n",
      "step 28027, loss is 4.817083358764648\n",
      "(64, 33)\n",
      "step 28028, loss is 4.9354777336120605\n",
      "(64, 33)\n",
      "step 28029, loss is 4.703052997589111\n",
      "(64, 33)\n",
      "step 28030, loss is 4.872843265533447\n",
      "(64, 33)\n",
      "step 28031, loss is 4.928154945373535\n",
      "(64, 33)\n",
      "step 28032, loss is 4.746903419494629\n",
      "(64, 33)\n",
      "step 28033, loss is 4.611263751983643\n",
      "(64, 33)\n",
      "step 28034, loss is 4.867116928100586\n",
      "(64, 33)\n",
      "step 28035, loss is 4.707191467285156\n",
      "(64, 33)\n",
      "step 28036, loss is 4.794170379638672\n",
      "(64, 33)\n",
      "step 28037, loss is 4.845033168792725\n",
      "(64, 33)\n",
      "step 28038, loss is 4.63331937789917\n",
      "(64, 33)\n",
      "step 28039, loss is 4.848503112792969\n",
      "(64, 33)\n",
      "step 28040, loss is 4.795200824737549\n",
      "(64, 33)\n",
      "step 28041, loss is 4.877816677093506\n",
      "(64, 33)\n",
      "step 28042, loss is 4.518495082855225\n",
      "(64, 33)\n",
      "step 28043, loss is 4.792976379394531\n",
      "(64, 33)\n",
      "step 28044, loss is 4.969052314758301\n",
      "(64, 33)\n",
      "step 28045, loss is 5.0205793380737305\n",
      "(64, 33)\n",
      "step 28046, loss is 4.672086238861084\n",
      "(64, 33)\n",
      "step 28047, loss is 4.627952575683594\n",
      "(64, 33)\n",
      "step 28048, loss is 4.52929162979126\n",
      "(64, 33)\n",
      "step 28049, loss is 4.766121864318848\n",
      "(64, 33)\n",
      "step 28050, loss is 4.7127861976623535\n",
      "(64, 33)\n",
      "step 28051, loss is 4.867328643798828\n",
      "(64, 33)\n",
      "step 28052, loss is 4.858200550079346\n",
      "(64, 33)\n",
      "step 28053, loss is 4.698880672454834\n",
      "(64, 33)\n",
      "step 28054, loss is 4.735888481140137\n",
      "(64, 33)\n",
      "step 28055, loss is 4.596330165863037\n",
      "(64, 33)\n",
      "step 28056, loss is 4.821774959564209\n",
      "(64, 33)\n",
      "step 28057, loss is 4.903344631195068\n",
      "(64, 33)\n",
      "step 28058, loss is 4.872446060180664\n",
      "(64, 33)\n",
      "step 28059, loss is 4.926715850830078\n",
      "(64, 33)\n",
      "step 28060, loss is 4.616364002227783\n",
      "(64, 33)\n",
      "step 28061, loss is 4.68556547164917\n",
      "(64, 33)\n",
      "step 28062, loss is 4.80795955657959\n",
      "(64, 33)\n",
      "step 28063, loss is 4.705027103424072\n",
      "(64, 33)\n",
      "step 28064, loss is 4.786637306213379\n",
      "(64, 33)\n",
      "step 28065, loss is 4.80875825881958\n",
      "(64, 33)\n",
      "step 28066, loss is 4.757218837738037\n",
      "(64, 33)\n",
      "step 28067, loss is 4.893993854522705\n",
      "(64, 33)\n",
      "step 28068, loss is 4.732715606689453\n",
      "(64, 33)\n",
      "step 28069, loss is 4.894035339355469\n",
      "(64, 33)\n",
      "step 28070, loss is 4.7446513175964355\n",
      "(64, 33)\n",
      "step 28071, loss is 4.863321304321289\n",
      "(64, 33)\n",
      "step 28072, loss is 4.759401798248291\n",
      "(64, 33)\n",
      "step 28073, loss is 5.020165920257568\n",
      "(64, 33)\n",
      "step 28074, loss is 4.698930263519287\n",
      "(64, 33)\n",
      "step 28075, loss is 5.016758918762207\n",
      "(64, 33)\n",
      "step 28076, loss is 4.719329833984375\n",
      "(64, 33)\n",
      "step 28077, loss is 4.696225643157959\n",
      "(64, 33)\n",
      "step 28078, loss is 4.782763957977295\n",
      "(64, 33)\n",
      "step 28079, loss is 4.594608783721924\n",
      "(64, 33)\n",
      "step 28080, loss is 5.029748916625977\n",
      "(64, 33)\n",
      "step 28081, loss is 4.869784355163574\n",
      "(64, 33)\n",
      "step 28082, loss is 4.722466945648193\n",
      "(64, 33)\n",
      "step 28083, loss is 4.814473628997803\n",
      "(64, 33)\n",
      "step 28084, loss is 4.7894158363342285\n",
      "(64, 33)\n",
      "step 28085, loss is 4.743255615234375\n",
      "(64, 33)\n",
      "step 28086, loss is 4.771503925323486\n",
      "(64, 33)\n",
      "step 28087, loss is 4.745792388916016\n",
      "(64, 33)\n",
      "step 28088, loss is 4.685451507568359\n",
      "(64, 33)\n",
      "step 28089, loss is 4.8028035163879395\n",
      "(64, 33)\n",
      "step 28090, loss is 4.87921667098999\n",
      "(64, 33)\n",
      "step 28091, loss is 4.738892078399658\n",
      "(64, 33)\n",
      "step 28092, loss is 4.956846237182617\n",
      "(64, 33)\n",
      "step 28093, loss is 4.709463596343994\n",
      "(64, 33)\n",
      "step 28094, loss is 4.857219219207764\n",
      "(64, 33)\n",
      "step 28095, loss is 4.736115455627441\n",
      "(64, 33)\n",
      "step 28096, loss is 4.4835028648376465\n",
      "(64, 33)\n",
      "step 28097, loss is 4.775273323059082\n",
      "(64, 33)\n",
      "step 28098, loss is 4.765625953674316\n",
      "(64, 33)\n",
      "step 28099, loss is 4.736258029937744\n",
      "(64, 33)\n",
      "step 28100, loss is 4.7384419441223145\n",
      "(64, 33)\n",
      "step 28101, loss is 4.875225067138672\n",
      "(64, 33)\n",
      "step 28102, loss is 4.851833820343018\n",
      "(64, 33)\n",
      "step 28103, loss is 4.675266265869141\n",
      "(64, 33)\n",
      "step 28104, loss is 4.80659818649292\n",
      "(64, 33)\n",
      "step 28105, loss is 4.740410804748535\n",
      "(64, 33)\n",
      "step 28106, loss is 4.930023670196533\n",
      "(64, 33)\n",
      "step 28107, loss is 4.774024963378906\n",
      "(64, 33)\n",
      "step 28108, loss is 4.868126392364502\n",
      "(64, 33)\n",
      "step 28109, loss is 4.67341947555542\n",
      "(64, 33)\n",
      "step 28110, loss is 4.685733795166016\n",
      "(64, 33)\n",
      "step 28111, loss is 4.687868595123291\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 28112, loss is 4.84415340423584\n",
      "(64, 33)\n",
      "step 28113, loss is 4.743563652038574\n",
      "(64, 33)\n",
      "step 28114, loss is 4.837811470031738\n",
      "(64, 33)\n",
      "step 28115, loss is 4.653160095214844\n",
      "(64, 33)\n",
      "step 28116, loss is 4.753569602966309\n",
      "(64, 33)\n",
      "step 28117, loss is 4.856739044189453\n",
      "(64, 33)\n",
      "step 28118, loss is 4.71712064743042\n",
      "(64, 33)\n",
      "step 28119, loss is 4.7897257804870605\n",
      "(64, 33)\n",
      "step 28120, loss is 4.740468502044678\n",
      "(64, 33)\n",
      "step 28121, loss is 4.886795997619629\n",
      "(64, 33)\n",
      "step 28122, loss is 4.715193271636963\n",
      "(64, 33)\n",
      "step 28123, loss is 4.754850387573242\n",
      "(64, 33)\n",
      "step 28124, loss is 4.922264099121094\n",
      "(64, 33)\n",
      "step 28125, loss is 4.850333213806152\n",
      "(64, 33)\n",
      "step 28126, loss is 4.706921100616455\n",
      "(64, 33)\n",
      "step 28127, loss is 4.814785003662109\n",
      "(64, 33)\n",
      "step 28128, loss is 4.865966320037842\n",
      "(64, 33)\n",
      "step 28129, loss is 4.710757255554199\n",
      "(64, 33)\n",
      "step 28130, loss is 4.721946716308594\n",
      "(64, 33)\n",
      "step 28131, loss is 4.978847503662109\n",
      "(64, 33)\n",
      "step 28132, loss is 4.874937057495117\n",
      "(64, 33)\n",
      "step 28133, loss is 4.841525077819824\n",
      "(64, 33)\n",
      "step 28134, loss is 4.93803596496582\n",
      "(64, 33)\n",
      "step 28135, loss is 4.9358439445495605\n",
      "(64, 33)\n",
      "step 28136, loss is 4.763688564300537\n",
      "(64, 33)\n",
      "step 28137, loss is 4.599663257598877\n",
      "(64, 33)\n",
      "step 28138, loss is 4.837922096252441\n",
      "(64, 33)\n",
      "step 28139, loss is 4.677977561950684\n",
      "(64, 33)\n",
      "step 28140, loss is 4.926962375640869\n",
      "(64, 33)\n",
      "step 28141, loss is 4.914037227630615\n",
      "(64, 33)\n",
      "step 28142, loss is 4.937011241912842\n",
      "(64, 33)\n",
      "step 28143, loss is 4.850523948669434\n",
      "(64, 33)\n",
      "step 28144, loss is 4.717623710632324\n",
      "(64, 33)\n",
      "step 28145, loss is 4.732355117797852\n",
      "(64, 33)\n",
      "step 28146, loss is 4.866034507751465\n",
      "(64, 33)\n",
      "step 28147, loss is 4.794270992279053\n",
      "(64, 33)\n",
      "step 28148, loss is 4.770861625671387\n",
      "(64, 33)\n",
      "step 28149, loss is 4.811185359954834\n",
      "(64, 33)\n",
      "step 28150, loss is 4.647675037384033\n",
      "(64, 33)\n",
      "step 28151, loss is 4.7930707931518555\n",
      "(64, 33)\n",
      "step 28152, loss is 4.811557292938232\n",
      "(64, 33)\n",
      "step 28153, loss is 4.61959171295166\n",
      "(64, 33)\n",
      "step 28154, loss is 4.826859474182129\n",
      "(64, 33)\n",
      "step 28155, loss is 4.6303019523620605\n",
      "(64, 33)\n",
      "step 28156, loss is 4.73634672164917\n",
      "(64, 33)\n",
      "step 28157, loss is 4.63562536239624\n",
      "(64, 33)\n",
      "step 28158, loss is 4.746723651885986\n",
      "(64, 33)\n",
      "step 28159, loss is 4.940145492553711\n",
      "(64, 33)\n",
      "step 28160, loss is 4.765843391418457\n",
      "(64, 33)\n",
      "step 28161, loss is 4.7928853034973145\n",
      "(64, 33)\n",
      "step 28162, loss is 4.7165093421936035\n",
      "(64, 33)\n",
      "step 28163, loss is 4.6093549728393555\n",
      "(64, 33)\n",
      "step 28164, loss is 4.742878437042236\n",
      "(64, 33)\n",
      "step 28165, loss is 4.763890266418457\n",
      "(64, 33)\n",
      "step 28166, loss is 4.881722450256348\n",
      "(64, 33)\n",
      "step 28167, loss is 4.72352933883667\n",
      "(64, 33)\n",
      "step 28168, loss is 4.7248969078063965\n",
      "(64, 33)\n",
      "step 28169, loss is 4.87680196762085\n",
      "(64, 33)\n",
      "step 28170, loss is 4.681935787200928\n",
      "(64, 33)\n",
      "step 28171, loss is 4.789266586303711\n",
      "(64, 33)\n",
      "step 28172, loss is 4.723095417022705\n",
      "(64, 33)\n",
      "step 28173, loss is 4.741228103637695\n",
      "(64, 33)\n",
      "step 28174, loss is 4.900624752044678\n",
      "(64, 33)\n",
      "step 28175, loss is 4.612570762634277\n",
      "(64, 33)\n",
      "step 28176, loss is 4.663397312164307\n",
      "(64, 33)\n",
      "step 28177, loss is 4.7460808753967285\n",
      "(64, 33)\n",
      "step 28178, loss is 4.835474014282227\n",
      "(64, 33)\n",
      "step 28179, loss is 4.83572244644165\n",
      "(64, 33)\n",
      "step 28180, loss is 4.813225746154785\n",
      "(64, 33)\n",
      "step 28181, loss is 4.84602689743042\n",
      "(64, 33)\n",
      "step 28182, loss is 4.6155571937561035\n",
      "(64, 33)\n",
      "step 28183, loss is 4.818055152893066\n",
      "(64, 33)\n",
      "step 28184, loss is 4.906381130218506\n",
      "(64, 33)\n",
      "step 28185, loss is 4.936100006103516\n",
      "(64, 33)\n",
      "step 28186, loss is 4.792643070220947\n",
      "(64, 33)\n",
      "step 28187, loss is 4.745491981506348\n",
      "(64, 33)\n",
      "step 28188, loss is 4.547706127166748\n",
      "(64, 33)\n",
      "step 28189, loss is 4.675271987915039\n",
      "(64, 33)\n",
      "step 28190, loss is 4.6956095695495605\n",
      "(64, 33)\n",
      "step 28191, loss is 4.836727142333984\n",
      "(64, 33)\n",
      "step 28192, loss is 4.918581008911133\n",
      "(64, 33)\n",
      "step 28193, loss is 4.701411247253418\n",
      "(64, 33)\n",
      "step 28194, loss is 4.815680980682373\n",
      "(64, 33)\n",
      "step 28195, loss is 4.6867499351501465\n",
      "(64, 33)\n",
      "step 28196, loss is 4.718259811401367\n",
      "(64, 33)\n",
      "step 28197, loss is 4.8384013175964355\n",
      "(64, 33)\n",
      "step 28198, loss is 4.988860607147217\n",
      "(64, 33)\n",
      "step 28199, loss is 4.672625541687012\n",
      "(64, 33)\n",
      "step 28200, loss is 4.755008220672607\n",
      "(64, 33)\n",
      "step 28201, loss is 4.795310020446777\n",
      "(64, 33)\n",
      "step 28202, loss is 4.956571578979492\n",
      "(64, 33)\n",
      "step 28203, loss is 4.716404914855957\n",
      "(64, 33)\n",
      "step 28204, loss is 4.67936897277832\n",
      "(64, 33)\n",
      "step 28205, loss is 4.793812274932861\n",
      "(64, 33)\n",
      "step 28206, loss is 4.977378845214844\n",
      "(64, 33)\n",
      "step 28207, loss is 4.769842624664307\n",
      "(64, 33)\n",
      "step 28208, loss is 4.875854969024658\n",
      "(64, 33)\n",
      "step 28209, loss is 4.902035713195801\n",
      "(64, 33)\n",
      "step 28210, loss is 4.87764835357666\n",
      "(64, 33)\n",
      "step 28211, loss is 4.851704120635986\n",
      "(64, 33)\n",
      "step 28212, loss is 4.740344524383545\n",
      "(64, 33)\n",
      "step 28213, loss is 4.580842971801758\n",
      "(64, 33)\n",
      "step 28214, loss is 4.694492340087891\n",
      "(64, 33)\n",
      "step 28215, loss is 4.704567909240723\n",
      "(64, 33)\n",
      "step 28216, loss is 4.862093925476074\n",
      "(64, 33)\n",
      "step 28217, loss is 4.652448654174805\n",
      "(64, 33)\n",
      "step 28218, loss is 4.7245330810546875\n",
      "(64, 33)\n",
      "step 28219, loss is 4.794788360595703\n",
      "(64, 33)\n",
      "step 28220, loss is 5.033623695373535\n",
      "(64, 33)\n",
      "step 28221, loss is 4.885634422302246\n",
      "(64, 33)\n",
      "step 28222, loss is 4.8785786628723145\n",
      "(64, 33)\n",
      "step 28223, loss is 4.781212329864502\n",
      "(64, 33)\n",
      "step 28224, loss is 4.778646945953369\n",
      "(64, 33)\n",
      "step 28225, loss is 4.808888912200928\n",
      "(64, 33)\n",
      "step 28226, loss is 4.972105979919434\n",
      "(64, 33)\n",
      "step 28227, loss is 4.842989921569824\n",
      "(64, 33)\n",
      "step 28228, loss is 4.813365459442139\n",
      "(64, 33)\n",
      "step 28229, loss is 4.96146821975708\n",
      "(64, 33)\n",
      "step 28230, loss is 4.7865705490112305\n",
      "(64, 33)\n",
      "step 28231, loss is 4.807374954223633\n",
      "(64, 33)\n",
      "step 28232, loss is 4.842767238616943\n",
      "(64, 33)\n",
      "step 28233, loss is 4.900981903076172\n",
      "(64, 33)\n",
      "step 28234, loss is 4.932315349578857\n",
      "(64, 33)\n",
      "step 28235, loss is 4.732760429382324\n",
      "(64, 33)\n",
      "step 28236, loss is 4.808340072631836\n",
      "(64, 33)\n",
      "step 28237, loss is 4.751287937164307\n",
      "(64, 33)\n",
      "step 28238, loss is 4.801589012145996\n",
      "(64, 33)\n",
      "step 28239, loss is 4.895223140716553\n",
      "(64, 33)\n",
      "step 28240, loss is 4.758507251739502\n",
      "(64, 33)\n",
      "step 28241, loss is 4.896134376525879\n",
      "(64, 33)\n",
      "step 28242, loss is 4.808172702789307\n",
      "(64, 33)\n",
      "step 28243, loss is 4.8275275230407715\n",
      "(64, 33)\n",
      "step 28244, loss is 4.922091007232666\n",
      "(64, 33)\n",
      "step 28245, loss is 4.859874725341797\n",
      "(64, 33)\n",
      "step 28246, loss is 4.9137187004089355\n",
      "(64, 33)\n",
      "step 28247, loss is 4.831695079803467\n",
      "(64, 33)\n",
      "step 28248, loss is 4.8787713050842285\n",
      "(64, 33)\n",
      "step 28249, loss is 4.824706554412842\n",
      "(64, 33)\n",
      "step 28250, loss is 4.785229682922363\n",
      "(64, 33)\n",
      "step 28251, loss is 4.619173526763916\n",
      "(64, 33)\n",
      "step 28252, loss is 4.929563045501709\n",
      "(64, 33)\n",
      "step 28253, loss is 4.915441036224365\n",
      "(64, 33)\n",
      "step 28254, loss is 4.776151180267334\n",
      "(64, 33)\n",
      "step 28255, loss is 4.839388847351074\n",
      "(64, 33)\n",
      "step 28256, loss is 4.7756876945495605\n",
      "(64, 33)\n",
      "step 28257, loss is 4.840489387512207\n",
      "(64, 33)\n",
      "step 28258, loss is 4.735798358917236\n",
      "(64, 33)\n",
      "step 28259, loss is 5.032642841339111\n",
      "(64, 33)\n",
      "step 28260, loss is 4.828334808349609\n",
      "(64, 33)\n",
      "step 28261, loss is 4.759580135345459\n",
      "(64, 33)\n",
      "step 28262, loss is 4.8333001136779785\n",
      "(64, 33)\n",
      "step 28263, loss is 4.961386680603027\n",
      "(64, 33)\n",
      "step 28264, loss is 4.647398471832275\n",
      "(64, 33)\n",
      "step 28265, loss is 4.831219673156738\n",
      "(64, 33)\n",
      "step 28266, loss is 4.730196952819824\n",
      "(64, 33)\n",
      "step 28267, loss is 4.807359218597412\n",
      "(64, 33)\n",
      "step 28268, loss is 5.005377292633057\n",
      "(64, 33)\n",
      "step 28269, loss is 4.829859256744385\n",
      "(64, 33)\n",
      "step 28270, loss is 4.617564678192139\n",
      "(64, 33)\n",
      "step 28271, loss is 4.763544082641602\n",
      "(64, 33)\n",
      "step 28272, loss is 4.984592914581299\n",
      "(64, 33)\n",
      "step 28273, loss is 4.7482500076293945\n",
      "(64, 33)\n",
      "step 28274, loss is 4.741543769836426\n",
      "(64, 33)\n",
      "step 28275, loss is 4.687887668609619\n",
      "(64, 33)\n",
      "step 28276, loss is 4.802780628204346\n",
      "(64, 33)\n",
      "step 28277, loss is 4.876505374908447\n",
      "(64, 33)\n",
      "step 28278, loss is 4.807367324829102\n",
      "(64, 33)\n",
      "step 28279, loss is 4.660569667816162\n",
      "(64, 33)\n",
      "step 28280, loss is 4.777358531951904\n",
      "(64, 33)\n",
      "step 28281, loss is 4.965908527374268\n",
      "(64, 33)\n",
      "step 28282, loss is 4.76138973236084\n",
      "(64, 33)\n",
      "step 28283, loss is 4.846976280212402\n",
      "(64, 33)\n",
      "step 28284, loss is 4.726353645324707\n",
      "(64, 33)\n",
      "step 28285, loss is 4.7846598625183105\n",
      "(64, 33)\n",
      "step 28286, loss is 4.7333855628967285\n",
      "(64, 33)\n",
      "step 28287, loss is 4.706132888793945\n",
      "(64, 33)\n",
      "step 28288, loss is 4.7844157218933105\n",
      "(64, 33)\n",
      "step 28289, loss is 4.4500956535339355\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 28290, loss is 4.807467937469482\n",
      "(64, 33)\n",
      "step 28291, loss is 4.593882083892822\n",
      "(64, 33)\n",
      "step 28292, loss is 4.893504619598389\n",
      "(64, 33)\n",
      "step 28293, loss is 4.7798309326171875\n",
      "(64, 33)\n",
      "step 28294, loss is 5.103146076202393\n",
      "(64, 33)\n",
      "step 28295, loss is 4.7751898765563965\n",
      "(64, 33)\n",
      "step 28296, loss is 4.899368762969971\n",
      "(64, 33)\n",
      "step 28297, loss is 4.813526630401611\n",
      "(64, 33)\n",
      "step 28298, loss is 4.747501850128174\n",
      "(64, 33)\n",
      "step 28299, loss is 4.860607624053955\n",
      "(64, 33)\n",
      "step 28300, loss is 4.846444129943848\n",
      "(64, 33)\n",
      "step 28301, loss is 4.887150764465332\n",
      "(64, 33)\n",
      "step 28302, loss is 4.515421390533447\n",
      "(64, 33)\n",
      "step 28303, loss is 4.8001227378845215\n",
      "(64, 33)\n",
      "step 28304, loss is 4.864713668823242\n",
      "(64, 33)\n",
      "step 28305, loss is 4.665718078613281\n",
      "(64, 33)\n",
      "step 28306, loss is 4.786596775054932\n",
      "(64, 33)\n",
      "step 28307, loss is 4.879507064819336\n",
      "(64, 33)\n",
      "step 28308, loss is 4.761925220489502\n",
      "(64, 33)\n",
      "step 28309, loss is 4.855864524841309\n",
      "(64, 33)\n",
      "step 28310, loss is 4.804710865020752\n",
      "(64, 33)\n",
      "step 28311, loss is 4.790763854980469\n",
      "(64, 33)\n",
      "step 28312, loss is 4.928275108337402\n",
      "(64, 33)\n",
      "step 28313, loss is 4.89209508895874\n",
      "(64, 33)\n",
      "step 28314, loss is 4.8553080558776855\n",
      "(64, 33)\n",
      "step 28315, loss is 4.857270240783691\n",
      "(64, 33)\n",
      "step 28316, loss is 4.775716781616211\n",
      "(64, 33)\n",
      "step 28317, loss is 4.837420463562012\n",
      "(64, 33)\n",
      "step 28318, loss is 4.89567232131958\n",
      "(64, 33)\n",
      "step 28319, loss is 4.620397567749023\n",
      "(64, 33)\n",
      "step 28320, loss is 4.867015361785889\n",
      "(64, 33)\n",
      "step 28321, loss is 4.8836669921875\n",
      "(64, 33)\n",
      "step 28322, loss is 4.872928619384766\n",
      "(64, 33)\n",
      "step 28323, loss is 5.012839317321777\n",
      "(64, 33)\n",
      "step 28324, loss is 4.60488224029541\n",
      "(64, 33)\n",
      "step 28325, loss is 4.7410502433776855\n",
      "(64, 33)\n",
      "step 28326, loss is 4.728094100952148\n",
      "(64, 33)\n",
      "step 28327, loss is 4.717271327972412\n",
      "(64, 33)\n",
      "step 28328, loss is 4.7888689041137695\n",
      "(64, 33)\n",
      "step 28329, loss is 4.970134735107422\n",
      "(64, 33)\n",
      "step 28330, loss is 4.635799407958984\n",
      "(64, 33)\n",
      "step 28331, loss is 4.805089950561523\n",
      "(64, 33)\n",
      "step 28332, loss is 4.750636577606201\n",
      "(64, 33)\n",
      "step 28333, loss is 4.682586193084717\n",
      "(64, 33)\n",
      "step 28334, loss is 4.702206611633301\n",
      "(64, 33)\n",
      "step 28335, loss is 4.805687427520752\n",
      "(64, 33)\n",
      "step 28336, loss is 4.962152004241943\n",
      "(64, 33)\n",
      "step 28337, loss is 4.79410457611084\n",
      "(64, 33)\n",
      "step 28338, loss is 4.6452412605285645\n",
      "(64, 33)\n",
      "step 28339, loss is 4.864980220794678\n",
      "(64, 33)\n",
      "step 28340, loss is 4.53908109664917\n",
      "(64, 33)\n",
      "step 28341, loss is 4.8289875984191895\n",
      "(64, 33)\n",
      "step 28342, loss is 4.733862400054932\n",
      "(64, 33)\n",
      "step 28343, loss is 4.751236915588379\n",
      "(64, 33)\n",
      "step 28344, loss is 4.543347358703613\n",
      "(64, 33)\n",
      "step 28345, loss is 4.73215913772583\n",
      "(64, 33)\n",
      "step 28346, loss is 4.756868839263916\n",
      "(64, 33)\n",
      "step 28347, loss is 4.822334289550781\n",
      "(64, 33)\n",
      "step 28348, loss is 4.618537902832031\n",
      "(64, 33)\n",
      "step 28349, loss is 4.827269077301025\n",
      "(64, 33)\n",
      "step 28350, loss is 4.769341945648193\n",
      "(64, 33)\n",
      "step 28351, loss is 4.717009544372559\n",
      "(64, 33)\n",
      "step 28352, loss is 4.8229570388793945\n",
      "(64, 33)\n",
      "step 28353, loss is 4.8143534660339355\n",
      "(64, 33)\n",
      "step 28354, loss is 4.632371425628662\n",
      "(64, 33)\n",
      "step 28355, loss is 4.761162281036377\n",
      "(64, 33)\n",
      "step 28356, loss is 4.691778182983398\n",
      "(64, 33)\n",
      "step 28357, loss is 4.874231338500977\n",
      "(64, 33)\n",
      "step 28358, loss is 4.780802249908447\n",
      "(64, 33)\n",
      "step 28359, loss is 4.871689319610596\n",
      "(64, 33)\n",
      "step 28360, loss is 4.708587646484375\n",
      "(64, 33)\n",
      "step 28361, loss is 4.62986421585083\n",
      "(64, 33)\n",
      "step 28362, loss is 4.930954456329346\n",
      "(64, 33)\n",
      "step 28363, loss is 4.858577728271484\n",
      "(64, 33)\n",
      "step 28364, loss is 4.642457008361816\n",
      "(64, 33)\n",
      "step 28365, loss is 4.714086532592773\n",
      "(64, 33)\n",
      "step 28366, loss is 4.8715009689331055\n",
      "(64, 33)\n",
      "step 28367, loss is 4.887274742126465\n",
      "(64, 33)\n",
      "step 28368, loss is 4.8496880531311035\n",
      "(64, 33)\n",
      "step 28369, loss is 4.771675109863281\n",
      "(64, 33)\n",
      "step 28370, loss is 4.73941707611084\n",
      "(64, 33)\n",
      "step 28371, loss is 4.626387119293213\n",
      "(64, 33)\n",
      "step 28372, loss is 4.8810224533081055\n",
      "(64, 33)\n",
      "step 28373, loss is 4.84862756729126\n",
      "(64, 33)\n",
      "step 28374, loss is 4.814827919006348\n",
      "(64, 33)\n",
      "step 28375, loss is 4.965240478515625\n",
      "(64, 33)\n",
      "step 28376, loss is 4.709681510925293\n",
      "(64, 33)\n",
      "step 28377, loss is 4.779335021972656\n",
      "(64, 33)\n",
      "step 28378, loss is 4.791492462158203\n",
      "(64, 33)\n",
      "step 28379, loss is 4.6916823387146\n",
      "(64, 33)\n",
      "step 28380, loss is 4.773818016052246\n",
      "(64, 33)\n",
      "step 28381, loss is 4.747578144073486\n",
      "(64, 33)\n",
      "step 28382, loss is 4.664791107177734\n",
      "(64, 33)\n",
      "step 28383, loss is 4.722447395324707\n",
      "(64, 33)\n",
      "step 28384, loss is 4.984494209289551\n",
      "(64, 33)\n",
      "step 28385, loss is 4.752982139587402\n",
      "(64, 33)\n",
      "step 28386, loss is 4.714062213897705\n",
      "(64, 33)\n",
      "step 28387, loss is 4.781366348266602\n",
      "(64, 33)\n",
      "step 28388, loss is 4.83884859085083\n",
      "(64, 33)\n",
      "step 28389, loss is 4.818532943725586\n",
      "(64, 33)\n",
      "step 28390, loss is 4.894582271575928\n",
      "(64, 33)\n",
      "step 28391, loss is 4.619105339050293\n",
      "(64, 33)\n",
      "step 28392, loss is 4.696681499481201\n",
      "(64, 33)\n",
      "step 28393, loss is 4.955020904541016\n",
      "(64, 33)\n",
      "step 28394, loss is 4.823297500610352\n",
      "(64, 33)\n",
      "step 28395, loss is 4.85395622253418\n",
      "(64, 33)\n",
      "step 28396, loss is 4.950544834136963\n",
      "(64, 33)\n",
      "step 28397, loss is 4.696071624755859\n",
      "(64, 33)\n",
      "step 28398, loss is 4.7035322189331055\n",
      "(64, 33)\n",
      "step 28399, loss is 4.974422454833984\n",
      "(64, 33)\n",
      "step 28400, loss is 4.983467102050781\n",
      "(64, 33)\n",
      "step 28401, loss is 4.728481292724609\n",
      "(64, 33)\n",
      "step 28402, loss is 4.718839645385742\n",
      "(64, 33)\n",
      "step 28403, loss is 4.802317142486572\n",
      "(64, 33)\n",
      "step 28404, loss is 4.66086483001709\n",
      "(64, 33)\n",
      "step 28405, loss is 5.061887741088867\n",
      "(64, 33)\n",
      "step 28406, loss is 4.837979316711426\n",
      "(64, 33)\n",
      "step 28407, loss is 4.9217529296875\n",
      "(64, 33)\n",
      "step 28408, loss is 4.5440592765808105\n",
      "(64, 33)\n",
      "step 28409, loss is 4.775550842285156\n",
      "(64, 33)\n",
      "step 28410, loss is 4.715444564819336\n",
      "(64, 33)\n",
      "step 28411, loss is 4.8680033683776855\n",
      "(64, 33)\n",
      "step 28412, loss is 4.688032150268555\n",
      "(64, 33)\n",
      "step 28413, loss is 4.682967662811279\n",
      "(64, 33)\n",
      "step 28414, loss is 4.652963638305664\n",
      "(64, 33)\n",
      "step 28415, loss is 4.9141035079956055\n",
      "(64, 33)\n",
      "step 28416, loss is 5.0004096031188965\n",
      "(64, 33)\n",
      "step 28417, loss is 4.695103645324707\n",
      "(64, 33)\n",
      "step 28418, loss is 4.705628395080566\n",
      "(64, 33)\n",
      "step 28419, loss is 4.750011920928955\n",
      "(64, 33)\n",
      "step 28420, loss is 4.743009567260742\n",
      "(64, 33)\n",
      "step 28421, loss is 4.903739929199219\n",
      "(64, 33)\n",
      "step 28422, loss is 4.72287654876709\n",
      "(64, 33)\n",
      "step 28423, loss is 4.78387451171875\n",
      "(64, 33)\n",
      "step 28424, loss is 4.813391208648682\n",
      "(64, 33)\n",
      "step 28425, loss is 4.760796070098877\n",
      "(64, 33)\n",
      "step 28426, loss is 4.988680362701416\n",
      "(64, 33)\n",
      "step 28427, loss is 4.657448768615723\n",
      "(64, 33)\n",
      "step 28428, loss is 4.859125137329102\n",
      "(64, 33)\n",
      "step 28429, loss is 4.644062519073486\n",
      "(64, 33)\n",
      "step 28430, loss is 4.76765775680542\n",
      "(64, 33)\n",
      "step 28431, loss is 4.724160194396973\n",
      "(64, 33)\n",
      "step 28432, loss is 4.747169017791748\n",
      "(64, 33)\n",
      "step 28433, loss is 4.755945205688477\n",
      "(64, 33)\n",
      "step 28434, loss is 4.7302446365356445\n",
      "(64, 33)\n",
      "step 28435, loss is 4.881365776062012\n",
      "(64, 33)\n",
      "step 28436, loss is 4.586231708526611\n",
      "(64, 33)\n",
      "step 28437, loss is 4.867250919342041\n",
      "(64, 33)\n",
      "step 28438, loss is 4.607785701751709\n",
      "(64, 33)\n",
      "step 28439, loss is 4.574690818786621\n",
      "(64, 33)\n",
      "step 28440, loss is 4.829075813293457\n",
      "(64, 33)\n",
      "step 28441, loss is 4.739825248718262\n",
      "(64, 33)\n",
      "step 28442, loss is 4.770430088043213\n",
      "(64, 33)\n",
      "step 28443, loss is 4.691169738769531\n",
      "(64, 33)\n",
      "step 28444, loss is 4.870584964752197\n",
      "(64, 33)\n",
      "step 28445, loss is 4.834561824798584\n",
      "(64, 33)\n",
      "step 28446, loss is 4.887503147125244\n",
      "(64, 33)\n",
      "step 28447, loss is 4.751926422119141\n",
      "(64, 33)\n",
      "step 28448, loss is 4.567122936248779\n",
      "(64, 33)\n",
      "step 28449, loss is 4.5754899978637695\n",
      "(64, 33)\n",
      "step 28450, loss is 4.948230266571045\n",
      "(64, 33)\n",
      "step 28451, loss is 4.867411136627197\n",
      "(64, 33)\n",
      "step 28452, loss is 4.89967155456543\n",
      "(64, 33)\n",
      "step 28453, loss is 4.756688594818115\n",
      "(64, 33)\n",
      "step 28454, loss is 4.6496405601501465\n",
      "(64, 33)\n",
      "step 28455, loss is 4.664268970489502\n",
      "(64, 33)\n",
      "step 28456, loss is 4.742469787597656\n",
      "(64, 33)\n",
      "step 28457, loss is 4.781851291656494\n",
      "(64, 33)\n",
      "step 28458, loss is 4.736750602722168\n",
      "(64, 33)\n",
      "step 28459, loss is 4.874608039855957\n",
      "(64, 33)\n",
      "step 28460, loss is 4.891289710998535\n",
      "(64, 33)\n",
      "step 28461, loss is 4.572906017303467\n",
      "(64, 33)\n",
      "step 28462, loss is 4.834200382232666\n",
      "(64, 33)\n",
      "step 28463, loss is 4.848396301269531\n",
      "(64, 33)\n",
      "step 28464, loss is 4.596505641937256\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 28465, loss is 4.9612812995910645\n",
      "(64, 33)\n",
      "step 28466, loss is 4.758885383605957\n",
      "(64, 33)\n",
      "step 28467, loss is 4.691288471221924\n",
      "(64, 33)\n",
      "step 28468, loss is 4.8631815910339355\n",
      "(64, 33)\n",
      "step 28469, loss is 5.021263122558594\n",
      "(64, 33)\n",
      "step 28470, loss is 4.738961219787598\n",
      "(64, 33)\n",
      "step 28471, loss is 4.875810146331787\n",
      "(64, 33)\n",
      "step 28472, loss is 4.804070949554443\n",
      "(64, 33)\n",
      "step 28473, loss is 4.833141326904297\n",
      "(64, 33)\n",
      "step 28474, loss is 4.773136138916016\n",
      "(64, 33)\n",
      "step 28475, loss is 4.812162399291992\n",
      "(64, 33)\n",
      "step 28476, loss is 4.6940765380859375\n",
      "(64, 33)\n",
      "step 28477, loss is 4.684494495391846\n",
      "(64, 33)\n",
      "step 28478, loss is 4.831385135650635\n",
      "(64, 33)\n",
      "step 28479, loss is 4.973581790924072\n",
      "(64, 33)\n",
      "step 28480, loss is 4.676275730133057\n",
      "(64, 33)\n",
      "step 28481, loss is 4.571626663208008\n",
      "(64, 33)\n",
      "step 28482, loss is 4.742552280426025\n",
      "(64, 33)\n",
      "step 28483, loss is 4.67439603805542\n",
      "(64, 33)\n",
      "step 28484, loss is 4.767538070678711\n",
      "(64, 33)\n",
      "step 28485, loss is 4.852166652679443\n",
      "(64, 33)\n",
      "step 28486, loss is 4.869904518127441\n",
      "(64, 33)\n",
      "step 28487, loss is 4.9064202308654785\n",
      "(64, 33)\n",
      "step 28488, loss is 4.658050537109375\n",
      "(64, 33)\n",
      "step 28489, loss is 4.848194599151611\n",
      "(64, 33)\n",
      "step 28490, loss is 4.668201446533203\n",
      "(64, 33)\n",
      "step 28491, loss is 4.7298078536987305\n",
      "(64, 33)\n",
      "step 28492, loss is 4.919455051422119\n",
      "(64, 33)\n",
      "step 28493, loss is 4.728137493133545\n",
      "(64, 33)\n",
      "step 28494, loss is 4.860843658447266\n",
      "(64, 33)\n",
      "step 28495, loss is 4.884242057800293\n",
      "(64, 33)\n",
      "step 28496, loss is 4.729283332824707\n",
      "(64, 33)\n",
      "step 28497, loss is 4.765382289886475\n",
      "(64, 33)\n",
      "step 28498, loss is 4.796660423278809\n",
      "(64, 33)\n",
      "step 28499, loss is 4.6822991371154785\n",
      "(64, 33)\n",
      "step 28500, loss is 4.681993007659912\n",
      "(64, 33)\n",
      "step 28501, loss is 4.891140937805176\n",
      "(64, 33)\n",
      "step 28502, loss is 4.703458786010742\n",
      "(64, 33)\n",
      "step 28503, loss is 4.878783702850342\n",
      "(64, 33)\n",
      "step 28504, loss is 4.777712821960449\n",
      "(64, 33)\n",
      "step 28505, loss is 4.663212299346924\n",
      "(64, 33)\n",
      "step 28506, loss is 4.735701560974121\n",
      "(64, 33)\n",
      "step 28507, loss is 4.837899208068848\n",
      "(64, 33)\n",
      "step 28508, loss is 4.733953952789307\n",
      "(64, 33)\n",
      "step 28509, loss is 4.568434238433838\n",
      "(64, 33)\n",
      "step 28510, loss is 4.712246894836426\n",
      "(64, 33)\n",
      "step 28511, loss is 4.869258880615234\n",
      "(64, 33)\n",
      "step 28512, loss is 4.631907939910889\n",
      "(64, 33)\n",
      "step 28513, loss is 4.772522926330566\n",
      "(64, 33)\n",
      "step 28514, loss is 4.598149299621582\n",
      "(64, 33)\n",
      "step 28515, loss is 4.739474296569824\n",
      "(64, 33)\n",
      "step 28516, loss is 4.769537448883057\n",
      "(64, 33)\n",
      "step 28517, loss is 4.8248724937438965\n",
      "(64, 33)\n",
      "step 28518, loss is 4.97913932800293\n",
      "(64, 33)\n",
      "step 28519, loss is 4.842777252197266\n",
      "(64, 33)\n",
      "step 28520, loss is 4.794610500335693\n",
      "(64, 33)\n",
      "step 28521, loss is 4.801036357879639\n",
      "(64, 33)\n",
      "step 28522, loss is 4.66458797454834\n",
      "(64, 33)\n",
      "step 28523, loss is 4.584949970245361\n",
      "(64, 33)\n",
      "step 28524, loss is 4.825596332550049\n",
      "(64, 33)\n",
      "step 28525, loss is 4.625661849975586\n",
      "(64, 33)\n",
      "step 28526, loss is 4.757018566131592\n",
      "(64, 33)\n",
      "step 28527, loss is 4.753968238830566\n",
      "(64, 33)\n",
      "step 28528, loss is 4.654798984527588\n",
      "(64, 33)\n",
      "step 28529, loss is 4.651303768157959\n",
      "(64, 33)\n",
      "step 28530, loss is 4.776700496673584\n",
      "(64, 33)\n",
      "step 28531, loss is 4.839390277862549\n",
      "(64, 33)\n",
      "step 28532, loss is 4.88092041015625\n",
      "(64, 33)\n",
      "step 28533, loss is 4.817949295043945\n",
      "(64, 33)\n",
      "step 28534, loss is 4.622183799743652\n",
      "(64, 33)\n",
      "step 28535, loss is 4.700563430786133\n",
      "(64, 33)\n",
      "step 28536, loss is 4.668893337249756\n",
      "(64, 33)\n",
      "step 28537, loss is 4.8778252601623535\n",
      "(64, 33)\n",
      "step 28538, loss is 4.799376487731934\n",
      "(64, 33)\n",
      "step 28539, loss is 4.701667308807373\n",
      "(64, 33)\n",
      "step 28540, loss is 4.803417682647705\n",
      "(64, 33)\n",
      "step 28541, loss is 4.757500171661377\n",
      "(64, 33)\n",
      "step 28542, loss is 4.686045169830322\n",
      "(64, 33)\n",
      "step 28543, loss is 4.789592266082764\n",
      "(64, 33)\n",
      "step 28544, loss is 4.682238578796387\n",
      "(64, 33)\n",
      "step 28545, loss is 4.633659362792969\n",
      "(64, 33)\n",
      "step 28546, loss is 4.804084777832031\n",
      "(64, 33)\n",
      "step 28547, loss is 4.760344982147217\n",
      "(64, 33)\n",
      "step 28548, loss is 4.868952751159668\n",
      "(64, 33)\n",
      "step 28549, loss is 4.888496398925781\n",
      "(64, 33)\n",
      "step 28550, loss is 4.680765151977539\n",
      "(64, 33)\n",
      "step 28551, loss is 4.816514492034912\n",
      "(64, 33)\n",
      "step 28552, loss is 4.8012776374816895\n",
      "(64, 33)\n",
      "step 28553, loss is 4.9169230461120605\n",
      "(64, 33)\n",
      "step 28554, loss is 4.858448028564453\n",
      "(64, 33)\n",
      "step 28555, loss is 4.8076491355896\n",
      "(64, 33)\n",
      "step 28556, loss is 4.728567600250244\n",
      "(64, 33)\n",
      "step 28557, loss is 4.985149383544922\n",
      "(64, 33)\n",
      "step 28558, loss is 4.678742408752441\n",
      "(64, 33)\n",
      "step 28559, loss is 4.817747116088867\n",
      "(64, 33)\n",
      "step 28560, loss is 4.773068904876709\n",
      "(64, 33)\n",
      "step 28561, loss is 4.867914199829102\n",
      "(64, 33)\n",
      "step 28562, loss is 4.713848114013672\n",
      "(64, 33)\n",
      "step 28563, loss is 4.802230358123779\n",
      "(64, 33)\n",
      "step 28564, loss is 4.757486343383789\n",
      "(64, 33)\n",
      "step 28565, loss is 4.831791400909424\n",
      "(64, 33)\n",
      "step 28566, loss is 4.777714729309082\n",
      "(64, 33)\n",
      "step 28567, loss is 4.678474426269531\n",
      "(64, 33)\n",
      "step 28568, loss is 4.614784240722656\n",
      "(64, 33)\n",
      "step 28569, loss is 4.74668550491333\n",
      "(64, 33)\n",
      "step 28570, loss is 4.646058559417725\n",
      "(64, 33)\n",
      "step 28571, loss is 4.766499996185303\n",
      "(64, 33)\n",
      "step 28572, loss is 4.727216720581055\n",
      "(64, 33)\n",
      "step 28573, loss is 4.782092571258545\n",
      "(64, 33)\n",
      "step 28574, loss is 4.911337375640869\n",
      "(64, 33)\n",
      "step 28575, loss is 4.982901573181152\n",
      "(64, 33)\n",
      "step 28576, loss is 4.575798034667969\n",
      "(64, 33)\n",
      "step 28577, loss is 4.787043571472168\n",
      "(64, 33)\n",
      "step 28578, loss is 4.646331310272217\n",
      "(64, 33)\n",
      "step 28579, loss is 4.677024841308594\n",
      "(64, 33)\n",
      "step 28580, loss is 4.913419723510742\n",
      "(64, 33)\n",
      "step 28581, loss is 4.514629364013672\n",
      "(64, 33)\n",
      "step 28582, loss is 4.616306304931641\n",
      "(64, 33)\n",
      "step 28583, loss is 4.894495487213135\n",
      "(64, 33)\n",
      "step 28584, loss is 4.592743396759033\n",
      "(64, 33)\n",
      "step 28585, loss is 4.734963417053223\n",
      "(64, 33)\n",
      "step 28586, loss is 4.703916072845459\n",
      "(64, 33)\n",
      "step 28587, loss is 4.668274879455566\n",
      "(64, 33)\n",
      "step 28588, loss is 4.795325756072998\n",
      "(64, 33)\n",
      "step 28589, loss is 4.679871559143066\n",
      "(64, 33)\n",
      "step 28590, loss is 4.625309467315674\n",
      "(64, 33)\n",
      "step 28591, loss is 4.864105701446533\n",
      "(64, 33)\n",
      "step 28592, loss is 4.768650054931641\n",
      "(64, 33)\n",
      "step 28593, loss is 4.87362003326416\n",
      "(64, 33)\n",
      "step 28594, loss is 4.888827800750732\n",
      "(64, 33)\n",
      "step 28595, loss is 4.802065849304199\n",
      "(64, 33)\n",
      "step 28596, loss is 4.744736671447754\n",
      "(64, 33)\n",
      "step 28597, loss is 4.876308441162109\n",
      "(64, 33)\n",
      "step 28598, loss is 4.906134605407715\n",
      "(64, 33)\n",
      "step 28599, loss is 4.6671223640441895\n",
      "(64, 33)\n",
      "step 28600, loss is 4.651834011077881\n",
      "(64, 33)\n",
      "step 28601, loss is 4.746707916259766\n",
      "(64, 33)\n",
      "step 28602, loss is 4.916730880737305\n",
      "(64, 33)\n",
      "step 28603, loss is 4.709455966949463\n",
      "(64, 33)\n",
      "step 28604, loss is 4.735244274139404\n",
      "(64, 33)\n",
      "step 28605, loss is 4.784872531890869\n",
      "(64, 33)\n",
      "step 28606, loss is 4.79597282409668\n",
      "(64, 33)\n",
      "step 28607, loss is 4.837717056274414\n",
      "(64, 33)\n",
      "step 28608, loss is 4.667943954467773\n",
      "(64, 33)\n",
      "step 28609, loss is 4.7398881912231445\n",
      "(64, 33)\n",
      "step 28610, loss is 4.834460735321045\n",
      "(64, 33)\n",
      "step 28611, loss is 4.8724365234375\n",
      "(64, 33)\n",
      "step 28612, loss is 4.599752902984619\n",
      "(64, 33)\n",
      "step 28613, loss is 4.819769859313965\n",
      "(64, 33)\n",
      "step 28614, loss is 4.920495986938477\n",
      "(64, 33)\n",
      "step 28615, loss is 4.862854957580566\n",
      "(64, 33)\n",
      "step 28616, loss is 4.767702102661133\n",
      "(64, 33)\n",
      "step 28617, loss is 4.781289577484131\n",
      "(64, 33)\n",
      "step 28618, loss is 4.779529094696045\n",
      "(64, 33)\n",
      "step 28619, loss is 4.729728698730469\n",
      "(64, 33)\n",
      "step 28620, loss is 4.728172302246094\n",
      "(64, 33)\n",
      "step 28621, loss is 4.82695198059082\n",
      "(64, 33)\n",
      "step 28622, loss is 4.848116874694824\n",
      "(64, 33)\n",
      "step 28623, loss is 4.568779468536377\n",
      "(64, 33)\n",
      "step 28624, loss is 4.681551933288574\n",
      "(64, 33)\n",
      "step 28625, loss is 4.922919750213623\n",
      "(64, 33)\n",
      "step 28626, loss is 4.831962585449219\n",
      "(64, 33)\n",
      "step 28627, loss is 4.595731735229492\n",
      "(64, 33)\n",
      "step 28628, loss is 4.849446773529053\n",
      "(64, 33)\n",
      "step 28629, loss is 4.832468032836914\n",
      "(64, 33)\n",
      "step 28630, loss is 4.687932014465332\n",
      "(64, 33)\n",
      "step 28631, loss is 4.882478713989258\n",
      "(64, 33)\n",
      "step 28632, loss is 4.695682525634766\n",
      "(64, 33)\n",
      "step 28633, loss is 4.676969528198242\n",
      "(64, 33)\n",
      "step 28634, loss is 4.647859573364258\n",
      "(64, 33)\n",
      "step 28635, loss is 4.810438632965088\n",
      "(64, 33)\n",
      "step 28636, loss is 4.701735496520996\n",
      "(64, 33)\n",
      "step 28637, loss is 4.833618640899658\n",
      "(64, 33)\n",
      "step 28638, loss is 4.733634948730469\n",
      "(64, 33)\n",
      "step 28639, loss is 4.8467936515808105\n",
      "(64, 33)\n",
      "step 28640, loss is 4.796402454376221\n",
      "(64, 33)\n",
      "step 28641, loss is 4.80368709564209\n",
      "(64, 33)\n",
      "step 28642, loss is 4.800019264221191\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 28643, loss is 4.627325057983398\n",
      "(64, 33)\n",
      "step 28644, loss is 4.727497577667236\n",
      "(64, 33)\n",
      "step 28645, loss is 4.658094882965088\n",
      "(64, 33)\n",
      "step 28646, loss is 4.805325508117676\n",
      "(64, 33)\n",
      "step 28647, loss is 4.802839756011963\n",
      "(64, 33)\n",
      "step 28648, loss is 4.880437850952148\n",
      "(64, 33)\n",
      "step 28649, loss is 4.813439846038818\n",
      "(64, 33)\n",
      "step 28650, loss is 4.781628131866455\n",
      "(64, 33)\n",
      "step 28651, loss is 4.724642276763916\n",
      "(64, 33)\n",
      "step 28652, loss is 4.869589328765869\n",
      "(64, 33)\n",
      "step 28653, loss is 4.774110794067383\n",
      "(64, 33)\n",
      "step 28654, loss is 4.681857109069824\n",
      "(64, 33)\n",
      "step 28655, loss is 4.634557723999023\n",
      "(64, 33)\n",
      "step 28656, loss is 4.850286483764648\n",
      "(64, 33)\n",
      "step 28657, loss is 4.862408638000488\n",
      "(64, 33)\n",
      "step 28658, loss is 4.773990631103516\n",
      "(64, 33)\n",
      "step 28659, loss is 4.693471431732178\n",
      "(64, 33)\n",
      "step 28660, loss is 4.796075344085693\n",
      "(64, 33)\n",
      "step 28661, loss is 4.771478652954102\n",
      "(64, 33)\n",
      "step 28662, loss is 4.601004123687744\n",
      "(64, 33)\n",
      "step 28663, loss is 4.87971305847168\n",
      "(64, 33)\n",
      "step 28664, loss is 4.758625507354736\n",
      "(64, 33)\n",
      "step 28665, loss is 4.797049522399902\n",
      "(64, 33)\n",
      "step 28666, loss is 4.752417087554932\n",
      "(64, 33)\n",
      "step 28667, loss is 4.6794867515563965\n",
      "(64, 33)\n",
      "step 28668, loss is 4.921860694885254\n",
      "(64, 33)\n",
      "step 28669, loss is 4.88046932220459\n",
      "(64, 33)\n",
      "step 28670, loss is 4.716545581817627\n",
      "(64, 33)\n",
      "step 28671, loss is 4.7107253074646\n",
      "(64, 33)\n",
      "step 28672, loss is 4.847973346710205\n",
      "(64, 33)\n",
      "step 28673, loss is 4.701960563659668\n",
      "(64, 33)\n",
      "step 28674, loss is 4.797826766967773\n",
      "(64, 33)\n",
      "step 28675, loss is 4.9349589347839355\n",
      "(64, 33)\n",
      "step 28676, loss is 4.778704643249512\n",
      "(64, 33)\n",
      "step 28677, loss is 4.725499629974365\n",
      "(64, 33)\n",
      "step 28678, loss is 4.80422306060791\n",
      "(64, 33)\n",
      "step 28679, loss is 4.766726493835449\n",
      "(64, 33)\n",
      "step 28680, loss is 4.757326126098633\n",
      "(64, 33)\n",
      "step 28681, loss is 4.908182621002197\n",
      "(64, 33)\n",
      "step 28682, loss is 4.838696479797363\n",
      "(64, 33)\n",
      "step 28683, loss is 4.665015697479248\n",
      "(64, 33)\n",
      "step 28684, loss is 4.58555269241333\n",
      "(64, 33)\n",
      "step 28685, loss is 4.857613563537598\n",
      "(64, 33)\n",
      "step 28686, loss is 4.892638206481934\n",
      "(64, 33)\n",
      "step 28687, loss is 4.808804988861084\n",
      "(64, 33)\n",
      "step 28688, loss is 4.904902458190918\n",
      "(64, 33)\n",
      "step 28689, loss is 4.765322685241699\n",
      "(64, 33)\n",
      "step 28690, loss is 4.63136100769043\n",
      "(64, 33)\n",
      "step 28691, loss is 4.79459810256958\n",
      "(64, 33)\n",
      "step 28692, loss is 4.869866371154785\n",
      "(64, 33)\n",
      "step 28693, loss is 4.813913345336914\n",
      "(64, 33)\n",
      "step 28694, loss is 4.732946395874023\n",
      "(64, 33)\n",
      "step 28695, loss is 4.770013809204102\n",
      "(64, 33)\n",
      "step 28696, loss is 4.766019821166992\n",
      "(64, 33)\n",
      "step 28697, loss is 4.744577407836914\n",
      "(64, 33)\n",
      "step 28698, loss is 4.865197658538818\n",
      "(64, 33)\n",
      "step 28699, loss is 4.874878406524658\n",
      "(64, 33)\n",
      "step 28700, loss is 4.709057807922363\n",
      "(64, 33)\n",
      "step 28701, loss is 4.797552585601807\n",
      "(64, 33)\n",
      "step 28702, loss is 4.701787948608398\n",
      "(64, 33)\n",
      "step 28703, loss is 4.65531587600708\n",
      "(64, 33)\n",
      "step 28704, loss is 4.798375129699707\n",
      "(64, 33)\n",
      "step 28705, loss is 4.683473587036133\n",
      "(64, 33)\n",
      "step 28706, loss is 4.721655368804932\n",
      "(64, 33)\n",
      "step 28707, loss is 4.699464321136475\n",
      "(64, 33)\n",
      "step 28708, loss is 4.839949607849121\n",
      "(64, 33)\n",
      "step 28709, loss is 4.978216171264648\n",
      "(64, 33)\n",
      "step 28710, loss is 4.861961364746094\n",
      "(64, 33)\n",
      "step 28711, loss is 4.888561248779297\n",
      "(64, 33)\n",
      "step 28712, loss is 4.847219944000244\n",
      "(64, 33)\n",
      "step 28713, loss is 4.627236366271973\n",
      "(64, 33)\n",
      "step 28714, loss is 4.882781505584717\n",
      "(64, 33)\n",
      "step 28715, loss is 4.711350440979004\n",
      "(64, 33)\n",
      "step 28716, loss is 4.7061076164245605\n",
      "(64, 33)\n",
      "step 28717, loss is 4.912528991699219\n",
      "(64, 33)\n",
      "step 28718, loss is 4.7767653465271\n",
      "(64, 33)\n",
      "step 28719, loss is 4.766313076019287\n",
      "(64, 33)\n",
      "step 28720, loss is 4.858940124511719\n",
      "(64, 33)\n",
      "step 28721, loss is 4.718015670776367\n",
      "(64, 33)\n",
      "step 28722, loss is 4.698310852050781\n",
      "(64, 33)\n",
      "step 28723, loss is 4.786758899688721\n",
      "(64, 33)\n",
      "step 28724, loss is 4.762050151824951\n",
      "(64, 33)\n",
      "step 28725, loss is 4.735363483428955\n",
      "(64, 33)\n",
      "step 28726, loss is 4.881877422332764\n",
      "(64, 33)\n",
      "step 28727, loss is 4.921494960784912\n",
      "(64, 33)\n",
      "step 28728, loss is 4.757814884185791\n",
      "(64, 33)\n",
      "step 28729, loss is 4.838083744049072\n",
      "(64, 33)\n",
      "step 28730, loss is 4.465987682342529\n",
      "(64, 33)\n",
      "step 28731, loss is 4.719494342803955\n",
      "(64, 33)\n",
      "step 28732, loss is 4.675434589385986\n",
      "(64, 33)\n",
      "step 28733, loss is 4.727726459503174\n",
      "(64, 33)\n",
      "step 28734, loss is 4.916914939880371\n",
      "(64, 33)\n",
      "step 28735, loss is 4.893166542053223\n",
      "(64, 33)\n",
      "step 28736, loss is 4.928713321685791\n",
      "(64, 33)\n",
      "step 28737, loss is 4.812536239624023\n",
      "(64, 33)\n",
      "step 28738, loss is 4.77805233001709\n",
      "(64, 33)\n",
      "step 28739, loss is 4.780101299285889\n",
      "(64, 33)\n",
      "step 28740, loss is 4.888319492340088\n",
      "(64, 33)\n",
      "step 28741, loss is 4.685854911804199\n",
      "(64, 33)\n",
      "step 28742, loss is 4.803181171417236\n",
      "(64, 33)\n",
      "step 28743, loss is 4.9137864112854\n",
      "(64, 33)\n",
      "step 28744, loss is 4.833133697509766\n",
      "(64, 33)\n",
      "step 28745, loss is 4.616343021392822\n",
      "(64, 33)\n",
      "step 28746, loss is 4.6900715827941895\n",
      "(64, 33)\n",
      "step 28747, loss is 4.831258296966553\n",
      "(64, 33)\n",
      "step 28748, loss is 4.677565574645996\n",
      "(64, 33)\n",
      "step 28749, loss is 4.7522382736206055\n",
      "(64, 33)\n",
      "step 28750, loss is 4.636425495147705\n",
      "(64, 33)\n",
      "step 28751, loss is 4.786271095275879\n",
      "(64, 33)\n",
      "step 28752, loss is 4.640678405761719\n",
      "(64, 33)\n",
      "step 28753, loss is 4.7886552810668945\n",
      "(64, 33)\n",
      "step 28754, loss is 4.65397834777832\n",
      "(64, 33)\n",
      "step 28755, loss is 4.718812942504883\n",
      "(64, 33)\n",
      "step 28756, loss is 4.653091907501221\n",
      "(64, 33)\n",
      "step 28757, loss is 4.81334114074707\n",
      "(64, 33)\n",
      "step 28758, loss is 4.899166584014893\n",
      "(64, 33)\n",
      "step 28759, loss is 4.775078296661377\n",
      "(64, 33)\n",
      "step 28760, loss is 4.696934700012207\n",
      "(64, 33)\n",
      "step 28761, loss is 4.641783714294434\n",
      "(64, 33)\n",
      "step 28762, loss is 4.8675079345703125\n",
      "(64, 33)\n",
      "step 28763, loss is 4.715081214904785\n",
      "(64, 33)\n",
      "step 28764, loss is 4.755125522613525\n",
      "(64, 33)\n",
      "step 28765, loss is 4.597469806671143\n",
      "(64, 33)\n",
      "step 28766, loss is 4.898680210113525\n",
      "(64, 33)\n",
      "step 28767, loss is 4.905043125152588\n",
      "(64, 33)\n",
      "step 28768, loss is 4.940948009490967\n",
      "(64, 33)\n",
      "step 28769, loss is 4.7350616455078125\n",
      "(64, 33)\n",
      "step 28770, loss is 4.906527519226074\n",
      "(64, 33)\n",
      "step 28771, loss is 4.689162731170654\n",
      "(64, 33)\n",
      "step 28772, loss is 4.801852226257324\n",
      "(64, 33)\n",
      "step 28773, loss is 4.969584941864014\n",
      "(64, 33)\n",
      "step 28774, loss is 4.71686315536499\n",
      "(64, 33)\n",
      "step 28775, loss is 4.731795310974121\n",
      "(64, 33)\n",
      "step 28776, loss is 4.862089157104492\n",
      "(64, 33)\n",
      "step 28777, loss is 4.704503059387207\n",
      "(64, 33)\n",
      "step 28778, loss is 4.803838729858398\n",
      "(64, 33)\n",
      "step 28779, loss is 4.684484481811523\n",
      "(64, 33)\n",
      "step 28780, loss is 4.741869926452637\n",
      "(64, 33)\n",
      "step 28781, loss is 4.868128776550293\n",
      "(64, 33)\n",
      "step 28782, loss is 4.9615912437438965\n",
      "(64, 33)\n",
      "step 28783, loss is 4.861259460449219\n",
      "(64, 33)\n",
      "step 28784, loss is 4.9062581062316895\n",
      "(64, 33)\n",
      "step 28785, loss is 4.852299213409424\n",
      "(64, 33)\n",
      "step 28786, loss is 4.832932949066162\n",
      "(64, 33)\n",
      "step 28787, loss is 4.939135551452637\n",
      "(64, 33)\n",
      "step 28788, loss is 4.799165725708008\n",
      "(64, 33)\n",
      "step 28789, loss is 4.731889724731445\n",
      "(64, 33)\n",
      "step 28790, loss is 4.74976110458374\n",
      "(64, 33)\n",
      "step 28791, loss is 4.764013290405273\n",
      "(64, 33)\n",
      "step 28792, loss is 4.8680830001831055\n",
      "(64, 33)\n",
      "step 28793, loss is 4.779962539672852\n",
      "(64, 33)\n",
      "step 28794, loss is 4.8515777587890625\n",
      "(64, 33)\n",
      "step 28795, loss is 4.786121845245361\n",
      "(64, 33)\n",
      "step 28796, loss is 4.775747776031494\n",
      "(64, 33)\n",
      "step 28797, loss is 4.854988098144531\n",
      "(64, 33)\n",
      "step 28798, loss is 4.8841776847839355\n",
      "(64, 33)\n",
      "step 28799, loss is 4.64442777633667\n",
      "(64, 33)\n",
      "step 28800, loss is 4.827755928039551\n",
      "(64, 33)\n",
      "step 28801, loss is 4.711195468902588\n",
      "(64, 33)\n",
      "step 28802, loss is 4.860954284667969\n",
      "(64, 33)\n",
      "step 28803, loss is 4.7171854972839355\n",
      "(64, 33)\n",
      "step 28804, loss is 4.779691696166992\n",
      "(64, 33)\n",
      "step 28805, loss is 4.6710734367370605\n",
      "(64, 33)\n",
      "step 28806, loss is 4.789403438568115\n",
      "(64, 33)\n",
      "step 28807, loss is 4.617337226867676\n",
      "(64, 33)\n",
      "step 28808, loss is 4.905129909515381\n",
      "(64, 33)\n",
      "step 28809, loss is 4.688285827636719\n",
      "(64, 33)\n",
      "step 28810, loss is 4.623260498046875\n",
      "(64, 33)\n",
      "step 28811, loss is 4.860052108764648\n",
      "(64, 33)\n",
      "step 28812, loss is 4.5465850830078125\n",
      "(64, 33)\n",
      "step 28813, loss is 4.623882293701172\n",
      "(64, 33)\n",
      "step 28814, loss is 4.70442008972168\n",
      "(64, 33)\n",
      "step 28815, loss is 4.829102039337158\n",
      "(64, 33)\n",
      "step 28816, loss is 4.7328314781188965\n",
      "(64, 33)\n",
      "step 28817, loss is 4.713837623596191\n",
      "(64, 33)\n",
      "step 28818, loss is 4.547940254211426\n",
      "(64, 33)\n",
      "step 28819, loss is 4.681358814239502\n",
      "(64, 33)\n",
      "step 28820, loss is 4.8347039222717285\n",
      "(64, 33)\n",
      "step 28821, loss is 4.7000322341918945\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 28822, loss is 4.774807929992676\n",
      "(64, 33)\n",
      "step 28823, loss is 4.671509742736816\n",
      "(64, 33)\n",
      "step 28824, loss is 4.637575149536133\n",
      "(64, 33)\n",
      "step 28825, loss is 4.971594333648682\n",
      "(64, 33)\n",
      "step 28826, loss is 4.80762243270874\n",
      "(64, 33)\n",
      "step 28827, loss is 4.834773540496826\n",
      "(64, 33)\n",
      "step 28828, loss is 4.7282586097717285\n",
      "(64, 33)\n",
      "step 28829, loss is 4.714106559753418\n",
      "(64, 33)\n",
      "step 28830, loss is 4.8119306564331055\n",
      "(64, 33)\n",
      "step 28831, loss is 4.836193084716797\n",
      "(64, 33)\n",
      "step 28832, loss is 4.839776992797852\n",
      "(64, 33)\n",
      "step 28833, loss is 4.7352471351623535\n",
      "(64, 33)\n",
      "step 28834, loss is 4.741090774536133\n",
      "(64, 33)\n",
      "step 28835, loss is 4.754502773284912\n",
      "(64, 33)\n",
      "step 28836, loss is 4.8242621421813965\n",
      "(64, 33)\n",
      "step 28837, loss is 4.88538122177124\n",
      "(64, 33)\n",
      "step 28838, loss is 4.820755481719971\n",
      "(64, 33)\n",
      "step 28839, loss is 4.728574752807617\n",
      "(64, 33)\n",
      "step 28840, loss is 4.815133571624756\n",
      "(64, 33)\n",
      "step 28841, loss is 4.933615684509277\n",
      "(64, 33)\n",
      "step 28842, loss is 4.965292930603027\n",
      "(64, 33)\n",
      "step 28843, loss is 4.812774181365967\n",
      "(64, 33)\n",
      "step 28844, loss is 4.846229076385498\n",
      "(64, 33)\n",
      "step 28845, loss is 4.717513084411621\n",
      "(64, 33)\n",
      "step 28846, loss is 4.7264404296875\n",
      "(64, 33)\n",
      "step 28847, loss is 4.767539978027344\n",
      "(64, 33)\n",
      "step 28848, loss is 4.779751777648926\n",
      "(64, 33)\n",
      "step 28849, loss is 4.922420501708984\n",
      "(64, 33)\n",
      "step 28850, loss is 4.67829704284668\n",
      "(64, 33)\n",
      "step 28851, loss is 4.947749614715576\n",
      "(64, 33)\n",
      "step 28852, loss is 4.665915489196777\n",
      "(64, 33)\n",
      "step 28853, loss is 4.857256889343262\n",
      "(64, 33)\n",
      "step 28854, loss is 4.6335930824279785\n",
      "(64, 33)\n",
      "step 28855, loss is 4.675225257873535\n",
      "(64, 33)\n",
      "step 28856, loss is 4.7666096687316895\n",
      "(64, 33)\n",
      "step 28857, loss is 4.764011383056641\n",
      "(64, 33)\n",
      "step 28858, loss is 4.910778999328613\n",
      "(64, 33)\n",
      "step 28859, loss is 4.852865219116211\n",
      "(64, 33)\n",
      "step 28860, loss is 4.9250712394714355\n",
      "(64, 33)\n",
      "step 28861, loss is 4.714250087738037\n",
      "(64, 33)\n",
      "step 28862, loss is 4.8665032386779785\n",
      "(64, 33)\n",
      "step 28863, loss is 4.698568820953369\n",
      "(64, 33)\n",
      "step 28864, loss is 4.710798263549805\n",
      "(64, 33)\n",
      "step 28865, loss is 4.777790546417236\n",
      "(64, 33)\n",
      "step 28866, loss is 4.7921671867370605\n",
      "(64, 33)\n",
      "step 28867, loss is 4.906944274902344\n",
      "(64, 33)\n",
      "step 28868, loss is 4.886417388916016\n",
      "(64, 33)\n",
      "step 28869, loss is 4.639344692230225\n",
      "(64, 33)\n",
      "step 28870, loss is 4.90101957321167\n",
      "(64, 33)\n",
      "step 28871, loss is 4.8056464195251465\n",
      "(64, 33)\n",
      "step 28872, loss is 4.780282497406006\n",
      "(64, 33)\n",
      "step 28873, loss is 4.69766902923584\n",
      "(64, 33)\n",
      "step 28874, loss is 4.908329963684082\n",
      "(64, 33)\n",
      "step 28875, loss is 4.921963214874268\n",
      "(64, 33)\n",
      "step 28876, loss is 4.936770439147949\n",
      "(64, 33)\n",
      "step 28877, loss is 4.748024940490723\n",
      "(64, 33)\n",
      "step 28878, loss is 4.680194854736328\n",
      "(64, 33)\n",
      "step 28879, loss is 4.709120273590088\n",
      "(64, 33)\n",
      "step 28880, loss is 4.815659523010254\n",
      "(64, 33)\n",
      "step 28881, loss is 4.974503517150879\n",
      "(64, 33)\n",
      "step 28882, loss is 4.620096683502197\n",
      "(64, 33)\n",
      "step 28883, loss is 4.816958427429199\n",
      "(64, 33)\n",
      "step 28884, loss is 4.851966381072998\n",
      "(64, 33)\n",
      "step 28885, loss is 4.851448059082031\n",
      "(64, 33)\n",
      "step 28886, loss is 4.6437153816223145\n",
      "(64, 33)\n",
      "step 28887, loss is 4.941585063934326\n",
      "(64, 33)\n",
      "step 28888, loss is 5.010714054107666\n",
      "(64, 33)\n",
      "step 28889, loss is 4.91611385345459\n",
      "(64, 33)\n",
      "step 28890, loss is 4.909969806671143\n",
      "(64, 33)\n",
      "step 28891, loss is 4.775611877441406\n",
      "(64, 33)\n",
      "step 28892, loss is 4.899527549743652\n",
      "(64, 33)\n",
      "step 28893, loss is 4.868020057678223\n",
      "(64, 33)\n",
      "step 28894, loss is 4.736721992492676\n",
      "(64, 33)\n",
      "step 28895, loss is 4.427696228027344\n",
      "(64, 33)\n",
      "step 28896, loss is 4.769950866699219\n",
      "(64, 33)\n",
      "step 28897, loss is 4.786327362060547\n",
      "(64, 33)\n",
      "step 28898, loss is 4.803993225097656\n",
      "(64, 33)\n",
      "step 28899, loss is 4.587766647338867\n",
      "(64, 33)\n",
      "step 28900, loss is 4.708826541900635\n",
      "(64, 33)\n",
      "step 28901, loss is 4.790823936462402\n",
      "(64, 33)\n",
      "step 28902, loss is 4.960944652557373\n",
      "(64, 33)\n",
      "step 28903, loss is 4.770796298980713\n",
      "(64, 33)\n",
      "step 28904, loss is 4.7873358726501465\n",
      "(64, 33)\n",
      "step 28905, loss is 4.704070091247559\n",
      "(64, 33)\n",
      "step 28906, loss is 4.688307285308838\n",
      "(64, 33)\n",
      "step 28907, loss is 4.779059410095215\n",
      "(64, 33)\n",
      "step 28908, loss is 4.729708194732666\n",
      "(64, 33)\n",
      "step 28909, loss is 4.670471668243408\n",
      "(64, 33)\n",
      "step 28910, loss is 4.815361976623535\n",
      "(64, 33)\n",
      "step 28911, loss is 4.692984104156494\n",
      "(64, 33)\n",
      "step 28912, loss is 4.759956359863281\n",
      "(64, 33)\n",
      "step 28913, loss is 4.647519111633301\n",
      "(64, 33)\n",
      "step 28914, loss is 4.786355495452881\n",
      "(64, 33)\n",
      "step 28915, loss is 4.58145809173584\n",
      "(64, 33)\n",
      "step 28916, loss is 4.814174652099609\n",
      "(64, 33)\n",
      "step 28917, loss is 4.7025580406188965\n",
      "(64, 33)\n",
      "step 28918, loss is 4.945957660675049\n",
      "(64, 33)\n",
      "step 28919, loss is 4.647205829620361\n",
      "(64, 33)\n",
      "step 28920, loss is 4.677151203155518\n",
      "(64, 33)\n",
      "step 28921, loss is 4.852823257446289\n",
      "(64, 33)\n",
      "step 28922, loss is 4.872878074645996\n",
      "(64, 33)\n",
      "step 28923, loss is 4.63886022567749\n",
      "(64, 33)\n",
      "step 28924, loss is 4.674895286560059\n",
      "(64, 33)\n",
      "step 28925, loss is 4.830374240875244\n",
      "(64, 33)\n",
      "step 28926, loss is 4.8409576416015625\n",
      "(64, 33)\n",
      "step 28927, loss is 4.737312316894531\n",
      "(64, 33)\n",
      "step 28928, loss is 4.8489179611206055\n",
      "(64, 33)\n",
      "step 28929, loss is 4.865776062011719\n",
      "(64, 33)\n",
      "step 28930, loss is 4.820568561553955\n",
      "(64, 33)\n",
      "step 28931, loss is 4.698848724365234\n",
      "(64, 33)\n",
      "step 28932, loss is 4.812187671661377\n",
      "(64, 33)\n",
      "step 28933, loss is 4.8342509269714355\n",
      "(64, 33)\n",
      "step 28934, loss is 4.780601501464844\n",
      "(64, 33)\n",
      "step 28935, loss is 4.776609897613525\n",
      "(64, 33)\n",
      "step 28936, loss is 4.740372180938721\n",
      "(64, 33)\n",
      "step 28937, loss is 4.868876934051514\n",
      "(64, 33)\n",
      "step 28938, loss is 4.815386772155762\n",
      "(64, 33)\n",
      "step 28939, loss is 4.623446941375732\n",
      "(64, 33)\n",
      "step 28940, loss is 4.7587571144104\n",
      "(64, 33)\n",
      "step 28941, loss is 5.041910171508789\n",
      "(64, 33)\n",
      "step 28942, loss is 4.781266212463379\n",
      "(64, 33)\n",
      "step 28943, loss is 4.779996871948242\n",
      "(64, 33)\n",
      "step 28944, loss is 4.764654636383057\n",
      "(64, 33)\n",
      "step 28945, loss is 4.69824743270874\n",
      "(64, 33)\n",
      "step 28946, loss is 4.827953815460205\n",
      "(64, 33)\n",
      "step 28947, loss is 4.773392200469971\n",
      "(64, 33)\n",
      "step 28948, loss is 4.704316139221191\n",
      "(64, 33)\n",
      "step 28949, loss is 4.6493611335754395\n",
      "(64, 33)\n",
      "step 28950, loss is 4.810400009155273\n",
      "(64, 33)\n",
      "step 28951, loss is 4.693614482879639\n",
      "(64, 33)\n",
      "step 28952, loss is 4.7584757804870605\n",
      "(64, 33)\n",
      "step 28953, loss is 4.739471435546875\n",
      "(64, 33)\n",
      "step 28954, loss is 4.637701034545898\n",
      "(64, 33)\n",
      "step 28955, loss is 4.731579780578613\n",
      "(64, 33)\n",
      "step 28956, loss is 4.816528797149658\n",
      "(64, 33)\n",
      "step 28957, loss is 4.855684757232666\n",
      "(64, 33)\n",
      "step 28958, loss is 4.80582857131958\n",
      "(64, 33)\n",
      "step 28959, loss is 4.68690824508667\n",
      "(64, 33)\n",
      "step 28960, loss is 4.771030426025391\n",
      "(64, 33)\n",
      "step 28961, loss is 4.790993690490723\n",
      "(64, 33)\n",
      "step 28962, loss is 4.821069240570068\n",
      "(64, 33)\n",
      "step 28963, loss is 4.667325496673584\n",
      "(64, 33)\n",
      "step 28964, loss is 4.845848083496094\n",
      "(64, 33)\n",
      "step 28965, loss is 4.726875305175781\n",
      "(64, 33)\n",
      "step 28966, loss is 4.805685043334961\n",
      "(64, 33)\n",
      "step 28967, loss is 4.947184085845947\n",
      "(64, 33)\n",
      "step 28968, loss is 4.671356201171875\n",
      "(64, 33)\n",
      "step 28969, loss is 4.8562188148498535\n",
      "(64, 33)\n",
      "step 28970, loss is 4.774184226989746\n",
      "(64, 33)\n",
      "step 28971, loss is 4.550211429595947\n",
      "(64, 33)\n",
      "step 28972, loss is 4.763764381408691\n",
      "(64, 33)\n",
      "step 28973, loss is 4.904356002807617\n",
      "(64, 33)\n",
      "step 28974, loss is 4.693731307983398\n",
      "(64, 33)\n",
      "step 28975, loss is 4.675167083740234\n",
      "(64, 33)\n",
      "step 28976, loss is 4.831649303436279\n",
      "(64, 33)\n",
      "step 28977, loss is 4.811936378479004\n",
      "(64, 33)\n",
      "step 28978, loss is 4.903088569641113\n",
      "(64, 33)\n",
      "step 28979, loss is 4.6115288734436035\n",
      "(64, 33)\n",
      "step 28980, loss is 4.855844020843506\n",
      "(64, 33)\n",
      "step 28981, loss is 4.706006050109863\n",
      "(64, 33)\n",
      "step 28982, loss is 4.77451229095459\n",
      "(64, 33)\n",
      "step 28983, loss is 4.845938205718994\n",
      "(64, 33)\n",
      "step 28984, loss is 4.773435115814209\n",
      "(64, 33)\n",
      "step 28985, loss is 4.774652481079102\n",
      "(64, 33)\n",
      "step 28986, loss is 4.753147125244141\n",
      "(64, 33)\n",
      "step 28987, loss is 4.553839683532715\n",
      "(64, 33)\n",
      "step 28988, loss is 4.688133716583252\n",
      "(64, 33)\n",
      "step 28989, loss is 4.84931755065918\n",
      "(64, 33)\n",
      "step 28990, loss is 4.840146064758301\n",
      "(64, 33)\n",
      "step 28991, loss is 4.723952770233154\n",
      "(64, 33)\n",
      "step 28992, loss is 4.722821235656738\n",
      "(64, 33)\n",
      "step 28993, loss is 4.787210941314697\n",
      "(64, 33)\n",
      "step 28994, loss is 4.834442138671875\n",
      "(64, 33)\n",
      "step 28995, loss is 4.783229827880859\n",
      "(64, 33)\n",
      "step 28996, loss is 4.669016361236572\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 28997, loss is 4.755679607391357\n",
      "(64, 33)\n",
      "step 28998, loss is 4.832063674926758\n",
      "(64, 33)\n",
      "step 28999, loss is 4.706270217895508\n",
      "(64, 33)\n",
      "step 29000, loss is 4.8191914558410645\n",
      "(64, 33)\n",
      "step 29001, loss is 4.611238956451416\n",
      "(64, 33)\n",
      "step 29002, loss is 4.751408576965332\n",
      "(64, 33)\n",
      "step 29003, loss is 4.779488563537598\n",
      "(64, 33)\n",
      "step 29004, loss is 4.8967132568359375\n",
      "(64, 33)\n",
      "step 29005, loss is 4.715421676635742\n",
      "(64, 33)\n",
      "step 29006, loss is 4.8096747398376465\n",
      "(64, 33)\n",
      "step 29007, loss is 4.908953666687012\n",
      "(64, 33)\n",
      "step 29008, loss is 4.97755765914917\n",
      "(64, 33)\n",
      "step 29009, loss is 4.885719299316406\n",
      "(64, 33)\n",
      "step 29010, loss is 4.766330718994141\n",
      "(64, 33)\n",
      "step 29011, loss is 4.743076324462891\n",
      "(64, 33)\n",
      "step 29012, loss is 4.9025092124938965\n",
      "(64, 33)\n",
      "step 29013, loss is 4.781039237976074\n",
      "(64, 33)\n",
      "step 29014, loss is 4.609352111816406\n",
      "(64, 33)\n",
      "step 29015, loss is 4.854541778564453\n",
      "(64, 33)\n",
      "step 29016, loss is 4.882028102874756\n",
      "(64, 33)\n",
      "step 29017, loss is 4.563976287841797\n",
      "(64, 33)\n",
      "step 29018, loss is 4.959461688995361\n",
      "(64, 33)\n",
      "step 29019, loss is 4.7815728187561035\n",
      "(64, 33)\n",
      "step 29020, loss is 5.0362548828125\n",
      "(64, 33)\n",
      "step 29021, loss is 4.612648963928223\n",
      "(64, 33)\n",
      "step 29022, loss is 4.75583553314209\n",
      "(64, 33)\n",
      "step 29023, loss is 4.8274407386779785\n",
      "(64, 33)\n",
      "step 29024, loss is 4.779775142669678\n",
      "(64, 33)\n",
      "step 29025, loss is 4.796305179595947\n",
      "(64, 33)\n",
      "step 29026, loss is 4.869680404663086\n",
      "(64, 33)\n",
      "step 29027, loss is 4.754608154296875\n",
      "(64, 33)\n",
      "step 29028, loss is 4.958803176879883\n",
      "(64, 33)\n",
      "step 29029, loss is 4.765981197357178\n",
      "(64, 33)\n",
      "step 29030, loss is 4.773859977722168\n",
      "(64, 33)\n",
      "step 29031, loss is 4.688811779022217\n",
      "(64, 33)\n",
      "step 29032, loss is 4.733740329742432\n",
      "(64, 33)\n",
      "step 29033, loss is 4.930599212646484\n",
      "(64, 33)\n",
      "step 29034, loss is 4.80439567565918\n",
      "(64, 33)\n",
      "step 29035, loss is 4.9181742668151855\n",
      "(64, 33)\n",
      "step 29036, loss is 4.639148712158203\n",
      "(64, 33)\n",
      "step 29037, loss is 4.820001125335693\n",
      "(64, 33)\n",
      "step 29038, loss is 4.768252849578857\n",
      "(64, 33)\n",
      "step 29039, loss is 4.714042663574219\n",
      "(64, 33)\n",
      "step 29040, loss is 4.8805012702941895\n",
      "(64, 33)\n",
      "step 29041, loss is 4.820202350616455\n",
      "(64, 33)\n",
      "step 29042, loss is 5.040328502655029\n",
      "(64, 33)\n",
      "step 29043, loss is 4.867581367492676\n",
      "(64, 33)\n",
      "step 29044, loss is 4.934612274169922\n",
      "(64, 33)\n",
      "step 29045, loss is 4.714672088623047\n",
      "(64, 33)\n",
      "step 29046, loss is 4.686814785003662\n",
      "(64, 33)\n",
      "step 29047, loss is 4.762432098388672\n",
      "(64, 33)\n",
      "step 29048, loss is 4.82017183303833\n",
      "(64, 33)\n",
      "step 29049, loss is 4.745673179626465\n",
      "(64, 33)\n",
      "step 29050, loss is 4.779032230377197\n",
      "(64, 33)\n",
      "step 29051, loss is 4.850740432739258\n",
      "(64, 33)\n",
      "step 29052, loss is 4.573458671569824\n",
      "(64, 33)\n",
      "step 29053, loss is 4.737328052520752\n",
      "(64, 33)\n",
      "step 29054, loss is 4.9044928550720215\n",
      "(64, 33)\n",
      "step 29055, loss is 4.696937561035156\n",
      "(64, 33)\n",
      "step 29056, loss is 4.773367404937744\n",
      "(64, 33)\n",
      "step 29057, loss is 4.734461307525635\n",
      "(64, 33)\n",
      "step 29058, loss is 4.94135856628418\n",
      "(64, 33)\n",
      "step 29059, loss is 4.769506931304932\n",
      "(64, 33)\n",
      "step 29060, loss is 4.714871406555176\n",
      "(64, 33)\n",
      "step 29061, loss is 4.887601852416992\n",
      "(64, 33)\n",
      "step 29062, loss is 4.797873497009277\n",
      "(64, 33)\n",
      "step 29063, loss is 4.748686790466309\n",
      "(64, 33)\n",
      "step 29064, loss is 4.807883262634277\n",
      "(64, 33)\n",
      "step 29065, loss is 4.868797302246094\n",
      "(64, 33)\n",
      "step 29066, loss is 4.854972839355469\n",
      "(64, 33)\n",
      "step 29067, loss is 4.9246745109558105\n",
      "(64, 33)\n",
      "step 29068, loss is 4.858340263366699\n",
      "(64, 33)\n",
      "step 29069, loss is 4.864978790283203\n",
      "(64, 33)\n",
      "step 29070, loss is 4.748770236968994\n",
      "(64, 33)\n",
      "step 29071, loss is 4.735198497772217\n",
      "(64, 33)\n",
      "step 29072, loss is 4.740904331207275\n",
      "(64, 33)\n",
      "step 29073, loss is 4.680853366851807\n",
      "(64, 33)\n",
      "step 29074, loss is 4.676835536956787\n",
      "(64, 33)\n",
      "step 29075, loss is 4.707403182983398\n",
      "(64, 33)\n",
      "step 29076, loss is 4.910920143127441\n",
      "(64, 33)\n",
      "step 29077, loss is 4.67083215713501\n",
      "(64, 33)\n",
      "step 29078, loss is 4.748719215393066\n",
      "(64, 33)\n",
      "step 29079, loss is 4.9256205558776855\n",
      "(64, 33)\n",
      "step 29080, loss is 4.732599258422852\n",
      "(64, 33)\n",
      "step 29081, loss is 4.839020252227783\n",
      "(64, 33)\n",
      "step 29082, loss is 4.810401916503906\n",
      "(64, 33)\n",
      "step 29083, loss is 5.0252485275268555\n",
      "(64, 33)\n",
      "step 29084, loss is 4.705002784729004\n",
      "(64, 33)\n",
      "step 29085, loss is 4.721556186676025\n",
      "(64, 33)\n",
      "step 29086, loss is 4.660948753356934\n",
      "(64, 33)\n",
      "step 29087, loss is 4.6529059410095215\n",
      "(64, 33)\n",
      "step 29088, loss is 4.7696380615234375\n",
      "(64, 33)\n",
      "step 29089, loss is 4.732041358947754\n",
      "(64, 33)\n",
      "step 29090, loss is 4.684811592102051\n",
      "(64, 33)\n",
      "step 29091, loss is 4.731717586517334\n",
      "(64, 33)\n",
      "step 29092, loss is 4.792446613311768\n",
      "(64, 33)\n",
      "step 29093, loss is 4.7963643074035645\n",
      "(64, 33)\n",
      "step 29094, loss is 4.857295036315918\n",
      "(64, 33)\n",
      "step 29095, loss is 4.793802738189697\n",
      "(64, 33)\n",
      "step 29096, loss is 4.653277397155762\n",
      "(64, 33)\n",
      "step 29097, loss is 4.7639570236206055\n",
      "(64, 33)\n",
      "step 29098, loss is 4.808248996734619\n",
      "(64, 33)\n",
      "step 29099, loss is 4.686810493469238\n",
      "(64, 33)\n",
      "step 29100, loss is 4.9139180183410645\n",
      "(64, 33)\n",
      "step 29101, loss is 4.908674716949463\n",
      "(64, 33)\n",
      "step 29102, loss is 4.716896057128906\n",
      "(64, 33)\n",
      "step 29103, loss is 4.59952974319458\n",
      "(64, 33)\n",
      "step 29104, loss is 4.932258129119873\n",
      "(64, 33)\n",
      "step 29105, loss is 4.794056415557861\n",
      "(64, 33)\n",
      "step 29106, loss is 4.886988639831543\n",
      "(64, 33)\n",
      "step 29107, loss is 4.677407741546631\n",
      "(64, 33)\n",
      "step 29108, loss is 4.852676868438721\n",
      "(64, 33)\n",
      "step 29109, loss is 4.795628547668457\n",
      "(64, 33)\n",
      "step 29110, loss is 4.699654579162598\n",
      "(64, 33)\n",
      "step 29111, loss is 4.566524505615234\n",
      "(64, 33)\n",
      "step 29112, loss is 4.874878883361816\n",
      "(64, 33)\n",
      "step 29113, loss is 4.67901086807251\n",
      "(64, 33)\n",
      "step 29114, loss is 4.997541427612305\n",
      "(64, 33)\n",
      "step 29115, loss is 4.793037414550781\n",
      "(64, 33)\n",
      "step 29116, loss is 4.816238880157471\n",
      "(64, 33)\n",
      "step 29117, loss is 4.726648330688477\n",
      "(64, 33)\n",
      "step 29118, loss is 4.853898048400879\n",
      "(64, 33)\n",
      "step 29119, loss is 4.751933574676514\n",
      "(64, 33)\n",
      "step 29120, loss is 4.747839450836182\n",
      "(64, 33)\n",
      "step 29121, loss is 5.014869689941406\n",
      "(64, 33)\n",
      "step 29122, loss is 4.640178680419922\n",
      "(64, 33)\n",
      "step 29123, loss is 4.785620212554932\n",
      "(64, 33)\n",
      "step 29124, loss is 4.750464916229248\n",
      "(64, 33)\n",
      "step 29125, loss is 4.721686363220215\n",
      "(64, 33)\n",
      "step 29126, loss is 4.596502304077148\n",
      "(64, 33)\n",
      "step 29127, loss is 4.693577766418457\n",
      "(64, 33)\n",
      "step 29128, loss is 4.925786972045898\n",
      "(64, 33)\n",
      "step 29129, loss is 4.697328567504883\n",
      "(64, 33)\n",
      "step 29130, loss is 4.716796398162842\n",
      "(64, 33)\n",
      "step 29131, loss is 4.772526264190674\n",
      "(64, 33)\n",
      "step 29132, loss is 4.320858955383301\n",
      "(64, 33)\n",
      "step 29133, loss is 4.614212989807129\n",
      "(64, 33)\n",
      "step 29134, loss is 4.6291680335998535\n",
      "(64, 33)\n",
      "step 29135, loss is 4.894796848297119\n",
      "(64, 33)\n",
      "step 29136, loss is 4.583556175231934\n",
      "(64, 33)\n",
      "step 29137, loss is 4.799589157104492\n",
      "(64, 33)\n",
      "step 29138, loss is 4.704070568084717\n",
      "(64, 33)\n",
      "step 29139, loss is 4.676520824432373\n",
      "(64, 33)\n",
      "step 29140, loss is 4.80518913269043\n",
      "(64, 33)\n",
      "step 29141, loss is 4.77871036529541\n",
      "(64, 33)\n",
      "step 29142, loss is 4.791347503662109\n",
      "(64, 33)\n",
      "step 29143, loss is 4.837482929229736\n",
      "(64, 33)\n",
      "step 29144, loss is 4.931896686553955\n",
      "(64, 33)\n",
      "step 29145, loss is 4.80387020111084\n",
      "(64, 33)\n",
      "step 29146, loss is 4.863515853881836\n",
      "(64, 33)\n",
      "step 29147, loss is 4.6290106773376465\n",
      "(64, 33)\n",
      "step 29148, loss is 4.734973430633545\n",
      "(64, 33)\n",
      "step 29149, loss is 4.840634822845459\n",
      "(64, 33)\n",
      "step 29150, loss is 4.968728542327881\n",
      "(64, 33)\n",
      "step 29151, loss is 4.88456916809082\n",
      "(64, 33)\n",
      "step 29152, loss is 4.660289764404297\n",
      "(64, 33)\n",
      "step 29153, loss is 4.736112594604492\n",
      "(64, 33)\n",
      "step 29154, loss is 4.8044304847717285\n",
      "(64, 33)\n",
      "step 29155, loss is 4.846187591552734\n",
      "(64, 33)\n",
      "step 29156, loss is 4.848870277404785\n",
      "(64, 33)\n",
      "step 29157, loss is 4.506814956665039\n",
      "(64, 33)\n",
      "step 29158, loss is 4.799966812133789\n",
      "(64, 33)\n",
      "step 29159, loss is 4.649228096008301\n",
      "(64, 33)\n",
      "step 29160, loss is 4.661298751831055\n",
      "(64, 33)\n",
      "step 29161, loss is 4.902888298034668\n",
      "(64, 33)\n",
      "step 29162, loss is 4.899291038513184\n",
      "(64, 33)\n",
      "step 29163, loss is 4.854288101196289\n",
      "(64, 33)\n",
      "step 29164, loss is 4.80302619934082\n",
      "(64, 33)\n",
      "step 29165, loss is 4.687716484069824\n",
      "(64, 33)\n",
      "step 29166, loss is 4.6318230628967285\n",
      "(64, 33)\n",
      "step 29167, loss is 4.788774013519287\n",
      "(64, 33)\n",
      "step 29168, loss is 4.879484176635742\n",
      "(64, 33)\n",
      "step 29169, loss is 4.837662696838379\n",
      "(64, 33)\n",
      "step 29170, loss is 4.870118141174316\n",
      "(64, 33)\n",
      "step 29171, loss is 4.934553146362305\n",
      "(64, 33)\n",
      "step 29172, loss is 4.893643379211426\n",
      "(64, 33)\n",
      "step 29173, loss is 4.767416477203369\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29174, loss is 4.753871440887451\n",
      "(64, 33)\n",
      "step 29175, loss is 4.669742584228516\n",
      "(64, 33)\n",
      "step 29176, loss is 4.787363529205322\n",
      "(64, 33)\n",
      "step 29177, loss is 4.565524578094482\n",
      "(64, 33)\n",
      "step 29178, loss is 4.82974910736084\n",
      "(64, 33)\n",
      "step 29179, loss is 4.847649097442627\n",
      "(64, 33)\n",
      "step 29180, loss is 4.757216453552246\n",
      "(64, 33)\n",
      "step 29181, loss is 4.881044387817383\n",
      "(64, 33)\n",
      "step 29182, loss is 4.692949295043945\n",
      "(64, 33)\n",
      "step 29183, loss is 4.6971755027771\n",
      "(64, 33)\n",
      "step 29184, loss is 4.774875640869141\n",
      "(64, 33)\n",
      "step 29185, loss is 4.6809210777282715\n",
      "(64, 33)\n",
      "step 29186, loss is 4.672016143798828\n",
      "(64, 33)\n",
      "step 29187, loss is 4.763399124145508\n",
      "(64, 33)\n",
      "step 29188, loss is 4.638334274291992\n",
      "(64, 33)\n",
      "step 29189, loss is 4.687083721160889\n",
      "(64, 33)\n",
      "step 29190, loss is 4.651739597320557\n",
      "(64, 33)\n",
      "step 29191, loss is 4.763761520385742\n",
      "(64, 33)\n",
      "step 29192, loss is 5.077674865722656\n",
      "(64, 33)\n",
      "step 29193, loss is 5.051395416259766\n",
      "(64, 33)\n",
      "step 29194, loss is 4.639379024505615\n",
      "(64, 33)\n",
      "step 29195, loss is 4.571932792663574\n",
      "(64, 33)\n",
      "step 29196, loss is 4.870237350463867\n",
      "(64, 33)\n",
      "step 29197, loss is 4.715462684631348\n",
      "(64, 33)\n",
      "step 29198, loss is 4.998408317565918\n",
      "(64, 33)\n",
      "step 29199, loss is 4.592801094055176\n",
      "(64, 33)\n",
      "step 29200, loss is 4.799159526824951\n",
      "(64, 33)\n",
      "step 29201, loss is 4.770235538482666\n",
      "(64, 33)\n",
      "step 29202, loss is 4.781618118286133\n",
      "(64, 33)\n",
      "step 29203, loss is 4.798975467681885\n",
      "(64, 33)\n",
      "step 29204, loss is 4.928175449371338\n",
      "(64, 33)\n",
      "step 29205, loss is 4.669790267944336\n",
      "(64, 33)\n",
      "step 29206, loss is 4.556353569030762\n",
      "(64, 33)\n",
      "step 29207, loss is 4.7158708572387695\n",
      "(64, 33)\n",
      "step 29208, loss is 4.772069454193115\n",
      "(64, 33)\n",
      "step 29209, loss is 4.8942155838012695\n",
      "(64, 33)\n",
      "step 29210, loss is 4.434326171875\n",
      "(64, 33)\n",
      "step 29211, loss is 4.650131702423096\n",
      "(64, 33)\n",
      "step 29212, loss is 4.8741984367370605\n",
      "(64, 33)\n",
      "step 29213, loss is 4.741305828094482\n",
      "(64, 33)\n",
      "step 29214, loss is 4.7065205574035645\n",
      "(64, 33)\n",
      "step 29215, loss is 4.771209239959717\n",
      "(64, 33)\n",
      "step 29216, loss is 4.764591217041016\n",
      "(64, 33)\n",
      "step 29217, loss is 4.510346412658691\n",
      "(64, 33)\n",
      "step 29218, loss is 4.778869152069092\n",
      "(64, 33)\n",
      "step 29219, loss is 4.894040584564209\n",
      "(64, 33)\n",
      "step 29220, loss is 4.803684234619141\n",
      "(64, 33)\n",
      "step 29221, loss is 4.826868534088135\n",
      "(64, 33)\n",
      "step 29222, loss is 4.687095642089844\n",
      "(64, 33)\n",
      "step 29223, loss is 4.6979289054870605\n",
      "(64, 33)\n",
      "step 29224, loss is 4.6988959312438965\n",
      "(64, 33)\n",
      "step 29225, loss is 4.762010097503662\n",
      "(64, 33)\n",
      "step 29226, loss is 4.732677936553955\n",
      "(64, 33)\n",
      "step 29227, loss is 4.875216007232666\n",
      "(64, 33)\n",
      "step 29228, loss is 4.759971618652344\n",
      "(64, 33)\n",
      "step 29229, loss is 4.711607456207275\n",
      "(64, 33)\n",
      "step 29230, loss is 4.817898273468018\n",
      "(64, 33)\n",
      "step 29231, loss is 4.77862024307251\n",
      "(64, 33)\n",
      "step 29232, loss is 4.623960494995117\n",
      "(64, 33)\n",
      "step 29233, loss is 4.66273832321167\n",
      "(64, 33)\n",
      "step 29234, loss is 4.837325096130371\n",
      "(64, 33)\n",
      "step 29235, loss is 4.689695358276367\n",
      "(64, 33)\n",
      "step 29236, loss is 4.698382377624512\n",
      "(64, 33)\n",
      "step 29237, loss is 4.640617370605469\n",
      "(64, 33)\n",
      "step 29238, loss is 4.739053249359131\n",
      "(64, 33)\n",
      "step 29239, loss is 4.76309061050415\n",
      "(64, 33)\n",
      "step 29240, loss is 4.6376752853393555\n",
      "(64, 33)\n",
      "step 29241, loss is 4.812386989593506\n",
      "(64, 33)\n",
      "step 29242, loss is 4.752614974975586\n",
      "(64, 33)\n",
      "step 29243, loss is 4.758311748504639\n",
      "(64, 33)\n",
      "step 29244, loss is 4.820314884185791\n",
      "(64, 33)\n",
      "step 29245, loss is 4.7594170570373535\n",
      "(64, 33)\n",
      "step 29246, loss is 4.629803657531738\n",
      "(64, 33)\n",
      "step 29247, loss is 4.797964096069336\n",
      "(64, 33)\n",
      "step 29248, loss is 4.862577438354492\n",
      "(64, 33)\n",
      "step 29249, loss is 4.785841464996338\n",
      "(64, 33)\n",
      "step 29250, loss is 4.627303123474121\n",
      "(64, 33)\n",
      "step 29251, loss is 4.708818435668945\n",
      "(64, 33)\n",
      "step 29252, loss is 4.6702752113342285\n",
      "(64, 33)\n",
      "step 29253, loss is 4.62033748626709\n",
      "(64, 33)\n",
      "step 29254, loss is 4.786113739013672\n",
      "(64, 33)\n",
      "step 29255, loss is 4.69417667388916\n",
      "(64, 33)\n",
      "step 29256, loss is 4.812233924865723\n",
      "(64, 33)\n",
      "step 29257, loss is 4.651044845581055\n",
      "(64, 33)\n",
      "step 29258, loss is 4.713128566741943\n",
      "(64, 33)\n",
      "step 29259, loss is 4.949518203735352\n",
      "(64, 33)\n",
      "step 29260, loss is 4.741606712341309\n",
      "(64, 33)\n",
      "step 29261, loss is 4.614579677581787\n",
      "(64, 33)\n",
      "step 29262, loss is 4.7328314781188965\n",
      "(64, 33)\n",
      "step 29263, loss is 4.914992332458496\n",
      "(64, 33)\n",
      "step 29264, loss is 4.596461772918701\n",
      "(64, 33)\n",
      "step 29265, loss is 4.807576656341553\n",
      "(64, 33)\n",
      "step 29266, loss is 4.659199237823486\n",
      "(64, 33)\n",
      "step 29267, loss is 4.9795308113098145\n",
      "(64, 33)\n",
      "step 29268, loss is 4.839636325836182\n",
      "(64, 33)\n",
      "step 29269, loss is 4.569538593292236\n",
      "(64, 33)\n",
      "step 29270, loss is 4.861382961273193\n",
      "(64, 33)\n",
      "step 29271, loss is 4.64162015914917\n",
      "(64, 33)\n",
      "step 29272, loss is 4.61137056350708\n",
      "(64, 33)\n",
      "step 29273, loss is 4.953107833862305\n",
      "(64, 33)\n",
      "step 29274, loss is 4.762986183166504\n",
      "(64, 33)\n",
      "step 29275, loss is 4.708077907562256\n",
      "(64, 33)\n",
      "step 29276, loss is 4.773218154907227\n",
      "(64, 33)\n",
      "step 29277, loss is 5.0272722244262695\n",
      "(64, 33)\n",
      "step 29278, loss is 4.6577935218811035\n",
      "(64, 33)\n",
      "step 29279, loss is 4.5301032066345215\n",
      "(64, 33)\n",
      "step 29280, loss is 4.93223237991333\n",
      "(64, 33)\n",
      "step 29281, loss is 4.803097724914551\n",
      "(64, 33)\n",
      "step 29282, loss is 4.629342555999756\n",
      "(64, 33)\n",
      "step 29283, loss is 4.809285640716553\n",
      "(64, 33)\n",
      "step 29284, loss is 4.71729850769043\n",
      "(64, 33)\n",
      "step 29285, loss is 4.574251174926758\n",
      "(64, 33)\n",
      "step 29286, loss is 4.681913375854492\n",
      "(64, 33)\n",
      "step 29287, loss is 4.6241302490234375\n",
      "(64, 33)\n",
      "step 29288, loss is 4.8725199699401855\n",
      "(64, 33)\n",
      "step 29289, loss is 4.886164665222168\n",
      "(64, 33)\n",
      "step 29290, loss is 4.653450012207031\n",
      "(64, 33)\n",
      "step 29291, loss is 4.875021457672119\n",
      "(64, 33)\n",
      "step 29292, loss is 4.796114444732666\n",
      "(64, 33)\n",
      "step 29293, loss is 4.7922043800354\n",
      "(64, 33)\n",
      "step 29294, loss is 4.825759410858154\n",
      "(64, 33)\n",
      "step 29295, loss is 4.855035305023193\n",
      "(64, 33)\n",
      "step 29296, loss is 4.861419200897217\n",
      "(64, 33)\n",
      "step 29297, loss is 4.816987991333008\n",
      "(64, 33)\n",
      "step 29298, loss is 4.956077575683594\n",
      "(64, 33)\n",
      "step 29299, loss is 4.780831336975098\n",
      "(64, 33)\n",
      "step 29300, loss is 4.820245265960693\n",
      "(64, 33)\n",
      "step 29301, loss is 4.721686840057373\n",
      "(64, 33)\n",
      "step 29302, loss is 4.819175720214844\n",
      "(64, 33)\n",
      "step 29303, loss is 4.6162004470825195\n",
      "(64, 33)\n",
      "step 29304, loss is 4.630825042724609\n",
      "(64, 33)\n",
      "step 29305, loss is 4.851296901702881\n",
      "(64, 33)\n",
      "step 29306, loss is 4.696824550628662\n",
      "(64, 33)\n",
      "step 29307, loss is 5.090447425842285\n",
      "(64, 33)\n",
      "step 29308, loss is 4.688753604888916\n",
      "(64, 33)\n",
      "step 29309, loss is 4.6895060539245605\n",
      "(64, 33)\n",
      "step 29310, loss is 4.7915496826171875\n",
      "(64, 33)\n",
      "step 29311, loss is 4.575701713562012\n",
      "(64, 33)\n",
      "step 29312, loss is 4.586747169494629\n",
      "(64, 33)\n",
      "step 29313, loss is 4.77181339263916\n",
      "(64, 33)\n",
      "step 29314, loss is 4.8560309410095215\n",
      "(64, 33)\n",
      "step 29315, loss is 4.711389541625977\n",
      "(64, 33)\n",
      "step 29316, loss is 4.951813697814941\n",
      "(64, 33)\n",
      "step 29317, loss is 4.70344352722168\n",
      "(64, 33)\n",
      "step 29318, loss is 4.842297077178955\n",
      "(64, 33)\n",
      "step 29319, loss is 4.900092124938965\n",
      "(64, 33)\n",
      "step 29320, loss is 4.765478134155273\n",
      "(64, 33)\n",
      "step 29321, loss is 4.836023807525635\n",
      "(64, 33)\n",
      "step 29322, loss is 4.654729843139648\n",
      "(64, 33)\n",
      "step 29323, loss is 4.8087334632873535\n",
      "(64, 33)\n",
      "step 29324, loss is 4.792445182800293\n",
      "(64, 33)\n",
      "step 29325, loss is 4.792605876922607\n",
      "(64, 33)\n",
      "step 29326, loss is 4.7708048820495605\n",
      "(64, 33)\n",
      "step 29327, loss is 4.909358024597168\n",
      "(64, 33)\n",
      "step 29328, loss is 4.548070907592773\n",
      "(64, 33)\n",
      "step 29329, loss is 4.718735218048096\n",
      "(64, 33)\n",
      "step 29330, loss is 4.7760467529296875\n",
      "(64, 33)\n",
      "step 29331, loss is 4.8683671951293945\n",
      "(64, 33)\n",
      "step 29332, loss is 4.727292060852051\n",
      "(64, 33)\n",
      "step 29333, loss is 4.540011882781982\n",
      "(64, 33)\n",
      "step 29334, loss is 4.728458404541016\n",
      "(64, 33)\n",
      "step 29335, loss is 4.772251605987549\n",
      "(64, 33)\n",
      "step 29336, loss is 4.890318393707275\n",
      "(64, 33)\n",
      "step 29337, loss is 4.514642238616943\n",
      "(64, 33)\n",
      "step 29338, loss is 4.810597896575928\n",
      "(64, 33)\n",
      "step 29339, loss is 4.771818161010742\n",
      "(64, 33)\n",
      "step 29340, loss is 4.925662517547607\n",
      "(64, 33)\n",
      "step 29341, loss is 4.803232669830322\n",
      "(64, 33)\n",
      "step 29342, loss is 4.673890113830566\n",
      "(64, 33)\n",
      "step 29343, loss is 4.689316749572754\n",
      "(64, 33)\n",
      "step 29344, loss is 4.6855645179748535\n",
      "(64, 33)\n",
      "step 29345, loss is 4.549183368682861\n",
      "(64, 33)\n",
      "step 29346, loss is 4.879461765289307\n",
      "(64, 33)\n",
      "step 29347, loss is 4.881008148193359\n",
      "(64, 33)\n",
      "step 29348, loss is 4.703609466552734\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29349, loss is 4.723244667053223\n",
      "(64, 33)\n",
      "step 29350, loss is 4.633552074432373\n",
      "(64, 33)\n",
      "step 29351, loss is 4.815372943878174\n",
      "(64, 33)\n",
      "step 29352, loss is 4.922111511230469\n",
      "(64, 33)\n",
      "step 29353, loss is 4.856612205505371\n",
      "(64, 33)\n",
      "step 29354, loss is 4.749457359313965\n",
      "(64, 33)\n",
      "step 29355, loss is 4.872519493103027\n",
      "(64, 33)\n",
      "step 29356, loss is 4.658689975738525\n",
      "(64, 33)\n",
      "step 29357, loss is 4.898336887359619\n",
      "(64, 33)\n",
      "step 29358, loss is 4.936718463897705\n",
      "(64, 33)\n",
      "step 29359, loss is 4.790779113769531\n",
      "(64, 33)\n",
      "step 29360, loss is 4.975872993469238\n",
      "(64, 33)\n",
      "step 29361, loss is 4.827083587646484\n",
      "(64, 33)\n",
      "step 29362, loss is 4.941197395324707\n",
      "(64, 33)\n",
      "step 29363, loss is 4.755491256713867\n",
      "(64, 33)\n",
      "step 29364, loss is 4.826354026794434\n",
      "(64, 33)\n",
      "step 29365, loss is 4.852843284606934\n",
      "(64, 33)\n",
      "step 29366, loss is 4.629059791564941\n",
      "(64, 33)\n",
      "step 29367, loss is 4.866458892822266\n",
      "(64, 33)\n",
      "step 29368, loss is 4.882957458496094\n",
      "(64, 33)\n",
      "step 29369, loss is 4.860145092010498\n",
      "(64, 33)\n",
      "step 29370, loss is 4.754299640655518\n",
      "(64, 33)\n",
      "step 29371, loss is 4.658079624176025\n",
      "(64, 33)\n",
      "step 29372, loss is 4.693992614746094\n",
      "(64, 33)\n",
      "step 29373, loss is 4.658876419067383\n",
      "(64, 33)\n",
      "step 29374, loss is 4.778149127960205\n",
      "(64, 33)\n",
      "step 29375, loss is 4.61818265914917\n",
      "(64, 33)\n",
      "step 29376, loss is 4.76100492477417\n",
      "(64, 33)\n",
      "step 29377, loss is 4.879061698913574\n",
      "(64, 33)\n",
      "step 29378, loss is 4.954750061035156\n",
      "(64, 33)\n",
      "step 29379, loss is 4.975964069366455\n",
      "(64, 33)\n",
      "step 29380, loss is 4.792360305786133\n",
      "(64, 33)\n",
      "step 29381, loss is 4.712888717651367\n",
      "(64, 33)\n",
      "step 29382, loss is 4.67589807510376\n",
      "(64, 33)\n",
      "step 29383, loss is 4.755343914031982\n",
      "(64, 33)\n",
      "step 29384, loss is 4.725122451782227\n",
      "(64, 33)\n",
      "step 29385, loss is 4.9996161460876465\n",
      "(64, 33)\n",
      "step 29386, loss is 4.691114902496338\n",
      "(64, 33)\n",
      "step 29387, loss is 4.968070983886719\n",
      "(64, 33)\n",
      "step 29388, loss is 4.77963399887085\n",
      "(64, 33)\n",
      "step 29389, loss is 4.6924147605896\n",
      "(64, 33)\n",
      "step 29390, loss is 4.766617298126221\n",
      "(64, 33)\n",
      "step 29391, loss is 4.747212886810303\n",
      "(64, 33)\n",
      "step 29392, loss is 4.850480079650879\n",
      "(64, 33)\n",
      "step 29393, loss is 4.698124885559082\n",
      "(64, 33)\n",
      "step 29394, loss is 4.706613063812256\n",
      "(64, 33)\n",
      "step 29395, loss is 4.670718193054199\n",
      "(64, 33)\n",
      "step 29396, loss is 4.890600204467773\n",
      "(64, 33)\n",
      "step 29397, loss is 4.928339004516602\n",
      "(64, 33)\n",
      "step 29398, loss is 4.712925434112549\n",
      "(64, 33)\n",
      "step 29399, loss is 4.841606616973877\n",
      "(64, 33)\n",
      "step 29400, loss is 4.784486293792725\n",
      "(64, 33)\n",
      "step 29401, loss is 4.709756851196289\n",
      "(64, 33)\n",
      "step 29402, loss is 4.819908142089844\n",
      "(64, 33)\n",
      "step 29403, loss is 4.881310939788818\n",
      "(64, 33)\n",
      "step 29404, loss is 4.943840503692627\n",
      "(64, 33)\n",
      "step 29405, loss is 4.63848876953125\n",
      "(64, 33)\n",
      "step 29406, loss is 4.749435901641846\n",
      "(64, 33)\n",
      "step 29407, loss is 4.682086944580078\n",
      "(64, 33)\n",
      "step 29408, loss is 4.508575439453125\n",
      "(64, 33)\n",
      "step 29409, loss is 4.7829999923706055\n",
      "(64, 33)\n",
      "step 29410, loss is 4.761206150054932\n",
      "(64, 33)\n",
      "step 29411, loss is 4.864579200744629\n",
      "(64, 33)\n",
      "step 29412, loss is 4.750698089599609\n",
      "(64, 33)\n",
      "step 29413, loss is 4.797761917114258\n",
      "(64, 33)\n",
      "step 29414, loss is 5.0437188148498535\n",
      "(64, 33)\n",
      "step 29415, loss is 4.992242336273193\n",
      "(64, 33)\n",
      "step 29416, loss is 4.6746134757995605\n",
      "(64, 33)\n",
      "step 29417, loss is 4.846095085144043\n",
      "(64, 33)\n",
      "step 29418, loss is 4.674440383911133\n",
      "(64, 33)\n",
      "step 29419, loss is 4.789254665374756\n",
      "(64, 33)\n",
      "step 29420, loss is 4.817298412322998\n",
      "(64, 33)\n",
      "step 29421, loss is 4.832814693450928\n",
      "(64, 33)\n",
      "step 29422, loss is 4.711747646331787\n",
      "(64, 33)\n",
      "step 29423, loss is 4.854918003082275\n",
      "(64, 33)\n",
      "step 29424, loss is 4.747993469238281\n",
      "(64, 33)\n",
      "step 29425, loss is 4.720527648925781\n",
      "(64, 33)\n",
      "step 29426, loss is 4.8363728523254395\n",
      "(64, 33)\n",
      "step 29427, loss is 4.732658386230469\n",
      "(64, 33)\n",
      "step 29428, loss is 4.893333435058594\n",
      "(64, 33)\n",
      "step 29429, loss is 4.891138076782227\n",
      "(64, 33)\n",
      "step 29430, loss is 4.7559895515441895\n",
      "(64, 33)\n",
      "step 29431, loss is 4.752087593078613\n",
      "(64, 33)\n",
      "step 29432, loss is 4.742677211761475\n",
      "(64, 33)\n",
      "step 29433, loss is 4.7336602210998535\n",
      "(64, 33)\n",
      "step 29434, loss is 4.85711145401001\n",
      "(64, 33)\n",
      "step 29435, loss is 4.7357025146484375\n",
      "(64, 33)\n",
      "step 29436, loss is 4.838763236999512\n",
      "(64, 33)\n",
      "step 29437, loss is 4.790470123291016\n",
      "(64, 33)\n",
      "step 29438, loss is 4.67987060546875\n",
      "(64, 33)\n",
      "step 29439, loss is 4.550229072570801\n",
      "(64, 33)\n",
      "step 29440, loss is 4.759312629699707\n",
      "(64, 33)\n",
      "step 29441, loss is 4.805093288421631\n",
      "(64, 33)\n",
      "step 29442, loss is 4.866736888885498\n",
      "(64, 33)\n",
      "step 29443, loss is 4.755101203918457\n",
      "(64, 33)\n",
      "step 29444, loss is 4.615448951721191\n",
      "(64, 33)\n",
      "step 29445, loss is 4.749886512756348\n",
      "(64, 33)\n",
      "step 29446, loss is 4.813301086425781\n",
      "(64, 33)\n",
      "step 29447, loss is 4.7389445304870605\n",
      "(64, 33)\n",
      "step 29448, loss is 4.883204936981201\n",
      "(64, 33)\n",
      "step 29449, loss is 4.7913360595703125\n",
      "(64, 33)\n",
      "step 29450, loss is 4.777614116668701\n",
      "(64, 33)\n",
      "step 29451, loss is 4.846147060394287\n",
      "(64, 33)\n",
      "step 29452, loss is 4.988326072692871\n",
      "(64, 33)\n",
      "step 29453, loss is 4.874717712402344\n",
      "(64, 33)\n",
      "step 29454, loss is 4.9933013916015625\n",
      "(64, 33)\n",
      "step 29455, loss is 4.724392890930176\n",
      "(64, 33)\n",
      "step 29456, loss is 4.8094868659973145\n",
      "(64, 33)\n",
      "step 29457, loss is 4.757776260375977\n",
      "(64, 33)\n",
      "step 29458, loss is 4.6714396476745605\n",
      "(64, 33)\n",
      "step 29459, loss is 4.782867908477783\n",
      "(64, 33)\n",
      "step 29460, loss is 4.743330478668213\n",
      "(64, 33)\n",
      "step 29461, loss is 4.947644233703613\n",
      "(64, 33)\n",
      "step 29462, loss is 4.649111747741699\n",
      "(64, 33)\n",
      "step 29463, loss is 4.648877143859863\n",
      "(64, 33)\n",
      "step 29464, loss is 4.842202663421631\n",
      "(64, 33)\n",
      "step 29465, loss is 4.835855007171631\n",
      "(64, 33)\n",
      "step 29466, loss is 4.708790302276611\n",
      "(64, 33)\n",
      "step 29467, loss is 4.897209644317627\n",
      "(64, 33)\n",
      "step 29468, loss is 4.77083158493042\n",
      "(64, 33)\n",
      "step 29469, loss is 4.879162311553955\n",
      "(64, 33)\n",
      "step 29470, loss is 4.922076225280762\n",
      "(64, 33)\n",
      "step 29471, loss is 4.717925548553467\n",
      "(64, 33)\n",
      "step 29472, loss is 4.953763961791992\n",
      "(64, 33)\n",
      "step 29473, loss is 4.704135894775391\n",
      "(64, 33)\n",
      "step 29474, loss is 4.848850250244141\n",
      "(64, 33)\n",
      "step 29475, loss is 4.750696182250977\n",
      "(64, 33)\n",
      "step 29476, loss is 4.710851192474365\n",
      "(64, 33)\n",
      "step 29477, loss is 4.819793701171875\n",
      "(64, 33)\n",
      "step 29478, loss is 4.865616321563721\n",
      "(64, 33)\n",
      "step 29479, loss is 4.81410551071167\n",
      "(64, 33)\n",
      "step 29480, loss is 4.780641555786133\n",
      "(64, 33)\n",
      "step 29481, loss is 4.702306747436523\n",
      "(64, 33)\n",
      "step 29482, loss is 4.716650009155273\n",
      "(64, 33)\n",
      "step 29483, loss is 4.99423885345459\n",
      "(64, 33)\n",
      "step 29484, loss is 4.798731803894043\n",
      "(64, 33)\n",
      "step 29485, loss is 4.877408027648926\n",
      "(64, 33)\n",
      "step 29486, loss is 4.741241931915283\n",
      "(64, 33)\n",
      "step 29487, loss is 4.876350402832031\n",
      "(64, 33)\n",
      "step 29488, loss is 4.783904552459717\n",
      "(64, 33)\n",
      "step 29489, loss is 4.918262004852295\n",
      "(64, 33)\n",
      "step 29490, loss is 4.972569465637207\n",
      "(64, 33)\n",
      "step 29491, loss is 4.922695159912109\n",
      "(64, 33)\n",
      "step 29492, loss is 4.739818096160889\n",
      "(64, 33)\n",
      "step 29493, loss is 5.031439304351807\n",
      "(64, 33)\n",
      "step 29494, loss is 4.631172180175781\n",
      "(64, 33)\n",
      "step 29495, loss is 4.6969428062438965\n",
      "(64, 33)\n",
      "step 29496, loss is 4.593155384063721\n",
      "(64, 33)\n",
      "step 29497, loss is 4.969681262969971\n",
      "(64, 33)\n",
      "step 29498, loss is 4.855050563812256\n",
      "(64, 33)\n",
      "step 29499, loss is 4.729704856872559\n",
      "(64, 33)\n",
      "step 29500, loss is 4.877186298370361\n",
      "(64, 33)\n",
      "step 29501, loss is 4.842942237854004\n",
      "(64, 33)\n",
      "step 29502, loss is 4.951719284057617\n",
      "(64, 33)\n",
      "step 29503, loss is 4.662344455718994\n",
      "(64, 33)\n",
      "step 29504, loss is 4.937796115875244\n",
      "(64, 33)\n",
      "step 29505, loss is 4.857162952423096\n",
      "(64, 33)\n",
      "step 29506, loss is 4.9683637619018555\n",
      "(64, 33)\n",
      "step 29507, loss is 4.789008140563965\n",
      "(64, 33)\n",
      "step 29508, loss is 4.634823799133301\n",
      "(64, 33)\n",
      "step 29509, loss is 4.940411567687988\n",
      "(64, 33)\n",
      "step 29510, loss is 4.846489906311035\n",
      "(64, 33)\n",
      "step 29511, loss is 4.735345363616943\n",
      "(64, 33)\n",
      "step 29512, loss is 4.787245273590088\n",
      "(64, 33)\n",
      "step 29513, loss is 4.796201705932617\n",
      "(64, 33)\n",
      "step 29514, loss is 4.613979816436768\n",
      "(64, 33)\n",
      "step 29515, loss is 4.949709892272949\n",
      "(64, 33)\n",
      "step 29516, loss is 4.724068641662598\n",
      "(64, 33)\n",
      "step 29517, loss is 4.6837334632873535\n",
      "(64, 33)\n",
      "step 29518, loss is 4.813911437988281\n",
      "(64, 33)\n",
      "step 29519, loss is 4.647267818450928\n",
      "(64, 33)\n",
      "step 29520, loss is 4.68581485748291\n",
      "(64, 33)\n",
      "step 29521, loss is 4.861732482910156\n",
      "(64, 33)\n",
      "step 29522, loss is 4.6928935050964355\n",
      "(64, 33)\n",
      "step 29523, loss is 4.773506164550781\n",
      "(64, 33)\n",
      "step 29524, loss is 5.027738094329834\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29525, loss is 4.709148406982422\n",
      "(64, 33)\n",
      "step 29526, loss is 4.785695552825928\n",
      "(64, 33)\n",
      "step 29527, loss is 4.75347375869751\n",
      "(64, 33)\n",
      "step 29528, loss is 4.614692687988281\n",
      "(64, 33)\n",
      "step 29529, loss is 4.836080551147461\n",
      "(64, 33)\n",
      "step 29530, loss is 4.777125835418701\n",
      "(64, 33)\n",
      "step 29531, loss is 4.926167011260986\n",
      "(64, 33)\n",
      "step 29532, loss is 4.782740592956543\n",
      "(64, 33)\n",
      "step 29533, loss is 4.629866600036621\n",
      "(64, 33)\n",
      "step 29534, loss is 4.79380464553833\n",
      "(64, 33)\n",
      "step 29535, loss is 4.617739200592041\n",
      "(64, 33)\n",
      "step 29536, loss is 4.795064449310303\n",
      "(64, 33)\n",
      "step 29537, loss is 4.813022136688232\n",
      "(64, 33)\n",
      "step 29538, loss is 4.708393573760986\n",
      "(64, 33)\n",
      "step 29539, loss is 4.98883056640625\n",
      "(64, 33)\n",
      "step 29540, loss is 4.763105392456055\n",
      "(64, 33)\n",
      "step 29541, loss is 4.7401227951049805\n",
      "(64, 33)\n",
      "step 29542, loss is 4.690863609313965\n",
      "(64, 33)\n",
      "step 29543, loss is 4.6070780754089355\n",
      "(64, 33)\n",
      "step 29544, loss is 4.622589111328125\n",
      "(64, 33)\n",
      "step 29545, loss is 4.796973705291748\n",
      "(64, 33)\n",
      "step 29546, loss is 4.914106369018555\n",
      "(64, 33)\n",
      "step 29547, loss is 4.889414310455322\n",
      "(64, 33)\n",
      "step 29548, loss is 4.677606105804443\n",
      "(64, 33)\n",
      "step 29549, loss is 4.961422443389893\n",
      "(64, 33)\n",
      "step 29550, loss is 5.037641525268555\n",
      "(64, 33)\n",
      "step 29551, loss is 4.871175765991211\n",
      "(64, 33)\n",
      "step 29552, loss is 4.8169379234313965\n",
      "(64, 33)\n",
      "step 29553, loss is 4.8187456130981445\n",
      "(64, 33)\n",
      "step 29554, loss is 4.751960277557373\n",
      "(64, 33)\n",
      "step 29555, loss is 4.560796737670898\n",
      "(64, 33)\n",
      "step 29556, loss is 4.760942459106445\n",
      "(64, 33)\n",
      "step 29557, loss is 4.765722751617432\n",
      "(64, 33)\n",
      "step 29558, loss is 4.798868179321289\n",
      "(64, 33)\n",
      "step 29559, loss is 4.915094375610352\n",
      "(64, 33)\n",
      "step 29560, loss is 4.850588798522949\n",
      "(64, 33)\n",
      "step 29561, loss is 4.837939739227295\n",
      "(64, 33)\n",
      "step 29562, loss is 4.679769992828369\n",
      "(64, 33)\n",
      "step 29563, loss is 4.724699974060059\n",
      "(64, 33)\n",
      "step 29564, loss is 4.6473870277404785\n",
      "(64, 33)\n",
      "step 29565, loss is 4.81850528717041\n",
      "(64, 33)\n",
      "step 29566, loss is 4.775074481964111\n",
      "(64, 33)\n",
      "step 29567, loss is 4.575767517089844\n",
      "(64, 33)\n",
      "step 29568, loss is 4.859103202819824\n",
      "(64, 33)\n",
      "step 29569, loss is 4.846041202545166\n",
      "(64, 33)\n",
      "step 29570, loss is 4.752112865447998\n",
      "(64, 33)\n",
      "step 29571, loss is 4.8735480308532715\n",
      "(64, 33)\n",
      "step 29572, loss is 4.725632190704346\n",
      "(64, 33)\n",
      "step 29573, loss is 4.852437496185303\n",
      "(64, 33)\n",
      "step 29574, loss is 4.912250518798828\n",
      "(64, 33)\n",
      "step 29575, loss is 4.700002193450928\n",
      "(64, 33)\n",
      "step 29576, loss is 4.9227728843688965\n",
      "(64, 33)\n",
      "step 29577, loss is 4.613672256469727\n",
      "(64, 33)\n",
      "step 29578, loss is 4.865516185760498\n",
      "(64, 33)\n",
      "step 29579, loss is 4.6500020027160645\n",
      "(64, 33)\n",
      "step 29580, loss is 4.784397602081299\n",
      "(64, 33)\n",
      "step 29581, loss is 4.780214786529541\n",
      "(64, 33)\n",
      "step 29582, loss is 4.680388927459717\n",
      "(64, 33)\n",
      "step 29583, loss is 4.793483734130859\n",
      "(64, 33)\n",
      "step 29584, loss is 4.878645896911621\n",
      "(64, 33)\n",
      "step 29585, loss is 4.68107795715332\n",
      "(64, 33)\n",
      "step 29586, loss is 4.649257183074951\n",
      "(64, 33)\n",
      "step 29587, loss is 4.575089454650879\n",
      "(64, 33)\n",
      "step 29588, loss is 4.802613735198975\n",
      "(64, 33)\n",
      "step 29589, loss is 4.811642169952393\n",
      "(64, 33)\n",
      "step 29590, loss is 4.866164207458496\n",
      "(64, 33)\n",
      "step 29591, loss is 4.74440336227417\n",
      "(64, 33)\n",
      "step 29592, loss is 4.8020548820495605\n",
      "(64, 33)\n",
      "step 29593, loss is 4.724298000335693\n",
      "(64, 33)\n",
      "step 29594, loss is 4.561100959777832\n",
      "(64, 33)\n",
      "step 29595, loss is 4.68595552444458\n",
      "(64, 33)\n",
      "step 29596, loss is 4.798609256744385\n",
      "(64, 33)\n",
      "step 29597, loss is 4.927251815795898\n",
      "(64, 33)\n",
      "step 29598, loss is 4.727184295654297\n",
      "(64, 33)\n",
      "step 29599, loss is 4.71488094329834\n",
      "(64, 33)\n",
      "step 29600, loss is 4.871207237243652\n",
      "(64, 33)\n",
      "step 29601, loss is 4.669050216674805\n",
      "(64, 33)\n",
      "step 29602, loss is 4.676774978637695\n",
      "(64, 33)\n",
      "step 29603, loss is 4.735877990722656\n",
      "(64, 33)\n",
      "step 29604, loss is 4.735318183898926\n",
      "(64, 33)\n",
      "step 29605, loss is 4.721816539764404\n",
      "(64, 33)\n",
      "step 29606, loss is 4.893770217895508\n",
      "(64, 33)\n",
      "step 29607, loss is 4.875741481781006\n",
      "(64, 33)\n",
      "step 29608, loss is 4.808261394500732\n",
      "(64, 33)\n",
      "step 29609, loss is 4.870822906494141\n",
      "(64, 33)\n",
      "step 29610, loss is 4.926358222961426\n",
      "(64, 33)\n",
      "step 29611, loss is 4.830440044403076\n",
      "(64, 33)\n",
      "step 29612, loss is 4.85434103012085\n",
      "(64, 33)\n",
      "step 29613, loss is 5.01330041885376\n",
      "(64, 33)\n",
      "step 29614, loss is 4.803495407104492\n",
      "(64, 33)\n",
      "step 29615, loss is 4.800899028778076\n",
      "(64, 33)\n",
      "step 29616, loss is 4.811638832092285\n",
      "(64, 33)\n",
      "step 29617, loss is 4.767561912536621\n",
      "(64, 33)\n",
      "step 29618, loss is 4.9660773277282715\n",
      "(64, 33)\n",
      "step 29619, loss is 4.977489948272705\n",
      "(64, 33)\n",
      "step 29620, loss is 4.9785542488098145\n",
      "(64, 33)\n",
      "step 29621, loss is 4.902610778808594\n",
      "(64, 33)\n",
      "step 29622, loss is 4.8983635902404785\n",
      "(64, 33)\n",
      "step 29623, loss is 4.847151756286621\n",
      "(64, 33)\n",
      "step 29624, loss is 4.8886237144470215\n",
      "(64, 33)\n",
      "step 29625, loss is 4.778205871582031\n",
      "(64, 33)\n",
      "step 29626, loss is 4.8829874992370605\n",
      "(64, 33)\n",
      "step 29627, loss is 4.819258213043213\n",
      "(64, 33)\n",
      "step 29628, loss is 4.835579872131348\n",
      "(64, 33)\n",
      "step 29629, loss is 4.862477779388428\n",
      "(64, 33)\n",
      "step 29630, loss is 4.789089202880859\n",
      "(64, 33)\n",
      "step 29631, loss is 4.591001987457275\n",
      "(64, 33)\n",
      "step 29632, loss is 4.794249057769775\n",
      "(64, 33)\n",
      "step 29633, loss is 4.766788005828857\n",
      "(64, 33)\n",
      "step 29634, loss is 4.694265842437744\n",
      "(64, 33)\n",
      "step 29635, loss is 4.811112403869629\n",
      "(64, 33)\n",
      "step 29636, loss is 4.71356725692749\n",
      "(64, 33)\n",
      "step 29637, loss is 4.8733906745910645\n",
      "(64, 33)\n",
      "step 29638, loss is 4.655440807342529\n",
      "(64, 33)\n",
      "step 29639, loss is 4.681813716888428\n",
      "(64, 33)\n",
      "step 29640, loss is 4.616113662719727\n",
      "(64, 33)\n",
      "step 29641, loss is 4.863011360168457\n",
      "(64, 33)\n",
      "step 29642, loss is 4.629178047180176\n",
      "(64, 33)\n",
      "step 29643, loss is 4.741944313049316\n",
      "(64, 33)\n",
      "step 29644, loss is 4.956851959228516\n",
      "(64, 33)\n",
      "step 29645, loss is 4.809296607971191\n",
      "(64, 33)\n",
      "step 29646, loss is 4.829823017120361\n",
      "(64, 33)\n",
      "step 29647, loss is 4.65504789352417\n",
      "(64, 33)\n",
      "step 29648, loss is 4.930056095123291\n",
      "(64, 33)\n",
      "step 29649, loss is 4.806215286254883\n",
      "(64, 33)\n",
      "step 29650, loss is 4.827474117279053\n",
      "(64, 33)\n",
      "step 29651, loss is 4.824981689453125\n",
      "(64, 33)\n",
      "step 29652, loss is 4.890074729919434\n",
      "(64, 33)\n",
      "step 29653, loss is 4.841424942016602\n",
      "(64, 33)\n",
      "step 29654, loss is 4.758249282836914\n",
      "(64, 33)\n",
      "step 29655, loss is 4.96126127243042\n",
      "(64, 33)\n",
      "step 29656, loss is 4.6413044929504395\n",
      "(64, 33)\n",
      "step 29657, loss is 4.803042411804199\n",
      "(64, 33)\n",
      "step 29658, loss is 4.7438788414001465\n",
      "(64, 33)\n",
      "step 29659, loss is 4.731391906738281\n",
      "(64, 33)\n",
      "step 29660, loss is 4.746253490447998\n",
      "(64, 33)\n",
      "step 29661, loss is 4.802674293518066\n",
      "(64, 33)\n",
      "step 29662, loss is 4.907238483428955\n",
      "(64, 33)\n",
      "step 29663, loss is 4.864844799041748\n",
      "(64, 33)\n",
      "step 29664, loss is 4.931186676025391\n",
      "(64, 33)\n",
      "step 29665, loss is 4.779054641723633\n",
      "(64, 33)\n",
      "step 29666, loss is 4.796102523803711\n",
      "(64, 33)\n",
      "step 29667, loss is 4.938584327697754\n",
      "(64, 33)\n",
      "step 29668, loss is 4.892695903778076\n",
      "(64, 33)\n",
      "step 29669, loss is 4.605657577514648\n",
      "(64, 33)\n",
      "step 29670, loss is 4.832108020782471\n",
      "(64, 33)\n",
      "step 29671, loss is 4.7287702560424805\n",
      "(64, 33)\n",
      "step 29672, loss is 4.93184232711792\n",
      "(64, 33)\n",
      "step 29673, loss is 4.576405048370361\n",
      "(64, 33)\n",
      "step 29674, loss is 4.906453609466553\n",
      "(64, 33)\n",
      "step 29675, loss is 4.668974876403809\n",
      "(64, 33)\n",
      "step 29676, loss is 4.907567024230957\n",
      "(64, 33)\n",
      "step 29677, loss is 4.84039306640625\n",
      "(64, 33)\n",
      "step 29678, loss is 4.633364677429199\n",
      "(64, 33)\n",
      "step 29679, loss is 4.669765949249268\n",
      "(64, 33)\n",
      "step 29680, loss is 4.799858093261719\n",
      "(64, 33)\n",
      "step 29681, loss is 4.864753723144531\n",
      "(64, 33)\n",
      "step 29682, loss is 4.751679420471191\n",
      "(64, 33)\n",
      "step 29683, loss is 4.804690361022949\n",
      "(64, 33)\n",
      "step 29684, loss is 4.724860191345215\n",
      "(64, 33)\n",
      "step 29685, loss is 4.514762878417969\n",
      "(64, 33)\n",
      "step 29686, loss is 4.845301628112793\n",
      "(64, 33)\n",
      "step 29687, loss is 4.902979850769043\n",
      "(64, 33)\n",
      "step 29688, loss is 4.6354146003723145\n",
      "(64, 33)\n",
      "step 29689, loss is 4.779855728149414\n",
      "(64, 33)\n",
      "step 29690, loss is 4.797735214233398\n",
      "(64, 33)\n",
      "step 29691, loss is 4.814601421356201\n",
      "(64, 33)\n",
      "step 29692, loss is 4.724403381347656\n",
      "(64, 33)\n",
      "step 29693, loss is 4.946814060211182\n",
      "(64, 33)\n",
      "step 29694, loss is 4.613164901733398\n",
      "(64, 33)\n",
      "step 29695, loss is 4.864399433135986\n",
      "(64, 33)\n",
      "step 29696, loss is 4.745355606079102\n",
      "(64, 33)\n",
      "step 29697, loss is 4.844490051269531\n",
      "(64, 33)\n",
      "step 29698, loss is 4.765904903411865\n",
      "(64, 33)\n",
      "step 29699, loss is 4.835275650024414\n",
      "(64, 33)\n",
      "step 29700, loss is 4.719970703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33)\n",
      "step 29701, loss is 4.751053333282471\n",
      "(64, 33)\n",
      "step 29702, loss is 4.935075283050537\n",
      "(64, 33)\n",
      "step 29703, loss is 4.9503936767578125\n",
      "(64, 33)\n",
      "step 29704, loss is 4.727180480957031\n",
      "(64, 33)\n",
      "step 29705, loss is 4.930083274841309\n",
      "(64, 33)\n",
      "step 29706, loss is 4.702582359313965\n",
      "(64, 33)\n",
      "step 29707, loss is 4.7661285400390625\n",
      "(64, 33)\n",
      "step 29708, loss is 4.866079330444336\n",
      "(64, 33)\n",
      "step 29709, loss is 4.802918434143066\n",
      "(64, 33)\n",
      "step 29710, loss is 4.862913608551025\n",
      "(64, 33)\n",
      "step 29711, loss is 4.857268333435059\n",
      "(64, 33)\n",
      "step 29712, loss is 4.826772689819336\n",
      "(64, 33)\n",
      "step 29713, loss is 4.771202564239502\n",
      "(64, 33)\n",
      "step 29714, loss is 4.8448286056518555\n",
      "(64, 33)\n",
      "step 29715, loss is 4.636619567871094\n",
      "(64, 33)\n",
      "step 29716, loss is 4.915836334228516\n",
      "(64, 33)\n",
      "step 29717, loss is 4.895190238952637\n",
      "(64, 33)\n",
      "step 29718, loss is 4.799150466918945\n",
      "(64, 33)\n",
      "step 29719, loss is 4.713394641876221\n",
      "(64, 33)\n",
      "step 29720, loss is 4.74240779876709\n",
      "(64, 33)\n",
      "step 29721, loss is 4.836788177490234\n",
      "(64, 33)\n",
      "step 29722, loss is 4.699038982391357\n",
      "(64, 33)\n",
      "step 29723, loss is 4.892593860626221\n",
      "(64, 33)\n",
      "step 29724, loss is 4.750329971313477\n",
      "(64, 33)\n",
      "step 29725, loss is 4.920200824737549\n",
      "(64, 33)\n",
      "step 29726, loss is 4.870573043823242\n",
      "(64, 33)\n",
      "step 29727, loss is 4.69227409362793\n",
      "(64, 33)\n",
      "step 29728, loss is 4.809492588043213\n",
      "(64, 33)\n",
      "step 29729, loss is 4.775373935699463\n",
      "(64, 33)\n",
      "step 29730, loss is 4.786353588104248\n",
      "(64, 33)\n",
      "step 29731, loss is 4.538165092468262\n",
      "(64, 33)\n",
      "step 29732, loss is 4.6969685554504395\n",
      "(64, 33)\n",
      "step 29733, loss is 4.869977951049805\n",
      "(64, 33)\n",
      "step 29734, loss is 4.785360813140869\n",
      "(64, 33)\n",
      "step 29735, loss is 4.695411682128906\n",
      "(64, 33)\n",
      "step 29736, loss is 4.691187381744385\n",
      "(64, 33)\n",
      "step 29737, loss is 4.68779182434082\n",
      "(64, 33)\n",
      "step 29738, loss is 4.727384567260742\n",
      "(64, 33)\n",
      "step 29739, loss is 4.894613265991211\n",
      "(64, 33)\n",
      "step 29740, loss is 4.905856132507324\n",
      "(64, 33)\n",
      "step 29741, loss is 4.773089408874512\n",
      "(64, 33)\n",
      "step 29742, loss is 4.778332233428955\n",
      "(64, 33)\n",
      "step 29743, loss is 4.6176042556762695\n",
      "(64, 33)\n",
      "step 29744, loss is 4.72566032409668\n",
      "(64, 33)\n",
      "step 29745, loss is 4.78642463684082\n",
      "(64, 33)\n",
      "step 29746, loss is 4.804481029510498\n",
      "(64, 33)\n",
      "step 29747, loss is 4.7210001945495605\n",
      "(64, 33)\n",
      "step 29748, loss is 4.901238441467285\n",
      "(64, 33)\n",
      "step 29749, loss is 4.84877347946167\n",
      "(64, 33)\n",
      "step 29750, loss is 4.795967102050781\n",
      "(64, 33)\n",
      "step 29751, loss is 4.857191562652588\n",
      "(64, 33)\n",
      "step 29752, loss is 4.605595588684082\n",
      "(64, 33)\n",
      "step 29753, loss is 4.727027893066406\n",
      "(64, 33)\n",
      "step 29754, loss is 4.605528831481934\n",
      "(64, 33)\n",
      "step 29755, loss is 4.895755290985107\n",
      "(64, 33)\n",
      "step 29756, loss is 4.8705973625183105\n",
      "(64, 33)\n",
      "step 29757, loss is 4.852600574493408\n",
      "(64, 33)\n",
      "step 29758, loss is 4.731495380401611\n",
      "(64, 33)\n",
      "step 29759, loss is 4.802050590515137\n",
      "(64, 33)\n",
      "step 29760, loss is 4.801907062530518\n",
      "(64, 33)\n",
      "step 29761, loss is 4.786508083343506\n",
      "(64, 33)\n",
      "step 29762, loss is 4.756628513336182\n",
      "(64, 33)\n",
      "step 29763, loss is 4.650666236877441\n",
      "(64, 33)\n",
      "step 29764, loss is 4.76060152053833\n",
      "(64, 33)\n",
      "step 29765, loss is 4.725047588348389\n",
      "(64, 33)\n",
      "step 29766, loss is 4.768207550048828\n",
      "(64, 33)\n",
      "step 29767, loss is 4.739114284515381\n",
      "(64, 33)\n",
      "step 29768, loss is 4.631889343261719\n",
      "(64, 33)\n",
      "step 29769, loss is 4.740370273590088\n",
      "(64, 33)\n",
      "step 29770, loss is 4.683001518249512\n",
      "(64, 33)\n",
      "step 29771, loss is 4.878473281860352\n",
      "(64, 33)\n",
      "step 29772, loss is 4.811614036560059\n",
      "(64, 33)\n",
      "step 29773, loss is 4.808663845062256\n",
      "(64, 33)\n",
      "step 29774, loss is 4.67647123336792\n",
      "(64, 33)\n",
      "step 29775, loss is 4.707503795623779\n",
      "(64, 33)\n",
      "step 29776, loss is 4.638026714324951\n",
      "(64, 33)\n",
      "step 29777, loss is 4.738772392272949\n",
      "(64, 33)\n",
      "step 29778, loss is 4.817959785461426\n",
      "(64, 33)\n",
      "step 29779, loss is 4.659539699554443\n",
      "(64, 33)\n",
      "step 29780, loss is 4.738539695739746\n",
      "(64, 33)\n",
      "step 29781, loss is 4.739753723144531\n",
      "(64, 33)\n",
      "step 29782, loss is 4.7282233238220215\n",
      "(64, 33)\n",
      "step 29783, loss is 4.661692142486572\n",
      "(64, 33)\n",
      "step 29784, loss is 4.625091075897217\n",
      "(64, 33)\n",
      "step 29785, loss is 4.98224401473999\n",
      "(64, 33)\n",
      "step 29786, loss is 4.771073818206787\n",
      "(64, 33)\n",
      "step 29787, loss is 4.664407253265381\n",
      "(64, 33)\n",
      "step 29788, loss is 4.638821601867676\n",
      "(64, 33)\n",
      "step 29789, loss is 4.688721656799316\n",
      "(64, 33)\n",
      "step 29790, loss is 4.709534645080566\n",
      "(64, 33)\n",
      "step 29791, loss is 4.731081008911133\n",
      "(64, 33)\n",
      "step 29792, loss is 4.787703037261963\n",
      "(64, 33)\n",
      "step 29793, loss is 4.801351070404053\n",
      "(64, 33)\n",
      "step 29794, loss is 4.7979416847229\n",
      "(64, 33)\n",
      "step 29795, loss is 4.705071449279785\n",
      "(64, 33)\n",
      "step 29796, loss is 4.694343090057373\n",
      "(64, 33)\n",
      "step 29797, loss is 4.900712490081787\n",
      "(64, 33)\n",
      "step 29798, loss is 4.760443687438965\n",
      "(64, 33)\n",
      "step 29799, loss is 4.838258266448975\n",
      "(64, 33)\n",
      "step 29800, loss is 4.733340740203857\n",
      "(64, 33)\n",
      "step 29801, loss is 4.754091262817383\n",
      "(64, 33)\n",
      "step 29802, loss is 4.6571855545043945\n",
      "(64, 33)\n",
      "step 29803, loss is 4.736794471740723\n",
      "(64, 33)\n",
      "step 29804, loss is 4.772195339202881\n",
      "(64, 33)\n",
      "step 29805, loss is 4.728459358215332\n",
      "(64, 33)\n",
      "step 29806, loss is 4.873415946960449\n",
      "(64, 33)\n",
      "step 29807, loss is 4.664674758911133\n",
      "(64, 33)\n",
      "step 29808, loss is 4.6409525871276855\n",
      "(64, 33)\n",
      "step 29809, loss is 4.854269981384277\n",
      "(64, 33)\n",
      "step 29810, loss is 4.797604560852051\n",
      "(64, 33)\n",
      "step 29811, loss is 4.6510162353515625\n",
      "(64, 33)\n",
      "step 29812, loss is 4.724639892578125\n",
      "(64, 33)\n",
      "step 29813, loss is 4.696410655975342\n",
      "(64, 33)\n",
      "step 29814, loss is 4.731819152832031\n",
      "(64, 33)\n",
      "step 29815, loss is 4.810928821563721\n",
      "(64, 33)\n",
      "step 29816, loss is 4.653622627258301\n",
      "(64, 33)\n",
      "step 29817, loss is 4.707101345062256\n",
      "(64, 33)\n",
      "step 29818, loss is 4.718301773071289\n",
      "(64, 33)\n",
      "step 29819, loss is 4.7708258628845215\n",
      "(64, 33)\n",
      "step 29820, loss is 4.9631218910217285\n",
      "(64, 33)\n",
      "step 29821, loss is 4.742860794067383\n",
      "(64, 33)\n",
      "step 29822, loss is 4.873079776763916\n",
      "(64, 33)\n",
      "step 29823, loss is 4.702857494354248\n",
      "(64, 33)\n",
      "step 29824, loss is 4.981996536254883\n",
      "(64, 33)\n",
      "step 29825, loss is 4.822898864746094\n",
      "(64, 33)\n",
      "step 29826, loss is 4.624305725097656\n",
      "(64, 33)\n",
      "step 29827, loss is 4.835671901702881\n",
      "(64, 33)\n",
      "step 29828, loss is 4.767746448516846\n",
      "(64, 33)\n",
      "step 29829, loss is 4.875424861907959\n",
      "(64, 33)\n",
      "step 29830, loss is 4.7668633460998535\n",
      "(64, 33)\n",
      "step 29831, loss is 4.504089832305908\n",
      "(64, 33)\n",
      "step 29832, loss is 4.704829692840576\n",
      "(64, 33)\n",
      "step 29833, loss is 4.710752964019775\n",
      "(64, 33)\n",
      "step 29834, loss is 4.755184173583984\n",
      "(64, 33)\n",
      "step 29835, loss is 4.640158176422119\n",
      "(64, 33)\n",
      "step 29836, loss is 4.813024997711182\n",
      "(64, 33)\n",
      "step 29837, loss is 4.517765522003174\n",
      "(64, 33)\n",
      "step 29838, loss is 4.778685092926025\n",
      "(64, 33)\n",
      "step 29839, loss is 4.734257698059082\n",
      "(64, 33)\n",
      "step 29840, loss is 4.695628643035889\n",
      "(64, 33)\n",
      "step 29841, loss is 4.728596210479736\n",
      "(64, 33)\n",
      "step 29842, loss is 4.71302604675293\n",
      "(64, 33)\n",
      "step 29843, loss is 4.897861480712891\n",
      "(64, 33)\n",
      "step 29844, loss is 4.747126579284668\n",
      "(64, 33)\n",
      "step 29845, loss is 4.622037410736084\n",
      "(64, 33)\n",
      "step 29846, loss is 4.731335639953613\n",
      "(64, 33)\n",
      "step 29847, loss is 4.901760578155518\n",
      "(64, 33)\n",
      "step 29848, loss is 4.758581161499023\n",
      "(64, 33)\n",
      "step 29849, loss is 4.819571495056152\n",
      "(64, 33)\n",
      "step 29850, loss is 4.748257637023926\n",
      "(64, 33)\n",
      "step 29851, loss is 4.806088447570801\n",
      "(64, 33)\n",
      "step 29852, loss is 4.791093826293945\n",
      "(64, 33)\n",
      "step 29853, loss is 4.7295823097229\n",
      "(64, 33)\n",
      "step 29854, loss is 4.836562633514404\n",
      "(64, 33)\n",
      "step 29855, loss is 4.8617377281188965\n",
      "(64, 33)\n",
      "step 29856, loss is 4.9688920974731445\n",
      "(64, 33)\n",
      "step 29857, loss is 4.821692943572998\n",
      "(64, 33)\n",
      "step 29858, loss is 4.683422565460205\n",
      "(64, 33)\n",
      "step 29859, loss is 4.77487325668335\n",
      "(64, 33)\n",
      "step 29860, loss is 4.764627933502197\n",
      "(64, 33)\n",
      "step 29861, loss is 4.753964424133301\n",
      "(64, 33)\n",
      "step 29862, loss is 4.839785099029541\n",
      "(64, 33)\n",
      "step 29863, loss is 4.711609840393066\n",
      "(64, 33)\n",
      "step 29864, loss is 4.679792881011963\n",
      "(64, 33)\n",
      "step 29865, loss is 4.817591190338135\n",
      "(64, 33)\n",
      "step 29866, loss is 4.900711536407471\n",
      "(64, 33)\n",
      "step 29867, loss is 4.806187152862549\n",
      "(64, 33)\n",
      "step 29868, loss is 4.8747406005859375\n",
      "(64, 33)\n",
      "step 29869, loss is 4.797361850738525\n",
      "(64, 33)\n",
      "step 29870, loss is 4.662795543670654\n",
      "(64, 33)\n",
      "step 29871, loss is 4.832287788391113\n",
      "(64, 33)\n",
      "step 29872, loss is 4.734960079193115\n",
      "(64, 33)\n",
      "step 29873, loss is 4.929388523101807\n",
      "(64, 33)\n",
      "step 29874, loss is 4.653559684753418\n",
      "(64, 33)\n",
      "step 29875, loss is 4.8148193359375\n",
      "(64, 33)\n",
      "step 29876, loss is 4.7821478843688965\n",
      "(64, 33)\n",
      "step 29877, loss is 4.94856595993042\n",
      "(64, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29878, loss is 4.842081546783447\n",
      "(64, 33)\n",
      "step 29879, loss is 4.598847389221191\n",
      "(64, 33)\n",
      "step 29880, loss is 4.828158855438232\n",
      "(64, 33)\n",
      "step 29881, loss is 4.7292962074279785\n",
      "(64, 33)\n",
      "step 29882, loss is 4.897801399230957\n",
      "(64, 33)\n",
      "step 29883, loss is 4.7100090980529785\n",
      "(64, 33)\n",
      "step 29884, loss is 4.911215782165527\n",
      "(64, 33)\n",
      "step 29885, loss is 4.825595378875732\n",
      "(64, 33)\n",
      "step 29886, loss is 4.883553981781006\n",
      "(64, 33)\n",
      "step 29887, loss is 4.795073986053467\n",
      "(64, 33)\n",
      "step 29888, loss is 4.756414890289307\n",
      "(64, 33)\n",
      "step 29889, loss is 4.808561325073242\n",
      "(64, 33)\n",
      "step 29890, loss is 4.867471694946289\n",
      "(64, 33)\n",
      "step 29891, loss is 4.911922454833984\n",
      "(64, 33)\n",
      "step 29892, loss is 4.759373188018799\n",
      "(64, 33)\n",
      "step 29893, loss is 4.64255428314209\n",
      "(64, 33)\n",
      "step 29894, loss is 4.805435657501221\n",
      "(64, 33)\n",
      "step 29895, loss is 4.823724746704102\n",
      "(64, 33)\n",
      "step 29896, loss is 4.890425205230713\n",
      "(64, 33)\n",
      "step 29897, loss is 4.688248157501221\n",
      "(64, 33)\n",
      "step 29898, loss is 4.6525726318359375\n",
      "(64, 33)\n",
      "step 29899, loss is 4.8628363609313965\n",
      "(64, 33)\n",
      "step 29900, loss is 4.713798522949219\n",
      "(64, 33)\n",
      "step 29901, loss is 4.91148567199707\n",
      "(64, 33)\n",
      "step 29902, loss is 4.822208404541016\n",
      "(64, 33)\n",
      "step 29903, loss is 4.776228427886963\n",
      "(64, 33)\n",
      "step 29904, loss is 4.734855651855469\n",
      "(64, 33)\n",
      "step 29905, loss is 4.868017673492432\n",
      "(64, 33)\n",
      "step 29906, loss is 4.619736194610596\n",
      "(64, 33)\n",
      "step 29907, loss is 4.656006336212158\n",
      "(64, 33)\n",
      "step 29908, loss is 4.8732194900512695\n",
      "(64, 33)\n",
      "step 29909, loss is 4.636974334716797\n",
      "(64, 33)\n",
      "step 29910, loss is 4.8336992263793945\n",
      "(64, 33)\n",
      "step 29911, loss is 4.930650234222412\n",
      "(64, 33)\n",
      "step 29912, loss is 4.708672523498535\n",
      "(64, 33)\n",
      "step 29913, loss is 4.705920219421387\n",
      "(64, 33)\n",
      "step 29914, loss is 4.907078742980957\n",
      "(64, 33)\n",
      "step 29915, loss is 4.81405782699585\n",
      "(64, 33)\n",
      "step 29916, loss is 4.9666314125061035\n",
      "(64, 33)\n",
      "step 29917, loss is 4.698781490325928\n",
      "(64, 33)\n",
      "step 29918, loss is 4.768207550048828\n",
      "(64, 33)\n",
      "step 29919, loss is 4.759186744689941\n",
      "(64, 33)\n",
      "step 29920, loss is 4.6847333908081055\n",
      "(64, 33)\n",
      "step 29921, loss is 4.851070880889893\n",
      "(64, 33)\n",
      "step 29922, loss is 4.809092998504639\n",
      "(64, 33)\n",
      "step 29923, loss is 4.847238063812256\n",
      "(64, 33)\n",
      "step 29924, loss is 4.884011745452881\n",
      "(64, 33)\n",
      "step 29925, loss is 4.654199123382568\n",
      "(64, 33)\n",
      "step 29926, loss is 4.806539058685303\n",
      "(64, 33)\n",
      "step 29927, loss is 4.780632972717285\n",
      "(64, 33)\n",
      "step 29928, loss is 4.907140731811523\n",
      "(64, 33)\n",
      "step 29929, loss is 4.852828502655029\n",
      "(64, 33)\n",
      "step 29930, loss is 4.796552658081055\n",
      "(64, 33)\n",
      "step 29931, loss is 4.796115875244141\n",
      "(64, 33)\n",
      "step 29932, loss is 4.83738899230957\n",
      "(64, 33)\n",
      "step 29933, loss is 4.835777759552002\n",
      "(64, 33)\n",
      "step 29934, loss is 4.707035064697266\n",
      "(64, 33)\n",
      "step 29935, loss is 4.840901851654053\n",
      "(64, 33)\n",
      "step 29936, loss is 4.799773693084717\n",
      "(64, 33)\n",
      "step 29937, loss is 4.785030364990234\n",
      "(64, 33)\n",
      "step 29938, loss is 4.892965316772461\n",
      "(64, 33)\n",
      "step 29939, loss is 4.839554786682129\n",
      "(64, 33)\n",
      "step 29940, loss is 4.661925792694092\n",
      "(64, 33)\n",
      "step 29941, loss is 4.960075378417969\n",
      "(64, 33)\n",
      "step 29942, loss is 4.969429016113281\n",
      "(64, 33)\n",
      "step 29943, loss is 4.629740238189697\n",
      "(64, 33)\n",
      "step 29944, loss is 4.762710094451904\n",
      "(64, 33)\n",
      "step 29945, loss is 4.940906524658203\n",
      "(64, 33)\n",
      "step 29946, loss is 4.647022724151611\n",
      "(64, 33)\n",
      "step 29947, loss is 4.905948162078857\n",
      "(64, 33)\n",
      "step 29948, loss is 4.955863952636719\n",
      "(64, 33)\n",
      "step 29949, loss is 4.818825721740723\n",
      "(64, 33)\n",
      "step 29950, loss is 4.895286560058594\n",
      "(64, 33)\n",
      "step 29951, loss is 4.729989051818848\n",
      "(64, 33)\n",
      "step 29952, loss is 4.710254192352295\n",
      "(64, 33)\n",
      "step 29953, loss is 4.578974723815918\n",
      "(64, 33)\n",
      "step 29954, loss is 4.621681213378906\n",
      "(64, 33)\n",
      "step 29955, loss is 4.933119297027588\n",
      "(64, 33)\n",
      "step 29956, loss is 4.789238452911377\n",
      "(64, 33)\n",
      "step 29957, loss is 4.746096134185791\n",
      "(64, 33)\n",
      "step 29958, loss is 4.777734279632568\n",
      "(64, 33)\n",
      "step 29959, loss is 4.931673526763916\n",
      "(64, 33)\n",
      "step 29960, loss is 4.9263176918029785\n",
      "(64, 33)\n",
      "step 29961, loss is 4.870668411254883\n",
      "(64, 33)\n",
      "step 29962, loss is 4.764958381652832\n",
      "(64, 33)\n",
      "step 29963, loss is 4.796083927154541\n",
      "(64, 33)\n",
      "step 29964, loss is 4.795338153839111\n",
      "(64, 33)\n",
      "step 29965, loss is 4.834346771240234\n",
      "(64, 33)\n",
      "step 29966, loss is 4.775418281555176\n",
      "(64, 33)\n",
      "step 29967, loss is 4.853115081787109\n",
      "(64, 33)\n",
      "step 29968, loss is 4.578824520111084\n",
      "(64, 33)\n",
      "step 29969, loss is 4.734809875488281\n",
      "(64, 33)\n",
      "step 29970, loss is 4.767586708068848\n",
      "(64, 33)\n",
      "step 29971, loss is 4.8532633781433105\n",
      "(64, 33)\n",
      "step 29972, loss is 4.604252815246582\n",
      "(64, 33)\n",
      "step 29973, loss is 4.870143413543701\n",
      "(64, 33)\n",
      "step 29974, loss is 4.739136695861816\n",
      "(64, 33)\n",
      "step 29975, loss is 4.880899906158447\n",
      "(64, 33)\n",
      "step 29976, loss is 4.84534215927124\n",
      "(64, 33)\n",
      "step 29977, loss is 4.638286590576172\n",
      "(64, 33)\n",
      "step 29978, loss is 4.620715618133545\n",
      "(64, 33)\n",
      "step 29979, loss is 4.767040252685547\n",
      "(64, 33)\n",
      "step 29980, loss is 4.65078067779541\n",
      "(64, 33)\n",
      "step 29981, loss is 4.835139751434326\n",
      "(64, 33)\n",
      "step 29982, loss is 4.731289863586426\n",
      "(64, 33)\n",
      "step 29983, loss is 4.732925891876221\n",
      "(64, 33)\n",
      "step 29984, loss is 4.868896007537842\n",
      "(64, 33)\n",
      "step 29985, loss is 4.886131286621094\n",
      "(64, 33)\n",
      "step 29986, loss is 4.954471111297607\n",
      "(64, 33)\n",
      "step 29987, loss is 4.821496963500977\n",
      "(64, 33)\n",
      "step 29988, loss is 5.014484882354736\n",
      "(64, 33)\n",
      "step 29989, loss is 4.799795627593994\n",
      "(64, 33)\n",
      "step 29990, loss is 4.893141269683838\n",
      "(64, 33)\n",
      "step 29991, loss is 5.026421070098877\n",
      "(64, 33)\n",
      "step 29992, loss is 4.871209144592285\n",
      "(64, 33)\n",
      "step 29993, loss is 4.692764759063721\n",
      "(64, 33)\n",
      "step 29994, loss is 4.684526443481445\n",
      "(64, 33)\n",
      "step 29995, loss is 4.582969665527344\n",
      "(64, 33)\n",
      "step 29996, loss is 4.823590278625488\n",
      "(64, 33)\n",
      "step 29997, loss is 4.727088928222656\n",
      "(64, 33)\n",
      "step 29998, loss is 4.980075359344482\n",
      "(64, 33)\n",
      "step 29999, loss is 4.79255485534668\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "import tensorflow as tf\n",
    "from model import TrainModel\n",
    "import dataset\n",
    "import utils\n",
    "import setting\n",
    "\n",
    "TRAIN_TIMES = 30000  # 迭代总次数（没有计算epoch）\n",
    "SHOW_STEP = 1  # 显示loss频率\n",
    "SAVE_STEP = 100  # 保存模型参数频率\n",
    "\n",
    "x_data = tf.placeholder(tf.int32, [setting.BATCH_SIZE, None])  # 输入数据\n",
    "y_data = tf.placeholder(tf.int32, [setting.BATCH_SIZE, None])  # 标签\n",
    "emb_keep = tf.placeholder(tf.float32)  # embedding层dropout保留率\n",
    "rnn_keep = tf.placeholder(tf.float32)  # lstm层dropout保留率\n",
    "\n",
    "data = dataset.Dataset(setting.BATCH_SIZE)  # 创建数据集\n",
    "\n",
    "model = TrainModel(x_data, y_data, emb_keep, rnn_keep)  # 创建训练模型\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())  # 初始化\n",
    "    for step in range(TRAIN_TIMES):\n",
    "        # 获取训练batch\n",
    "        x, y = data.next_batch()\n",
    "        # 计算loss\n",
    "        loss, _ = sess.run([model.loss, model.optimize],\n",
    "                           {model.data: x, model.labels: y, model.emb_keep: setting.EMB_KEEP,\n",
    "                            model.rnn_keep: setting.RNN_KEEP})\n",
    "        if step % SHOW_STEP == 0:\n",
    "            print ('step {}, loss is {}'.format(step, loss))\n",
    "        # 保存模型\n",
    "        if step % SAVE_STEP == 0:\n",
    "            saver.save(sess, setting.CKPT_PATH, global_step=model.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
